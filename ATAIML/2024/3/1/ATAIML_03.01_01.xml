<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-BeTj2Tq8HNzCqNWFFw6AAVyKEk9HbeM3</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml030101</article-id>
      <title-group>
        <article-title>Enhanced Pest and Disease Detection in Agriculture Using Deep Learning-Enabled Drones</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Li</surname>
            <given-names>Wenqi</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-5895-0300</contrib-id>
          <email>lwq18300607503@163.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Han</surname>
            <given-names>Xixi</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0006-5266-7909</contrib-id>
          <email>6580@zut.edu.cn</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Lin</surname>
            <given-names>Zhibo</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0008-3079-3220</contrib-id>
          <email>2426775074@qq.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="2">2</xref>
          <name>
            <surname>Rahman</surname>
            <given-names>Atta-Ur</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6696-277X</contrib-id>
          <email>aaurrahman@iau.edu.sa</email>
        </contrib>
        <aff id="1">School of Electronic and Information Engineering, Zhongyuan University of Technology, 451191 Zhengzhou, China</aff>
        <aff id="2">College of Computer Science and Information Technology, Imam Abdulrahman Bin Faisal University, 31441 Dammam, Saudi Arabia</aff>
      </contrib-group>
      <year>2024</year>
      <volume>3</volume>
      <issue>1</issue>
      <fpage>1</fpage>
      <lpage>10</lpage>
      <page-range>1-10</page-range>
      <history>
        <date date-type="received">
          <month>10</month>
          <day>12</day>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <month>01</month>
          <day>01</day>
          <year>2024</year>
        </date>
        <date date-type="pub">
          <month>01</month>
          <day>11</day>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license>. Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the <a href='https://creativecommons.org/licenses/by/4.0/' target='_blank' class='text-yellow-700 hover:underline'>CC BY 4.0 license</a>.</license>
      </permissions>
      <abstract><p>In this study, an integrated pest and disease recognition system for agricultural drones has been developed, leveraging deep learning technologies to significantly improve the accuracy and efficiency of pest and disease detection in agricultural settings. By employing convolutional neural networks (CNN) in conjunction with high-definition image acquisition and wireless data transmission, the system demonstrates proficiency in the effective identification and classification of various agricultural pests and diseases. Methodologically, a deep learning framework has been innovatively applied, incorporating critical modules such as image acquisition, data transmission, and pest and disease identification. This comprehensive approach facilitates rapid and precise classification of agricultural pests and diseases, while catering to the needs of remote operation and real-time data processing, thus ensuring both system efficiency and data security. Comparative analyses reveal that this system offers a notable enhancement in both accuracy and response time for pest and disease recognition, surpassing traditional detection methods and optimizing the management of agricultural pests and diseases. The significant contribution of this research is the successful integration of deep learning into the domain of agricultural pest and disease detection, marking a new era in smart agriculture technology. The findings of this study bear substantial theoretical and practical implications, advancing precision agriculture practices and contributing to the sustainability and efficiency of agricultural production.</p></abstract>
      <kwd-group>
        <kwd>Deep learning</kwd>
        <kwd>Agricultural drones</kwd>
        <kwd>Pest and disease recognition</kwd>
        <kwd>Convolutional neural networks</kwd>
        <kwd>Recognition accuracy</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors">4</count>
        <fig-count>6</fig-count>
        <table-count>2</table-count>
        <ref-count>21</ref-count>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec disp-level="level1" sec-type="intro">
      <title>1. Introduction</title>
      <p>In China, known for its robust agricultural sector, favorable climatic conditions, abundant sunlight, and ample water resources are recognized as key factors promoting agricultural development [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>]. These conditions, however, also contribute to the proliferation of pests and diseases, positioning pest and disease detection as a critical factor in advancing agricultural practices. Traditional approaches, which predominantly rely on experts or technicians for the identification of pests and diseases, are characterized by their low efficiency, susceptibility to subjective biases, and high labor intensity, rendering them inadequate for practical needs [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>].</p><p>The advent of machine vision, the Internet of Things (IoT), and drone technologies has led to a significant expansion in the use of drones within the agricultural sector [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>]. Machine vision, substituting human observation with computerized systems, imitates human visual capabilities to measure and evaluate the identified objects. This process involves capturing images through imaging devices, which are then processed by a series of algorithms within an image processing unit, ultimately yielding detection results [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>]. The role of target detection is fundamental in the machine vision system. These technologies introduce innovative methods for detecting agricultural pests and diseases, thereby significantly enhancing detection efficiency [<xref ref-type="bibr" rid="ref_9">9</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>].</p><p>In traditional drone-based pest and disease recognition systems, machine vision plays a pivotal role, primarily processing extracted images via algorithms to identify attributes such as texture, shape, and color. These systems then employ various algorithms for classification. Conventional methods involved manual design of pest characteristics and classifiers, utilizing techniques like threshold segmentation, edge detection with Sobel, Roberts, Prewitt methods, and feature extraction methods including Scale-Invariant Feature Transform (SIFT), Histogram of Oriented Gradients (HOG), as well as shape, color, and texture analysis. Classification is typically performed using methods such as Support Vector Machine (SVM), Back Propagation (BP), Bayesian analysis, among others [<xref ref-type="bibr" rid="ref_11">11</xref>], [<xref ref-type="bibr" rid="ref_12">12</xref>]. The imaging requirements for object detection in these systems are stringent, demanding a stark contrast between pests and their surrounding environment. However, the diversity of pest types and their morphological changes during different growth stages, along with the possibility of similar characteristics being displayed by different pests at various stages, pose significant challenges. These challenges are compounded by the complexities of natural light and background similarities in real-world detection scenarios, which can compromise the robustness and effectiveness of traditional machine vision recognition algorithms [<xref ref-type="bibr" rid="ref_13">13</xref>], [<xref ref-type="bibr" rid="ref_14">14</xref>].</p><p>Deep learning technology, which mimics the structure and functionality of human brain neural networks, outperforms traditional machine learning by autonomously learning from extensive datasets, thus exhibiting formidable computational capabilities. Provided with ample learning data and high-performance computing units, deep learning demonstrates enhanced robustness, superior detection performance, and greater recognition accuracy [<xref ref-type="bibr" rid="ref_15">15</xref>], [<xref ref-type="bibr" rid="ref_16">16</xref>]. The evolution of drone technology has markedly advanced the deployment of agricultural drones for pest control. The application of these drones, particularly in pesticide spraying, not only utilizes machine vision technology for efficient planning of spray routes but also significantly reduces human exposure to pesticides. This advancement facilitates increased spray efficiency, uniformity, and the automation and precision of pesticide application. The role of pest and disease detection and control in agricultural production is crucial, directly influencing crop yield. Therefore, the integration of deep learning in pest and disease recognition systems for agricultural drones is vital for the advancement of agricultural practices.</p><p>In this study, the has been selected as the deep learning model for the pest and disease recognition system tailored to agricultural drones. CNN is meticulously designed to take into account the spatial structure and local correlation of images, thereby enhancing the efficiency of image data processing. These networks are adept at tasks such as object detection, image classification, and image recognition, excelling in the extraction of features from detected images, which equips them with robust image recognition and analysis capabilities. Particularly proficient in extracting deep features from images, CNN significantly bolsters image recognition performance, exhibiting formidable robustness [<xref ref-type="bibr" rid="ref_17">17</xref>], [<xref ref-type="bibr" rid="ref_18">18</xref>]. This research delves into the critical technologies integral to deep learning in agricultural drone pest and disease recognition systems, encompassing deep learning models, algorithms, and evaluation metrics for such systems. The deep learning-based algorithms employed here are characterized by their potent feature extraction ability, high detection precision, and recognition efficiency, surpassing traditional algorithms and offering substantial guidance in identifying crop pests and diseases.</p><p>The application of CNN in crop pest and disease recognition by various researchers has yielded noteworthy results. However, challenges persist due to the complex and variable backgrounds encountered in real crop pest and disease scenarios, difficulties in acquiring agricultural disease image data, and the constraints posed by limited data volume. Enhancing the accuracy and robustness of recognition algorithms remains an ongoing focus of research. Striking a balance between heightened recognition precision and maintaining processing speed is an ongoing challenge in the field.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>2. Methodology</title>
      
        <sec disp-level="level2">
          
            <title>2.1. Overall system design</title>
          
          <p>In this research, a pest and disease recognition system designed for agricultural drones is structured into a three-tier architecture. This architecture encompasses the sensing layer, transmission layer, and application layer. The structural framework of the system is systematically represented in <xref ref-type="fig" rid="fig_1">Figure 1</xref>.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>Structural framework of the recognition system</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_J2kH37eUc1b3BBXI.jpeg"/>
            </fig>
          
          
            <sec disp-level="level3">
              
                <title>2.1.1 Sensing layer</title>
              
              <p>The sensing layer forms the foundation of the recognition system, incorporating the drone system, ground monitoring station, and mobile client. This layer is primarily dedicated to fulfilling the dynamic recognition demands of agricultural drones.</p><p>Within the drone system, a series of components are coordinated to control device rotation, movement, focus adjustment, and image acquisition. Key components include a structural frame, a main processor of Digital Signal Processor (DSP), a data collection module, a power supply module, a Charge Coupled Device (CCD) camera, and a propulsion system. The drone system's control framework is comprehensively illustrated in <xref ref-type="fig" rid="fig_2">Figure 2</xref>.</p>
              
                <fig id="fig_2">
                  <label>Figure 2</label>
                  <caption>Control framework of the drone system</caption>
                  <abstract/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_9fWf6YC9MD63Zo4C.png"/>
                </fig>
              
              <p>The central processing unit of the drone system is a DM643 model, specifically engineered for digital multimedia applications. This unit functions as the heart of the drone's flight control system, managing tasks such as receiving flight data, orchestrating trajectory planning, and processing the images captured. The data collection module is integral to this system, tasked with the real-time acquisition of flight status data. This data is then relayed to a microcontroller responsible for monitoring and controlling the drone's posture, thereby ensuring the safety of its flight. Constituting this module are various components, including an altitude sensor, a Global Positioning System (GPS), angle sensor, angular velocity sensor, accelerometer, and airspeed indicator. Proportion Integration Differentiation (PID) control technology is implemented to precisely manage the drone's flight posture while minimizing systemic errors. This control mechanism is detailed in <xref ref-type="fig" rid="fig_3">Figure 3</xref>.</p>
              
                <fig id="fig_3">
                  <label>Figure 3</label>
                  <caption>Drone flight attitude control mechanism</caption>
                  <abstract/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_LaWDH-iMlntqX6CY.png"/>
                </fig>
              
              <p>The propulsion system of the drone, crucial for its operation, comprises a motor, an Electronic Speed Controller (ESC), and a propeller. The motor selected, known for its precise controllability, is of the brushless variety. The ESC, with its 20A rated current, is utilized to modulate the motor's speed. The propeller, driven by the motor, generates necessary lift, ensuring that the motor's operational capacity is not exceeded.</p><p>For imaging purposes, a CCD camera, model DS-2DC4423IW-D, is employed. This camera is characterized by its spherical shape and expansive field of view. The GPS system plays a pivotal role in facilitating the drone's navigational accuracy.</p><p>The ground monitoring station serves as a control hub for the drone, overseeing its flight status and missions. Its primary components include an imaging system and a Liquid Crystal Display (LCD). The imaging system is tasked with adjusting and controlling the CCD camera, modifying parameters like resolution, shooting mode, and focal length to suit various area characteristics, thus ensuring optimal image acquisition.</p>
            </sec>
          
          
            <sec disp-level="level3">
              
                <title>2.1.2 Transmission layer</title>
              
              <p>The transmission layer is designed with two essential functions. Firstly, it is responsible for the secure and stable transfer of flight data from the recognition drone and the image data collected by the CCD camera. Secondly, it facilitates the transmission of commands from ground personnel to the drone. Wireless networking is employed for data transmission to accommodate the requirements of long-distance drone operations, enhancing both the safety and stability of the data transmission process. The wireless network operates on the Ubuntu system, utilizing SDCC, EC2Tools, and MonoDeveloper as its primary development tools. Data transmission is executed in packet form, ensuring efficient and reliable delivery of information. The sequential steps of data transmission within this wireless network are detailed in <xref ref-type="fig" rid="fig_4">Figure 4</xref>.</p>
              
                <fig id="fig_4">
                  <label>Figure 4</label>
                  <caption>Steps of data transmission in the wireless network</caption>
                  <abstract/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_ulOE7axMA2DjZgkl.png"/>
                </fig>
              
            </sec>
          
          
            <sec disp-level="level3">
              
                <title>2.1.3 Application layer</title>
              
              <p>The application layer's primary role is the analysis and processing of the information acquired. It employs deep learning algorithms for the recognition of diseases, yielding definitive results. To align with the functional demands of this recognition system, the application layer is segregated into six distinct modules: the image acquisition module, storage module, virus detection module, disease recognition module, flight route planning module, and control module.</p>
            </sec>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>2.2. Core algorithm design of the system</title>
          
          <p>Deep learning integrates low-level features to create abstract high-level features, representing categories or characteristics of attributes, thereby enabling complex classification tasks to be performed using simplified models. This technique grants machines capabilities akin to human analytical skills, advancing the development of artificial intelligence [<xref ref-type="bibr" rid="ref_19">19</xref>], [<xref ref-type="bibr" rid="ref_20">20</xref>]. In the realm of deep learning, CNN stand out as a quintessential model, automating the learning of image features and patterns. This automation is realized through an amalgamation of convolutional layers, pooling layers, and fully connected layers, which collectively facilitate the automatic extraction and classification of image features. Central to the CNN is the convolutional layer, comprised of multiple convolutional kernels. Each kernel interacts with the input image to extract local features, capturing edges, textures, and other localized aspects of the image. The pooling layer plays a pivotal role in reducing the spatial dimensions of the output from the convolutional layer, thereby diminishing the number of parameters and highlighting the key features. CNN, compared to other deep learning algorithms, demands fewer parameters and demonstrate superior generalization abilities [<xref ref-type="bibr" rid="ref_21">21</xref>]. Given their extensive application in target detection within machine vision, CNN has been deemed appropriate for this study. Consequently, CNN has been selected as the foundational deep learning algorithm in the development of the pest and disease recognition system for agricultural drones.</p>
          
            <sec disp-level="level3">
              
                <title>2.2.1 Cnn design</title>
              
              <p>CNN is comprised of several layers: convolutional layers, activation function layers, pooling layers, fully connected layers, and output layers. At the heart of the CNN architecture lies the convolutional layer, entrusted with the critical task of extracting local features from images. This layer is recognized as the most computationally demanding and time-intensive component, yet it is essential for the network's functionality. The core principle of the convolutional layer involves sliding a window across the input image data and performing calculations to extract image features. The convolution operation process is systematically illustrated in <xref ref-type="fig" rid="fig_5">Figure 5</xref>.</p>
              
                <fig id="fig_5">
                  <label>Figure 5</label>
                  <caption>Illustration of the convolution operation process</caption>
                  <abstract/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_vVUuixXifT8LXcjl.png"/>
                </fig>
              
              <p>In the convolution operation, the input pixels are summed with the corresponding data in the convolution kernel, along with an added bias term. The network algorithm design employs various convolution kernels to achieve specific functionalities. The dimensions <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <msub>
        <mrow data-mjx-texclass="ORD">
          <mi>n</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mi>o</mi>
          <mi>u</mi>
          <mi>t</mi>
        </mrow>
      </msub>
    </mrow>
  </math>
</inline-formula> of the resulting image are determined by the following equation:</p>
              
                <disp-formula>
                  <label>(1)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <msub>
                        <mrow data-mjx-texclass="ORD">
                          <mi>n</mi>
                        </mrow>
                        <mrow data-mjx-texclass="ORD">
                          <mi>o</mi>
                          <mi>u</mi>
                          <mi>t</mi>
                        </mrow>
                      </msub>
                    </mrow>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">[</mo>
                      <mo data-mjx-texclass="CLOSE">]</mo>
                      <mfrac>
                        <mrow>
                          <mrow data-mjx-texclass="ORD">
                            <msub>
                              <mrow data-mjx-texclass="ORD">
                                <mi>n</mi>
                              </mrow>
                              <mrow data-mjx-texclass="ORD">
                                <mi>i</mi>
                                <mi>n</mi>
                              </mrow>
                            </msub>
                          </mrow>
                          <mo>+</mo>
                          <mo>−</mo>
                          <mn>2</mn>
                          <mi>q</mi>
                          <mi>f</mi>
                        </mrow>
                        <mi>s</mi>
                      </mfrac>
                    </mrow>
                    <mo>=</mo>
                    <mo>+</mo>
                    <mn>1</mn>
                  </math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <msub>
        <mrow data-mjx-texclass="ORD">
          <mi>n</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mi>i</mi>
          <mi>n</mi>
        </mrow>
      </msub>
    </mrow>
  </math>
</inline-formula> and $f<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>d</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>o</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>u</mi>
    <mi>m</mi>
    <mi>b</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>f</mi>
    <mi>c</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>v</mi>
    <mi>o</mi>
    <mi>l</mi>
    <mi>u</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>k</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>n</mi>
    <mi>e</mi>
    <mi>l</mi>
    <mi>s</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>d</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>i</mi>
    <mi>r</mi>
    <mi>r</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>p</mi>
    <mi>e</mi>
    <mi>c</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>v</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>i</mi>
    <mi>z</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mo>,</mo>
  </math>
</inline-formula>s<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>r</mi>
    <mi>e</mi>
    <mi>p</mi>
    <mi>r</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>t</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>p</mi>
    <mi>s</mi>
    <mi>i</mi>
    <mi>z</mi>
    <mi>e</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>d</mi>
    <mo>,</mo>
  </math>
</inline-formula>q$ is the padding number, all classified as hyperparameters within the convolutional layer.</p><p>The activation function layer is pivotal in addressing data non-linearities, thus equipping the CNN to handle more complex tasks. Prominent activation functions utilized include the Sigmoid, Tanh, and ReLU activation functions, each described by their respective mathematical expressions:</p>
              
                <disp-formula>
                  <label>(2)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <mi>S</mi>
                      <mi>i</mi>
                      <mi>g</mi>
                      <mi>m</mi>
                      <mi>o</mi>
                      <mi>i</mi>
                      <mi>d</mi>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mi>S</mi>
                      <mi>i</mi>
                      <mi>g</mi>
                      <mi>m</mi>
                      <mi>o</mi>
                      <mi>i</mi>
                      <msup>
                        <mrow data-mjx-texclass="ORD">
                          <mi>d</mi>
                        </mrow>
                        <mo data-mjx-alternate="1">′</mo>
                      </msup>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mi>S</mi>
                      <mi>i</mi>
                      <mi>g</mi>
                      <mi>m</mi>
                      <mi>o</mi>
                      <mi>i</mi>
                      <mi>d</mi>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mi>S</mi>
                      <mi>i</mi>
                      <mi>g</mi>
                      <mi>m</mi>
                      <mi>o</mi>
                      <mi>i</mi>
                      <mi>d</mi>
                    </mrow>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo>,</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo stretchy="false">[</mo>
                    <mo>−</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo stretchy="false">]</mo>
                    <mi>x</mi>
                    <mi>x</mi>
                    <mi>x</mi>
                    <mi>x</mi>
                    <mfrac>
                      <mn>1</mn>
                      <mrow>
                        <mn>1</mn>
                        <mo>+</mo>
                        <mrow data-mjx-texclass="ORD">
                          <msup>
                            <mrow data-mjx-texclass="ORD">
                              <mi>e</mi>
                            </mrow>
                            <mrow data-mjx-texclass="ORD">
                              <mo>−</mo>
                              <mi>x</mi>
                            </mrow>
                          </msup>
                        </mrow>
                      </mrow>
                    </mfrac>
                    <mn>1</mn>
                  </math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(3)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <mi>T</mi>
                      <mi>a</mi>
                      <mi>n</mi>
                      <mi>h</mi>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <msup>
                        <mrow data-mjx-texclass="ORD">
                          <mrow data-mjx-texclass="ORD">
                            <mi>T</mi>
                            <mi>a</mi>
                            <mi>n</mi>
                            <mi>h</mi>
                          </mrow>
                        </mrow>
                        <mrow data-mjx-texclass="ORD">
                          <mn>2</mn>
                        </mrow>
                      </msup>
                    </mrow>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo>,</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo>−</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mi>x</mi>
                    <mi>x</mi>
                    <mi>x</mi>
                    <mfrac>
                      <mrow>
                        <mrow data-mjx-texclass="ORD">
                          <msup>
                            <mrow data-mjx-texclass="ORD">
                              <mi>e</mi>
                            </mrow>
                            <mrow data-mjx-texclass="ORD">
                              <mi>x</mi>
                            </mrow>
                          </msup>
                        </mrow>
                        <mrow data-mjx-texclass="ORD">
                          <msup>
                            <mrow data-mjx-texclass="ORD">
                              <mi>e</mi>
                            </mrow>
                            <mrow data-mjx-texclass="ORD">
                              <mo>−</mo>
                              <mi>x</mi>
                            </mrow>
                          </msup>
                        </mrow>
                        <mo>−</mo>
                      </mrow>
                      <mrow>
                        <mrow data-mjx-texclass="ORD">
                          <msup>
                            <mrow data-mjx-texclass="ORD">
                              <mi>e</mi>
                            </mrow>
                            <mrow data-mjx-texclass="ORD">
                              <mi>x</mi>
                            </mrow>
                          </msup>
                        </mrow>
                        <mrow data-mjx-texclass="ORD">
                          <msup>
                            <mrow data-mjx-texclass="ORD">
                              <mi>e</mi>
                            </mrow>
                            <mrow data-mjx-texclass="ORD">
                              <mo>−</mo>
                              <mi>x</mi>
                            </mrow>
                          </msup>
                        </mrow>
                        <mo>+</mo>
                      </mrow>
                    </mfrac>
                    <msup>
                      <mrow data-mjx-texclass="ORD">
                        <mi>T</mi>
                        <mi>a</mi>
                        <mi>n</mi>
                        <mi>h</mi>
                      </mrow>
                      <mo data-mjx-alternate="1">′</mo>
                    </msup>
                    <mn>1</mn>
                  </math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(4)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <mi>R</mi>
                      <mi>e</mi>
                      <mi>L</mi>
                      <mi>u</mi>
                    </mrow>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo data-mjx-texclass="OP" movablelimits="true">max</mo>
                    <mo stretchy="false">(</mo>
                    <mo>,</mo>
                    <mo stretchy="false">)</mo>
                    <mi>x</mi>
                    <mi>x</mi>
                    <mn>0</mn>
                  </math>
                </disp-formula>
              
              <p>The pooling layer, often referred to as the subsampling layer, employs mathematical methods to process data derived from the convolutional and activation function layers, ultimately producing a feature map. This layer plays a crucial role in data and parameter compression by reducing the feature map's size, which consequently diminishes the consumption of computational resources and aids in preventing overfitting. The size of the output feature map is determined by the formula provided below:</p>
              
                <disp-formula>
                  <label>(5)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <msub>
                        <mrow data-mjx-texclass="ORD">
                          <mi>n</mi>
                        </mrow>
                        <mrow data-mjx-texclass="ORD">
                          <mi>o</mi>
                          <mi>u</mi>
                          <mi>t</mi>
                        </mrow>
                      </msub>
                    </mrow>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">[</mo>
                      <mo data-mjx-texclass="CLOSE">]</mo>
                      <mfrac>
                        <mrow>
                          <mrow data-mjx-texclass="ORD">
                            <msub>
                              <mrow data-mjx-texclass="ORD">
                                <mi>n</mi>
                              </mrow>
                              <mrow data-mjx-texclass="ORD">
                                <mi>i</mi>
                                <mi>n</mi>
                              </mrow>
                            </msub>
                          </mrow>
                          <mo>−</mo>
                          <mi>f</mi>
                        </mrow>
                        <mi>s</mi>
                      </mfrac>
                    </mrow>
                    <mo>=</mo>
                    <mo>+</mo>
                    <mn>1</mn>
                  </math>
                </disp-formula>
              
              <p>For the augmentation of image classification, recognition, and detection accuracy, the network architecture may be further enhanced by incorporating additional layers, such as fully connected layers and loss function layers. These layers are instrumental in classifying the features extracted from the images, thereby yielding a quantitative description of the input feature map.</p>
            </sec>
          
          
            <sec disp-level="level3">
              
                <title>2.2.2 Design of evaluation metrics for the recognition system</title>
              
              <p>Upon the extraction of image features and completion of localization via the CNN, establishing evaluation metrics for the system becomes imperative to assess whether the output aligns with the specified criteria. In instances where the results adhere to the predefined range, they are considered valid outputs; otherwise, a further assessment of image features is necessitated. Key evaluation metrics adopted include the Intersection over Union (IoU), precision, recall, F-measure, and the precision-recall (PR) curve.</p><p>The IoU metric quantitatively assesses the extent of overlap between the predicted outcomes and the actual results. It is computed using the following formula:</p>
              
                <disp-formula>
                  <label>(6)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>I</mi>
                    <mi>o</mi>
                    <mi>U</mi>
                    <mo>=</mo>
                    <mfrac>
                      <mrow data-mjx-texclass="ORD">
                        <msub>
                          <mrow data-mjx-texclass="ORD">
                            <mi>S</mi>
                          </mrow>
                          <mrow data-mjx-texclass="ORD">
                            <mi>O</mi>
                            <mi>v</mi>
                            <mi>e</mi>
                            <mi>r</mi>
                            <mi>l</mi>
                            <mi>a</mi>
                            <mi>p</mi>
                          </mrow>
                        </msub>
                      </mrow>
                      <mrow data-mjx-texclass="ORD">
                        <msub>
                          <mrow data-mjx-texclass="ORD">
                            <mi>S</mi>
                          </mrow>
                          <mrow data-mjx-texclass="ORD">
                            <mi>U</mi>
                            <mi>n</mi>
                            <mi>i</mi>
                            <mi>o</mi>
                            <mi>n</mi>
                          </mrow>
                        </msub>
                      </mrow>
                    </mfrac>
                  </math>
                </disp-formula>
              
              <p>Here, IoU is expressed as the proportion of the intersection <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <msub>
        <mrow data-mjx-texclass="ORD">
          <mi>S</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mi>O</mi>
          <mi>v</mi>
          <mi>e</mi>
          <mi>r</mi>
          <mi>l</mi>
          <mi>a</mi>
          <mi>p</mi>
        </mrow>
      </msub>
    </mrow>
  </math>
</inline-formula> compared to the union <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <msub>
        <mrow data-mjx-texclass="ORD">
          <mi>S</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mi>U</mi>
          <mi>n</mi>
          <mi>i</mi>
          <mi>o</mi>
          <mi>n</mi>
        </mrow>
      </msub>
    </mrow>
  </math>
</inline-formula> of the predicted and actual results. </p>
              <p>Precision in this context is defined as the system's accuracy in identifying image features and is calculated as:</p>
              
                <disp-formula>
                  <label>(7)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <mrow data-mjx-texclass="ORD">
                        <mi>P</mi>
                        <mi>r</mi>
                        <mi>e</mi>
                        <mi>c</mi>
                        <mi>i</mi>
                        <mi>s</mi>
                        <mi>i</mi>
                        <mi>o</mi>
                        <mi>n</mi>
                      </mrow>
                    </mrow>
                    <mo>=</mo>
                    <mfrac>
                      <mrow>
                        <mi>T</mi>
                        <mi>P</mi>
                      </mrow>
                      <mrow>
                        <mi>Z</mi>
                        <mi>P</mi>
                      </mrow>
                    </mfrac>
                  </math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>T</mi>
    <mi>P</mi>
  </math>
</inline-formula> denotes the count of samples accurately identified as positive by the system, and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>Z</mi>
    <mi>P</mi>
  </math>
</inline-formula> represents the total number of samples the system classifies as positive. Recall is the measure of the system’s ability to re-identify, computed using:</p>
              
                <disp-formula>
                  <label>(8)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <mi>R</mi>
                      <mi>e</mi>
                      <mi>c</mi>
                      <mi>a</mi>
                      <mi>l</mi>
                      <mi>l</mi>
                    </mrow>
                    <mo>=</mo>
                    <mfrac>
                      <mrow>
                        <mi>T</mi>
                        <mi>P</mi>
                      </mrow>
                      <mrow>
                        <mi>Z</mi>
                        <mi>N</mi>
                      </mrow>
                    </mfrac>
                  </math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>Z</mi>
    <mi>N</mi>
  </math>
</inline-formula> signifies the actual count of positive samples. For practical application, a balance between precision and recall is sought, achieved through the F-measure, which is articulated as:</p>
              
                <disp-formula>
                  <label>(9)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>F</mi>
                    <mo>=</mo>
                    <mfrac>
                      <mrow>
                        <mo stretchy="false">(</mo>
                        <mo>+</mo>
                        <mo stretchy="false">)</mo>
                        <mo>⋅</mo>
                        <mo>⋅</mo>
                        <mn>1</mn>
                        <mi>β</mi>
                        <mtext mathvariant="italic">Precision</mtext>
                        <mtext mathvariant="italic">Recall</mtext>
                      </mrow>
                      <mrow>
                        <msup>
                          <mi>β</mi>
                          <mn>2</mn>
                        </msup>
                        <mo>⋅</mo>
                        <mo>+</mo>
                        <mtext mathvariant="italic">Precision</mtext>
                        <mtext mathvariant="italic">Recall</mtext>
                      </mrow>
                    </mfrac>
                  </math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>β</mi>
  </math>
</inline-formula> is the harmonic mean of precision and recall.</p><p>In the PR curve, precision and recall are delineated as the x and y coordinates, respectively. This curve illustrates the relationship between precision and recall at varying threshold values, as demonstrated in <xref ref-type="fig" rid="fig_6">Figure 6</xref>.</p><p>Within this curve, the comparative analysis of feature curves is conducted, where larger features are indicative of enhanced performance. As depicted in <xref ref-type="fig" rid="fig_6">Figure 6</xref>, the performance denoted by Curves A and B is observed to surpass that of Curve C. The equilibrium point of the curve serves as an additional comparative metric; a higher equilibrium point suggests more favorable features. Utilizing these analytical methods enables the determination of image feature values and the achievement of accurate localization.</p>
              
                <fig id="fig_6">
                  <label>Figure 6</label>
                  <caption>PR curve</caption>
                  <abstract/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_ugir1-0JL28Q1mpq.jpeg"/>
                </fig>
              
            </sec>
          
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>3. Experiments</title>
      <p>To evaluate the effectiveness of the pest and disease recognition system developed for agricultural drones, a comprehensive series of experiments was undertaken. Initially, the system required training with a specific dataset, followed by a testing phase. Additionally, an emphasis was placed on the system's portability, namely its capability to adapt to entirely new datasets. Consequently, the experimental design encompassed both training and testing trials, as well as the application of the model to alternative datasets.</p>
      
        <sec disp-level="level2">
          
            <title>3.1. Training and testing experiments</title>
          
          <p>For the training and testing phase, grape leaves, characterized by their distinct pest features, were selected as the primary subject. Although the Plant Village dataset is extensively utilized in the domain of plant leaf pest and disease visual detection, it predominantly comprises images acquired under laboratory conditions rather than natural environments. Therefore, in this study, images captured by agricultural drones, representing pest and disease instances in actual natural settings, were employed. To resolve issues related to dataset accessibility and nomenclature, the downloaded data files were meticulously processed and renamed, facilitating the formation of a dataset suitable for subsequent training. The selected pest images underwent preprocessing and data augmentation procedures, including rotation, flipping, scaling, and translation. These processes were essential to conform to the size requirements of the chosen model and to mitigate the limitations posed by a smaller dataset size, which could potentially impact the efficacy of model training. This methodology also aimed to enhance the generalization capability of the model trained on this dataset. Post preprocessing, pest image samples were inputted for model training, which was conducted in alignment with the target detection recognition algorithm. Following the completion of the model training, pest and disease recognition and detection were executed using the obtained model weight files. The dataset utilized in the system included 1000 grape leaf images, encompassing 184 images of healthy leaves, 328 images of brown spot disease leaves, 301 images of black rot disease leaves, and 187 images of ring spot disease leaves. A random selection of 70% of these images served as the training data, while the remaining 30% were used for testing purposes. The outcomes of these experiments are detailed in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>Results of training and testing experiments</caption>
              <abstract/>
              <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center"></p></td><td colspan="3" rowspan="1"><p style="text-align: center">Healthy Leaves</p></td><td colspan="3" rowspan="1"><p style="text-align: center">Leaves Affected by Diseases and Pests</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"></p></td><td colspan="1" rowspan="1"><p style="text-align: center">Number of samples</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Correct identifications</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Identification rate %</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Number of samples</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Correct identifications</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Identification rate %</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center">Training set</p></td><td colspan="1" rowspan="1"><p style="text-align: center">130</p></td><td colspan="1" rowspan="1"><p style="text-align: center">122</p></td><td colspan="1" rowspan="1"><p style="text-align: center">93.8</p></td><td colspan="1" rowspan="1"><p style="text-align: center">57</p></td><td colspan="1" rowspan="1"><p style="text-align: center">55</p></td><td colspan="1" rowspan="1"><p style="text-align: center">96.4</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center">Testing</p><p style="text-align: center">set</p></td><td colspan="1" rowspan="1"><p style="text-align: center">54</p></td><td colspan="1" rowspan="1"><p style="text-align: center">51</p></td><td colspan="1" rowspan="1"><p style="text-align: center">94.4</p></td><td colspan="1" rowspan="1"><p style="text-align: center">246</p></td><td colspan="1" rowspan="1"><p style="text-align: center">230</p></td><td colspan="1" rowspan="1"><p style="text-align: center">I93.5</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>As presented in <xref ref-type="table" rid="table_1">Table 1</xref>, the results from both the training and testing sets of the experiment demonstrate a noteworthy achievement in identification accuracy, surpassing 93.5%. This high level of accuracy, coupled with the system's automation and intelligence, aligns with the stipulated accuracy requirements for the pest and disease recognition system.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.2. Testing the recognition system on other training sets</title>
          
          <p>The evaluation of the recognition system's portability was conducted by applying it to tomato, corn, and potato leaves for pest and disease identification. This phase encompassed an initial training with respective samples, followed by a subsequent testing phase. The results of these tests are detailed in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>Test results of the recognition system on other training sets</caption>
              <abstract/>
              <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span>Plant Type</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>Leaf Type</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>Number of Samples</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>Correct Identifications</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>Identification Rate %</span></p></td></tr><tr><td colspan="1" rowspan="2"><p style="text-align: center"><span>Tomato leaf</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>Healthy</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>320</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>309</span></p></td><td colspan="1" rowspan="2"><p style="text-align: center"><span>96.5</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span>Affected by diseases and pests</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>250</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>241</span></p></td></tr><tr><td colspan="1" rowspan="2"><p style="text-align: center"><span>Corn leaf</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>Healthy</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>310</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>303</span></p></td><td colspan="1" rowspan="2"><p style="text-align: center"><span>97.2</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span>Affected by diseases and pests</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>240</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>232</span></p></td></tr><tr><td colspan="1" rowspan="2"><p style="text-align: center"><span>Potato leaf</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>Healthy</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>300</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>290</span></p></td><td colspan="1" rowspan="2"><p style="text-align: center"><span>97.4</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span>Affected by diseases and pests</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>230</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>226</span></p></td></tr></tbody></table>
            </table-wrap>
          
          <p>As illustrated in <xref ref-type="table" rid="table_2">Table 2</xref>, the system exhibited identification accuracies of 96.5% for tomato leaves, 97.2% for corn leaves, and 97.4% for potato leaves, underscoring its remarkable portability.</p><p>The outcomes from both the training and testing experiments, along with the tests on additional training sets, corroborate that the deep learning-based pest and disease detection system developed in this study substantially bolsters the implementation of deep learning in crop pest and disease recognition. This enhancement markedly improves the efficiency and accuracy of pest and disease identification and detection, thus providing valuable insights for the furtherance of pest and disease detection technologies in practical applications.</p>
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="conclusions">
      <title>4. Conclusions</title>
      <p>This study addressed the challenges of manual pest and disease identification in agriculture, characterized by low accuracy, slow speed, and high labor costs, by exploring a deep learning-based recognition system for agricultural drones. Deep learning, a leading technology in image detection, has been effectively applied to agricultural drones, enabling intelligent, automated detection and identification of pests and diseases. This approach significantly surpasses traditional image processing methods by autonomously extracting image features, thereby enhancing the accuracy in identifying characteristics of pests and diseases. Such advancements contribute a novel methodology to the prevention and treatment of crop pests and diseases, holding substantial value for the advancement of intelligent and precision agriculture. The developed recognition system encompasses a three-tier architecture: the sensing layer, transmission layer, and application layer. Deep learning algorithms were instrumental in the artificial intelligence recognition process, with specific focus on designing both the CNN and the recognition system's evaluation metrics. Comprehensive training and testing trials were conducted on the system, supplemented by testing on various datasets. The findings reveal that the system not only possesses a high recognition rate for viruses but also demonstrates commendable portability, effectively addressing the challenges associated with detecting crop pests and diseases using agricultural drones. The system's potential for broad application signifies a significant step towards the further intelligent evolution of agriculture. Additionally, the integration of drone technology and deep learning presents expansive possibilities, such as in the detection and analysis of environmental elements like land surfaces and water quality, thus enhancing the efficiency and precision in areas like environmental monitoring, disaster assessment, and land use planning.</p><p>The methodologies employed in this research were constrained by limited datasets, presenting significant challenges for future studies. In real-world agricultural settings, the diversity of crop types and the plethora of pest and disease species necessitate the recognition and detection of a wider array of crops and their afflictions, marking a critical direction for future research endeavors. The dataset utilized in this study, while pivotal, was not exhaustive, and there is a noticeable scarcity of images depicting crop pests and diseases under varied environmental conditions. Future research should focus on continuously augmenting and developing these datasets, expanding their scope to surmount the limitations inherent in current research methodologies, thereby enhancing the stability and adaptability of the algorithms. Investigating the states of crops prior to the onset of pests and diseases, with the aim of achieving preventative measures, emerges as a prospective trend in this field. Moreover, a paramount task in the evolution of the pest and disease recognition system for agricultural drones is the refinement of algorithms. The objective is to achieve an optimal balance between recognition accuracy and processing speed, which remains a pivotal area of focus for upcoming research initiatives.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p>The study was funded by Scientific Research Fund Project of Zhengzhou University of Economics and Trade Young in 2022 (Grant No.: QK2228), and Henan Province Higher Education Institution College Student Innovation Training Program Project in 2023 (Grant No.: 202310465013).</p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>665</volume>
          <page-range>1017-1025</page-range>
          <issue/>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>L.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Pang</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Lu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.scitotenv.2019.02.162</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Carbon emissions, energy consumption and economic growth: Evidence from the agricultural sector of China's main grain-producing areas</article-title>
          <source>Sci. Total Environ.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>9259</page-range>
          <issue>21</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>H. O.</given-names>
              <surname>Guo</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Xu</surname>
            </name>
            <name>
              <given-names>C. L.</given-names>
              <surname>Pan</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/su12219259</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Measurement of the spatial complexity and its influencing factors of agricultural green development in China</article-title>
          <source>Sustainability</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>27-51</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>L. C.</given-names>
              <surname>Ngugi</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Abelwahab</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Abo-Zahhad</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.inpa.2020.04.004</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Recent advances in image processing techniques for automated leaf pest and disease recognition–A review</article-title>
          <source>Inf. Process. Agric.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>185-189</page-range>
          <issue/>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A.</given-names>
              <surname>Kumar</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Sarkar</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Pradhan</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/ICCSP.2019.8698099</pub-id>
          <article-title>Recommendation system for crop identification and pest control technique in agriculture</article-title>
          <source>2019 International Conference on Communication and Signal Processing (ICCSP), Chennai, India</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>48-59</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Y.</given-names>
              <surname>Yuan</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>H. R.</given-names>
              <surname>Wu</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Lin</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.inpa.2021.01.003</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Advanced agricultural disease image recognition technologies: A review</article-title>
          <source>Inf. Process. Agric.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>20</volume>
          <page-range>1487</page-range>
          <issue>5</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>D. M.</given-names>
              <surname>Gao</surname>
            </name>
            <name>
              <given-names>Q.</given-names>
              <surname>Sun</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Hu</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Zhang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s20051487</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A framework for agricultural pest and disease monitoring based on Internet-of-Things and unmanned aerial vehicles</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>28</volume>
          <page-range>49-62</page-range>
          <issue>13</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>T.</given-names>
              <surname>Daniya</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Vigneshwari</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>A review on machine learning techniques for rice plant disease detection in agricultural research</article-title>
          <source>Int. J. Adv. Sci. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>164</volume>
          <page-range>104906</page-range>
          <issue/>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>K.</given-names>
              <surname>Thenmozhi</surname>
            </name>
            <name>
              <given-names>U. S.</given-names>
              <surname>Reddy</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compag.2019.104906</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Crop pest classification based on deep convolutional neural network and transfer learning</article-title>
          <source>Comput. Electron. Agric.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>21986-21997</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C. J.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>Y. Y.</given-names>
              <surname>Huang</surname>
            </name>
            <name>
              <given-names>Y. S.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>C. Y.</given-names>
              <surname>Chang</surname>
            </name>
            <name>
              <given-names>Y. C.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>Y. M.</given-names>
              <surname>Huang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2021.3056082</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Identification of fruit tree pests with deep learning on embedded drone to achieve accurate pesticide spraying</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>897739</page-range>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>G. J.</given-names>
              <surname>Sun</surname>
            </name>
            <name>
              <given-names>S. H.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>H. L.</given-names>
              <surname>Luo</surname>
            </name>
            <name>
              <surname>others</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fpls.2022.897739</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Intelligent monitoring system of migratory pests based on searchlight trap and machine vision</article-title>
          <source>Front. Plant Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>749-753</page-range>
          <issue/>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A.</given-names>
              <surname>Devaraj</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Rathan</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Jaahnavi</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Indira</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/ICCSP.2019.8698056</pub-id>
          <article-title>Identification of plant disease using image processing technique</article-title>
          <source>2019 International Conference on Communication and Signal Processing (ICCSP), Chennai, India</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>313-316</page-range>
          <issue/>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S. S.</given-names>
              <surname>Kumar</surname>
            </name>
            <name>
              <given-names>B. K.</given-names>
              <surname>Raghavendra</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/ICACCS.2019.8728325</pub-id>
          <article-title>Diseases detection of various plant leaf using image processing techniques: A review</article-title>
          <source>2019 5th International Conference on Advanced Computing &amp; Communication Systems (ICACCS), Coimbatore, India</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>171686-171693</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Y.</given-names>
              <surname>Ai</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Sun</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Tie</surname>
            </name>
            <name>
              <given-names>X. T.</given-names>
              <surname>Cai</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3025325</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Research on recognition model of crop diseases and insect pests based on deep learning in harsh environments</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>27</volume>
          <page-range>1636-1651</page-range>
          <issue>3</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Türkoğlu</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Hanbay</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3906/elk-1809-181</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Plant disease and pest detection using deep learning-based features</article-title>
          <source>Turk. J. Electr. Eng. Comput. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>145</volume>
          <page-range>206-222</page-range>
          <issue>3</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>D. J. A.</given-names>
              <surname>Rustia</surname>
            </name>
            <name>
              <given-names>J. J.</given-names>
              <surname>Chao</surname>
            </name>
            <name>
              <given-names>L. Y.</given-names>
              <surname>Chiu</surname>
            </name>
            <name>
              <given-names>Y. F.</given-names>
              <surname>Wu</surname>
            </name>
            <name>
              <given-names>J. Y.</given-names>
              <surname>Chung</surname>
            </name>
            <name>
              <given-names>J. C.</given-names>
              <surname>Hsu</surname>
            </name>
            <name>
              <given-names>T. T.</given-names>
              <surname>Lin</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1111/jen.12834</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Automatic greenhouse insect pest detection and recognition based on a cascaded deep learning classification method</article-title>
          <source>J. Appl. Entomol.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>194</volume>
          <page-range>112-120</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C. R.</given-names>
              <surname>Rahman</surname>
            </name>
            <name>
              <given-names>P. S.</given-names>
              <surname>Arko</surname>
            </name>
            <name>
              <given-names>M. E.</given-names>
              <surname>Ali</surname>
            </name>
            <name>
              <given-names>M. A. I.</given-names>
              <surname>Khan</surname>
            </name>
            <name>
              <given-names>S. H.</given-names>
              <surname>Apon</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Nowrin</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Wasif</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.biosystemseng.2020.03.020</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Identification and recognition of rice diseases and pests using convolutional neural networks</article-title>
          <source>Biosyst. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>69</volume>
          <page-range>1731-1739</page-range>
          <issue>9</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S. H.</given-names>
              <surname>Lee</surname>
            </name>
            <name>
              <given-names>S. R.</given-names>
              <surname>Lin</surname>
            </name>
            <name>
              <given-names>S. F.</given-names>
              <surname>Chen</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1111/ppa.13251</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Identification of tea foliar diseases and pest damage under practical field conditions using a convolutional neural network</article-title>
          <source>Plant Pathol.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>709-714</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>H.</given-names>
              <surname>Kuzuhara</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Takimoto</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Sato</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Kanagawa</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.23919/SICE48898.2020.9240458</pub-id>
          <article-title>Insect pest detection and identification method based on deep learning for realizing a pest control system</article-title>
          <source>2020 59th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE), Chiang Mai, Thailand</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>500</page-range>
          <issue>4</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J. L.</given-names>
              <surname>Kong</surname>
            </name>
            <name>
              <given-names>H. X.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>C. C.</given-names>
              <surname>Yang</surname>
            </name>
            <name>
              <given-names>X. B.</given-names>
              <surname>Jin</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Zuo</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Zhang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/agriculture12040500</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A spatial feature-enhanced attention neural network with high-order pooling representation for application in pest and disease recognition</article-title>
          <source>Agriculture</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>72</volume>
          <page-range>101846</page-range>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Y. S.</given-names>
              <surname>Peng</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Wang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ecoinf.2022.101846</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>CNN and transformer framework for insect pest classification</article-title>
          <source>Ecol. Inform.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>33</volume>
          <page-range>6999-7019</page-range>
          <issue>12</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Z.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>W. J.</given-names>
              <surname>Yang</surname>
            </name>
            <name>
              <given-names>S. H.</given-names>
              <surname>Peng</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Zhou</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TNNLS.2021.3084827</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A survey of convolutional neural networks: Analysis, applications, and prospects</article-title>
          <source>IEEE Trans. Neural Netw. Learn. Syst.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>