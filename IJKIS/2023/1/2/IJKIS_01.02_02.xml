<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IJKIS</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>International Journal of Knowledge and Innovation Studies</journal-title>
        <abbrev-journal-title abbrev-type="issn">Int J. Knowl. Innov Stud.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IJKIS</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">3005-6101</issn>
      <issn publication-format="print">3005-6098</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-yp4dD-yRbhwF8IsgFbgVHjfnKCu3kOlu</article-id>
      <article-id pub-id-type="doi">10.56578/ijkis010202</article-id>
      <title-group>
        <article-title>Racism and Hate Speech Detection on Twitter: A QAHA-Based Hybrid Deep Learning Approach Using LSTM-CNN</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Jayapal</surname>
            <given-names>Praveen Kumar</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4940-2946</contrib-id>
          <email>praveen.jayapal@smart.mit.edu</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="2">2</xref>
          <name>
            <surname>Ramachandraiah</surname>
            <given-names>Kumar Raja Depa</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7593-4519</contrib-id>
          <email>depa@utem.edu.my</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="3">3</xref>
          <name>
            <surname>Lella</surname>
            <given-names>Kranthi Kumar</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7736-5321</contrib-id>
          <email>kranthi1231@gmail.com</email>
        </contrib>
        <aff id="1">DiSTAP, Singapore-MIT Alliance for Research and Technology, 138602 Singapore, Singapore</aff>
        <aff id="2">Faculty of Information and Communications Technology, Universiti Teknikal Malaysia Melaka, 76100 Melaka, Malaysia</aff>
        <aff id="3">School of Computer Science and Engineering, VIT-AP University, 522237 Vijayawada, India</aff>
      </contrib-group>
      <year>2023</year>
      <volume>1</volume>
      <issue>2</issue>
      <fpage>89</fpage>
      <lpage>102</lpage>
      <page-range>89-102</page-range>
      <history>
        <date date-type="received">
          <month>10</month>
          <day>11</day>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <month>11</month>
          <day>22</day>
          <year>2023</year>
        </date>
        <date date-type="pub">
          <month>12</month>
          <day>07</day>
          <year>2023</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2023 by the authors</copyright-statement>
        <copyright-year>2023</copyright-year>
        <license>. Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the <a href='https://creativecommons.org/licenses/by/4.0/' target='_blank' class='text-yellow-700 hover:underline'>CC BY 4.0 license</a>.</license>
      </permissions>
      <abstract><p>Twitter, a predominant platform for instantaneous communication and idea dissemination, is often exploited by cybercriminals for victim harassment through sexism, racism, hate speech, and trolling using pseudony-mous accounts. The propagation of racially charged online discourse poses significant threats to the social, political, and cultural fabric of many societies. Monitoring and prompt eradication of such content from social media, a breeding ground for racist ideologies, are imperative. This study introduces an advanced hybrid forecasting model, utilizing convolutional neural networks (CNNs) and long-short-term memory (LSTM) neural networks, for the efficient and accurate detection of racist and hate speech in English on Twitter. Unlabelled tweets, collated via the Twitter API, formed the basis of the initial investigation. Feature vectors were extracted from these tweets using the TF-IDF (Term Frequency-Inverse Document Frequency) feature extraction technique. This research contrasts the proposed model with existing intelligent classification algorithms in supervised learning. The HateMotiv corpus, a publicly available dataset annotated with types of hate crimes and ideological motivations, was employed, emphasizing Twitter as the primary social media context. A novel aspect of this study is the introduction of a revised artificial hummingbird algorithm (AHA), supplemented by quantum-based optimization (QBO). This quantum-based artificial hummingbird algorithm (QAHA) aims to augment exploration capabilities and reveal potential solution spaces. Employing QAHA resulted in a detection accuracy of approximately 98%, compared to 95.97% without its application. The study's principal contribution lies in the significant advancements achieved in the field of racism and hate speech detection in English through the application of hybrid deep learning methodologies.</p></abstract>
      <kwd-group>
        <kwd>Cyberstalkers</kwd>
        <kwd>Artificial hummingbird algorithm</kwd>
        <kwd>Quantum-based optimization</kwd>
        <kwd>Long short-term memory</kwd>
        <kwd>Racism detection</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors">3</count>
        <fig-count>8</fig-count>
        <table-count>3</table-count>
        <ref-count>37</ref-count>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec disp-level="level1" sec-type="intro">
      <title>1. Introduction</title>
      <p>In the digital age, the prevalence of social media has revolutionized the way individuals communicate and express themselves. A notable trend observed is the uninhibited expression of thoughts and opinions by users, often leading to the oversharing of personal information [<xref ref-type="bibr" rid="ref_1">1</xref>]. The anonymity provided by social networks emboldens many users to post their emotions and thoughts without filters, sometimes disregarding the potential harm to others [<xref ref-type="bibr" rid="ref_2">2</xref>]. Particularly, individuals with racist ideologies exploit social media to disseminate their beliefs, asserting their right to free expression [<xref ref-type="bibr" rid="ref_3">3</xref>]. This unfettered expression is not confined to fanatical religious, racial, or political views; it extends to extreme bigotry, including sexist behavior that transgresses the bounds of hate speech [<xref ref-type="bibr" rid="ref_4">4</xref>]. The voluminous nature of interactions on social media platforms renders manual monitoring and response to the myriad of comments, messages, and data virtually unfeasible [<xref ref-type="bibr" rid="ref_5">5</xref>]. Furthermore, the scarcity of official data on hate crimes underscores the prevailing issues in accurately addressing such content on these platforms [<xref ref-type="bibr" rid="ref_6">6</xref>]. Despite these challenges, the rich data available on social media are pivotal for user data processing. Data mining plays a critical role in this context, uncovering hitherto unknown patterns within datasets and enabling rapid, informed research and decision-making processes [<xref ref-type="bibr" rid="ref_7">7</xref>]. In the realm of Natural Language Processing (NLP), LSTM and CNN are prominent neural network architectures. LSTM excels in handling sequential data, while CNN is adept at detecting patterns and features in text data. The amalgamation of these two architectures suggests a hybrid model that capitalizes on the strengths of both LSTM and CNN for effective hate speech detection.</p><p>Social media platforms, notably Facebook and Twitter, have raised significant concerns regarding the prevalence of inappropriate language in user posts [<xref ref-type="bibr" rid="ref_8">8</xref>]. The manifestation of racism on these platforms is multifaceted, encompassing both overt and covert forms [<xref ref-type="bibr" rid="ref_9">9</xref>]. Instances include the utilization of counterfeit profiles for disseminating racist remarks. Historically linked to ethnicity, racism now proliferates based on skin tone, country of origin, language, cultural background, and predominantly religious beliefs. Online remarks and posts inciting racial tensions have threatened the social, political, and cultural equilibrium of various nations [<xref ref-type="bibr" rid="ref_10">10</xref>]. The rapid dissemination of racist ideologies via social media underscores the urgency of identifying and eliminating such content [<xref ref-type="bibr" rid="ref_11">11</xref>].</p><p>Exposure to racist comments and tweets on social media has been associated with various mental and physical health conditions, leading to adverse health outcomes [<xref ref-type="bibr" rid="ref_12">12</xref>]. Three distinct forms of racism identified in online interactions include institutional racism, personally mediated racism, and internalized racism [<xref ref-type="bibr" rid="ref_13">13</xref>]. Personally mediated racism occurs when an individual experiences or witnesses prejudice based on race [<xref ref-type="bibr" rid="ref_14">14</xref>]. Consequently, racism in society inflicts psychological stress on individuals, heightening the risk of chronic diseases [<xref ref-type="bibr" rid="ref_15">15</xref>]. Racist groups and individuals are increasingly employing sophisticated methods to promote cyber racism [<xref ref-type="bibr" rid="ref_16">16</xref>]. Sentiment analysis has gained prominence for its application in analyzing social media content for purposes like hate speech detection and racism identification [<xref ref-type="bibr" rid="ref_17">17</xref>]. Recent advancements in automatic detection methods aim to address the issue of abusive content [<xref ref-type="bibr" rid="ref_18">18</xref>]. Machine and deep learning approaches have proven their efficacy in various domains, including sentiment analysis [<xref ref-type="bibr" rid="ref_19">19</xref>], [<xref ref-type="bibr" rid="ref_20">20</xref>].</p><p>This study, therefore, employs a hybrid deep learning model to analyze racist tweets, with the following contributions: first, implementation of pre-processing techniques, such as TF-IDF and Bag of Words, for enhanced classification accuracy; second, utilization of the LSTM model within CNN-LSTM for effective detection of racist and offensive language; third, application of the QAHA to refine AHA performance, thereby optimizing the hyper-parameters of CNN-LSTM; finally, deployment of multiple methods for the detection of racist and offensive speech on a publicly available dataset. The subsequent sections of this study are structured as follows: Section 2 reviews related works on Twitter data analysis. Section 3 elucidates the proposed model, while Section 4 presents the experimental analysis. Section 5 concludes the study and outlines future research directions.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>2. Related works</title>
      <p>The pervasive nature of hate speech on social media has catalyzed significant research efforts. Lee et al. [<xref ref-type="bibr" rid="ref_4">4</xref>] explored sentiment analysis to detect tweets laden with racist content, employing Gated Convolutional Recurrent Neural Networks (GCR-NNs). This stacked ensemble model, integrating Gated Recurrent Units (GRUs), CNNs, and Recurrent Neural Networks (RNNs), demonstrates a synergistic improvement over individual components. In GCR-NNs, GRUs serve to extract salient features from raw text, which are then processed by CNNs to assist RNNs in making accurate predictions. Comparative analyses with existing models underscored the GCR-NN's efficacy, achieving an accuracy of 98% and a 97% detection rate for racist tweets.</p><p>Peng et al. [<xref ref-type="bibr" rid="ref_21">21</xref>] employed a sophisticated Bidirectional Encoder Representations from Transformers (BERT) model, specifically optimized for Twitter sentiment classification, to analyze the tone of approximately one million tweets related to the Black Lives Matter (BLM) movement from July 2013 to March 2021. This model, tested on the Sentiment 140 dataset, achieved unparalleled results among machine learning models, registering an accuracy of 0.94 in the testing phase. The study utilized metrics such as retweet counts and word counts in tweets to visualize key concepts and milestones of the BLM movement. Public opinion analysis revealed varied degrees of support for issues like social justice and police brutality. The implications of this research extend to the promotion and analysis of social and political movements.</p><p>Ghosal and Jain [<xref ref-type="bibr" rid="ref_22">22</xref>] introduced the first unsupervised detection system, which encompasses the HateCircle algorithm, hate tweet classification, and code-switch data preparation techniques. The HateCircle method, employing word co-occurrence analysis, determines the hate orientation of each phrase. A multi-class system for hate tweet categorization was developed using part-of-speech tagging, Euclidean distance, and Geometric median methods. The system proved more effective in identifying hate content in local scripts compared to Roman script, advocating its use in code-switch data preparation. Utilizing an enhanced hate lexicon in conjunction with various dictionaries, the system achieved a maximum F1-score of 0.74 in the Hindi dataset and 0.88 in the Bengali dataset. Comparative evaluation of the proposed parts of speech tagging and Geometric detection strategies with the HateCircle method and hate tweet identification framework showed that HateCircle attained maximum accuracies of 0.73 and 0.78 on the Hindi and Bengali datasets, respectively. This study demonstrates the efficacy of contextual detection research incorporating a language-independent component in combating the spread of subtly harmful content on social media.</p><p>Ali et al. [<xref ref-type="bibr" rid="ref_23">23</xref>] proposed novel graph-based algorithms for identifying hate material on social media. Utilizing Twitter, a dataset was created for testing and validation purposes, involving the extraction and annotation of tweets by language experts. The authors introduced a custom LSTM-GRU model to categorize hate speech into distinct classes. Applied to the compiled dataset, the model achieved an accuracy of 98.14 percent. The Girvan-Newman method was employed to identify key individuals and intraclass communities on Twitter. This approach is significant for monitoring social media to detect potential disruptions, including the identification of hate tweets and groups.</p><p>In a separate study, Agarwal et al. [<xref ref-type="bibr" rid="ref_24">24</xref>] explored enhancing the efficiency of automatic hate speech detection on Social Media Platforms (SMPs) by parallelizing traditional ensemble learning techniques. The research involved parallelizing three popular hate speech detection algorithms—bagging, A-stacking, and random sub-space—and their performance was compared with their serial counterparts across various high-dimensional datasets. These datasets encompassed diverse topics such as the COVID-19 pandemic and the 2020 US farmers' agitation in India (2021). The parallel models demonstrated a considerable increase in speed and efficiency, validating their suitability for the intended applications. This study underscored the importance of generalization by testing the models in a cross-dataset environment, finding that the parallelized algorithms maintained accuracy comparable to their serial versions.</p><p>Joloudari et al. [<xref ref-type="bibr" rid="ref_25">25</xref>] proposed future research directions for the development of a BERT-based model tailored for sentiment analysis. In this approach, a deep CNN architecture captures the hierarchical structure of tweet embeddings, while the BERT model accumulates contextual representations of words, efficiently delineating the intricate semantics of tweets related to COVID-19. Comparative analysis with existing sentiment analysis techniques demonstrated that the BERT-deep CNN models excel in real-time classification of sentiments in COVID-19-related tweets. The study's findings contribute significantly to understanding public sentiment, offering insights that are crucial for policymakers in discerning public opinion, identifying misinformation, and formulating emergency response strategies. This research sets a new benchmark for future studies in sentiment analysis of social media data in crisis contexts and furthers the development of sentiment analysis methodologies.</p><p>Saleh et al. [<xref ref-type="bibr" rid="ref_26">26</xref>] explored the effectiveness of using domain-specific word embedding for the automatic identification of hate speech. This method assigns negative connotations to specific terms to detect coded words effectively. Additionally, the application of the transfer learning language model (BERT), known for its proficiency in various NLP tasks, was examined for hate speech classification. Experimental results revealed that a bidirectional LSTM-based model, employing domain-specific word embedding, achieved an F1-score of 93% on a balanced dataset comprising existing hate speech datasets. In contrast, the BERT model attained a 96% F1-score. The performance of pre-trained models was found to be influenced by the volume of training data. Despite the disparity in corpus size, the first method, focusing on domain-specific data during training, outperformed the BERT model, trained on a larger corpus. The study highlighted the advantage of creating large pre-trained models from rich domain-specific content for contemporary social media platforms.</p><p>Nagar et al. [<xref ref-type="bibr" rid="ref_27">27</xref>] introduced a novel methodology for the detection of hate dialogue on Twitter. This method integrates the author's content, social context, and linguistic characteristics to enhance the accuracy of hate speech detection. Incorporating textual content and the surrounding social environment, the approach employs an encoder to assimilate the unified features of the authors. The adaptability of this framework allows the use of various text encoders to capture the textual properties of the material, rendering it suitable for a broad spectrum of existing and future language models. The efficacy of this method was validated on two distinct Twitter datasets, demonstrating significant improvements over current state-of-the-art approaches. The results highlighted the importance of considering social context in enhancing the identification of hate speech on Twitter.</p><p>Liu et al. [<xref ref-type="bibr" rid="ref_28">28</xref>] proposed BotMoE, a system designed to detect fraudulent bots on Twitter by using multiple modalities of user information, including metadata, textual content, and network structure. The system incorporates a Mixture-of-Experts (MoE) layer, which considers the Twitter communities to augment domain generalization and adaptability. BotMoE constructs modal-specific encoders for metadata attributes and textual structure, subsequently employing a MoE layer that categorizes users into appropriate groups based on community expertise. The final stage involves an expert fusion layer that amalgamates user representations from metadata, text, and graph perspectives, ensuring consistency across all modalities. Extensive trials indicated that BotMoE significantly surpassed existing methods in identifying sophisticated and stealthy bots, demonstrating reduced reliance on training data and enhanced generalization capabilities for new and unknown user populations.</p><p>Mnassri et al. [<xref ref-type="bibr" rid="ref_29">29</xref>] addressed the challenge of unbalanced and sparsely labeled datasets by introducing a learning strategy that incorporates external emotional variables from diverse corpora. This study utilized BERT and mBERT, the latter focusing on cross-lingual identification of abusive material. Leveraging the shared encoder of transformers, the model concurrently recognizes abusive content and incorporates emotional aspects. This approach facilitates rapid learning through the use of auxiliary information and enhances data efficiency by minimizing overfitting through shared representations. The research indicated that incorporating emotional intelligence significantly improved the accuracy of databases in recognizing hate speech and abusive language. Notably, multi-task models exhibited fewer errors than single-task models in both hate speech identification and aggressive language detection tasks, presenting an intriguing development in this field.</p><p>Almaliki et al. [<xref ref-type="bibr" rid="ref_30">30</xref>] introduced a method for the precise identification of Arabic anti-Semitism on Twitter. This study implemented the Arabic BERT-Mini Model (ABMM) for detecting online bigotry. Twitter data were analyzed using bidirectional encoder representations from the model, categorizing the findings into typical, abusive, and hateful classes. Comparative tests were conducted against current state-of-the-art methods, and the results demonstrated that the ABMM model excelled in detecting Arabic hate speech, achieving a highly encouraging score of 0.986.</p><p>Gite et al. [<xref ref-type="bibr" rid="ref_31">31</xref>] explored the application of Ant Colony Optimization (ACO) as an optimization strategy, integrating it with four machine learning models utilizing various feature selection and extraction methods on K-Nearest Neighbour (KNN) and Logistic Regression (LR). The objective was to demonstrate the differences between findings using comparative analysis. The proposed feature selection and extraction methods facilitated the improvement of the machine learning models' efficiency. This study considered both numerical datasets for stroke prediction and textual datasets for hate speech detection. The text dataset, compiled from Twitter API data encompassing tweets with positive, negative, and neutral emotions, utilized the TF-IDF method in conjunction with ACO. The application of ACO to the Random Forest model resulted in a significant accuracy enhancement, reaching up to 10.07 percent.</p><p>Fazil et al. [<xref ref-type="bibr" rid="ref_32">32</xref>] presented a Bidirectional LSTM (BiLSTM) network for identifying xenophobic content. The model employed a multi-channel setup using contemporary word representation techniques to capture semantic relationships across various time frames using multiple filters of different kernel widths. The network processed encoded representations from several channels, with the output of a stacked 2-layer BiLSTM being combined and transmitted through a dense layer, subsequently weighted by an attention layer. The classification was conducted using a sigmoid function in the output layer. The performance of this model was evaluated on three Twitter datasets using four assessment metrics. Comparative analysis with five state-of-the-art models and an aggregate of baseline models indicated superior performance of the proposed model. The ablation study revealed that the removal of channels and the attention mechanism significantly impacted the model's performance. An empirical study was conducted to determine the optimal settings for the model's word representation methods, optimization algorithms, activation functions, and batch size.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>3. Material and methods</title>
      <p>For the automatic detection of online hate crimes, access to annotated corpora is indispensable. In the absence of a standardized benchmark, researchers have been compelled to collect and categorize data on hate crimes independently. This research aims to fill the gap in literature focusing specifically on the identification and motivations of hate crimes, which has been previously unexplored, thus hampering the understanding of prevalent hate crime causes and their mitigation.</p>
      
        <sec disp-level="level2">
          
            <title>3.1. Corpus construction</title>
          
          <p>The HateMotiv corpus was developed through the collection of Twitter posts spanning nine years (1 January 2010–30 December 2019), using the TweetScraper too [<xref ref-type="bibr" rid="ref_6">6</xref>]. It is important to note that the presence of terms such as “hate crime” or “hate crimes” in a tweet does not necessarily imply the endorsement or incitement of violence against a specific group. Twitter users commonly employ hashtags to associate their posts with specific events or topics [<xref ref-type="bibr" rid="ref_33">33</xref>]. Consequently, prevalent hashtags related to hate crimes were identified using the “Hashtagify” application (https://hashtagify.me/) [<xref ref-type="bibr" rid="ref_34">34</xref>]. Hashtags including “hate crime”, “racist”, “racism”, and “Islamophobia” were among those selected for compiling relevant tweets. These hashtags, found to align closely with the FBI’s categorization of hate crimes, were employed as keywords to extract suitable tweets. An English instructor with extensive annotation experience selected these keywords, resulting in a query that returned 23,179 tweets containing the specified hashtags. To optimize the resources for manual annotation, a subset of 5,000 tweets was randomly selected for further analysis.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.2. Annotation process</title>
          
          <p>Each tweet was annotated by two native English-speaking annotators [<xref ref-type="bibr" rid="ref_35">35</xref>]. Uniform standards were applied in the annotation process, focusing on identifying the type of hate crime and the motivation behind it. The corpus was annotated for four categories and causes of hate crimes, as outlined in <xref ref-type="table" rid="table_1">Table 1</xref>. Regular discussions were held between the annotators and the judge overseeing the annotation process to address any arising inconsistencies or challenges.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>Glossed entity classes for HateMotiv corpus</caption>
              <abstract/>
              <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Class Category</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Explanation</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Hate crime type</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Hate crime type refers to categories identified by the FBI, including physical assault, verbal abuse, and incitement to hatred</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Motivation</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Motivation refers to the underlying motive for hate crimes, such as bias related to racism, religion, disability, and unknown</span></p></td></tr></tbody></table>
            </table-wrap>
          
          <p>The HateMotiv corpus analysis revealed that physical assault constitutes the most common type of hate crime, while verbal abuse is the least frequently recorded category on Twitter. A notable observation from the data is the primary role played by the inability to accept diversity in terms of skin color and nationality in the perpetration of various forms of hate crimes. Conversely, hate crimes attributed to disability and negative attitudes towards disabled individuals constituted a minor percentage of the overall causes.</p><p>Furthermore, sexism or gender-based discrimination emerged as the second most prevalent justification for hate crimes, following racial prejudice. Notably, a proportion of hate crimes were committed with no apparent motive, reflecting the perpetrators' inherent biases or mental health issues. This is captured in the corpus under the term “unknown motive". The data indicated that assaults were the most frequent type of hate crime committed for reasons classified as unknown. However, the incidence of crimes committed for indeterminate reasons remains relatively low, accounting for approximately 0.011% of all categorized hate crimes.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.3. Cleaning and visualizing data</title>
          
          <p>The analysis of emojis within tweets was employed as a preliminary method to gauge the tone of the message. However, the primary focus was on textual data, which required extensive cleaning and preprocessing. This process involved the following four steps:</p><p>Step 1: Filtering. The first step involved the removal of hypertext links (e.g., http://google.com) and user handles typically starting with the “@” symbol on Twitter. This was crucial to eliminate irrelevant data and focus on the content of the tweets.</p><p>Step 2: Tokenization. In the second step, a Bag of Words representation was created. This involved the exclusion of punctuation and question marks to facilitate the appropriate representation of large datasets.</p><p>Step 3: Stop-word removal. Commonly occurring words such as “a”, “an”, and “the”, which do not contribute to the analysis, were eliminated in the third step.</p><p>Step 4: N-gram creation. This step involved generating n-grams, defined as sequences of ‘n' words or characters extracted from the text. While unigrams and bigrams have distinct utilities, this study utilized unigram tokens for tweet preparation. The decision to focus on unigrams was based on their comprehensive data coverage.</p><p>The selection between constructing unigrams or bigrams should be guided by the specific objectives of the study. Bigrams, such as “not good”, are effective in conveying emotions succinctly, making them particularly suitable for sentiment analysis and product reviews. Conversely, unigrams offer comprehensive data coverage. In this research, the focus was on unigram tokens for tweet preparation, to evaluate the efficacy of various stemmers and lemmatizers. It was found that while lemmatizers could deconstruct compound words into their elements, this process did not significantly enhance accuracy compared to the categorization models applied. Post-cleanup of text documents, tokenization was employed for more detailed analysis, necessitating the transformation of these tokens into feature vectors. Feature vectors serve as a crucial representation in the training of classification algorithms.</p><p>Two transformation techniques were compared: the Bag of Words method and the TF-IDF method. The Bag of Words approach, a straightforward transformation strategy, utilizes the corpus's diverse words as features, with each column indicating the frequency of a specific term's occurrence. Despite its computational simplicity, this method provides limited insights beyond word frequency. The TF-IDF method, on the other hand, combines the frequency of a term's occurrence in the text with its distribution across different document types to assign a weight to each word. This implies that commonly occurring words across various text types are assigned a lower weight. The feature vectors generated using these methods were successfully prepared for use in training classification models.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.4. Classification using the proposed architectures</title>
          
          <p>This study introduced three distinct deep learning models for hate speech classification: a LSTM network, a CNN, and a hybrid model combining both. The performance of each model in executing classification tasks was evaluated comprehensively. The CNN model, characterized by higher efficiency and a manageable number of trainable parameters, was ultimately selected for deployment on System-on-Chip Field Programmable Gate Arrays (SoC-FPGAs). The choice was influenced by hardware limitations, as SoC-FPGAs, known for their high performance and low power consumption, are suitable for edge computing applications, but only the CNN model was compatible with this hardware.</p>
          
            <sec disp-level="level3">
              
                <title>3.4.1 Lstm architecture</title>
              
              <p>As depicted in <xref ref-type="fig" rid="fig_1">Figure 1</xref>, the employed LSTM architecture incorporates two LSTM layers, each consisting of 100 LSTM cells. These layers are designed to accurately represent the sequential relationships between the features and the labels of hate speech data. The LSTM layers process the incoming data, discerning complex connections between characteristics and labels. Subsequently, two fully connected layers receive the output from the LSTM layers and generate a final prediction based on the processed data.</p>
              
                <fig id="fig_1">
                  <label>Figure 1</label>
                  <caption>Proposed LSTM architecture</caption>
                  <abstract/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_5HrlyMJnujKB2IV4.png"/>
                </fig>
              
            </sec>
          
          
            <sec disp-level="level3">
              
                <title>3.4.2 Cnn architecture</title>
              
              <p> <xref ref-type="fig" rid="fig_2">Figure 2</xref> presents the CNN model structure, comprising several CNN layers followed by pooling layers, originally developed for image classification tasks. For the purpose of hate speech classification, data arrays, represented as images, were input into the model. The CNN was then trained to identify pertinent features and make predictions about the input data concerning the classification of racist content.</p>
              
                <fig id="fig_2">
                  <label>Figure 2</label>
                  <caption>Proposed CNN architecture</caption>
                  <abstract/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_W_ljpQ_W7JF09XZz.png"/>
                </fig>
              
            </sec>
          
          
            <sec disp-level="level3">
              
                <title>3.4.3 Cnn-lstm architecture</title>
              
              <p>The hybrid architecture, depicted in <xref ref-type="fig" rid="fig_3">Figure 3</xref>, amalgamates the strengths of CNN and LSTM networks to enhance outcomes for complex deep learning tasks. This model harnesses the LSTM's ability to model temporal correlations among features in conjunction with the CNN's capacity to extract pivotal information from the data. The CNN simplifies the feature extraction process by identifying key elements, while the LSTM maintains the temporal relationships within the data.</p><p>The CNN model was ultimately chosen for hardware implementation due to its superior performance and efficiency compared to the LSTM and CNN-LSTM models. The selection was also influenced by the model's simplicity in terms of understanding and implementation on Field Programmable Gate Arrays (FPGAs). Hardware limitations were a contributing factor, as the FPGA was compatible only with the CNN architecture.</p><p>In summary, the study presented three distinct deep learning models - CNN, LSTM, and their hybrid - for the classification of racist content. Extensive testing led to the selection of the CNN model for implementation on SoC-FPGAs due to its high performance and computational efficiency. The CNN model demonstrated superiority over LSTM models in terms of accuracy and processing efficiency, with all models capable of analyzing temporal aspects of data.</p>
              
                <fig id="fig_3">
                  <label>Figure 3</label>
                  <caption>Proposed CNN-LSTM architecture</caption>
                  <abstract/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_fZBa6Flp-zLazRPQ.png"/>
                </fig>
              
            </sec>
          
          
            <sec disp-level="level3">
              
                <title>3.4.4 Cnn-lstm architecture</title>
              
              <p>Three models were implemented for comparative analysis: an LSTM model, a CNN, and a CNN-LSTM hybrid. The LSTM model consists of two layers, each with 100 units, following an input layer that processes 1200 features sequentially. The initial LSTM layer receives data in the format of (1, 1200) and outputs in the form of (1, 100), which is then fed into the subsequent LSTM layer. A series of fully connected layers compute the probability distribution for each class after the network learns the association between features and labels.</p><p>The proposed CNN architecture comprises six CNN blocks, each containing a CNN layer followed by a max pooling layer. After the CNN blocks, a flatten layer converts the aggregated features into a one-dimensional array. A dropout layer with a 50% rate is introduced to prevent overfitting, randomly eliminating neurons to reduce dependency on training data. The output from the CNN layers is passed to fully connected layers, which produce the probability distribution for each class.</p><p>In the CNN-LSTM hybrid:</p><p>Each of the four CNN processing units includes a CNN layer for feature extraction and a max pooling layer to capture the most salient features.</p><p>A reshape layer then transforms the output from the CNN blocks, converting 3D CNN output into 2D LSTM input.</p><p>The CNN block's output is fed into an LSTM layer to learn features that evolve over time.</p><p>Finally, a flatten layer followed by two fully connected layers generates a probability distribution for the classes.</p>
            </sec>
          
          
            <sec disp-level="level3">
              
                <title>3.4.5 Parameter tuning</title>
              
              <p>The performance of the implemented deep learning models can be significantly influenced by the tuning of hyperparameters. These models necessitate meticulous adjustment of several hyperparameters, which affect memory and compute complexity. This section outlines the additional hyperparameters that facilitate the selection of a specific approach for given scenarios. It is observed that superior outcomes often require extensive tuning of these hyperparameters.</p><p>Hyperparameter optimization can be mathematically defined as:</p>
              
                <disp-formula>
                  <label>(1)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msup>
                      <mi>x</mi>
                      <mo>∗</mo>
                    </msup>
                    <mo>=</mo>
                    <mo data-mjx-texclass="NONE">⁡</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <msubsup>
                      <mi>arg</mi>
                      <mi>m</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mi>x</mi>
                        <mi>X</mi>
                        <mo>∈</mo>
                      </mrow>
                    </msubsup>
                    <mi>f</mi>
                    <mi>x</mi>
                  </math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>f</mi>
    <mi>x</mi>
    <mo stretchy="false">(</mo>
    <mo stretchy="false">)</mo>
  </math>
</inline-formula> represents the objective function to minimize the error measured on the validation set, and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msup>
      <mi>x</mi>
      <mo>∗</mo>
    </msup>
  </math>
</inline-formula> is the set of hyperparameters within the domain $X$ that yields the lowest error. The goal is to identify the hyperparameter values that result in optimal performance on the validation set metric. The selection between manual and automatic hyperparameter tuning establishes a balance between the maximal computational cost of automated models and the in-depth knowledge required for manual selection. In this study, a QBO model was utilized to fine-tune the CNN-LSTM model's hyperparameters.</p><p>QBO</p><p>This section delineates the principles of QBO, an approach utilized for feature selection in this study. In QBO, binary representation is used, where ‘1s' indicate features to be retained and ‘0s' denote features to be discarded. The quantum bit (Q-bit) operations in QBO involve each feature being represented as a superposition, characterized by a complex integer. This is mathematically expressed as:</p>
              
                <disp-formula>
                  <label>(2)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>q</mi>
                    <mi>α</mi>
                    <mi>i</mi>
                    <mi>β</mi>
                    <mi>a</mi>
                    <mi>β</mi>
                    <mo>=</mo>
                    <mo>+</mo>
                    <mo>=</mo>
                    <mo>,</mo>
                    <mo>+</mo>
                    <mo>=</mo>
                    <msup>
                      <mi>e</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mi>i</mi>
                        <mi>θ</mi>
                      </mrow>
                    </msup>
                    <msup>
                      <mrow data-mjx-texclass="ORD">
                        <mo stretchy="false">|</mo>
                      </mrow>
                      <mn>2</mn>
                    </msup>
                    <msup>
                      <mrow data-mjx-texclass="ORD">
                        <mo stretchy="false">|</mo>
                      </mrow>
                      <mn>2</mn>
                    </msup>
                    <mrow data-mjx-texclass="ORD">
                      <mo stretchy="false">|</mo>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mo stretchy="false">|</mo>
                    </mrow>
                    <mn>1</mn>
                  </math>
                </disp-formula>
              
              <p> where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>α</mi>
  </math>
</inline-formula> and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>β</mi>
  </math>
</inline-formula> correspond to the two potential states of the Q-bit, namely ‘0' and ‘1'. The angle of $q<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>i</mi>
    <mi>s</mi>
    <mi>a</mi>
    <mi>d</mi>
    <mi>j</mi>
    <mi>u</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>d</mi>
    <mi>u</mi>
    <mi>s</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>g</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>a</mi>
    <mi>r</mi>
    <mi>c</mi>
    <mi>t</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>f</mi>
    <mi>u</mi>
    <mi>n</mi>
    <mi>c</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>p</mi>
    <mi>p</mi>
    <mi>T</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>p</mi>
    <mi>r</mi>
    <mi>i</mi>
    <mi>m</mi>
    <mi>a</mi>
    <mi>r</mi>
    <mi>y</mi>
    <mi>o</mi>
    <mi>b</mi>
    <mi>j</mi>
    <mi>e</mi>
    <mi>c</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>v</mi>
    <mi>e</mi>
    <mi>o</mi>
    <mi>f</mi>
    <mi>Q</mi>
    <mi>B</mi>
    <mi>O</mi>
    <mi>i</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mi>o</mi>
    <mi>d</mi>
    <mi>e</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>m</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>e</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>c</mi>
    <mi>h</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>g</mi>
    <mi>e</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>v</mi>
    <mi>a</mi>
    <mi>l</mi>
    <mi>u</mi>
    <mi>e</mi>
    <mi>o</mi>
    <mi>f</mi>
    <mo>.</mo>
    <mo>&amp;lt;</mo>
    <mo>&amp;gt;&amp;lt;</mo>
    <mo>&amp;gt;</mo>
    <mrow data-mjx-texclass="ORD">
      <mo>/</mo>
    </mrow>
  </math>
</inline-formula>q<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mo>.</mo>
    <mi>T</mi>
    <mi>h</mi>
    <mi>i</mi>
    <mi>s</mi>
    <mi>p</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>c</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>s</mi>
    <mi>i</mi>
    <mi>s</mi>
    <mi>c</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>d</mi>
    <mi>u</mi>
    <mi>c</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>d</mi>
    <mi>u</mi>
    <mi>s</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>g</mi>
    <mi>a</mi>
    <mi>c</mi>
    <mi>a</mi>
    <mi>l</mi>
    <mi>c</mi>
    <mi>u</mi>
    <mi>l</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>m</mi>
    <mi>e</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>o</mi>
    <mi>d</mi>
  </math>
</inline-formula>\Delta \theta$.</p>
              
                <disp-formula>
                  <label>(3)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>q</mi>
                    <mi>t</mi>
                    <mi>q</mi>
                    <mi>t</mi>
                    <mi>R</mi>
                    <mi mathvariant="normal">Δ</mi>
                    <mi>θ</mi>
                    <mi>a</mi>
                    <mi>t</mi>
                    <mi>β</mi>
                    <mi>t</mi>
                    <mi>R</mi>
                    <mi mathvariant="normal">Δ</mi>
                    <mi>θ</mi>
                    <mo stretchy="false">(</mo>
                    <mo>+</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>×</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo stretchy="false">[</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo stretchy="false">]</mo>
                    <mo>×</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mn>1</mn>
                  </math>
                </disp-formula>
              
              <p> where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>R</mi>
    <mi mathvariant="normal">Δ</mi>
    <mi>θ</mi>
    <mo stretchy="false">(</mo>
    <mo stretchy="false">)</mo>
  </math>
</inline-formula> stands for the rotation matrix associated with a change of <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi mathvariant="normal">Δ</mi>
    <mi>θ</mi>
  </math>
</inline-formula> in the angle, defined as:</p>
              
                <disp-formula>
                  <label>(4)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>R</mi>
                    <mi mathvariant="normal">Δ</mi>
                    <mi>θ</mi>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">[</mo>
                      <mo data-mjx-texclass="CLOSE">]</mo>
                      <mtable columnalign="left" columnspacing="1em" rowspacing="4pt">
                        <mtr>
                          <mtd>
                            <mi>cos</mi>
                            <mi mathvariant="normal">Δ</mi>
                            <mi>θ</mi>
                            <mi>sin</mi>
                            <mi mathvariant="normal">Δ</mi>
                            <mi>θ</mi>
                            <mo data-mjx-texclass="NONE">⁡</mo>
                            <mo stretchy="false">(</mo>
                            <mo stretchy="false">)</mo>
                            <mo>−</mo>
                            <mo data-mjx-texclass="NONE">⁡</mo>
                            <mo stretchy="false">(</mo>
                            <mo stretchy="false">)</mo>
                          </mtd>
                        </mtr>
                        <mtr>
                          <mtd>
                            <mi>sin</mi>
                            <mi mathvariant="normal">Δ</mi>
                            <mi>θ</mi>
                            <mi>cos</mi>
                            <mi mathvariant="normal">Δ</mi>
                            <mi>θ</mi>
                            <mo data-mjx-texclass="NONE">⁡</mo>
                            <mo stretchy="false">(</mo>
                            <mo stretchy="false">)</mo>
                            <mo>−</mo>
                            <mo data-mjx-texclass="NONE">⁡</mo>
                            <mo stretchy="false">(</mo>
                            <mo stretchy="false">)</mo>
                          </mtd>
                        </mtr>
                      </mtable>
                    </mrow>
                  </math>
                </disp-formula>
              
              <p>The optimal solution, denoted as <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>X</mi>
      <mi>b</mi>
    </msub>
  </math>
</inline-formula>, is predetermined to set the value of the parameters influencing $q<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mo>.</mo>
    <mi>T</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>b</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>a</mi>
    <mi>r</mi>
    <mi>y</mi>
    <mi>r</mi>
    <mi>e</mi>
    <mi>p</mi>
    <mi>r</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>t</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>o</mi>
    <mi>f</mi>
    <mi>a</mi>
    <mi>s</mi>
    <mi>o</mi>
    <mi>l</mi>
    <mi>u</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
  </math>
</inline-formula>X_i<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>i</mi>
    <mi>s</mi>
    <mi>r</mi>
    <mi>e</mi>
    <mi>p</mi>
    <mi>r</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>d</mi>
    <mi>b</mi>
    <mi>y</mi>
    <mi>i</mi>
    <mi>t</mi>
    <mi>s</mi>
  </math>
</inline-formula>j<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mo>−</mo>
    <mi>t</mi>
    <mi>h</mi>
    <mi>b</mi>
    <mi>i</mi>
    <mi>t</mi>
  </math>
</inline-formula>X_{i j}<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mo>,</mo>
    <mi>w</mi>
    <mi>h</mi>
    <mi>i</mi>
    <mi>l</mi>
    <mi>e</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
  </math>
</inline-formula>j<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mo>−</mo>
    <mi>t</mi>
    <mi>h</mi>
    <mi>b</mi>
    <mi>i</mi>
    <mi>t</mi>
    <mi>o</mi>
    <mi>f</mi>
  </math>
</inline-formula>X_b<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>a</mi>
    <mi>t</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>m</mi>
    <mi>e</mi>
  </math>
</inline-formula>t<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>i</mi>
    <mi>s</mi>
    <mi>d</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>o</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>d</mi>
    <mi>a</mi>
    <mi>s</mi>
  </math>
</inline-formula>X_{b j}<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mo>.</mo>
    <mo stretchy="false">[</mo>
    <mo stretchy="false">]</mo>
    <mo>,</mo>
    <mo>,</mo>
    <mo>−</mo>
    <mo>.</mo>
    <mo>&amp;lt;</mo>
    <mo>&amp;gt;&amp;lt;</mo>
    <mo>&amp;gt;</mo>
    <mo>.</mo>
    <mo>,</mo>
    <mi>A</mi>
    <mi>s</mi>
    <mi>r</mi>
    <mi>e</mi>
    <mi>p</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>d</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mi>u</mi>
    <mi>d</mi>
    <mi>y</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>g</mi>
    <mi>l</mi>
    <mi>e</mi>
    <mi>v</mi>
    <mi>e</mi>
    <mi>c</mi>
    <mi>t</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>Q</mi>
    <mi>B</mi>
    <mi>O</mi>
    <mi>i</mi>
    <mi>s</mi>
    <mi>c</mi>
    <mi>a</mi>
    <mi>p</mi>
    <mi>a</mi>
    <mi>b</mi>
    <mi>l</mi>
    <mi>e</mi>
    <mi>o</mi>
    <mi>f</mi>
    <mi>a</mi>
    <mi>s</mi>
    <mi>s</mi>
    <mi>u</mi>
    <mi>m</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>g</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>e</mi>
    <mi>o</mi>
    <mi>f</mi>
    <mi>e</mi>
    <mi>i</mi>
    <mi>g</mi>
    <mi>h</mi>
    <mi>t</mi>
    <mi>d</mi>
    <mi>i</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>c</mi>
    <mi>t</mi>
    <mi>v</mi>
    <mi>a</mi>
    <mi>l</mi>
    <mi>u</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>a</mi>
    <mi>l</mi>
    <mi>l</mi>
    <mi>o</mi>
    <mi>w</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>g</mi>
    <mi>f</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mi>v</mi>
    <mi>a</mi>
    <mi>r</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>d</mi>
    <mi>a</mi>
    <mi>d</mi>
    <mi>j</mi>
    <mi>u</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mi>m</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>t</mi>
    <mi>s</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>Q</mi>
    <mi>b</mi>
    <mi>i</mi>
    <mi>t</mi>
    <mi>r</mi>
    <mi>e</mi>
    <mi>p</mi>
    <mi>r</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>t</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>p</mi>
    <mi>p</mi>
    <mi>T</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>p</mi>
    <mi>r</mi>
    <mi>i</mi>
    <mi>m</mi>
    <mi>a</mi>
    <mi>r</mi>
    <mi>y</mi>
    <mi>o</mi>
    <mi>b</mi>
    <mi>j</mi>
    <mi>e</mi>
    <mi>c</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>v</mi>
    <mi>e</mi>
    <mi>o</mi>
    <mi>f</mi>
    <mi>Q</mi>
    <mi>B</mi>
    <mi>O</mi>
    <mi>i</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mi>o</mi>
    <mi>b</mi>
    <mi>a</mi>
    <mi>l</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>c</mi>
    <mi>e</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>e</mi>
    <mi>x</mi>
    <mi>p</mi>
    <mi>l</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>d</mi>
    <mi>e</mi>
    <mi>x</mi>
    <mi>p</mi>
    <mi>l</mi>
    <mi>o</mi>
    <mi>i</mi>
    <mi>t</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>p</mi>
    <mi>o</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>a</mi>
    <mi>l</mi>
    <mi>I</mi>
    <mi>n</mi>
    <mi>i</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>a</mi>
    <mi>l</mi>
    <mi>l</mi>
    <mi>y</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>d</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>a</mi>
    <mi>i</mi>
    <mi>s</mi>
    <mi>s</mi>
    <mi>p</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>t</mi>
    <mi>o</mi>
    <mi>a</mi>
    <mn>36</mn>
    <mn>70</mn>
    <mrow data-mjx-texclass="ORD">
      <mo>/</mo>
    </mrow>
  </math>
</inline-formula>D<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>Q</mi>
    <mi>b</mi>
    <mi>i</mi>
    <mi>t</mi>
    <mi>s</mi>
    <mi>C</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>s</mi>
    <mi>e</mi>
    <mi>q</mi>
    <mi>u</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>t</mi>
    <mi>l</mi>
    <mi>y</mi>
    <mi>e</mi>
    <mi>a</mi>
    <mi>c</mi>
    <mi>h</mi>
    <mi>s</mi>
    <mi>o</mi>
    <mi>l</mi>
    <mi>u</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mo>−</mo>
    <mo>.</mo>
    <mo>,</mo>
  </math>
</inline-formula>X_i$ can be expressed as:</p>
              
                <disp-formula>
                  <label>(5)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msub>
                      <mi>X</mi>
                      <mi>i</mi>
                    </msub>
                    <mo>=</mo>
                    <mo>=</mo>
                    <mo>,</mo>
                    <mo>=</mo>
                    <mo>,</mo>
                    <mo>,</mo>
                    <mo>…</mo>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">[</mo>
                      <mo>…</mo>
                      <mo data-mjx-texclass="CLOSE">]</mo>
                      <msub>
                        <mi>q</mi>
                        <mrow data-mjx-texclass="ORD">
                          <mi>i</mi>
                          <mn>1</mn>
                        </mrow>
                      </msub>
                      <mrow data-mjx-texclass="INNER">
                        <mo data-mjx-texclass="OPEN">|</mo>
                        <mo data-mjx-texclass="CLOSE">|</mo>
                        <msub>
                          <mi>q</mi>
                          <mrow data-mjx-texclass="ORD">
                            <mi>i</mi>
                            <mn>2</mn>
                          </mrow>
                        </msub>
                      </mrow>
                      <mrow data-mjx-texclass="INNER">
                        <mo data-mjx-texclass="OPEN">|</mo>
                        <mo data-mjx-texclass="CLOSE">|</mo>
                        <msub>
                          <mi>q</mi>
                          <mrow data-mjx-texclass="ORD">
                            <mi>i</mi>
                            <mi>D</mi>
                          </mrow>
                        </msub>
                      </mrow>
                    </mrow>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">[</mo>
                      <mo>…</mo>
                      <mo>∣</mo>
                      <mo data-mjx-texclass="CLOSE">]</mo>
                      <msub>
                        <mi>θ</mi>
                        <mrow data-mjx-texclass="ORD">
                          <mi>i</mi>
                          <mn>1</mn>
                        </mrow>
                      </msub>
                      <msub>
                        <mi>θ</mi>
                        <mrow data-mjx-texclass="ORD">
                          <mi>i</mi>
                          <mi>D</mi>
                        </mrow>
                      </msub>
                      <mrow data-mjx-texclass="INNER">
                        <mo data-mjx-texclass="OPEN">|</mo>
                        <mo data-mjx-texclass="CLOSE">|</mo>
                        <msub>
                          <mi>θ</mi>
                          <mrow data-mjx-texclass="ORD">
                            <mi>i</mi>
                            <mn>2</mn>
                          </mrow>
                        </msub>
                      </mrow>
                    </mrow>
                    <mi>i</mi>
                    <mi>N</mi>
                    <mn>1</mn>
                    <mn>2</mn>
                  </math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>X</mi>
      <mi>i</mi>
    </msub>
  </math>
</inline-formula> refers to the superposition of probabilities of selecting or not selecting features.</p><p>B. Second stage</p><p>The process of updating agents until a specified criterion is met constitutes a critical phase in the application of the QAHA. Initially, the binary representation of each solution, denoted as <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>X</mi>
    <mi>i</mi>
  </math>
</inline-formula>, is determined through an equation. This representation involves a random charge rand <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mo>∈</mo>
    <mo stretchy="false">[</mo>
    <mo>,</mo>
    <mo stretchy="false">]</mo>
    <mn>0</mn>
    <mn>1</mn>
  </math>
</inline-formula> and a parameter <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>β</mi>
  </math>
</inline-formula> as defined in Eq. (2).</p>
              
                <disp-formula>
                  <label>(6)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>B</mi>
                    <msub>
                      <mi>X</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mi>i</mi>
                        <mi>j</mi>
                      </mrow>
                    </msub>
                    <mo>=</mo>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">{</mo>
                      <mo data-mjx-texclass="CLOSE" fence="true" stretchy="true" symmetric="true"/>
                      <mtable columnalign="left center center" columnspacing="1em" rowspacing="4pt">
                        <mtr>
                          <mtd>
                            <mn>1</mn>
                          </mtd>
                          <mtd>
                            <mtext> if </mtext>
                          </mtd>
                          <mtd>
                            <mo stretchy="false">|</mo>
                            <mo>&gt;</mo>
                            <mi>β</mi>
                            <msup>
                              <mrow data-mjx-texclass="ORD">
                                <mo stretchy="false">|</mo>
                              </mrow>
                              <mn>2</mn>
                            </msup>
                            <mtext> rand </mtext>
                          </mtd>
                        </mtr>
                        <mtr>
                          <mtd>
                            <mn>0</mn>
                          </mtd>
                          <mtd/>
                          <mtd>
                            <mtext> otherwise </mtext>
                          </mtd>
                        </mtr>
                      </mtable>
                    </mrow>
                  </math>
                </disp-formula>
              
              <p>Subsequently, the fitness value for each agent is computed. This computation is achieved by training a CNN-LSTM classifier, with the features derived from <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>B</mi>
    <msub>
      <mi>X</mi>
      <mrow data-mjx-texclass="ORD">
        <mi>i</mi>
        <mi>j</mi>
      </mrow>
    </msub>
  </math>
</inline-formula> serving as the model's hyperparameters. The fitness value is formulated as:</p>
              
                <disp-formula>
                  <label>(7)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msub>
                      <mtext> Fit </mtext>
                      <mi>i</mi>
                    </msub>
                    <mo>=</mo>
                    <mo>×</mo>
                    <mo>+</mo>
                    <mo stretchy="false">(</mo>
                    <mo>−</mo>
                    <mo stretchy="false">)</mo>
                    <mo>×</mo>
                    <mi>ρ</mi>
                    <mi>γ</mi>
                    <mi>ρ</mi>
                    <mn>1</mn>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mfrac>
                        <mrow data-mjx-texclass="INNER">
                          <mo data-mjx-texclass="OPEN">|</mo>
                          <mo data-mjx-texclass="CLOSE">|</mo>
                          <mi>B</mi>
                          <msub>
                            <mi>X</mi>
                            <mrow data-mjx-texclass="ORD">
                              <mi>i</mi>
                              <mi>j</mi>
                            </mrow>
                          </msub>
                        </mrow>
                        <mi>D</mi>
                      </mfrac>
                    </mrow>
                  </math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">|</mo>
      <mo data-mjx-texclass="CLOSE">|</mo>
      <mi>B</mi>
      <msub>
        <mi>X</mi>
        <mrow data-mjx-texclass="ORD">
          <mi>i</mi>
          <mi>j</mi>
        </mrow>
      </msub>
    </mrow>
  </math>
</inline-formula> represents the error rate in classifying features using the CNN-LSTM classifier, and denotes the total number of features employed. A normalization factor within the range <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mo stretchy="false">[</mo>
    <mo>,</mo>
    <mo stretchy="false">]</mo>
    <mn>0</mn>
    <mn>1</mn>
  </math>
</inline-formula> ensures parity in fitness levels across different agents. The LSTM model is preferred due to its simplicity, efficiency, and reliance on a singular tuning parameter. Its ability to retain information from the training set contributes to its effectiveness, particularly when other classifiers might not yield desired results.</p><p>The subsequent stage involves identifying the most optimal agent <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>X</mi>
      <mi>b</mi>
    </msub>
  </math>
</inline-formula>, characterized by the minimum fitness value <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>F</mi>
    <mi>i</mi>
    <msub>
      <mi>t</mi>
      <mi>b</mi>
    </msub>
  </math>
</inline-formula>. This step is pivotal in the QAHA process as it determines the most suitable set of features for the classification task at hand.</p><p>C. Third stage</p><p>The test set is narrowed down to features equivalent to those in the binary representation of <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>X</mi>
      <mi>b</mi>
    </msub>
  </math>
</inline-formula>. The reduceddimension test set is then used to apply the trained classifier for predictions. Subsequently, the output quality is thoroughly evaluated. The computational cost of QAHA is determined by the initial population size, population size $N<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mo>,</mo>
    <mi>f</mi>
    <mi>i</mi>
    <mi>t</mi>
    <mi>n</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>s</mi>
    <mi>e</mi>
    <mi>v</mi>
    <mi>a</mi>
    <mi>l</mi>
    <mi>u</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
  </math>
</inline-formula>N_{F it}$, and maximum iteration count.</p>
              
                <disp-formula>
                  <label>(8)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>O</mi>
                    <mi>Q</mi>
                    <mi>A</mi>
                    <mi>H</mi>
                    <mi>A</mi>
                    <mi>O</mi>
                    <mi>T</mi>
                    <mi>C</mi>
                    <mi>N</mi>
                    <mi>T</mi>
                    <mi>N</mi>
                    <mi>D</mi>
                    <mi>T</mi>
                    <mi>D</mi>
                    <mi>O</mi>
                    <mi>N</mi>
                    <mi>D</mi>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo>×</mo>
                    <mo>×</mo>
                    <mo>+</mo>
                    <mo>×</mo>
                    <mo>×</mo>
                    <mo>+</mo>
                    <mo>×</mo>
                    <mo stretchy="false">)</mo>
                    <mo>+</mo>
                    <mo stretchy="false">(</mo>
                    <mo>×</mo>
                    <mo stretchy="false">)</mo>
                    <mrow data-mjx-texclass="ORD">
                      <mo>/</mo>
                    </mrow>
                    <mn>2</mn>
                  </math>
                </disp-formula>
              
              <p>In summary, the complexity of QAHA is given by:</p>
              
                <disp-formula>
                  <label>(9)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>O</mi>
                    <mi>Q</mi>
                    <mi>A</mi>
                    <mi>H</mi>
                    <mi>A</mi>
                    <mi>O</mi>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo>×</mo>
                      <mo>×</mo>
                      <mo>+</mo>
                      <mo>×</mo>
                      <mo>×</mo>
                      <mo>+</mo>
                      <mo>×</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mi>T</mi>
                      <mi>N</mi>
                      <mi>T</mi>
                      <mi>N</mi>
                      <mi>D</mi>
                      <mi>T</mi>
                      <mi>D</mi>
                      <msub>
                        <mi>N</mi>
                        <mrow data-mjx-texclass="ORD">
                          <mi>F</mi>
                          <mi>i</mi>
                          <mi>t</mi>
                        </mrow>
                      </msub>
                      <mrow data-mjx-texclass="ORD">
                        <mo>/</mo>
                      </mrow>
                      <mn>2</mn>
                    </mrow>
                  </math>
                </disp-formula>
              
            </sec>
          
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>4. Results and discussion</title>
      
        <sec disp-level="level2">
          
            <title>4.1. Hardware and software used for the experiments</title>
          
          <p>For preprocessing the tweets and applying deep learning methodologies, a Jupyter notebook, scripted in Python 3.6, was employed. The computational tasks were executed on a workstation equipped with the following hardware specifications: an Intel Core i7-9700K processor operating at 3.60GHz, 32.0GB of RAM, and a 6GB NVIDIA GeForce graphics card. For text preprocessing, the spaCy and nltk libraries were utilized, facilitating the efficient processing of the datasets.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>4.2. Performances metrics</title>
          
          <p>The performance of the model was evaluated using the confusion matrix, a tool that provides four distinct outcomes: true positive (TP), true negative (TN), false positive (FP), and false negative (FN). The effectiveness of the model was determined through the calculation of various metrics derived from the confusion matrix.</p>
          
            <disp-formula>
              <label>(10)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mtext> Accuracy </mtext>
                <mo>=</mo>
                <mo stretchy="false">(</mo>
                <mo>+</mo>
                <mo stretchy="false">)</mo>
                <mo stretchy="false">(</mo>
                <mo>+</mo>
                <mo>+</mo>
                <mo>+</mo>
                <mo stretchy="false">)</mo>
                <mi>T</mi>
                <mi>N</mi>
                <mi>T</mi>
                <mi>P</mi>
                <mi>T</mi>
                <mi>N</mi>
                <mi>T</mi>
                <mi>P</mi>
                <mi>F</mi>
                <mi>N</mi>
                <mi>F</mi>
                <mi>P</mi>
                <mrow data-mjx-texclass="ORD">
                  <mo>/</mo>
                </mrow>
              </math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(11)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mtext> Sensitivity </mtext>
                <mo>=</mo>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <mfrac>
                    <mrow>
                      <mi>T</mi>
                      <mi>P</mi>
                    </mrow>
                    <mrow>
                      <mi>T</mi>
                      <mi>P</mi>
                      <mi>F</mi>
                      <mi>N</mi>
                      <mo>+</mo>
                    </mrow>
                  </mfrac>
                </mrow>
              </math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(12)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mtext> Specificity </mtext>
                <mo>=</mo>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <mfrac>
                    <mrow>
                      <mi>T</mi>
                      <mi>N</mi>
                    </mrow>
                    <mrow>
                      <mi>T</mi>
                      <mi>N</mi>
                      <mi>F</mi>
                      <mi>P</mi>
                      <mo>+</mo>
                    </mrow>
                  </mfrac>
                </mrow>
              </math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(13)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mtext> Precision </mtext>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                  </mrow>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                    <mi>F</mi>
                    <mi>P</mi>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(14)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>F</mi>
                <mo>−</mo>
                <mo>=</mo>
                <mtext> Measure </mtext>
                <mfrac>
                  <mrow>
                    <mn>2</mn>
                    <mi>T</mi>
                    <mi>P</mi>
                  </mrow>
                  <mrow>
                    <mn>2</mn>
                    <mi>T</mi>
                    <mi>P</mi>
                    <mi>F</mi>
                    <mi>P</mi>
                    <mi>F</mi>
                    <mi>N</mi>
                    <mo>+</mo>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>4.3. Analysis of the proposed classifier</title>
          
          <p>The efficacy of the proposed model was compared with that of generic deep learning models. The uniqueness of the dataset used in this study, which had not been previously employed for validation analysis, necessitated this comparison. The results of the analysis are presented in <xref ref-type="table" rid="table_2">Table 2</xref> and <xref ref-type="table" rid="table_3">Table 3</xref>.</p><p>The analysis of the hybrid model's performance is detailed in <xref ref-type="table" rid="table_2">Table 2</xref>. The Autoencoder (AE) model demonstrated an Area Under the Curve (AUC) of 0.758 and achieved an accuracy of 87.72%. Precision was recorded at 78.14%, recall at 89.92%, and the F-measure at 88.67%. Subsequently, the Deep Belief Network (DBN) model exhibited an AUC of 0.854, accuracy of 89.17%, precision of 70.91%, recall of 85.69%, and an F-measure of 82.33%. The RNN model registered an AUC of 0.687, with an accuracy of 90.28%. The precision was noted at 64.17%, recall at 86.66%, and F-measure at 80.24%. Following this, the CNN model showed an AUC of 0.947, an accuracy of 92.78%, precision of 91.94%, recall of 90.61%, and an F-measure of 86.86%. The LSTM model recorded an AUC of 0.957 and an accuracy of 94.34%. Its precision was 92.45%, recall 93.78%, and F-measure 91.36%. Finally, the combined CNN-LSTM model achieved the highest performance with an AUC of 0.967, accuracy of 95.97%, precision of 96.84%, recall of 97.24%, and an F-measure of 94.13%.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>Analysis of the proposed hybrid model without QAHA</caption>
              <abstract/>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Classification</p></td><td colspan="1" rowspan="1"><p>AUC</p></td><td colspan="1" rowspan="1"><p>Accuracy (%)</p></td><td colspan="1" rowspan="1"><p>Precision (%)</p></td><td colspan="1" rowspan="1"><p>Recall (%)</p></td><td colspan="1" rowspan="1"><p>F-Measure (%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>AE</p></td><td colspan="1" rowspan="1"><p>0.758</p></td><td colspan="1" rowspan="1"><p>87.72</p></td><td colspan="1" rowspan="1"><p>78.14</p></td><td colspan="1" rowspan="1"><p>89.92</p></td><td colspan="1" rowspan="1"><p>88.67</p></td></tr><tr><td colspan="1" rowspan="1"><p>DBN</p></td><td colspan="1" rowspan="1"><p>0.854</p></td><td colspan="1" rowspan="1"><p>89.17</p></td><td colspan="1" rowspan="1"><p>70.91</p></td><td colspan="1" rowspan="1"><p>85.69</p></td><td colspan="1" rowspan="1"><p>82.33</p></td></tr><tr><td colspan="1" rowspan="1"><p>RNN</p></td><td colspan="1" rowspan="1"><p>0.687</p></td><td colspan="1" rowspan="1"><p>90.28</p></td><td colspan="1" rowspan="1"><p>64.17</p></td><td colspan="1" rowspan="1"><p>86.66</p></td><td colspan="1" rowspan="1"><p>80.24</p></td></tr><tr><td colspan="1" rowspan="1"><p>CNN</p></td><td colspan="1" rowspan="1"><p>0.947</p></td><td colspan="1" rowspan="1"><p>92.78</p></td><td colspan="1" rowspan="1"><p>91.94</p></td><td colspan="1" rowspan="1"><p>90.61</p></td><td colspan="1" rowspan="1"><p>86.86</p></td></tr><tr><td colspan="1" rowspan="1"><p>LSTM</p></td><td colspan="1" rowspan="1"><p>0.957</p></td><td colspan="1" rowspan="1"><p>94.34</p></td><td colspan="1" rowspan="1"><p>92.45</p></td><td colspan="1" rowspan="1"><p>93.78</p></td><td colspan="1" rowspan="1"><p>91.36</p></td></tr><tr><td colspan="1" rowspan="1"><p>CNN-LSTM</p></td><td colspan="1" rowspan="1"><p>0.967</p></td><td colspan="1" rowspan="1"><p>95.97</p></td><td colspan="1" rowspan="1"><p>96.84</p></td><td colspan="1" rowspan="1"><p>97.24</p></td><td colspan="1" rowspan="1"><p>94.13</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>Analysis of the proposed hybrid model with QAHA</caption>
              <abstract/>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Classification</p></td><td colspan="1" rowspan="1"><p>AUC</p></td><td colspan="1" rowspan="1"><p>Accuracy (%)</p></td><td colspan="1" rowspan="1"><p>Precision (%)</p></td><td colspan="1" rowspan="1"><p>Recall (%)</p></td><td colspan="1" rowspan="1"><p>F-Measure (%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>AE</p></td><td colspan="1" rowspan="1"><p>0.9343</p></td><td colspan="1" rowspan="1"><p>91.56</p></td><td colspan="1" rowspan="1"><p>84.56</p></td><td colspan="1" rowspan="1"><p>90.66</p></td><td colspan="1" rowspan="1"><p>90.67</p></td></tr><tr><td colspan="1" rowspan="1"><p>DBN</p></td><td colspan="1" rowspan="1"><p>0.8923</p></td><td colspan="1" rowspan="1"><p>90.12</p></td><td colspan="1" rowspan="1"><p>85.2</p></td><td colspan="1" rowspan="1"><p>92.57</p></td><td colspan="1" rowspan="1"><p>86.54</p></td></tr><tr><td colspan="1" rowspan="1"><p>RNN</p></td><td colspan="1" rowspan="1"><p>0.9082</p></td><td colspan="1" rowspan="1"><p>93.22</p></td><td colspan="1" rowspan="1"><p>78.7</p></td><td colspan="1" rowspan="1"><p>90.67</p></td><td colspan="1" rowspan="1"><p>88.67</p></td></tr><tr><td colspan="1" rowspan="1"><p>CNN</p></td><td colspan="1" rowspan="1"><p>0.9135</p></td><td colspan="1" rowspan="1"><p>94.43</p></td><td colspan="1" rowspan="1"><p>93.6</p></td><td colspan="1" rowspan="1"><p>94.01</p></td><td colspan="1" rowspan="1"><p>91.54</p></td></tr><tr><td colspan="1" rowspan="1"><p>LSTM</p></td><td colspan="1" rowspan="1"><p>0.9544</p></td><td colspan="1" rowspan="1"><p>93.23</p></td><td colspan="1" rowspan="1"><p>94.7</p></td><td colspan="1" rowspan="1"><p>96.62</p></td><td colspan="1" rowspan="1"><p>95.09</p></td></tr><tr><td colspan="1" rowspan="1"><p>CNN-LSTM</p></td><td colspan="1" rowspan="1"><p>0.9829</p></td><td colspan="1" rowspan="1"><p>98.45</p></td><td colspan="1" rowspan="1"><p>98.8</p></td><td colspan="1" rowspan="1"><p>99.56</p></td><td colspan="1" rowspan="1"><p>97.70</p></td></tr></tbody></table>
            </table-wrap>
          
          <p><xref ref-type="table" rid="table_3">Table 3</xref> presents the outcomes from the assessment of the hybrid model integrated with QAHA. The AE model exhibited an AUC of 0.9343, achieving an accuracy of 91.56%. Precision was recorded at 84.56%, recall at 90.66%, and the F-measure at 90.67%. The DBN model attained an AUC of 0.8923, accuracy of 90.12%, precision of 85.2%, recall of 92.57%, and an F-measure of 86.54%. The RNN model demonstrated an AUC of 0.9082 and achieved an accuracy of 93.22%. Its precision was noted at 78.7%, recall at 90.67%, and F-measure at 88.67%. The CNN model showed an AUC of 0.9135, an accuracy of 94.43%, precision of 93.6%, recall of 94.01%, and an F-measure of 91.54%. Further, the LSTM model registered an AUC of 0.9544, with an accuracy of 93.23%, precision of 94.7%, recall of 96.62%, and an F-measure of 95.09%. Lastly, the CNN-LSTM model, representing the pinnacle of this research, achieved an AUC of 0.9829, an exceptional accuracy of 98.45%, precision of 98.8%, recall of 99.56%, and an F-measure of 97.70%.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>AUC comparison</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_jXWguv9oO1F6NYL8.png"/>
            </fig>
          
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>Accuracy analysis with and without the optimization model</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_hSKNGAKbcM219wun.png"/>
            </fig>
          
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>Graphical representation of various deep learning models</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_CKeTSeuEHuXkZ1Ov.png"/>
            </fig>
          
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>Recall analysis</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_eVEtaPxnMRLvox72.png"/>
            </fig>
          
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>Validation analysis of QAHA</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_xDJX8X87H0NDiMpD.png"/>
            </fig>
          
          <p><xref ref-type="fig" rid="fig_4">Figure 4</xref>, <xref ref-type="fig" rid="fig_5">Figure 5</xref>, <xref ref-type="fig" rid="fig_6">Figure 6</xref>, <xref ref-type="fig" rid="fig_7">Figure 7</xref> and <xref ref-type="fig" rid="fig_8">Figure 8</xref> provide graphical representations of these results, including AUC comparison (<xref ref-type="fig" rid="fig_4">Figure 4</xref>), accuracy analysis with and without the optimization model (<xref ref-type="fig" rid="fig_5">Figure 5</xref>), and recall analysis (<xref ref-type="fig" rid="fig_7">Figure 7</xref>), among others. The validation analysis of QAHA is depicted in <xref ref-type="fig" rid="fig_8">Figure 8</xref>. The analysis revealed that the implementation of QAHA significantly enhanced the performance of the deep learning models. The CNN-LSTM model with QAHA outperformed the other models, demonstrating the effectiveness of the proposed hybrid approach in the context of hate speech classification.</p>
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>5. Conclusions and future work</title>
      <p>The emergence of racist content on social media platforms, particularly Twitter, has necessitated the development of automated detection and removal mechanisms. This study has adopted a sentiment analysis approach to identify racist tweets, focusing on specific phrases and words. Following data preprocessing, neural network classification was conducted using LSTM, CNN, and a hybrid CNN-LSTM model. The experimental results demonstrated that the CNN and hybrid models significantly outperformed the LSTM model in both phases of the analysis. It was found that, despite its lower execution time, LSTM's complexity rendered it less suitable for SoC-FPGAs compared to the CNN model. The CNN's simpler architecture and high accuracy underscored its appropriateness for SoC-FPGA implementation. Furthermore, the QAHA was employed to optimize hyperparameters, enhancing the classification accuracy of the proposed model.</p><p>The dataset used in this study is publicly available, offering a valuable resource for future research into the automatic detection and prediction of hate crimes and their underlying motivations, including racism. This accessibility to the scientific community could spur further investigations into this domain. The experimental study revealed that the proposed model achieved superior performance compared to baseline models, with accuracy and recall rates exceeding 95% and 96%, respectively. Understanding the factors contributing to online hate crimes through advanced deep-learning techniques can be instrumental in curbing detrimental biases and reducing the incidence of crimes driven by such biases. Future work in this area aims to refine and employ sophisticated deep-learning methods to train models in recognizing the root causes of hate crimes shared online.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p></p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>e906</page-range>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>José Alberto</given-names>
              <surname>Benítez-Andrades</surname>
            </name>
            <name>
              <given-names>Álvaro</given-names>
              <surname>González-Jiménez</surname>
            </name>
            <name>
              <given-names>Álvaro</given-names>
              <surname>López-Brea</surname>
            </name>
            <name>
              <given-names>Jose</given-names>
              <surname>Aveleira-Mata</surname>
            </name>
            <name>
              <given-names>José Manuel</given-names>
              <surname>Alija-Pérez</surname>
            </name>
            <name>
              <given-names>María Teresa</given-names>
              <surname>García-Ordás</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.7717/peerj-cs.906</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Detecting racism and xenophobia using deep learning models on Twitter data: CNN, LSTM and BERT</article-title>
          <source>PeerJ Comput. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>114</volume>
          <page-range>120-129</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Sadiq</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Mehmood</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Ullah</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Ahmad</surname>
            </name>
            <name>
              <given-names>G. S.</given-names>
              <surname>Choi</surname>
            </name>
            <name>
              <given-names>B. W.</given-names>
              <surname>On</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.future.2020.07.050</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Aggression detection through deep neural model on Twitter</article-title>
          <source>Future Gener. Comput. Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>148-158</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J. A.</given-names>
              <surname>Benitez-Andrades</surname>
            </name>
            <name>
              <given-names>Á.</given-names>
              <surname>González-Jiménez</surname>
            </name>
            <name>
              <given-names>Á.</given-names>
              <surname>López-Brea</surname>
            </name>
            <name>
              <given-names>Benavides</given-names>
              <surname>C.</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Aveleira-Mata</surname>
            </name>
            <name>
              <given-names>J. M.</given-names>
              <surname>Alija-Pérez</surname>
            </name>
            <name>
              <given-names>M. T.</given-names>
              <surname>García-Ordás</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1007/978-3-030-98876-0_13</pub-id>
          <article-title>BERT model-based approach for detecting racism and xenophobia on Twitter data.</article-title>
          <source>Research Conference on Metadata and Semantics Research</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>9717-9728</page-range>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>E.</given-names>
              <surname>Lee</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Rustam</surname>
            </name>
            <name>
              <given-names>P. B.</given-names>
              <surname>Washington</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>El Barakaz</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Aljedaani</surname>
            </name>
            <name>
              <given-names>I.</given-names>
              <surname>Ashraf</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2022.3144266</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Racism detection by analyzing differential opinions through sentiment analysis of tweets using stacked ensemble GCR-NN model</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>28</volume>
          <page-range>433-441</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>H.</given-names>
              <surname>Macherla</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Kotapati</surname>
            </name>
            <name>
              <given-names>M. T.</given-names>
              <surname>Sunitha</surname>
            </name>
            <name>
              <given-names>K. R.</given-names>
              <surname>Chittipireddy</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Attuluri</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Vatambeti</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/isi.280219</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Deep learning framework-based chaotic hunger games search optimization algorithm for prediction of air quality index</article-title>
          <source>Ing. Syst. Inf.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>69</page-range>
          <issue>6</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>N.</given-names>
              <surname>Alnazzawi</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/data7060069</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Using Twitter to detect hate crimes and their motivations: The hatemotiv corpus</article-title>
          <source>Data</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>1-6</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>G. A.</given-names>
              <surname>De Souza</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Da Costa-Abreu</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/IJCNN48605.2020.9207652</pub-id>
          <article-title>Automatic offensive language detection from Twitter data using machine learning and feature selection of metadata.</article-title>
          <source>2020 International Joint Conference on Neural Networks (IJCNN), Glasgow, UK</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>318</page-range>
          <issue>7</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Natalia</given-names>
              <surname>Vanetik</surname>
            </name>
            <name>
              <given-names>Elisa</given-names>
              <surname>Mimoun</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/info13070318</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Detection of racist language in French tweets</article-title>
          <source>Inf.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>63</page-range>
          <issue>10</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Carlos</given-names>
              <surname>Arcila-Calderón</surname>
            </name>
            <name>
              <given-names>José J.</given-names>
              <surname>Amores</surname>
            </name>
            <name>
              <given-names>Pedro</given-names>
              <surname>Sánchez-Holgado</surname>
            </name>
            <name>
              <given-names>David</given-names>
              <surname>Blanco-Herrero</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/mti5100063</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Using shallow and deep learning to automatically detect hate motivated by gender and sexual orientation on Twitter in Spanish</article-title>
          <source>Multimodal Technol. Interact.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>28</volume>
          <page-range>1063-1071</page-range>
          <issue>4</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>N. V. R. S.</given-names>
              <surname>Reddy</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Chitteti</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Yesupadam</surname>
            </name>
            <name>
              <given-names>V.</given-names>
              <surname>Subbaiah</surname>
            </name>
            <name>
              <given-names>S. S. V.</given-names>
              <surname>Desanamukula</surname>
            </name>
            <name>
              <given-names>N. J.</given-names>
              <surname>Bommagani</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/isi.280426</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Enhanced speckle noise reduction in breast cancer ultrasound imagery using a hybrid deep learning model</article-title>
          <source>Ing. Syst. Inf.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <page-range>243-264</page-range>
          <issue>2</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>B.</given-names>
              <surname>Jia</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Dzitac</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Shrestha</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Turdaliev</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Seidaliev</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-981-15-2740-1_17</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>An ensemble machine learning approach to understanding the effect of a global pandemic on Twitter users' attitudes</article-title>
          <source>Int. J. Comput. Commun. Control.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>243-264</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A.</given-names>
              <surname>Bisht</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Singh</surname>
            </name>
            <name>
              <given-names>H. S.</given-names>
              <surname>Bhadauria</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Virmani</surname>
            </name>
            <name>
              <given-names/>
              <surname>Kriti</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1007/978-981-15-2740-1_17</pub-id>
          <article-title>Detection of hate speech and offensive language in Twitter data using LSTM model.</article-title>
          <source>Recent Trends in Image and Signal Processing in Computer Vision</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>932381</page-range>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A.</given-names>
              <surname>Toliyat</surname>
            </name>
            <name>
              <given-names>I.</given-names>
              <surname>Sarah  Levitan</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Peng</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Etemadpour</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/frai.2022.932381</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Asian hate speech detection on Twitter during COVID-19</article-title>
          <source>Front. Artif. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>5056-5067</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>H.</given-names>
              <surname>Herodotou</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Chatzakou</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Kourtellis</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/BigData50022.2020.9377980</pub-id>
          <article-title>A streaming machine learning framework for online aggression detection on Twitter.</article-title>
          <source>2020 IEEE International Conference on Big Data (Big Data), Atlanta, GA, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>95-99</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>O.</given-names>
              <surname>Istaiteh</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Al-Omoush</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Tedmori</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/IDSTA50958.2020.9264052</pub-id>
          <article-title>Racist and sexist hate speech detection: Literature review</article-title>
          <source>2020 International Conference on Intelligent Data Science Technologies and Applications (IDSTA), Valencia, Spain</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>87-92</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S. A.</given-names>
              <surname>Kokatnoor</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Krishnan</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/ICRCICN50933.2020.9296199</pub-id>
          <article-title>Twitter hate speech detection using stacked weighted ensemble (SWE) model</article-title>
          <source>2020 Fifth International Conference on Research in Computational Intelligence and Communication Networks (ICRCICN), Bangalore, India</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>5318-5328</page-range>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Kaya</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Alatas</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>A new hybrid LSTM-RNN deep learning based racism, xenomy, and genderism detection model in online social network</article-title>
          <source>Int. J. Adv. Netw. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>219563-219576</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>F.</given-names>
              <surname>Rodríguez-Sánchez</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Albornoz</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Plaza</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3042604</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Automatic classification of sexism in social networks: An empirical study on Twitter data</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>e0237861</page-range>
          <issue>8</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Mozafari</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Farahbakhsh</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Crespi</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0237861</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Hate speech detection and racial bias mitigation in social media based on BERT model</article-title>
          <source>PloS One</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>11</page-range>
          <issue>3</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>N.</given-names>
              <surname>Pitropakis</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Kokot</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Gkatzia</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Ludwiniak</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Mylonas</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Kandias</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/make2030011</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Monitoring users’ behavior: Anti-immigration speech detection on Twitter</article-title>
          <source>Mach. Learn. Knowl. Extr.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>56-66</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J.</given-names>
              <surname>Peng</surname>
            </name>
            <name>
              <given-names>J. S.</given-names>
              <surname>Fung</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Murtaza</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Rahman</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Walia</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Obande</surname>
            </name>
            <name>
              <given-names>A. R.</given-names>
              <surname>Verma</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.17975/sfj-2022-015</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A sentiment analysis of the Black Lives Matter movement using Twitter</article-title>
          <source>STEM Fellow. J.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>1-28</page-range>
          <issue>4</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Ghosal</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Jain</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3576913</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>HateCircle and unsupervised hate speech detection incorporating emotion and contextual semantics</article-title>
          <source>ACM Trans. Asian Low-Resour. Lang. Inf. Process.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>188</volume>
          <page-range>122252</page-range>
          <issue/>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Ali</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Hassan</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Kifayat</surname>
            </name>
            <name>
              <given-names>J. Y.</given-names>
              <surname>Kim</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Hakak</surname>
            </name>
            <name>
              <given-names>M. K.</given-names>
              <surname>Khan</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.techfore.2022.122252</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Social media content classification and community detection using deep learning and graph analytics</article-title>
          <source>Technol. Forecast. Social Change</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>230</volume>
          <page-range>120564</page-range>
          <issue/>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Agarwal</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Sonawane</surname>
            </name>
            <name>
              <given-names>C. R.</given-names>
              <surname>Chowdary</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2023.120564</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Accelerating automatic hate speech detection using parallelized ensemble learning models</article-title>
          <source>Expert Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>99</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J. H.</given-names>
              <surname>Joloudari</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Hussain</surname>
            </name>
            <name>
              <given-names>M. A.</given-names>
              <surname>Nematollahi</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Bagheri</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Fazl</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Alizadehsani</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Lashgari</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Talukder</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s13278-023-01102-y</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>BERT-deep CNN: State of the art for sentiment analysis of COVID-19 tweets</article-title>
          <source>Social Netw. Anal. Min.</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>37</volume>
          <page-range>2166719</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>H.</given-names>
              <surname>Saleh</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Alhothali</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Moria</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/08839514.2023.2166719</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Detection of hate speech using BERT and hate speech word embedding with deep model</article-title>
          <source>Appl. Artif. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>47</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Nagar</surname>
            </name>
            <name>
              <given-names>F. A.</given-names>
              <surname>Barbhuiya</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Dey</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s13278-023-01051-6</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Towards more robust hate speech detection: Using social context and user data</article-title>
          <source>Soc. Netw. Anal. Min.</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <volume>arXiv:2304.06280</volume>
          <page-range/>
          <issue/>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Y.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Tan</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Feng</surname>
            </name>
            <name>
              <given-names>Q.</given-names>
              <surname>Zheng</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Luo</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>BotMoE: Twitter bot detection with community-aware mixtures of modal-specific experts</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range/>
          <issue/>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>K.</given-names>
              <surname>Mnassri</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Rajapaksha</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Farahbakhsh</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Crespi</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>Hate speech and offensive language detection using an emotion-aware shared encoder</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>1048</page-range>
          <issue>4</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Almaliki</surname>
            </name>
            <name>
              <given-names>A. M.</given-names>
              <surname>Almars</surname>
            </name>
            <name>
              <given-names>I.</given-names>
              <surname>Gad</surname>
            </name>
            <name>
              <given-names>E. S.</given-names>
              <surname>Atlam</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/electronics12041048</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>ABMM: Arabic BERT-Mini model for hate-speech detection on social media</article-title>
          <source>Electronics</source>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>45</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Gite</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Patil</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Dharrao</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Yadav</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Basak</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Rajendran</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Kotecha</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/bdcc7010045</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Textual feature extraction using ant colony optimization for hate speech classification</article-title>
          <source>Big Data Cognitive Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>16801-16811</page-range>
          <issue/>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Fazil</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Khan</surname>
            </name>
            <name>
              <given-names>B. M.</given-names>
              <surname>Albahlal</surname>
            </name>
            <name>
              <given-names>R. M.</given-names>
              <surname>Alotaibi</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Siddiqui</surname>
            </name>
            <name>
              <given-names>M. A.</given-names>
              <surname>Shah</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2023.3246388</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Attentional multi-channel convolution with bidirectional LSTM cell toward hate speech prediction</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range/>
          <issue>1</issue>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <given-names>P.</given-names>
              <surname>Burnap</surname>
            </name>
            <name>
              <given-names>M. L.</given-names>
              <surname>Williams</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1140/epjds/s13688-016-0072-6</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Us and them: Identifying cyber hate on Twitter across multiple protected characteristics</article-title>
          <source>EPJ Data Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="webpage">
          <article-title>Search and find the best twitter hashtags</article-title>
          <source>, https://hashtagify.me/</source>
          <year/>
          <uri/>
        </element-citation>
      </ref>
      <ref id="ref_35">
        <label>35.</label>
        <element-citation publication-type="webpage">
          <article-title>Training data for AI, ML with human empowered automation</article-title>
          <source>, https://www.cogitotech.com/about-us</source>
          <year/>
          <uri/>
        </element-citation>
      </ref>
      <ref id="ref_36">
        <label>36.</label>
        <element-citation publication-type="journal">
          <volume>70</volume>
          <page-range>243-260</page-range>
          <issue/>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>K.</given-names>
              <surname>Srikanth</surname>
            </name>
            <name>
              <given-names>L. K.</given-names>
              <surname>Panwar</surname>
            </name>
            <name>
              <given-names>B. K.</given-names>
              <surname>Panigrahi</surname>
            </name>
            <name>
              <given-names>E.</given-names>
              <surname>Herrera-Viedma</surname>
            </name>
            <name>
              <given-names>A. K.</given-names>
              <surname>Sangaiah</surname>
            </name>
            <name>
              <given-names>G. G.</given-names>
              <surname>Wang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compeleceng.2017.07.023</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Meta-heuristic framework: Quantum inspired binary grey wolf optimizer for unit commitment problem</article-title>
          <source>Comput. Electr. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_37">
        <label>37.</label>
        <element-citation publication-type="journal">
          <volume>388</volume>
          <page-range>114194</page-range>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>W.</given-names>
              <surname>Zhao</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Mirjalili</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.cma.2021.114194</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Artificial hummingbird algorithm: A new bio-inspired optimizer with its engineering applications</article-title>
          <source>Comput. Meth. Appl. Mech. Eng.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>