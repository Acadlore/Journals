<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-ZlSpfx59qC9C0hlRWVgm9mf-hvbMITIe</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml020401</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Augmenting Diabetic Retinopathy Severity Prediction with a Dual-Level Deep Learning Approach Utilizing Customized MobileNet Feature Embeddings</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5185-882X</contrib-id>
          <name>
            <surname>Bodapati</surname>
            <given-names>Jyostna Devi</given-names>
          </name>
          <email>jyostna.bodapati82@gmail.com</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0006-3337-2398</contrib-id>
          <name>
            <surname>Konda</surname>
            <given-names>Rajasekhar</given-names>
          </name>
          <email>rajasekhar.konda@gmail.com</email>
          <xref ref-type="aff" rid="aff_2">2</xref>
        </contrib>
        <aff id="aff_1">Department of Advanced Computer Science and Engineering, Vignan’s Foundation for Science Technology and Research, 522213 Guntur, India</aff>
        <aff id="aff_2">Software Engineering Manager &amp; IEEE Senior Member, 94536 San Francisco, California, USA</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>11</day>
        <month>10</month>
        <year>2023</year>
      </pub-date>
      <volume>2</volume>
      <issue>4</issue>
      <fpage>182</fpage>
      <lpage>193</lpage>
      <page-range>182-193</page-range>
      <history>
        <date date-type="received">
          <day>07</day>
          <month>08</month>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <day>05</day>
          <month>10</month>
          <year>2023</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2023 by the author(s)</copyright-statement>
        <copyright-year>2023</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Diabetic retinopathy, a severe ocular disease correlated with elevated blood glucose levels in diabetic patients, carries a significant risk of visual impairment. The essentiality of its timely and precise severity classification is underscored for effective therapeutic intervention. Deep learning methodologies have been shown to yield encouraging results in the detection and categorisation of severity levels of diabetic retinopathy. This study proposes a dual-level approach, wherein the MobileNetV2 model is modified for a regression task, predicting retinopathy severity levels and subsequently fine-tuned on fundus images. The refined MobileNetV2 model is then utilised for learning feature embeddings, and a Support Vector Machine (SVM) classifier is trained for grading retinopathy severity. Upon implementation, this dual-level approach demonstrated remarkable performance, achieving an accuracy rate of 87% and a kappa value of 93.76% when evaluated on the APTOS19 benchmark dataset. Additionally, the efficacy of data augmentation and the handling of class imbalance issues were explored. These findings suggest that the novel dual-level approach provides an efficient and highly effective solution for the detection and classification of diabetic retinopathy severity levels.</p></abstract>
      <kwd-group>
        <kwd>Color fundus images</kwd>
        <kwd>Diabetic retinopathy (DR)</kwd>
        <kwd>Pre-trained ConvNet (PCN)</kwd>
        <kwd>Deep neural network (DNN)</kwd>
        <kwd>VGG Net</kwd>
        <kwd>Xception</kwd>
        <kwd>InceptionResNetV2 (Inception V4)</kwd>
        <kwd>Machine learning models (ML)</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="2"/>
        <fig-count count="4"/>
        <table-count count="5"/>
        <ref-count count="35"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Diabetic retinopathy (DR), a debilitating ocular condition, inflicts substantial damage on the retina and, if unchecked, can precipitate irreversible vision loss. This condition is a consequence of the body's impaired glucose processing and storage mechanisms, typically associated with diabetes. Elevated blood glucose levels wreak havoc on blood vessels, including those within the retina, instigating a series of pathological stages [<xref ref-type="bibr" rid="ref_1">1</xref>]. As delineated in <xref ref-type="table" rid="table_1">Table 1</xref>, distinct clinical manifestations correspond to various DR phases, initiating with non-proliferative diabetic retinopathy (NPDR). This stage is characterized by blurred or foggy vision resulting from retinal tissue swelling. If unaddressed, NPDR can escalate to proliferative diabetic retinopathy (PDR), marked by the formation of fragile blood vessels that hinder retinal and vitreous circulation [<xref ref-type="bibr" rid="ref_2">2</xref>]. Progression to PDR can evoke complications such as scar tissue formation, glaucoma, and vitreous hemorrhage, each of which can progressively damage the optic nerve and lead to severe vision loss or total blindness [<xref ref-type="bibr" rid="ref_3">3</xref>]. The paramount importance of early detection and intervention in DR is underscored by <xref ref-type="fig" rid="fig_1">Figure 1</xref>, which visually represents various lesion types observable in retinopathy patients.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Clinical signs of diabetic retinopathy at different stages</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center">Severity</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Clinical Signs</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Type</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center">No. DR</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Asymptotic</p></td><td colspan="1" rowspan="1"><p style="text-align: center">-</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center">Mild DR</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Dotted and blotted hemorrhages, Microaneurysms, Cotton wool spots</p></td><td colspan="1" rowspan="3"><p style="text-align: center">Non-Proliferative (NPDR)</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center">Moderate DR</p></td><td colspan="1" rowspan="1"><p style="text-align: center">More pronounced signs of Moderate DR</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center">Severe DR</p></td><td colspan="1" rowspan="1"><p style="text-align: center">notable increase in the severity of hemorrhages, venous beading, and intra-retinal microvascular abnormalities</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center">Proliferate DR</p></td><td colspan="1" rowspan="1"><p style="text-align: center">vitreous hemorrhage, Neovascularization, and retinal detachment</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Proliferative (PDR)</p></td></tr></tbody></table>
        </table-wrap>
      
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Color fundus imaging: Depicting various lesion types in the retinas of patients with retinopathy</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/9/img_IfGfWhsx5N-n2cE2.png"/>
        </fig>
      
      <p>Effective classification and staging of DR severity are vital for preventing vision loss, as proper management can avert over 90% of cases [<xref ref-type="bibr" rid="ref_4">4</xref>]. This is particularly pertinent given the anticipated escalation in global diabetes incidence and prevalence, which is projected to reach epidemic proportions by 2030 [<xref ref-type="bibr" rid="ref_5">5</xref>]. Therefore, the imperative for developing robust diagnostic and intervention strategies for DR is increasingly pressing.</p><p>In the context of this escalating public health challenge, this paper introduces a novel, dual-level deep learning approach to enhance the prediction of DR severity levels. Employing custom MobileNet feature embeddings, the approach promises to provide a highly effective solution for the early detection and classification of DR, thus significantly contributing to the global efforts to combat this debilitating condition.</p><p>Consistent dilated eye examinations are emphatically recommended for individuals with an extensive history of diabetes, primarily to detect early signs of diabetic retinopathy. Implementing preventative strategies, such as meticulously managing blood glucose levels through medication, dietary modifications, and physical activity, abstaining from alcohol and smoking, and controlling hypertension, may decelerate or even mitigate the onset of diabetic retinopathy [<xref ref-type="bibr" rid="ref_6">6</xref>]. The conventional diagnosis of DR primarily involves retinal screening, however, the incorporation of machine learning algorithms for automated diagnosis could potentially serve as a valuable adjunct for ophthalmologists. Comprehensive eye examinations encompass patient history, visual acuity measurements, and the assessment of ocular structures to determine the severity of DR. Additional diagnostic tests may include fluorescein angiography and retinal imaging or tomography. The integration of machine learning algorithms can augment the diagnostic process, serving as a supportive tool for ophthalmologists by facilitating the automated diagnosis of diabetic retinopathy. This could potentially enhance diagnostic precision and improve clinical decision-making processes. The therapeutic approach to DR is customized according to the disease's stage. During the initial stages, vigilant monitoring and efficient regulation of blood sugar levels are of paramount importance. Intraocular injections of medication can mitigate the progression of DR by inhibiting the formation of abnormal blood vessels. In more advanced stages, a series of laser treatments can be employed to obliterate abnormal blood vessels, potentially causing some peripheral vision loss to preserve the more crucial central vision [<xref ref-type="bibr" rid="ref_7">7</xref>].</p><p>The advent of automated methods for screening and diagnosing DR constitutes a promising stride in the realm of medical imaging. These methodologies, leveraging techniques such as deep learning, fuzzy logic, and deep feature extraction, have exhibited impressive proficiencies in identifying DR lesions, including hard exudates, hemorrhages, and microaneurysms [<xref ref-type="bibr" rid="ref_8">8</xref>]. The incorporation of these methodologies into clinical settings harbors the potential to significantly aid healthcare professionals in attaining accurate and prompt diagnoses, ultimately fostering improved patient outcomes. Furthermore, the Sequentially Embedded Surrogate (SeS) method, as proposed by Verma et al., presents a valuable approach for enhancing the efficiency of training deep learning models [<xref ref-type="bibr" rid="ref_9">9</xref>]. This is particularly pertinent in real-world applications where expedient training processes are paramount. By augmenting training efficiency, the SeS method bolsters the feasibility and practicality of deploying AI-equipped systems for DR diagnosis in clinical settings. Collectively, these studies highlight the substantial promise of AI in medical image analysis, specifically within the scope of fundus image analysis [<xref ref-type="bibr" rid="ref_10">10</xref>].</p><p>Automated models, including machine learning approaches, aimed at detecting and classifying diabetic retinopathy (DR), are presently in their nascent stages of development. Additional research is mandated to enhance accuracy and generalizability, and to address inherent limitations such as the necessity for voluminous training datasets and potential data bias [<xref ref-type="bibr" rid="ref_11">11</xref>]. The cost and accessibility of technological equipment capable of capturing high-resolution retinal images may pose implementation challenges in certain regions or amongst specific populations [<xref ref-type="bibr" rid="ref_12">12</xref>]. Despite these obstacles, the incorporation of machine learning techniques in the field of DR detection and classification exhibits exceptional potential [<xref ref-type="bibr" rid="ref_13">13</xref>].</p><p>Current research endeavors are vigorously pursuing an array of techniques and algorithms specifically designed to tackle various facets of the disease. These areas of focus encompass microaneurysm detection, identification of retinal lesions, disease stage classification, and the detection of new vessels in instances of proliferative diabetic retinopathy [<xref ref-type="bibr" rid="ref_14">14</xref>].</p><p>For example, Long et al. [<xref ref-type="bibr" rid="ref_1">1</xref>] proposed a method that involves dynamic thresholding and fuzzy C-means clustering (FCM), subsequent to which a support vector machine (SVM) is deployed for recognition. One limitation of this approach, however, is its dependence on manual parameter tuning, thereby restricting its applicability across diverse datasets [<xref ref-type="bibr" rid="ref_15">15</xref>].</p><p>Similarly, a technique proposed by Haloi et al. [<xref ref-type="bibr" rid="ref_2">2</xref>] leverages mathematical morphology, Gaussian scale space, and support vector machine classification for exudate detection and categorization. Despite this method demonstrating impressive sensitivity and prediction rates, it might be susceptible to image quality and may struggle to demonstrate robust generalizability across new datasets [<xref ref-type="bibr" rid="ref_16">16</xref>].</p><p>The two-step method introduced by Eftekhari et al. [<xref ref-type="bibr" rid="ref_3">3</xref>] presents significant advancements in microaneurysm (MA) detection. However, a potential pitfall of this CNN-based method is the risk of overfitting to the training dataset. Conversely, the method developed by Bodapati [<xref ref-type="bibr" rid="ref_17">17</xref>] aims to augment CNN training efficiency via the selective sampling of misclassified negative samples. A potential trade-off with this strategy, however, is that it may inadvertently extend the overall training time.</p><p>In the context of Srivastava et al. [<xref ref-type="bibr" rid="ref_5">5</xref>], their introduction of novel filters to distinguish red lesions from blood vessels in the detection of microaneurysms and hemorrhages exhibits superior performance compared to existing methodologies. It should be noted, though, that their approach relies on patch-level processing, which may make it vulnerable to variations in image quality [<xref ref-type="bibr" rid="ref_18">18</xref>].</p><p>Haloi et al. [<xref ref-type="bibr" rid="ref_2">2</xref>] introduced an innovative deep neural network model incorporating dropout layers and a maxout activation function, achieving exceptional accuracy in spotting microaneurysms (MAs) [<xref ref-type="bibr" rid="ref_19">19</xref>]. However, the detection of MAs alone may not provide a comprehensive diagnosis of diabetic retinopathy (DR). Several models, including those proposed by Akram et al. [<xref ref-type="bibr" rid="ref_7">7</xref>], and Welikala et al. [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>], have demonstrated encouraging results in discerning the severity of DR [<xref ref-type="bibr" rid="ref_20">20</xref>]. Despite these advances, these models could require manual parameter tuning, may lack broad applicability to new datasets, and carry the risk of overfitting to the training dataset [<xref ref-type="bibr" rid="ref_21">21</xref>]. Addressing these limitations and probing solutions that offer a more holistic and resilient evaluation of disease progression will undoubtedly fortify the effectiveness of DR diagnostic methodologies [<xref ref-type="bibr" rid="ref_22">22</xref>].</p><p>Our proposed method presents an innovative and efficient strategy for predicting the severity levels of diabetic retinopathy (DR), harnessing the capabilities of deep learning techniques. Our approach comprises a series of strategic steps, including fine-tuning, regression model training, and learning embeddings to establish a robust solution. The initial phase involves pre-processing the raw retinal images, followed by their integration into a custom-tailored MobileNetV2 architecture for fine-tuning. A key distinguishing feature of our approach is the adaptation of the MobileNet architecture into a regression model, explicitly designed to predict DR severity levels. Furthermore, the trained model is employed for learning embeddings from the fundus images. To rigorously assess the efficacy of our proposed methodology, comprehensive experiments were conducted using the APTOS19 dataset, which contains retinal images. The results of these experiments strongly endorse the effectiveness of our approach, as it achieves a remarkable accuracy rate of 87% in predicting DR severity levels. This significant level of accuracy underscores the potential of our methodology in facilitating precise predictions and early detection of the disease. By providing such accurate forecasts, our approach has the potential to play a crucial role in the early identification and subsequent treatment of diabetic retinopathy, ultimately leading to improved patient outcomes.</p>
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p>Diabetic retinopathy (DR) presents a significant risk to vision, emphasizing the critical need for early detection and management. This review delves into the evolving realm of automated DR detection methodologies, with the objective to scrutinize, categorize, and critically appraise their merits, limitations, and potential implications for clinical practice. The methodologies examined in the literature can be systematically categorized into distinct subgroups as displayed in <xref ref-type="table" rid="table_2">Table 2</xref>, each of which is analyzed in this section.</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>Sub-categories of the diabetic retinopathy methods explored in the literature</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center">Category</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Reference</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center">Traditional Techniques</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Long et al. [<xref ref-type="bibr" rid="ref_1">1</xref>]</p><p style="text-align: center">Haloi et al. [<xref ref-type="bibr" rid="ref_2">2</xref>]</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center">Deep Learning Paradigms</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Eftekhari et al. [<xref ref-type="bibr" rid="ref_3">3</xref>]</p><p>Bodapati and Balaji [<xref ref-type="bibr" rid="ref_4">4</xref>]</p><p>Zeng et al. [<xref ref-type="bibr" rid="ref_18">18</xref>]</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center">Hybrid Approaches</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Akram et al. [<xref ref-type="bibr" rid="ref_7">7</xref>]</p><p style="text-align: center">Casanova et al. [<xref ref-type="bibr" rid="ref_8">8</xref>]</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center">Microaneurysms and Hemorrhages</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Srivastava et al. [<xref ref-type="bibr" rid="ref_5">5</xref>]</p><p style="text-align: center">Welikala et al. [<xref ref-type="bibr" rid="ref_11">11</xref>]</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center">New Vessels and Proliferative DR (PDR)</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Welikala et al. [<xref ref-type="bibr" rid="ref_10">10</xref>]</p><p style="text-align: center">Welikala et al. [<xref ref-type="bibr" rid="ref_11">11</xref>]</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center">Deep Learning for DR Features</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Zeng et al. [<xref ref-type="bibr" rid="ref_18">18</xref>]</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center">Microaneurysm Detection</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Mateen et al. [<xref ref-type="bibr" rid="ref_19">19</xref>]</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>Traditional Techniques: Some studies have employed conventional image processing methods for DR detection. Long et al. [<xref ref-type="bibr" rid="ref_1">1</xref>] introduced an algorithm that amalgamates dynamic thresholding, fuzzy C-means clustering, and support vector machine (SVM) classification to accurately pinpoint hard exudates (HEs). While these methods demonstrate proficiency under controlled conditions, their susceptibility to fluctuations in lighting and image quality warrants attention. Haloi et al. [<xref ref-type="bibr" rid="ref_2">2</xref>] proposed a method that blends a Gaussian scale space approach with mathematical morphology and SVM classification for exudate detection. This category traces the historical trajectory of traditional methods and their contextual effectiveness.</p><p>Deep Learning Paradigms: A distinct group of studies has embraced deep learning, particularly Convolutional Neural Networks (CNNs), for DR detection [<xref ref-type="bibr" rid="ref_23">23</xref>]. Eftekhari et al. [<xref ref-type="bibr" rid="ref_3">3</xref>] designed a two-step CNN architecture for microaneurysm (MA) detection, explicitly addressing data imbalance to enhance sensitivity compared to prior models. Bodapati and Balaji [<xref ref-type="bibr" rid="ref_4">4</xref>] revolutionized CNN training by implementing selective sampling of misclassified negative samples, which expedited training without sacrificing accuracy. Zeng et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] delved into multiple neural network models—backpropagation, deep neural networks, and CNNs— for the identification of critical DR features. Their exploration underscores the dynamic and evolving nature of deep learning methodologies.</p><p>Hybrid Approaches: Some studies, such as those by Akram et al. [<xref ref-type="bibr" rid="ref_7">7</xref>], have ventured into hybrid models that amalgamate the strengths of diverse methodologies. These approaches adeptly fuse the Gaussian Mixture Model and m-Medoids based modeling with SVM and ensemble classifiers, achieving noteworthy accuracy in detecting MAs and hemorrhages.</p><p>Microaneurysms and Hemorrhages: Initiatives such as those by Srivastava et al. [<xref ref-type="bibr" rid="ref_5">5</xref>] and Welikala et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] delve deeply into microaneurysm and hemorrhage detection. Srivastava et al. [<xref ref-type="bibr" rid="ref_5">5</xref>] introduced innovative filters and utilized Multiple Kernel Learning to enhance feature extraction and machine learning. Welikala et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] strategically integrate vessel segmentation and Random Forests classification for a comprehensive DR analysis.</p><p>New Vessels and Proliferative DR (PDR): Welikala et al. [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>] offer dual classification systems for detecting new vessels, aligning with the clinical aim of diagnosing proliferative DR. These studies establish a critical foundation for tracking disease progression. Welikala et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] presented an automated method for detecting new vessels in retinal images, leveraging two vessel segmentation approaches and a dual classification system. This method proves effective as a screening tool for the early detection of proliferative diabetic retinopathy (PDR), delivering improved sensitivity and specificity compared to traditional line operator approaches [<xref ref-type="bibr" rid="ref_24">24</xref>].</p><p>Welikala et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] proposed an automated method for detecting new vessels in retinal images using a dual classification approach. Their methodology incorporates binary vessel maps and local morphological features to generate 21-D feature vectors. To enhance classification performance, they employ a genetic algorithm-based feature selection approach, achieving high sensitivity and specificity rates. This method demonstrates considerable potential for automated PDR diagnosis.</p><p>Deep Learning for DR Features: Zeng et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] introduced an automated deep learning model for identifying key precursors of diabetic retinopathy (DR) using fundus images. Their exploration included backpropagation neural networks (NN), deep neural networks (DNN), and convolutional neural networks (CNN) [<xref ref-type="bibr" rid="ref_24">24</xref>]. Deep learning models exhibited superior accuracy compared to NN models, effectively quantifying DR features and severity levels [<xref ref-type="bibr" rid="ref_25">25</xref>].</p><p>Microaneurysm Detection: Rahim et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] developed an automatic screening system focused on microaneurysm detection in color fundus images. Their approach blends feature extraction methods, the circular Hough transform, and fuzzy histogram equalization for preprocessing, resulting in enhanced microaneurysm detection during the fundus image preprocessing stage [<xref ref-type="bibr" rid="ref_26">26</xref>].</p><p>In a broader context, traditional techniques provide interpretability and context-specific efficacy, yet they are vulnerable to real-world variations [<xref ref-type="bibr" rid="ref_27">27</xref>]. Conversely, deep learning models excel in processing vast datasets and deciphering intricate patterns, albeit at the cost of significant computational resources [<xref ref-type="bibr" rid="ref_28">28</xref>]. The domain of hybrid models presents potential, but with the caveat of potential complexity and increased resource demands [<xref ref-type="bibr" rid="ref_29">29</xref>]. The synthesis of these diverse methodologies has the potential to transform DR screening and diagnosis, thereby facilitating timely intervention and ultimately improving patient outcomes [<xref ref-type="bibr" rid="ref_30">30</xref>].</p><p>The studies considered in this review exemplify the judicious application of diverse techniques, encompassing fuzzy C-means clustering, Gaussian scale space approaches, convolutional neural networks, and hybrid classifiers [<xref ref-type="bibr" rid="ref_31">31</xref>]. Collectively, these approaches yield high sensitivity, low false positive rates, and state-of-the-art accuracy, effectively addressing the critical need for early detection in publicly available datasets [<xref ref-type="bibr" rid="ref_32">32</xref>]. The potential of these algorithms to significantly enhance DR monitoring and early detection, while extending their applicability to other medical image analysis domains, remains an impressive testament to their innovation and impact [<xref ref-type="bibr" rid="ref_33">33</xref>].</p>
    </sec>
    <sec sec-type="">
      <title>3. Proposed methodology</title>
      <p>The primary aim of this study is to introduce a versatile classification model specifically designed for the identification of diabetic retinopathy severity levels. Given the inherent scarcity of available training data, our proposed methodology is strategically constructed to maximize the utility of the limited dataset at our disposal. In pursuit of our objective, we emphasized the creation of an adept feature representation mechanism for retinal images. Our methodology comprises several key steps, with the relationship between these steps depicted in <xref ref-type="fig" rid="fig_2">Figure 2</xref>.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Workflow of the proposed dual-level deep learning approach</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/9/img_okV13-ASRkjrRito.png"/>
        </fig>
      
      <p>Our proposed approach unfolds through a series of sequential stages, each contributing to the comprehensive framework: Image pre-processing, Data augmentation, Customizing the MobileNetV2 architecture, Fine-tuning the customized model, Learning Retinopathy Image Embeddings, Training a model for classification, and ultimately, model evaluation for disease prediction tasks. The process initiates with the resizing of retinal images, standardizing them to a resolution of 224×224 pixels. To enhance dataset diversity, we judiciously employ data augmentation techniques, ensuring robust model generalization. Notably, the MobileNetV2 architecture undergoes strategic customization. Following the model training phase, the model is readied for feature extraction from fundus images, capitalizing on the knowledge it has garnered. The customized MobileNetV2 model commences its training with the adoption of the pre-trained weights from the original MobileNetV2 model, which was trained on the comprehensive ImageNet dataset. During the fine-tuning process, the convolutional layer weights are frozen, while the fully-connected layers are updated by fine-tuning the model with the DR dataset. The model employs regression to predict the severity value, which is an ordinal variable. The extracted features are then classified using classifiers such as the Support Vector Machine (SVM). Below, we delve into the details of each of the modules used in our proposed work.</p><p>Image Pre-processing: The image pre-processing phase is a critical step where retinal images undergo a series of transformations, setting the stage for subsequent analysis. The initial step in this stage is the uniform resizing of retinal images to a standardized resolution of 224×224 pixels. This resizing is crucial to synchronize the images with the dimensions that the pre-trained MobileNetV2 architecture is designed to handle. As a result, compatibility is assured between the retinal images and the architecture, paving the way for seamless integration [<xref ref-type="bibr" rid="ref_33">33</xref>].</p><p>Data Augmentation: Data augmentation is a technique that amplifies the volume and diversity of data without necessitating additional data collection. This is accomplished by generating different variations of the existing data, thereby effectively increasing the dataset size. The proposed approach employs image processing techniques, such as zooming, cropping, flipping, and rotating, to augment the original images [<xref ref-type="bibr" rid="ref_34">34</xref>]. The augmented images are amalgamated with the initial dataset and used to train the models. This strategy aids in mitigating overfitting and enhancing model generalization. By exposing the model to variations of the training data, it can recognize the same object under different conditions, thereby increasing its robustness to real-world data variations.</p><p>Customizing MobileNetV2: In our proposed approach, MobileNetV2 serves as the fundamental architecture for feature extraction. A key attribute of MobileNetV2 is its employment of depth-wise separable convolutions, a technique which significantly reduces the volume of parameters and computations required for processing [<xref ref-type="bibr" rid="ref_35">35</xref>]. Despite this reduction, the methodology maintains an impressive level of accuracy. The unique combination of competitive accuracy, compact model size, and computational efficiency that MobileNetV2 displays positions it as an ideal candidate for the crucial role of feature extraction. This design choice optimizes resource utilization and contributes to faster inference times, which are vital for real-time applications. Compared to its predecessor, MobileNetV1, MobileNetV2 has exhibited superior performance and capability, solidifying its standing as an effective solution for feature embedding across diverse applications. Its incorporation within our proposed approach for diabetic retinopathy severity classification showcases its capacity to meet the demands of the task while maintaining efficiency and accuracy.</p><p>We perform a strategic customization of the MobileNetV2 architecture to adapt it for efficient feature extraction tasks. Specifically, the final output softmax layer is replaced with a dense layer featuring a sigmoid activation function. This architectural adjustment aligns the model's suitability for regression tasks, enabling it to predict severity grades as continuous values.</p><p>Fine-tuning Modified MobileNetV2: The customized MobileNetV2 model is subjected to fine-tuning to optimize its performance in predicting diabetic retinopathy (DR) severity levels. Before fine-tuning, the DR images are augmented, and the pre-trained weights of the MobileNetV2 architecture – which were originally trained on the extensive ImageNet dataset – are utilized as initial weights for the network. These foundational weights equip the customized model with base knowledge. During the fine-tuning process, the weights of the convolutional layers remain unaltered, while only the fully-connected layers are updated using the specific DR image dataset. This approach stems from the presumption that the convolutional layers of pre-trained networks, having been exposed to extensive datasets, inherently possess effective feature extraction capabilities. The fine-tuning phase is steered by the goal of minimizing the sum of squared loss, a metric that is particularly suitable for regression tasks. The formula for the loss function is defined in Eq. (1).</p>
      
        <disp-formula>
          <label>(1)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mrow data-mjx-texclass="ORD">
              <mi data-mjx-auto-op="false">MSE</mi>
            </mrow>
            <mo>=</mo>
            <mfrac>
              <mn>1</mn>
              <mi>n</mi>
            </mfrac>
            <munderover>
              <mo data-mjx-texclass="OP">∑</mo>
              <mrow data-mjx-texclass="ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>n</mi>
            </munderover>
            <msup>
              <mrow data-mjx-texclass="INNER">
                <mo data-mjx-texclass="OPEN">(</mo>
                <mo>−</mo>
                <mo data-mjx-texclass="CLOSE">)</mo>
                <msub>
                  <mi>Y</mi>
                  <mi>i</mi>
                </msub>
                <msub>
                  <mrow data-mjx-texclass="ORD">
                    <mover>
                      <mi>Y</mi>
                      <mo stretchy="false">^</mo>
                    </mover>
                  </mrow>
                  <mi>i</mi>
                </msub>
              </mrow>
              <mn>2</mn>
            </msup>
          </math>
        </disp-formula>
      
      <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>Y</mi>
      <mi>i</mi>
    </msub>
  </math>
</inline-formula> represents the target value to be predicted and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mover>
          <mi>Y</mi>
          <mo stretchy="false">^</mo>
        </mover>
      </mrow>
      <mi>i</mi>
    </msub>
  </math>
</inline-formula> denotes the prediction generated by the modified MobileNetV2 architecture. The fine-tuning process aims to adjust the network's parameters to minimize the deviation between the predicted and actual target values.</p><p>Learning Retinopathy Image Embeddings: In our proposed approach, the feature extraction process takes place after the fine-tuning of the Modified MobileNetV2 network. The pre-trained MobileNetV2 model, specifically fine-tuned for the task of DR severity classification, forms the foundation for this pivotal phase of feature extraction. As part of this process, fundus images are fed into the input layer of the network, commencing the extraction of the most pertinent features from the convolutional layers inherent to the MobileNetV2 architecture. For each input image, a total of 512 features are extracted, sourced from the outputs of the first and second fully connected layers (256 features from fc1 and 256 features from fc2) within the network. These specific layers have demonstrated proficiency in capturing and encapsulating essential patterns and information learned by the network during the fine-tuning process. The dimensions of the input remain constant, ensuring a consistent and uniform feature extraction process across a diverse range of images. By extracting 512 features from each image, a compact and efficient representation of the fundus image is attained. These features serve as a condensed representation of the image, capturing the most relevant information for DR severity classification.</p><p>MobileNet Features: MobileNetV2 is chosen for feature extraction in our proposed approach due to its competitive accuracy, smaller model size, and computational efficiency. It is specifically designed for mobile and embedded devices, which makes it suitable for efficient computation in environments with limited resources. MobileNetV2 employs depth-wise separable convolutions to reduce parameters and computations while preserving accuracy. Its architecture incorporates linear bottleneck blocks with inverted residuals and shortcut connections to enhance nonlinearity and gradient flow. With its flexibility in width and resolution parameters, MobileNetV2 provides improved performance over its predecessor, MobileNetV1, making it a favoured choice for feature embedding in various applications.</p><p>Disease Prediction Module: In the disease prediction module, the extracted features are processed and classified using classifiers such as Support Vector Machines (SVM). SVMs are widely used for disease prediction in medical imaging due to their versatility and ability to handle both linearly and non-linearly separable data. They aim to find a hyperplane that maximally separates data points in an N-dimensional space. For linearly separable data, a linear SVM can be used, while for non-linearly separable data, a non-linear SVM with a kernel function is employed to transform the data into a higher-dimensional space. Common kernel functions include linear, sigmoid, radial basis function (RBF), and polynomial kernels.</p><p>Common kernels within the context of SVM include the following in <xref ref-type="table" rid="table_3">Table 3</xref>:</p>
      
        <table-wrap id="table_3">
          <label>Table 3</label>
          <caption>
            <title>Types of popular kernels suitable with support vector machines </title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Kernel Name</p></td><td colspan="1" rowspan="1"><p>Mathematical Expression </p></td><td colspan="1" rowspan="1"><p>Parameters </p></td></tr><tr><td colspan="1" rowspan="1"><p>Linear kernel</p></td><td colspan="1" rowspan="1"><p><math>
  <mrow data-mjx-texclass="ORD">
    <mi>K</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi>x</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi>y</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi>y</mi>
  </mrow>
  <mo stretchy="false">(</mo>
  <mo>,</mo>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi>x</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mrow data-mjx-texclass="ORD">
        <mi>T</mi>
      </mrow>
    </mrow>
  </msup>
</math></p></td><td colspan="1" rowspan="1"><p>None</p></td></tr><tr><td colspan="1" rowspan="1"><p>Polynomial kernel</p></td><td colspan="1" rowspan="1"><p><math>
  <mrow data-mjx-texclass="ORD">
    <mi>K</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi>x</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi>y</mi>
  </mrow>
  <mo stretchy="false">(</mo>
  <mo>,</mo>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <msup>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">(</mo>
      <mo>+</mo>
      <mo data-mjx-texclass="CLOSE">)</mo>
      <msup>
        <mrow data-mjx-texclass="ORD">
          <mi>x</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mrow data-mjx-texclass="ORD">
            <mi>T</mi>
          </mrow>
        </mrow>
      </msup>
      <mrow data-mjx-texclass="ORD">
        <mi>y</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mi>c</mi>
      </mrow>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mrow data-mjx-texclass="ORD">
        <mi>d</mi>
      </mrow>
    </mrow>
  </msup>
</math></p></td><td colspan="1" rowspan="1"><p>c, d</p></td></tr><tr><td colspan="1" rowspan="1"><p>Radial basis function (RBF) kernel</p></td><td colspan="1" rowspan="1"><p><math>
  <mrow data-mjx-texclass="ORD">
    <mi>K</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi>x</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi>y</mi>
  </mrow>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">(</mo>
    <mo>−</mo>
    <mo data-mjx-texclass="CLOSE" fence="true" stretchy="true" symmetric="true"/>
  </mrow>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN" fence="true" stretchy="true" symmetric="true"/>
    <mo>∗</mo>
    <mo data-mjx-texclass="ORD" fence="false" stretchy="false">‖</mo>
    <mo>−</mo>
    <mo data-mjx-texclass="CLOSE">)</mo>
    <mrow data-mjx-texclass="ORD">
      <mi>x</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>y</mi>
    </mrow>
    <msup>
      <mo data-mjx-texclass="ORD" fence="false" stretchy="false">‖</mo>
      <mn>2</mn>
    </msup>
  </mrow>
  <mo stretchy="false">(</mo>
  <mo>,</mo>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mo data-mjx-texclass="NONE">⁡</mo>
  <mi>exp</mi>
  <mi>g</mi>
  <mi>a</mi>
  <mi>m</mi>
  <mi>m</mi>
  <mi>a</mi>
</math></p></td><td colspan="1" rowspan="1"><p>gamma</p></td></tr><tr><td colspan="1" rowspan="1"><p>Sigmoid kernel</p></td><td colspan="1" rowspan="1"><p><math>
  <mrow data-mjx-texclass="ORD">
    <mi>K</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi>x</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi>y</mi>
  </mrow>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">(</mo>
    <mo>∗</mo>
    <mo>+</mo>
    <mo data-mjx-texclass="CLOSE">)</mo>
    <mrow data-mjx-texclass="ORD">
      <mi>g</mi>
      <mi>a</mi>
      <mi>m</mi>
      <mi>m</mi>
      <mi>a</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>y</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>r</mi>
    </mrow>
    <msup>
      <mrow data-mjx-texclass="ORD">
        <mi>x</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mrow data-mjx-texclass="ORD">
          <mi>T</mi>
        </mrow>
      </mrow>
    </msup>
  </mrow>
  <mo stretchy="false">(</mo>
  <mo>,</mo>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mo data-mjx-texclass="NONE">⁡</mo>
  <mi>tanh</mi>
</math></p></td><td colspan="1" rowspan="1"><p>Gamma, r</p></td></tr><tr><td colspan="1" rowspan="1"><p>Laplacian kernel</p></td><td colspan="1" rowspan="1"><p><math>
  <mrow data-mjx-texclass="ORD">
    <mi>K</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi>x</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi>y</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi>x</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi>y</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mo>/</mo>
  </mrow>
  <mo stretchy="false">(</mo>
  <mo>,</mo>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mo data-mjx-texclass="NONE">⁡</mo>
  <mo stretchy="false">(</mo>
  <mo>−</mo>
  <mo data-mjx-texclass="ORD" fence="false" stretchy="false">‖</mo>
  <mo>−</mo>
  <mo data-mjx-texclass="ORD" fence="false" stretchy="false">‖</mo>
  <mo stretchy="false">)</mo>
  <mi>exp</mi>
  <mi>σ</mi>
</math></p></td><td colspan="1" rowspan="1"><p><math>
  <mi>σ</mi>
</math></p></td></tr></tbody></table>
        </table-wrap>
      
      <p>Indeed, kernel selection hinges on the unique features of the specific problem and the inherent characteristics of the data. The Radial Basis Function (RBF) kernel is particularly esteemed for its proficiency in manoeuvring complex patterns and nonlinear data. However, the pivotal aspect lies in the realm of experimentation, which aids in identifying the most suitable kernel and corresponding parameters tailored to each distinct problem scenario.</p>
    </sec>
    <sec sec-type="">
      <title>4. Experimental results</title>
      <p>Our experimental investigations were designed to evaluate the effectiveness of our proposed method for detecting and grading Diabetic Retinopathy (DR). This section provides a synopsis of the dataset utilized, the evaluation metrics, and the comparative studies conducted.</p>
      <p>Dataset Overview: The dataset is composed of retinal images captured through fundus photography, obtained from the APTOS 2019 Blindness Detection Challenge hosted on Kaggle. These images were meticulously graded on a scale of 0 to 4, each grade reflecting a different level of DR severity. The dataset encompasses images sourced from a variety of clinics, captured under diverse imaging conditions, and utilizing a range of scanners over an extensive timeframe. The distribution of severity levels within the APTOS2019 dataset is as follows: No DR – 180 samples, Mild DR: 370 samples, Moderate DR: 999 samples, Severe DR: 193 samples, Proliferative DR: 295 samples, with a total of 3662 samples. <xref ref-type="fig" rid="fig_3">Figure 3</xref> shows sample images from the APTOS 2019 dataset.</p>
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>Sample dataset images from APTOS 2019 diabetic retinopathy dataset</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/9/img_T80F4Wq3H9AROsKu.png"/>
        </fig>
      
      <p>Evaluation Metrics: Our experimental analysis employs crucial evaluation metrics including Accuracy, Precision, Recall, and F1 Score, to conduct an in-depth assessment of the performance of our proposed method for diabetic retinopathy (DR) severity prediction.</p><p>Accuracy serves as a measure of the proportion of correctly classified instances amongst the total number of samples. While it offers an overarching perspective of the model's correctness, its validity might be compromised in scenarios where class imbalance is present.</p><p>Precision measures the proportion of correctly predicted positive instances out of the total instances predicted as positive. In our context, it gauges the aptitude of the model in accurately predicting a specific DR severity level when it asserts that level.</p><p>Recall, also known as sensitivity or true positive rate, quantifies the ratio of correctly predicted positive instances to the total number of genuine positive instances. It signifies the model's capacity to identify all occurrences of a particular severity level.</p><p>The F1 Score represents the harmonic mean of Precision and Recall. It provides a balanced measure that takes into account both false positives and false negatives, making it an invaluable metric for evaluating classification models in circumstances where class imbalance or unequal misclassification costs are present.</p><p>The Kappa Statistic accounts for the potential of agreement occurring by chance. It quantifies the concurrence between predicted and expected classifications beyond what can be achieved by random chance. This statistic, with its range from -1 to 1, assists in understanding how well the model's predictions correspond with the true classes, considering the possibility of random agreement. The formula for calculating the Kappa Statistic is: Kappa = (Accuracy - Expected Accuracy) / (1 - Expected Accuracy).</p><p>Collectively, these metrics offer comprehensive insights into the efficacy of the model in accurately identifying and predicting the presence or absence of the disease.</p><p>Experimental Setup: For the purpose of our experimentation, we utilized a high-end GPU system equipped with an Intel Xenon processor, a 2 TB hard disk drive, and 32 GB of DDR4 RAM. The GPU in use is the NVIDIA GeForce GTX with a memory capacity of 16 GB. All experiments were conducted using Python 3.6.6, with TensorFlow 1.14 and Keras 2.3 serving as the backbone for building and training the neural network models.</p><p>Experimental Results: Our experimental exploration comprised two distinct phases, aimed at evaluating the performance of various machine learning models in predicting the severity of Diabetic Retinopathy (DR). Initially, the focus was on training a spectrum of traditional machine learning models using various feature embeddings. These embeddings encompassed representations from diverse pre-trained deep models as well as raw pixel values. In the subsequent phase, we employed renowned traditional machine learning models, including Support Vector Machines (SVM), Multi-Layer Perceptron (MLP), AdaBoost, K-Nearest Neighbors (KNN), and Random Forest classifiers. These models were harnessed to predict the severity of diabetic retinopathy.</p><p><xref ref-type="table" rid="table_4">Table 4</xref> elucidates the performance of various traditional machine learning models such as SVM, MLP, ADABOOST, KNN, and Random Forest classifiers, utilizing different types of feature embeddings for the task of diabetic retinopathy Severity Prediction. The embeddings employed include Raw Pixel Values, ResNet101, DenseNet169, VGG-19, VGG-16, and MobileNetV2.</p><p>From the derived results, it is unequivocally apparent that MobileNet embeddings outshine other embeddings, demonstrating superior performance across all classifiers. This suggests that MobileNet is particularly well-suited for the specific task of Diabetic Retinopathy Severity Prediction. Moreover, the SVM classifier transcends all other classifiers for this task, regardless of the type of embedding used, indicating SVM as a fitting choice of classifier for this task.</p><p>In examining the individual performances of embeddings, both DenseNet169 and VGG-16 exhibited commendable results across most classifiers. However, the ADABOOST and KNN classifiers didn't deliver satisfactory performances with these embeddings. This underscores the importance of classifier selection when working with different types of embeddings.</p>
      
        <table-wrap id="table_4">
          <label>Table 4</label>
          <caption>
            <title>Performance of traditional ML models with different embeddings for diabetic retinopathy severity prediction</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Embedding Type</p></td><td colspan="1" rowspan="1"><p>Classifier</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>Precision</p></td><td colspan="1" rowspan="1"><p>F1-Score</p></td><td colspan="1" rowspan="1"><p>Recall</p></td></tr><tr><td colspan="1" rowspan="5"><p>Raw Pixel Values</p></td><td colspan="1" rowspan="1"><p>SVM</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>61</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>MLP</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>ADABOOST</p></td><td colspan="1" rowspan="1"><p>74</p></td><td colspan="1" rowspan="1"><p>65</p></td><td colspan="1" rowspan="1"><p>68</p></td><td colspan="1" rowspan="1"><p>74</p></td></tr><tr><td colspan="1" rowspan="1"><p>KNN</p></td><td colspan="1" rowspan="1"><p>72</p></td><td colspan="1" rowspan="1"><p>67</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>72</p></td></tr><tr><td colspan="1" rowspan="1"><p>Random Forest</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>73</p></td><td colspan="1" rowspan="1"><p>70</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="5"><p>ResNet101</p></td><td colspan="1" rowspan="1"><p>SVM</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>MLP</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>78</p></td><td colspan="1" rowspan="1"><p>62</p></td></tr><tr><td colspan="1" rowspan="1"><p>ADABOOST</p></td><td colspan="1" rowspan="1"><p>77</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>71</p></td><td colspan="1" rowspan="1"><p>76</p></td></tr><tr><td colspan="1" rowspan="1"><p>KNN</p></td><td colspan="1" rowspan="1"><p>72</p></td><td colspan="1" rowspan="1"><p>66</p></td><td colspan="1" rowspan="1"><p>68</p></td><td colspan="1" rowspan="1"><p>72</p></td></tr><tr><td colspan="1" rowspan="1"><p>Random Forest</p></td><td colspan="1" rowspan="1"><p>77</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>71</p></td><td colspan="1" rowspan="1"><p>76</p></td></tr><tr><td colspan="1" rowspan="5"><p>DenseNet169</p></td><td colspan="1" rowspan="1"><p>SVM</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>MLP</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>ADABOOST</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>75</p></td><td colspan="1" rowspan="1"><p>73</p></td><td colspan="1" rowspan="1"><p>79</p></td></tr><tr><td colspan="1" rowspan="1"><p>KNN</p></td><td colspan="1" rowspan="1"><p>73</p></td><td colspan="1" rowspan="1"><p>68</p></td><td colspan="1" rowspan="1"><p>70</p></td><td colspan="1" rowspan="1"><p>73</p></td></tr><tr><td colspan="1" rowspan="1"><p>Random Forest</p></td><td colspan="1" rowspan="1"><p>78</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>70</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="5"><p>VGG-19</p></td><td colspan="1" rowspan="1"><p>SVM</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>MLP</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>ADABOOST</p></td><td colspan="1" rowspan="1"><p>74</p></td><td colspan="1" rowspan="1"><p>63</p></td><td colspan="1" rowspan="1"><p>67</p></td><td colspan="1" rowspan="1"><p>74</p></td></tr><tr><td colspan="1" rowspan="1"><p>KNN</p></td><td colspan="1" rowspan="1"><p>65</p></td><td colspan="1" rowspan="1"><p>63</p></td><td colspan="1" rowspan="1"><p>64</p></td><td colspan="1" rowspan="1"><p>66</p></td></tr><tr><td colspan="1" rowspan="1"><p>Random Forest</p></td><td colspan="1" rowspan="1"><p>77</p></td><td colspan="1" rowspan="1"><p>61</p></td><td colspan="1" rowspan="1"><p>68</p></td><td colspan="1" rowspan="1"><p>76</p></td></tr><tr><td colspan="1" rowspan="5"><p>VGG-16</p></td><td colspan="1" rowspan="1"><p>SVM</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>MLP</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>ADABOOST</p></td><td colspan="1" rowspan="1"><p>77</p></td><td colspan="1" rowspan="1"><p>70</p></td><td colspan="1" rowspan="1"><p>72</p></td><td colspan="1" rowspan="1"><p>76</p></td></tr><tr><td colspan="1" rowspan="1"><p>KNN</p></td><td colspan="1" rowspan="1"><p>71</p></td><td colspan="1" rowspan="1"><p>66</p></td><td colspan="1" rowspan="1"><p>68</p></td><td colspan="1" rowspan="1"><p>70</p></td></tr><tr><td colspan="1" rowspan="1"><p>Random Forest</p></td><td colspan="1" rowspan="1"><p>77</p></td><td colspan="1" rowspan="1"><p>70</p></td><td colspan="1" rowspan="1"><p>72</p></td><td colspan="1" rowspan="1"><p>76</p></td></tr><tr><td colspan="1" rowspan="5"><p>MobileNetV2</p></td><td colspan="1" rowspan="1"><p>SVM</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>MLP</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>ADABOOST</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>73</p></td><td colspan="1" rowspan="1"><p>72</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>KNN</p></td><td colspan="1" rowspan="1"><p>70</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>66</p></td><td colspan="1" rowspan="1"><p>70</p></td></tr><tr><td colspan="1" rowspan="1"><p>Random Forest</p></td><td colspan="1" rowspan="1"><p>77</p></td><td colspan="1" rowspan="1"><p>67</p></td><td colspan="1" rowspan="1"><p>70</p></td><td colspan="1" rowspan="1"><p>76</p></td></tr></tbody></table>
        </table-wrap>
      
      <p><xref ref-type="table" rid="table_5">Table 5</xref> reveals that the combination of MobileNet embeddings with the SVM classifier delivers an accuracy of 79%, precision of 62%, recall of 78%, and an F1-score of 69%. This combination emerges as the most effective for diabetic retinopathy Severity Prediction. These findings furnish crucial insights into the potential applicability of our proposed method for practical clinical implementations, particularly in the field of diabetic retinopathy severity evaluation. A Grid Search approach was employed during the experiments to ascertain optimal values for the hyperparameters, such as C, kernel type, and gamma values (<xref ref-type="fig" rid="fig_4">Figure 4</xref>).</p>
      
        <table-wrap id="table_5">
          <label>Table 5</label>
          <caption>
            <title>Performance comparison of SVM when different pre-trained embeddings are used to represent diabetic retinopathy images</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>Precision</p></td><td colspan="1" rowspan="1"><p>F1-Score</p></td><td colspan="1" rowspan="1"><p>Recall</p></td></tr><tr><td colspan="1" rowspan="1"><p>ResNet+SVM</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>DenseNet+SVM</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>VGG-19+SVM</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>VGG-16+SVM</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>69</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>MobileNetV2+SVM</p></td><td colspan="1" rowspan="1"><p>87</p></td><td colspan="1" rowspan="1"><p>79</p></td><td colspan="1" rowspan="1"><p>73</p></td><td colspan="1" rowspan="1"><p>70</p></td></tr></tbody></table>
        </table-wrap>
      
      
        <fig id="fig_4">
          <label>Figure 4</label>
          <caption>
            <title>Mean test scores of the model with respect to Gamma and C while performing the grid search</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/9/img_ylpM4qM3_vBzU59r.png"/>
        </fig>
      
      <p>It is noteworthy that traditional machine learning models, specifically SVM, consistently outperform the other classifiers used in this study, which include MLP, AdaBoost, KNN, and Random Forest. This result underscores the ongoing viability and competitiveness of traditional machine learning models in predicting diabetic retinopathy severity. Broadly, our study exhibits the potential of synergizing deep learning models with traditional machine learning models for the prediction of diabetic retinopathy severity. Among these, the combination of MobileNetV2 embeddings with SVM emerges as the most potent approach, showcasing its potential to bolster the precision and effectiveness of diabetic retinopathy severity prediction. This amalgamated insight positions our research at the vanguard of advancements in both the realms of deep learning and traditional machine learning for clinical applications, particularly in diabetic retinopathy assessment.</p>
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>This research endeavors to construct an automated diabetic retinopathy (DR) severity level prediction model utilizing retinal images. The proposed model harmoniously integrates an SVM classifier and deep feature extraction techniques to categorize images into varying severity levels based on the extracted features. The study offers a promising pathway towards automating DR screening, thereby alleviating the workload of ophthalmologists. Nevertheless, the model's efficacy could be further amplified by incorporating additional deep learning techniques and larger datasets. Experimental results corroborate that the amalgamation of the MobileNetV2 model and SVM classifier demonstrates superior performance relative to other models. Our proposed model, evaluated on the APTOS2019 dataset comprised of 3297 retinal images, achieves an admirable accuracy of 87%. Fine-tuning MobileNetV2 as a regression model prior to learning embeddings enhances performance, while the introduction of data augmentation and class imbalance management techniques further bolsters the accuracy of DR severity level classification. The proposed methodology holds considerable promise to expedite early DR detection and management, consequently reducing the risk of vision loss in diabetic patients.</p><p>In future research pursuits, addressing challenges associated with data imbalance, augmenting the model's robustness in real-world contexts, and emphasizing transparency in the decision-making processes inherent to automated DR detection systems should be prioritized. These efforts will significantly contribute to the evolution of accurate and ethically responsible automated DR assessment technologies.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p><span>The data used to support the research findings are available from the corresponding author upon request.</span></p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>2019</volume>
          <page-range>1-10</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Long</surname>
              <given-names>S. C.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>X. X.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Z. Q.</given-names>
            </name>
            <name>
              <surname>Pardhan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>D. C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2019/3926930</pub-id>
          <article-title>Automatic detection of hard exudates in color retinal images using dynamic threshold and SVM classification: Algorithm development and evaluation</article-title>
          <source>BioMed Res. Int.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Haloi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Dandapat</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Sinha</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1505.00737</pub-id>
          <article-title>A gaussian scale space approach for exudates detection, classification and severity prediction</article-title>
          <source>arXiv preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>18</volume>
          <page-range>67</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Eftekhari</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Pourreza</surname>
              <given-names>H. R.</given-names>
            </name>
            <name>
              <surname>Masoudi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Ghiasi-Shirazi</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Saeedi</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s12938-019-0675-9</pub-id>
          <article-title>Microaneurysm detection in fundus images using a two-step convolutional neural network</article-title>
          <source>BioMed. Eng. OnLine</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>2023</volume>
          <page-range>1-19</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bodapati</surname>
              <given-names>Jyostna Devi</given-names>
            </name>
            <name>
              <surname>Balaji</surname>
              <given-names>Bharadwaj Bagepalli</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-023-15557-w</pub-id>
          <article-title>Tumor AwareNet : Deep representation learning with attention based sparse convolutional denoising autoencoder for brain tumor recognition</article-title>
          <source>Multimed. Tools Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>138</volume>
          <page-range>83-91</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Srivastava</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Duan</surname>
              <given-names>L. X.</given-names>
            </name>
            <name>
              <surname>Wong</surname>
              <given-names>D. W. K.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wong</surname>
              <given-names>T. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.cmpb.2016.10.017</pub-id>
          <article-title>Detecting retinal microaneurysms and hemorrhages with robustness to the presence of blood vessels</article-title>
          <source>Comput. Methods Programs Biomed.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>4</volume>
          <page-range>290-294</page-range>
          <issue>6</issue>
          <year>2013</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Fernandez-Loaiza</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Sauma</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hernandez-Bogantes</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Masis</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.4239/wjd.v4.i6.290</pub-id>
          <article-title>Classification of diabetic retinopathy and diabetic macular edema</article-title>
          <source>World J. Diabetes</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>45</volume>
          <page-range>161-171</page-range>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Akram</surname>
              <given-names>M. U.</given-names>
            </name>
            <name>
              <surname>Khalid</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Tariq</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>S. A.</given-names>
            </name>
            <name>
              <surname>Azam</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compbiomed.2013.11.014</pub-id>
          <article-title>Detection and classification of retinal lesions for grading of diabetic retinopathy</article-title>
          <source>Comput. Biol. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>e98587</page-range>
          <issue>6</issue>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Casanova</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Saldana</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Chew</surname>
              <given-names>E. Y.</given-names>
            </name>
            <name>
              <surname>Danis</surname>
              <given-names>R. P.</given-names>
            </name>
            <name>
              <surname>Greven</surname>
              <given-names>C. M.</given-names>
            </name>
            <name>
              <surname>Ambrosius</surname>
              <given-names>W. T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0098587</pub-id>
          <article-title>Application of random forests methods to diabetic retinopathy classification analyses</article-title>
          <source>PLoS One</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-6</page-range>
          <year>2011</year>
          <person-group person-group-type="author">
            <name>
              <given-names>K.</given-names>
              <surname>Verma</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Deep</surname>
            </name>
            <name>
              <given-names>A. G.</given-names>
              <surname>Ramakrishnan</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/INDCON.2011.6139346</pub-id>
          <article-title>Detection and classification of diabetic retinopahy using retinal images</article-title>
          <source>2011 Annual IEEE India Conference, Hyderabad, India</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>114</volume>
          <page-range>247-261</page-range>
          <issue>3</issue>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Welikala</surname>
              <given-names>R. A.</given-names>
            </name>
            <name>
              <surname>Dehmeshki</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hoppe</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Tah</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Mann</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Williamson</surname>
              <given-names>T. H.</given-names>
            </name>
            <name>
              <surname>Barman</surname>
              <given-names>S. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.cmpb.2014.02.010</pub-id>
          <article-title>Automated detection of proliferative diabetic retinopathy using a modified line operator and dual classification</article-title>
          <source>Comput. Methods Programs Biomed.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>43</volume>
          <page-range>64-77</page-range>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Welikala</surname>
              <given-names>R. A.</given-names>
            </name>
            <name>
              <surname>Fraz</surname>
              <given-names>M. M.</given-names>
            </name>
            <name>
              <surname>Dehmeshki</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hoppe</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Tah</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Mann</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Williamson</surname>
              <given-names>T. H.</given-names>
            </name>
            <name>
              <surname>Barman</surname>
              <given-names>S. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compmedimag.2015.03.003</pub-id>
          <article-title>Genetic algorithm based feature selection combined with dual classification for the automated detection of proliferative diabetic retinopathy</article-title>
          <source>Comput. Med. Imaging Graph.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>18</volume>
          <page-range>1717-1728</page-range>
          <issue>5</issue>
          <year>2013</year>
          <person-group person-group-type="author">
            <name>
              <surname>Roy chowdhury</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Koozekanani</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Parhi</surname>
              <given-names>K. K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/JBHI.2013.2294635</pub-id>
          <article-title>Dream: Diabetic retinopathy analysis using machine learning</article-title>
          <source>IEEE J. Biomed. Health Inform.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>6</page-range>
          <issue>1</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Porter</surname>
              <given-names>L. F.</given-names>
            </name>
            <name>
              <surname>Saptarshi</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Fang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Rathi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Den Hollander</surname>
              <given-names>A. I.</given-names>
            </name>
            <name>
              <surname>De Jong</surname>
              <given-names>E. K.</given-names>
            </name>
            <name>
              <surname>Clark</surname>
              <given-names>S. J.</given-names>
            </name>
            <name>
              <surname>Bishop</surname>
              <given-names>P. N.</given-names>
            </name>
            <name>
              <surname>Olsen</surname>
              <given-names>T. W.</given-names>
            </name>
            <name>
              <surname>Liloglou</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Chavali</surname>
              <given-names>V. R. M.</given-names>
            </name>
            <name>
              <surname>Paraoan</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s13148-019-0608-2</pub-id>
          <article-title>Whole-genome methylation profiling of the retinal pigment epithelium of individuals with age-related macular degeneration reveals differential methylation of the SKI, GTF2H4, and TNXB genes</article-title>
          <source>Clin. Epigenetics</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>27</volume>
          <page-range>1149-1164</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Rahim</surname>
              <given-names>S. S.</given-names>
            </name>
            <name>
              <surname>Jayne</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Palade</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Shuttle worth</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00521-015-1929-5</pub-id>
          <article-title>Automatic detection of microaneurysms in colour fundus images for diabetic retinopathy screening</article-title>
          <source>Neural Comput. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>89-106</page-range>
          <issue>1</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Dutta</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Manideep</surname>
              <given-names>B. C. S.</given-names>
            </name>
            <name>
              <surname>Basha</surname>
              <given-names>S. M.</given-names>
            </name>
            <name>
              <surname>Caytiles</surname>
              <given-names>R. D.</given-names>
            </name>
            <name>
              <surname>Iyenger</surname>
              <given-names>N. C. S. N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.14257/ijgdc.2018.11.1.09</pub-id>
          <article-title>Classification of diabetic retinopathy images by using deep learning models</article-title>
          <source>Int. J. Grid Distrib. Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="conf-paper">
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>V.</given-names>
              <surname>Kakani</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Varun</surname>
            </name>
            <name>
              <given-names>J. D.</given-names>
              <surname>Bodapati</surname>
            </name>
            <name>
              <given-names>K. R.</given-names>
              <surname>Sekhar</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/I2CT57861.2023.10126288</pub-id>
          <article-title>Post-COVID chest disease monitoring using self-adaptive convolutional neural network</article-title>
          <source>2023 IEEE 8th International Conference for Convergence in Technology (I2CT), Lonavla, India</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>1453-1459</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bodapati</surname>
              <given-names>J. D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11760-021-01877-7</pub-id>
          <article-title>SAE-PD-Seq: Sequence autoencoder-based pre-training of decoder for sequence learning tasks</article-title>
          <source>Signal Image Video Process.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>30744-30753</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zeng</surname>
              <given-names>Xiang Long</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Hai Quan</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>Yuan</given-names>
            </name>
            <name>
              <surname>Ye</surname>
              <given-names>Wen Bin</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2019.2903171</pub-id>
          <article-title>Automated diabetic retinopathy detection based on binocular Siamese-like convolutional neural network</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>1</page-range>
          <issue>1</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mateen</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Wen</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Nasrullah</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Z. P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/sym11010001</pub-id>
          <article-title>Fundus image classification using VGG-19 architecture with PCA and SVD</article-title>
          <source>Symmetry</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <article-title>Very deep convolutional networks for large-scale image recognition</article-title>
          <source>arXiv preprint</source>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Simonyan</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zisserman</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.1409.1556</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1251-1258</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <given-names>François</given-names>
              <surname>Chollet</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CVPR.2017.195</pub-id>
          <article-title>Xception: Deep learning with depth wise separable convolutions</article-title>
          <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>2023</volume>
          <page-range>1-20</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bodapati</surname>
              <given-names>Jyostna Devi</given-names>
            </name>
            <name>
              <surname>Bagepalli Balaji</surname>
              <given-names>Bharadwaj</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-023-15120-7</pub-id>
          <article-title>Self-adaptive stacking ensemble approach with attention based deep neural network models for diabetic retinopathy severity prediction</article-title>
          <source>Multimed. Tools Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="conf-paper">
          <page-range>8697-8710</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Barret</given-names>
              <surname>Zoph</surname>
            </name>
            <name>
              <given-names>Vijay</given-names>
              <surname>Vasudevan</surname>
            </name>
            <name>
              <given-names>Jonathon</given-names>
              <surname>Shlens</surname>
            </name>
            <name>
              <given-names>Quoc V.</given-names>
              <surname>Le</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CVPR.2018.00907</pub-id>
          <article-title>Learning transferable architectures for scalable image recognition</article-title>
          <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, UT, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>104</volume>
          <page-range>569-578</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bodapati</surname>
              <given-names>Jyostna Devi</given-names>
            </name>
            <name>
              <surname>Sajja</surname>
              <given-names>RamaKrishna</given-names>
            </name>
            <name>
              <surname>Naralasetti</surname>
              <given-names>Veeranjaneyulu</given-names>
            </name>
          </person-group>
          <article-title>An efficient approach for semantic segmentation of salt domes in seismic images using improved UNET architecture</article-title>
          <source>J. Inst. Eng. India Ser. B</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>24</volume>
          <page-range>971-987</page-range>
          <issue>7</issue>
          <year>2002</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ojala</surname>
              <given-names>Timo</given-names>
            </name>
            <name>
              <surname>Pietikainen</surname>
              <given-names>Matti</given-names>
            </name>
            <name>
              <surname>Maenpaa</surname>
              <given-names>Topi</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TPAMI.2002.1017623</pub-id>
          <article-title>Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</article-title>
          <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>29</volume>
          <page-range>2352-2449</page-range>
          <issue>9</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Rawat</surname>
              <given-names>Waseem</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Zhenhua</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1162/neco_a_00990</pub-id>
          <article-title>Deep convolutional neural networks for image classification: A comprehensive review</article-title>
          <source>Neural Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>20</volume>
          <page-range>115101</page-range>
          <issue>11</issue>
          <year>2009</year>
          <person-group person-group-type="author">
            <name>
              <surname>Deng</surname>
              <given-names>Xiao Yong</given-names>
            </name>
            <name>
              <surname>Luan</surname>
              <given-names>Qi Xia</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Wen Ting</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Yan Li</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Ming Hong</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Hai Jiao</given-names>
            </name>
            <name>
              <surname>Jiao</surname>
              <given-names>Zheng</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1088/0957-4484/20/11/115101</pub-id>
          <article-title>Nanosized zinc oxide particles induce neural stem cell apoptosis</article-title>
          <source>Nanotechnology</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Canziani</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Paszke</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Culurciello</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.1605.07678</pub-id>
          <article-title>An analysis of deep neural network models for practical applications</article-title>
          <source>arXiv preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <volume>2018</volume>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Voulodimos</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Doulamis</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Doulamis</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Protopapadakis</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2018/7068349</pub-id>
          <article-title>Deep learning for computer vision: A brief review</article-title>
          <source>Comput. Intell. Neurosci.</source>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="conf-paper">
          <page-range>41-48</page-range>
          <year>2009</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Y.</given-names>
              <surname>Bengio</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Louradour</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Collobert</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Weston</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/1553374.1553380</pub-id>
          <article-title>Curriculum learning</article-title>
          <source>Proceedings of the 26th Annual International Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada</source>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="conf-paper">
          <page-range>3320-3328</page-range>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J.</given-names>
              <surname>Yosinski</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Clune</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Bengio</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Lipson</surname>
            </name>
          </person-group>
          <article-title>How transferable are features in deep neural networks?</article-title>
          <source>Proceedings of the 27th International Conference on Neural Information Processing Systems, Montreal, Canada</source>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="conf-paper">
          <page-range>4278-4284</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Szegedy</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Ioffe</surname>
            </name>
            <name>
              <given-names>V.</given-names>
              <surname>Vanhoucke</surname>
            </name>
            <name>
              <given-names>A. A.</given-names>
              <surname>Alemi</surname>
            </name>
          </person-group>
          <article-title>Inception-V4, inception-ResNet and the impact of residual connections on learning</article-title>
          <source>Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, San Francisco, California, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <volume>60</volume>
          <page-range>84-90</page-range>
          <issue>6</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Krizhevsky</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sutskever</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Hinton</surname>
              <given-names>G. E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3065386</pub-id>
          <article-title>Imagenet classification with deep convolutional neural networks</article-title>
          <source>Commun. ACM</source>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="conf-paper">
          <page-range>818-833</page-range>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M. D.</given-names>
              <surname>Zeiler</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Fergus</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-319-10590-1_53</pub-id>
          <article-title>Visualizing and understanding convolutional networks</article-title>
          <source>13th European Conference on Computer Vision, ECCV 2014, Zurich, Switzerland</source>
        </element-citation>
      </ref>
      <ref id="ref_35">
        <label>35.</label>
        <element-citation publication-type="journal">
          <volume>162</volume>
          <page-range>1566-1582</page-range>
          <issue>3</issue>
          <year>2013</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cheng</surname>
              <given-names>M. C.</given-names>
            </name>
            <name>
              <surname>Liao</surname>
              <given-names>P. M.</given-names>
            </name>
            <name>
              <surname>Kuo</surname>
              <given-names>W. W.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>T. P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1104/pp.113.221911</pub-id>
          <article-title>The Arabidopsis ETHYLENE RESPONSE FACTOR1 regulates abiotic stress-responsive gene expression by binding to different cis-acting elements in response to different stress signals</article-title>
          <source>Plant Physiol.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>