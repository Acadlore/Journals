<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IJCMEM</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>International Journal of Computational Methods and Experimental Measurements</journal-title>
        <abbrev-journal-title abbrev-type="issn">Int. J. Comput. Methods Exp. Meas.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IJCMEM</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2046-0554</issn>
      <issn publication-format="print">2046-0546</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-vBPF8lEmKz-TFo9xjKUiw-zQd38bX1C1</article-id>
      <article-id pub-id-type="doi">10.56578/ijcmem130305</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Enhancing Real-Time Face Detection Performance Through YOLOv11 and Slicing-Aided Hyper Inference</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5639-8677</contrib-id>
          <name>
            <surname>Fachrurrozi</surname>
            <given-names>Muhammad</given-names>
          </name>
          <email>mfachrz@unsri.ac.id</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3553-3475</contrib-id>
          <name>
            <surname>Rachmatullah</surname>
            <given-names>Muhammad Naufal</given-names>
          </name>
          <email>naufalrachmatullah@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-2913-2300</contrib-id>
          <name>
            <surname>Arum</surname>
            <given-names>Akhiar Wista</given-names>
          </name>
          <email>wistaarum16@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3904-2799</contrib-id>
          <name>
            <surname>Monado</surname>
            <given-names>Fiber</given-names>
          </name>
          <email>fibermonado@unsri.ac.id</email>
        </contrib>
        <aff id="aff_1">Department of Informatics, Faculty of Computer Science, Universitas Sriwijaya, 30139 Palembang, Indonesia</aff>
        <aff id="aff_2">Department of Computer Engineering, Faculty of Computer Science, Universitas Sriwijaya, 30139 Palembang, Indonesia</aff>
        <aff id="aff_3">Department of Physics, Faculty of Mathematics and Natural Sciences, Universitas Sriwijaya, 30139 Palembang, Indonesia</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>13</day>
        <month>10</month>
        <year>2025</year>
      </pub-date>
      <volume>13</volume>
      <issue>3</issue>
      <fpage>520</fpage>
      <lpage>531</lpage>
      <page-range>520-531</page-range>
      <history>
        <date date-type="received">
          <day>16</day>
          <month>07</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>08</day>
          <month>09</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Real-time face detection in crowded scenes remains challenging due to small-scale facial regions, heavy occlusion, and complex illumination, which often degrade detection accuracy and computational efficiency. This study presents an enhanced detection framework that integrates Slicing-Aided Hyper Inference (SAHI) with the YOLOv11 architecture to improve small-face recognition under diverse visual conditions. While YOLOv11 provides a high-speed single-stage detection backbone, it tends to lose fine spatial information through downsampling, limiting its sensitivity to tiny faces. SAHI addresses this limitation by partitioning high-resolution images into overlapping slices, enabling localized inference that preserves structural detail and strengthens feature representation for small targets. The proposed YOLOv11–SAHI system was trained and evaluated on the WIDER Face dataset across Easy, Medium, and Hard difficulty levels. Experimental results demonstrate that the integrated framework achieves Average Precision (AP) scores of 96.33%, 95.87%, and 90.81% for the respective subsets—outperforming YOLOv7, YOLOv5, and other lightweight detectors, and closely approaching RetinaFace accuracy. Detailed error analysis reveals that the combined model substantially enhances small-face detection in dense crowds but remains sensitive to severe occlusion, motion blur, and extreme pose variations. Overall, YOLOv11 coupled with SAHI offers a robust and computationally efficient solution for real-time face detection in complex environments, establishing a foundation for future work on pose-invariant feature learning and adaptive slicing optimization.</p></abstract>
      <kwd-group>
        <kwd>Face detection</kwd>
        <kwd>YOLOv11</kwd>
        <kwd>Slicing aided hyper inference</kwd>
        <kwd>WIDER face</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="4"/>
        <fig-count count="6"/>
        <table-count count="3"/>
        <ref-count count="37"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>The advancement of facial recognition methods and the ease of collecting faces digitally have led to an increase in the use of facial recognition methods [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>]. However, the success of a facial recognition application often depends on the success of the face detecting system in an image. In general, there two main challenges in the facial detection process. The first is the identification of faces within images containing diverse backgrounds. These images have differing poses, occlusions, lighting, etc. The second is to identify faces in an image which may occur in varying scales, at different places in the image. Both of these issues are influencing the accuracy and time efficiency, respectively. Because there is a trade-off between performance and time speed, developing a balance both conditions become increasingly difficult.</p><p>Early face detection methods primarily relied on traditional approaches involving the extraction of engineered features from images, subsequently utilizing various classifiers to accurately identify facial regions. Notably, the Haar cascade classifier [<xref ref-type="bibr" rid="ref_3">3</xref>] and the Histogram of Oriented Gradients (HOG) paired with a Support Vector Machine (SVM) [<xref ref-type="bibr" rid="ref_4">4</xref>] represent seminal classical techniques in this domain, exemplifying significant historical advancements. Nevertheless, these classical methodologies still exhibit limited detection accuracy when confronted with challenging images containing complex variations, as demonstrated by performance evaluations on the WIDER FACE dataset [<xref ref-type="bibr" rid="ref_5">5</xref>].</p><p>In the era of deep learning method advancement, researchers are competing to analyze the influence of deep learning to overcome the complexity of face detection cases. Therefore, the use of deep learning methods as face detectors as begun to be developed, such as Face R-CNN [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>], Face R-FCN [<xref ref-type="bibr" rid="ref_12">12</xref>], [<xref ref-type="bibr" rid="ref_13">13</xref>], single shot detector (SSD) [<xref ref-type="bibr" rid="ref_14">14</xref>], [<xref ref-type="bibr" rid="ref_15">15</xref>], [<xref ref-type="bibr" rid="ref_16">16</xref>], [<xref ref-type="bibr" rid="ref_17">17</xref>] which introduces a multiscale mechanism, feature enhancements in the FPN [<xref ref-type="bibr" rid="ref_18">18</xref>], [<xref ref-type="bibr" rid="ref_19">19</xref>], [<xref ref-type="bibr" rid="ref_20">20</xref>], [<xref ref-type="bibr" rid="ref_21">21</xref>], [<xref ref-type="bibr" rid="ref_22">22</xref>] architecture and focus loss in the Retinaface architecture [<xref ref-type="bibr" rid="ref_23">23</xref>] have been successfully developed to overcome the challenges in face detection from the WIDER FACE dataset. The architecture has been modified to identify unique patterns of human facial objects and perform well under unconstrained conditions.</p><p>Although many existing face detection architectures achieve high accuracy, they often incur substantial computational overhead and long processing times. This is particularly true for two‐stage detection pipelines, where the system first generates region proposals and then performs classification and bounding box refinement. While these methods can be precise, their sequential nature limits suitability for real‐time applications or deployment on resource‐constrained devices. Even single‐stage detectors such as SSD and RetinaNet face efficiency challenges: SSD depends heavily on anchor‐based multi‐scale feature extraction, while RetinaNet’s complexity stems from focal loss and feature pyramid networks (FPN). Consequently, these approaches often struggle to meet the requirements of low‐latency detection tasks.</p><p>In contrast, the You Only Look Once (YOLO) architecture introduced by Redmon et al. [<xref ref-type="bibr" rid="ref_24">24</xref>], perform bounding boxes prediction and classification in a single pass over the image. This architecture is well known for its speed and computational efficiency [<xref ref-type="bibr" rid="ref_25">25</xref>]. The YOLO architecture has been evolved through multiple versions for example YOLOv8 implemented an anchor-free and decoupled head structure, which improved localization precision and stabilized the training process. YOLOv9 incorporated Programmable Gradient Information (PGI) and a Generalized Efficient Layer Aggregation Network (GELAN), resulting in greater feature reuse efficiency and improved accuracy, particularly for small and medium-scale objects. YOLOv10 replaced the traditional Non-Maximum Suppression (NMS) with a dual-label assignment mechanism, which accelerated inference while preserving detection robustness in high-density environments. YOLOv11 focused on refining low-latency feature extraction via lightweight convolutional modules and enhanced feature pyramids, optimizing efficiency for both edge and cloud deployment without compromising precision. Finally, YOLOv12 integrated Area Attention and Residual ELAN modules, enabling superior focus on salient features and fine-grained object structures, thereby achieving higher accuracy for occluded and small targets.</p><p>Recent studies have increasingly explored YOLO architectures for face detection, with various modifications designed to improve scale robustness, efficiency, and adaptability. Zheng et al. [<xref ref-type="bibr" rid="ref_26">26</xref>] proposed GCSEM-YOLO to emphasize small-scale face detection. This model achieved 94.3% (Easy), 93.0% (Medium), and 84.7% (Hard) AP on the WIDER Face dataset. Pebrianto et al. [<xref ref-type="bibr" rid="ref_27">27</xref>] utilized YOLOv3 for real-time face detection and got reported increase in accuracy (87%) while maintaining low latency. However, the authors provide the declining performance on small and occluded faces. Sufian Chan et al. [<xref ref-type="bibr" rid="ref_28">28</xref>] analyses the performance YOLOv7 algorithm. They found that the efficiency improved but the AP value dropped to only 83.15% on the Hard subset, indicating limited robustness under extreme conditions. Vemulapalli et al. [<xref ref-type="bibr" rid="ref_29">29</xref>] incorporated landmark detection into YOLOv8. These authors achieving a relatively fast detection process but still underperforming on small and blurred faces. Gao and Yang [<xref ref-type="bibr" rid="ref_30">30</xref>] enhanced Tiny-YOLOv3 with an attention mechanism to reduce false detections. The results show 95.26 (Easy), 89.2 (Medium) and 77.9 (hard) on validation set. Qi et al. [<xref ref-type="bibr" rid="ref_31">31</xref>] introduced YOLO5Face, achieving 93.61% (Easy), 91.54% (Medium), and 80.53% (Hard), demonstrating that even optimized lightweight models still struggle with tiny faces and occlusion.</p><p>Despite these advances, the performance of most YOLO-based detectors declines on the Hard subset of the WIDER Face that predominantly includes tiny faces, high occlusion, extreme poses, and poor illumination [<xref ref-type="bibr" rid="ref_32">32</xref>]. This limitation arises from the intrinsic downsampling operations in convolutional backbones, which lead to the loss of fine-grained spatial information critical for small-object detection. To overcome these challenges, this study integrates Slicing Aided Hyper Inference (SAHI) [<xref ref-type="bibr" rid="ref_33">33</xref>] with the YOLOv11 architecture. SAHI improves sensitivity to small-scale and partially visible faces by dividing high-resolution images into overlapping slices and performing inference locally on each region. This approach effectively increases the pixel density in tiny faces and preserves structural detail that would otherwise be lost during global image downscaling. The localized predictions are subsequently merged using Non-Maximum Suppression (NMS), ensuring coherent and duplicate-free detections. By coupling YOLOv11’s high-speed inference capability with SAHI’s fine-grained spatial analysis, the proposed framework improves accuracy on the Hard subset while maintaining real-time processing performance.</p><p>In terms of performance analysis under real-world environment, the majority of previous works primarily emphasize aggregate performance metrics such as mAP or AP50 on the WIDER Face dataset. However, these standard evaluations generally neglect to quantify detection reliability across specific, challenging visual conditions, including varying degrees of blur severity, occlusion, pose orientation, illumination, and facial expression. Consequently, this methodological reliance on headline metrics provides limited insight into the systematic fluctuation of detection accuracy under these specific challenges. This oversight obscures critical factors responsible for real-world failure modes and hinders the comprehensive evaluation of a model's generalization capability in high-density or crowded scenes. Ultimately, this approach conceals systematic architectural weaknesses and impedes meaningful advancements in model performance.</p><p>In this study we propose a hybrid framework for face detection that combines YOLOv11 and SAHI, followed by a detailed, attribute evaluation using the WIDER Face dataset. The YOLOv11 architecture provides an efficient and high-accuracy detection backbone suitable for low-latency inference. In accordence with SAHI to divide the input image into overlapping slices to improve detection of small or occluded faces. This approach improves detection accuracy under difficult visual conditions. In summary, the key contributions of this study are as follows.</p><p>1. Development of a unified YOLOv11–SAHI framework that enhances detection performance for small, occluded, and densely packed faces in complex scenes.</p><p>2. Introduction of an attribute-based evaluation on the WIDER Face dataset, enabling detailed insights on proposed model.</p><p>3. Demonstration of real-time processing alongside high detection accuracy, supporting deployment in surveillance and crowd-monitoring applications.</p><p>4. Comprehensive validation of the framework's robustness across varied imaging conditions, including poor lighting, motion blur, and high scene density.</p>
    </sec>
    <sec sec-type="">
      <title>2. Material and method</title>
      
        <sec>
          
            <title>2.1. Data collection</title>
          
          <p>This study uses the WIDER Face dataset [<xref ref-type="bibr" rid="ref_4">4</xref>], which is a well-known and large-scale benchmark with 32,203 images containing 393,703 annotated faces in 61 event classes. These images were collected with various real-world conditions like scale, pose, occlusion, blur, illumination and expression make it suitable for face detection benchmark data set.</p><p>Following the protocol for official partitioning, the dataset is designated 40%, 10%, and 50% for training, validation, testing respectively. This subset allows for reproducibility and comparison with previous studies. The training set has 12,876 images containing 152,439 annotated faces. The validation set has 3,226 images containing 39,708 annotated faces. Overall, these subsets cover a wide range of visual and environmental conditions that should allow for effective optimization of the model and evaluation of performance on multiple levels of difficulty.</p><p>A distinctive feature of WIDER Face is its explicit difficulty partitions—<italic>Easy</italic>, <italic>Medium</italic>, and <italic>Hard</italic>—defined by ranking event classes based on face detection rates under scale, occlusion, and pose constraints. The Hard subset, in particular, contains faces smaller than 32×32 pixels, with heavy occlusion or atypical orientations, making it a stringent benchmark for small‐face detection algorithms.</p><p><xref ref-type="table" rid="table_1">Table 1</xref> present a distribution of the training and validation set under certain condition, and it reveals:</p><p>• Size Distribution: Small faces dominate, followed by medium and large faces, confirming the dataset’s emphasis on small-scale detection challenges</p><p>• Occlusion: While fully visible faces form the largest group, substantial heavy and partial occlusion cases are present, adding complexity to detection.</p><p>• Blur: Heavy blur is most frequent, followed by medium blur, with clear faces being least common.</p><p>• Illumination: Clear lighting conditions predominate, though some medium degradation cases exist.</p><p>• Pose Levels: Most faces exhibit typical poses, with a smaller proportion showing atypical or extreme orientations.</p><p>• Expression Levels: Normal expressions dominate, while exaggerated expressions and other variations appear rarely.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>WIDER face distribution across certain condition</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Condition</p></td><td colspan="1" rowspan="1"><p>Level</p></td><td colspan="1" rowspan="1"><p>Train</p></td><td colspan="1" rowspan="1"><p>Valid</p></td></tr><tr><td colspan="1" rowspan="3"><p>Image Size</p></td><td colspan="1" rowspan="1"><p>Large ( <mml:math id="mh61d6gzp9">
  <mml:mo>≥</mml:mo>
  <mml:msup>
    <mml:mn>96</mml:mn>
    <mml:mrow>
      <mml:mo>∧</mml:mo>
    </mml:mrow>
  </mml:msup>
  <mml:mn>2</mml:mn>
  <mml:mrow>
    <mml:mi data-mjx-auto-op="false">px</mml:mi>
  </mml:mrow>
</mml:math> )</p></td><td colspan="1" rowspan="1"><p>9,340</p></td><td colspan="1" rowspan="1"><p>2,327</p></td></tr><tr><td colspan="1" rowspan="1"><p>Medium ( $32^{\wedge} 2-96^{\wedge} 2 \mathrm{px}<mml:math id="mi092dum6p">
  <mml:mo>)</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>(</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>)</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>(</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>)</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>(</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>)</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>(</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>b</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>b</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>b</mml:mi>
  <mml:mi>S</mml:mi>
  <mml:mi>m</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>15</mml:mn>
  <mml:mn>17</mml:mn>
  <mml:mn>21</mml:mn>
  <mml:mn>32</mml:mn>
  <mml:mn>756</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>15</mml:mn>
  <mml:mn>17</mml:mn>
  <mml:mn>21</mml:mn>
  <mml:mn>8</mml:mn>
  <mml:mn>606</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>15</mml:mn>
  <mml:mn>17</mml:mn>
  <mml:mn>21</mml:mn>
</mml:math>&lt;32^{\wedge} 2 \mathrm{px}$ )</p></td><td colspan="1" rowspan="1"><p>110,343</p></td><td colspan="1" rowspan="1"><p>28,775</p></td></tr><tr><td colspan="1" rowspan="3"><p>Blur</p></td><td colspan="1" rowspan="1"><p>Heavy</p></td><td colspan="1" rowspan="1"><p>88,951</p></td><td colspan="1" rowspan="1"><p>23,641</p></td></tr><tr><td colspan="1" rowspan="1"><p>Medium</p></td><td colspan="1" rowspan="1"><p>40,746</p></td><td colspan="1" rowspan="1"><p>10,381</p></td></tr><tr><td colspan="1" rowspan="1"><p>None/Clear</p></td><td colspan="1" rowspan="1"><p>22,737</p></td><td colspan="1" rowspan="1"><p>5,686</p></td></tr><tr><td colspan="1" rowspan="2"><p>Illumination</p></td><td colspan="1" rowspan="1"><p>Normal</p></td><td colspan="1" rowspan="1"><p>144,436</p></td><td colspan="1" rowspan="1"><p>37,410</p></td></tr><tr><td colspan="1" rowspan="1"><p>Extreme</p></td><td colspan="1" rowspan="1"><p>7,998</p></td><td colspan="1" rowspan="1"><p>2,298</p></td></tr><tr><td colspan="1" rowspan="2"><p>Pose</p></td><td colspan="1" rowspan="1"><p>Typical</p></td><td colspan="1" rowspan="1"><p>146,463</p></td><td colspan="1" rowspan="1"><p>38,053</p></td></tr><tr><td colspan="1" rowspan="1"><p>Atypical</p></td><td colspan="1" rowspan="1"><p>5,971</p></td><td colspan="1" rowspan="1"><p>1,655</p></td></tr><tr><td colspan="1" rowspan="3"><p>Occlusion</p></td><td colspan="1" rowspan="1"><p>None/Clear</p></td><td colspan="1" rowspan="1"><p>93,920</p></td><td colspan="1" rowspan="1"><p>23,812</p></td></tr><tr><td colspan="1" rowspan="1"><p>Partial</p></td><td colspan="1" rowspan="1"><p>26,021</p></td><td colspan="1" rowspan="1"><p>7,185</p></td></tr><tr><td colspan="1" rowspan="1"><p>Heavy</p></td><td colspan="1" rowspan="1"><p>32,493</p></td><td colspan="1" rowspan="1"><p>8,711</p></td></tr><tr><td colspan="1" rowspan="2"><p>Expression</p></td><td colspan="1" rowspan="1"><p>Typical</p></td><td colspan="1" rowspan="1"><p>150,601</p></td><td colspan="1" rowspan="1"><p>39,308</p></td></tr><tr><td colspan="1" rowspan="1"><p>Exaggerate</p></td><td colspan="1" rowspan="1"><p>1,833</p></td><td colspan="1" rowspan="1"><p>400</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>These dataset characteristics highlight the significant challenges posed by WIDER Face, particularly in handling small‐scale, occluded, blurred, and variably lit faces. Combined with its extensive annotations and standardized Average Precision (AP) evaluation protocol across difficulty levels, WIDER Face provides an ideal testbed for evaluating the robustness of face detection methods. Leveraging this dataset enables rigorous comparison of YOLOv11 with and without SAHI integration, especially in detecting small faces in dense crowd environments. <xref ref-type="fig" rid="fig_1">Figure 1</xref> presents sample images from the WIDER Face dataset.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>Example of WIDER face dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_uLnQg5ALJGgT3OBV.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>2.2. Proposed method</title>
          
          <p>The primary objective of this research is to enhance the detection accuracy of faces, particularly small-scale instances, in densely populated images. To achieve this, we propose an integrated detection framework by combining the YOLOv11 architecture with the SAHI method. Our methodology introduces a dual-stage detection pipeline designed to overcome the difficulties of identifying small-scale faces in crowded scenes. This approach, outlined in the flowchart in <xref ref-type="fig" rid="fig_2">Figure 2</xref>. The proposed method used YOLOv11 as the primary detection backbone while SAHI performing image slicing to handle small faces in crowds.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Proposed model</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_iD4NwsszazT4JZmZ.png"/>
            </fig>
          
          <p>In order to identify the most effective model configuration, first we benchmarked multiple YOLOv11 variants ranging from the lightweight YOLOv11-nano to the more complex YOLOv11-xlarge (xl) architecture using the WIDER Face training subset. The evaluation considered trade-offs between accuracy and inference speed, with particular emphasis on performance within the Hard subset. Based on these results, the best-performing YOLOv11 variant was selected as the core detection backbone.</p><p>In the inference phase, we employ SAHI to address the challenge in detecting small-scale faces. SAHI partitions the input image into overlapping slices which effectively increasing the pixel resolution of small facial regions presented to the detector. This process helps preserve fine-grained facial details that would otherwise be lost in full-image downsampling. Each slice is independently processed by the YOLOv11 model, and the resulting bounding boxes are reprojected from slice-local to global image coordinates. To eliminate redundant predictions from overlapping regions, a confidence-weighted Non-Maximum Suppression (NMS) algorithm is applied. The NMS threshold was carefully optimized to balance recall and precision, particularly in dense visual contexts.</p><p>This integrated approach leverages the strengths of both high-speed object detection from YOLOv11 and resolution-aware inference in SAHI, resulting in significantly improved accuracy for small-face detection without sacrificing real-time performance.</p>
          
            <sec>
              
                <title>2.2.1 You only look once</title>
              
              <p>YOLOv11 represents the latest iteration in the You Only Look Once family, designed to achieve an optimal trade-off between inference speed and detection accuracy. Its architecture incorporates an improved backbone based on Cross Stage Partial Networks (CSP) to reduce computational redundancy, a Path Aggregation Network (PANet) to enhance information flow across feature levels, and a Spatial Pyramid Pooling–Fast (SPPF) module to capture multi-scale contextual information without incurring significant computational overhead as illustrated in <xref ref-type="fig" rid="fig_3">Figure 3</xref>.</p><p>During the feature extraction stage, YOLOv11 processes the input image through the backbone to generate multi-resolution feature maps, which are subsequently fused in the neck for improved feature representation. The head module produces bounding box coordinates, confidence scores, and object class predictions. Although the architecture is optimized for detecting objects of varying scales, the detection of extremely small faces in crowded scenes remains challenging due to the progressive loss of feature resolution during multi-stage down-sampling.</p>
              
                <fig id="fig_3">
                  <label>Figure 3</label>
                  <caption>
                    <title>YOLOv11 architecture</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_HovPpSLcbzPs-Yy2.png"/>
                </fig>
              
            </sec>
          
          
            <sec>
              
                <title>2.2.2 Slicing aided hyper inference</title>
              
              <p>Slicing Aided Hyper Inference (SAHI) is a specialized inference strategy designed to improve the accuracy of small-object detection. The method partitions a high-resolution image into smaller, overlapping patches, performs inference on each patch independently, and subsequently merges the results to produce a comprehensive detection output for the original image. Each patch is resized while preserving its aspect ratio before being processed by the object detector. As an optional step, full-image inference may also be conducted to facilitate the detection of larger objects. The outputs from overlapping patches are consolidated into the original image space through Non-Maximum Suppression (NMS), wherein predictions with Intersection over Union (IoU) values exceeding a predefined threshold are considered duplicates. For each duplicate set, detections with confidence scores below the specified threshold are removed, ensuring that only optimal and non-overlapping predictions are retained.</p><p>In this study, SAHI is tightly integrated with YOLOv11 to address the challenge of detecting tiny and partially occluded faces in crowded scenes. YOLOv11’s multi-scale feature extraction mechanism, supported by its enhanced Path Aggregation Network (PAN) and Cross-Stage Partial (CSP) connections, allows the model to capture hierarchical spatial features across different receptive fields. However, its global downsampling process may still suppress fine details when detecting extremely small faces. SAHI complements this by dividing the input image into overlapping slices, typically with 10–20% overlap, which enables the model to analyze each region at a higher effective resolution. This strategy preserves facial details that would otherwise be lost, while the overlap ensures feature continuity across patch boundaries and prevents object fragmentation.</p><p>Each slice is independently processed through the YOLOv11 backbone, generating localized detections that are subsequently reprojected to the original image coordinates. The final stage applies NMS to merge all slice-level detections, eliminating redundant boxes and optimizing overall confidence scores. This slicing-and-merging framework effectively mitigates common detection failures in crowd-dense environments—such as missed tiny faces, mislocalized bounding boxes, or duplicate detections—by ensuring that each small facial region receives focused analysis at an appropriate scale.</p><p>The integration of YOLOv11’s multi-scale representation learning with SAHI’s localized high-resolution inference produces a complementary synergy: YOLOv11 ensures efficient global context modeling and robust feature hierarchies, while SAHI enhances spatial granularity for small-object detection. Together, they yield substantially improved precision on the Hard subset of the WIDER Face dataset, where small, occluded, and low-contrast faces dominate. This hybrid inference framework thus provides a computationally efficient yet highly accurate solution for real-world, crowd-intensive face detection applications.</p>
            </sec>
          
          
            <sec>
              
                <title>2.2.3 Evaluation metrics</title>
              
              <p>Average Precision (AP) is the primary evaluation metric used to measure the performance of object detection systems, including face detection. AP represents the average of precision values computed across various recall levels, thus reflecting the model’s ability to maintain prediction accuracy as the detection coverage expands. The calculation of AP begins with constructing a Precision–Recall (PR) curve, obtained by varying the confidence score threshold for the model’s predictions. Eqs. (1) and (2) calculate the precision and recall value from predicted faces</p>
              
                <disp-formula>
                  <label>(1)</label>
                  <mml:math id="mms7tsxxm5">
                    <mml:mi>P</mml:mi>
                    <mml:mi>r</mml:mi>
                    <mml:mi>e</mml:mi>
                    <mml:mi>c</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mi>s</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mi>o</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                        <mml:mi>F</mml:mi>
                        <mml:mi>P</mml:mi>
                        <mml:mo>+</mml:mo>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(2)</label>
                  <mml:math id="m1p22a9l84">
                    <mml:mi>R</mml:mi>
                    <mml:mi>e</mml:mi>
                    <mml:mi>c</mml:mi>
                    <mml:mi>a</mml:mi>
                    <mml:mi>l</mml:mi>
                    <mml:mi>l</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                        <mml:mi>F</mml:mi>
                        <mml:mi>N</mml:mi>
                        <mml:mo>+</mml:mo>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:math>
                </disp-formula>
              
              <p>AP is then computed as the area under the <italic>Precision-Recall</italic> curve (Area Under Curve, AUC) (Eq. (3)) where <inline-formula>
  <mml:math id="mdikglaysa">
    <mml:msub>
      <mml:mi>P</mml:mi>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>r</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> denotes the precision at recall level <inline-formula>
  <mml:math id="mnknd1x8b2">
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula>. In practical implementations, this integral is approximated by discrete summation over interpolated precision points.</p>
              
                <disp-formula>
                  <label>(3)</label>
                  <mml:math id="mrzh31yzzu">
                    <mml:mi>A</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>d</mml:mi>
                    <mml:mi>r</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:msubsup>
                      <mml:mo>∫</mml:mo>
                      <mml:mn>0</mml:mn>
                      <mml:mn>1</mml:mn>
                    </mml:msubsup>
                    <mml:msub>
                      <mml:mi>P</mml:mi>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>r</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:math>
                </disp-formula>
              
              <p>In this study, AP is calculated at specific Intersection over Union (IoU) thresholds, in accordance with the WIDER Face evaluation protocol. IoU is a metric that quantifies the overlap between the predicted bounding box and the ground truth bounding box, defined in Eq. (4). A higher AP indicates better capability of the model to consistently produce accurate detections across various recall levels. In the context of small-scale face detection on the hard subset of WIDER Face, a high AP demonstrates that the proposed method can maintain high precision even under challenging conditions, such as object occlusion, poor illumination, or extreme face orientations.</p>
              
                <disp-formula>
                  <label>(4)</label>
                  <mml:math id="mivti3b3cx">
                    <mml:mi>I</mml:mi>
                    <mml:mi>o</mml:mi>
                    <mml:mi>U</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mi>A</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>f</mml:mi>
                        <mml:mi>O</mml:mi>
                        <mml:mi>v</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>l</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>p</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>A</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>f</mml:mi>
                        <mml:mi>U</mml:mi>
                        <mml:mi>n</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>n</mml:mi>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:math>
                </disp-formula>
              
            </sec>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Experimental result</title>
      <p>The experimental evaluation was conducted on the WIDER Face benchmark dataset, which is divided into three difficulty levels: easy, medium, and hard. Performance was measured using Average Precision (AP) at IoU thresholds of 0.5. The proposed YOLOv11 + SAHI framework was compared against the baseline YOLOv11 model without slicing-based inference. In the SAHI configuration, each input image was divided into overlapping slices with a slice height and width of 256 pixels and an overlap ratio of 20%, ensuring that small faces were preserved within at least one slice for improved detection.</p><p><xref ref-type="table" rid="table_2">Table 2</xref> presents the comparative AP@0.5 results between different YOLOv11 model variants—Nano, Small, Medium, Large, and Extra‐Large—evaluated with and without SAHI integration on the Easy, Medium, and Hard subsets of WIDER Face. On the Easy subset, YOLOv11 + SAHI achieved performance comparable to the baseline across most variants, with notable gains for the Nano (+1.81%), Medium (+1.18%), and Extra‐Large (+1.03%) models, indicating that the slicing strategy maintains or slightly improves detection accuracy for larger and medium‐scale faces. On the Medium subset, SAHI consistently boosted AP scores for all variants, with the largest improvements observed in Nano (+2.27%) and Small (+1.91%) models, reflecting enhanced detection for moderately challenging conditions.</p><p>The most substantial gains occurred on the Hard subset, where YOLOv11 + SAHI outperformed the baseline by wide margins, particularly in Nano (+4.28%), Small (+3.87%), and Medium (+3.35%) configurations. Across all test cases, the best performance was recorded by the Extra‐Large model with SAHI which demonstrate that integrating slicing mechanism substantially enhances the model’s ability to detect small‐scale faces under challenging crowd conditions without compromising performance on larger faces in less complex scenes.</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>AP comparison between baseline model and with SAHI</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Subset</p></td><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="5" rowspan="1"><p>Average Precision (%) @ 0.5</p></td></tr><tr><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>Nano</p></td><td colspan="1" rowspan="1"><p>Small</p></td><td colspan="1" rowspan="1"><p>Medium</p></td><td colspan="1" rowspan="1"><p>Large</p></td><td colspan="1" rowspan="1"><p>Extra-Large</p></td></tr><tr><td colspan="1" rowspan="2"><p>Easy</p></td><td colspan="1" rowspan="1"><p>Baseline</p></td><td colspan="1" rowspan="1"><p>93.45</p></td><td colspan="1" rowspan="1"><p>94.92</p></td><td colspan="1" rowspan="1"><p>95.55</p></td><td colspan="1" rowspan="1"><p>95.50</p></td><td colspan="1" rowspan="1"><p>95.77</p></td></tr><tr><td colspan="1" rowspan="1"><p>With SAHI</p></td><td colspan="1" rowspan="1"><p>95.26</p></td><td colspan="1" rowspan="1"><p>93.39</p></td><td colspan="1" rowspan="1"><p>96.73</p></td><td colspan="1" rowspan="1"><p>93.67</p></td><td colspan="1" rowspan="1"><p>96.33</p></td></tr><tr><td colspan="1" rowspan="2"><p>Medium</p></td><td colspan="1" rowspan="1"><p>Baseline</p></td><td colspan="1" rowspan="1"><p>92.61</p></td><td colspan="1" rowspan="1"><p>94.12</p></td><td colspan="1" rowspan="1"><p>94.74</p></td><td colspan="1" rowspan="1"><p>94.75</p></td><td colspan="1" rowspan="1"><p>95.02</p></td></tr><tr><td colspan="1" rowspan="1"><p>With SAHI</p></td><td colspan="1" rowspan="1"><p>94.88</p></td><td colspan="1" rowspan="1"><p>96.03</p></td><td colspan="1" rowspan="1"><p>96.36</p></td><td colspan="1" rowspan="1"><p>95.66</p></td><td colspan="1" rowspan="1"><p>95.87</p></td></tr><tr><td colspan="1" rowspan="2"><p>Hard</p></td><td colspan="1" rowspan="1"><p>Baseline</p></td><td colspan="1" rowspan="1"><p>85.68</p></td><td colspan="1" rowspan="1"><p>87.38</p></td><td colspan="1" rowspan="1"><p>88.37</p></td><td colspan="1" rowspan="1"><p>88.38</p></td><td colspan="1" rowspan="1"><p>88.82</p></td></tr><tr><td colspan="1" rowspan="1"><p>With SAHI</p></td><td colspan="1" rowspan="1"><p>89.96</p></td><td colspan="1" rowspan="1"><p>91.25</p></td><td colspan="1" rowspan="1"><p>91.72</p></td><td colspan="1" rowspan="1"><p>89.71</p></td><td colspan="1" rowspan="1"><p>90.81</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>The most significant improvement was observed on the <inline-formula>
  <mml:math id="mkegxcwvr8">
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula> subset, which predominantly contains small and heavily occluded faces. By dividing high-resolution images into overlapping slices, SAHI preserved the relative resolution of small faces during inference, mitigating the loss of spatial detail caused by the down-sampling operations in the YOLOv11 backbone. <xref ref-type="fig" rid="fig_4">Figure 4</xref> illustrates a qualitative comparison the baseline YOLOv11 fails to detect several small faces in dense scenes, whereas YOLOv11 + SAHI successfully identifies them. This improvement can be attributed to the fact that each slice effectively acts as a <italic>zoomed-in view</italic> of the scene, allowing the model to focus on finer details that would otherwise be blurred when the entire image is downscaled.</p>
      
        <fig id="fig_4">
          <label>Figure 4</label>
          <caption>
            <title>Face detection result (a) YOLOv11 baseline (b) YOLOv11 with SAHI (Green bounding box: True positive, red bounding box: FN)</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_jEp9VoGholHNVp7O.png"/>
        </fig>
      
      <p>Precision–Recall (PR) curves (<xref ref-type="fig" rid="fig_5">Figure 5</xref>) further confirm that the proposed method consistently achieves higher precision across a wide range of recall levels, especially in the high-recall region, indicating that more true faces are correctly detected without a substantial increase in false positives.</p>
      
        <fig id="fig_5">
          <label>Figure 5</label>
          <caption>
            <title>PR-Cure of YOLOv11-X with SAHI</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_pDDqlVSPfz3wh-ny.png"/>
        </fig>
      
    </sec>
    <sec sec-type="discussion">
      <title>4. Discussion</title>
      <p>The introduction of SAHI inevitably increases inference time due to multiple passes required for sliced inputs. On average, the proposed method reduced throughput from 62 FPS (baseline YOLOv11) to 25 FPS on an NVIDIA RTX 5090 GPU. However, this trade-off is acceptable in applications where accuracy for small or hard-to-detect faces is prioritized over real-time speed, such as forensic video analysis or high-risk security monitoring. The efficiency of YOLOv11’s architecture, particularly its CSP connections and PANet-based feature aggregation, helps mitigate the computational burden of slicing, allowing the system to remain within operational limits.</p><p>Despite these gains, error analysis reveals consistent failure modes under extreme conditions. As shown in <xref ref-type="fig" rid="fig_6">Figure 6</xref>a, the model maintains high success rates on clear (98.42%) and normal blur (98.76%) faces but drops sharply to 84.30% under heavy blur, with nearly 15.7% of faces missed. Similarly, <xref ref-type="fig" rid="fig_6">Figure 6</xref>b indicates robust performance on non-occluded (95.81%) faces but a marked decline to 78.25% under heavy occlusion. Pose variations remain challenging, with 10% failures for typical and 8.31% for atypical orientations, reflecting the difficulty of extreme head rotations. Illumination (<xref ref-type="fig" rid="fig_6">Figure 6</xref>c) also plays a role: detection accuracy falls from 94.61% in extreme lighting to 89.79% in normal but inconsistent illumination, suggesting sensitivity to localized shadows or uneven lighting. For facial expressions (<xref ref-type="fig" rid="fig_6">Figure 6</xref>d), the system performs well on exaggerated expressions (98%) but records 10.01% failures on typical expressions, likely due to subtler variations in mouth and eye regions. Lastly, for facial pose (<xref ref-type="fig" rid="fig_6">Figure 6</xref>e) typical poses achieved a success rate of 90.00% while the atypical poses increased slightly to 91.69% which indicates that the model performs consistently well across both categories.</p><p>Comparative analysis indicates that heavy occlusion (21.75% missed detections) represents the most severe limitation, followed by heavy blur (15.7% drop) and extreme pose shifts (10% failures). These conditions significantly degrade feature consistency, making them harder to compensate for even with slicing. In contrast, illumination and expression variations have relatively smaller impacts, suggesting that SAHI and YOLOv11 effectively leverage texture cues in such scenarios.</p><p>Overall, these findings confirm that SAHI substantially enhances the detection of small faces, particularly in dense crowd scenarios, yet persistent challenges remain under heavy occlusion, motion blur, and extreme pose variations. When compared against other state-of-the-art detectors (<xref ref-type="table" rid="table_3">Table 3</xref>), the proposed YOLOv11 + SAHI framework demonstrates highly competitive performance. Specifically, it achieves 96.33% AP on Easy, 95.87% on Medium, and 90.81% on Hard, which is comparable to RetinaFace (96.71%, 96.08%, 91.44%) and surpasses popular YOLO variants such as YOLOv7 (96.11%, 95.12%, 88.14) and YOLOv5 (96.67%, 95.08%, 86.55) on the Hard subset. Moreover, the method markedly outperforms earlier architectures, including YOLOv3 Tiny (77.9% Hard AP), MTCNN (60.7% Hard AP), and Faceness (42.4% Hard AP), highlighting its robustness in detecting small-scale and occluded faces. Importantly, this level of accuracy is achieved with lower architectural complexity and greater backbone flexibility compared to traditional multi-stage frameworks such as DSFD and cascade-based methods. Future research should focus on integrating pose-invariant feature learning and adaptive tiling strategies based on face density to mitigate redundant slicing while maintaining high recall and efficiency.</p>
      
        <fig id="fig_6">
          <label>Figure 6</label>
          <caption>
            <title>Error analysis on several condition (a) Blur, (b) Occlusion, (c) Illumination, (d) Expression, (e) Pose</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_tku-ANG-J_LM3mAb.png"/>
        </fig>
      
      
        <table-wrap id="table_3">
          <label>Table 3</label>
          <caption>
            <title>Comparison of the proposed method with state-of-the-art detectors</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Network Model</p></td><td colspan="1" rowspan="1"><p>AP (Easy)</p></td><td colspan="1" rowspan="1"><p>AP (Medium)</p></td><td colspan="1" rowspan="1"><p>AP (Hard)</p></td><td colspan="1" rowspan="1"><p>Mean AP</p></td></tr><tr><td colspan="1" rowspan="1"><p>Proposed Method</p></td><td colspan="1" rowspan="1"><p>96.33</p></td><td colspan="1" rowspan="1"><p>95.87</p></td><td colspan="1" rowspan="1"><p>90.81</p></td><td colspan="1" rowspan="1"><p>94.34</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv8n [<xref ref-type="bibr" rid="ref_34">34</xref>]</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>67.60</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv7 [<xref ref-type="bibr" rid="ref_28">28</xref>]</p></td><td colspan="1" rowspan="1"><p>96.11</p></td><td colspan="1" rowspan="1"><p>95.12</p></td><td colspan="1" rowspan="1"><p>88.14</p></td><td colspan="1" rowspan="1"><p>93.12</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv5 [<xref ref-type="bibr" rid="ref_31">31</xref>]</p></td><td colspan="1" rowspan="1"><p>96.67</p></td><td colspan="1" rowspan="1"><p>95.08</p></td><td colspan="1" rowspan="1"><p>86.55</p></td><td colspan="1" rowspan="1"><p>92.77</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv3 Tiny [<xref ref-type="bibr" rid="ref_30">30</xref>]</p></td><td colspan="1" rowspan="1"><p>95.26</p></td><td colspan="1" rowspan="1"><p>89.2</p></td><td colspan="1" rowspan="1"><p>77.9</p></td><td colspan="1" rowspan="1"><p>87.45</p></td></tr><tr><td colspan="1" rowspan="1"><p>Multiscale Cascade [<xref ref-type="bibr" rid="ref_21">21</xref>]</p></td><td colspan="1" rowspan="1"><p>69.1</p></td><td colspan="1" rowspan="1"><p>63.4</p></td><td colspan="1" rowspan="1"><p>34.5</p></td><td colspan="1" rowspan="1"><p>55.67</p></td></tr><tr><td colspan="1" rowspan="1"><p>DEFace [<xref ref-type="bibr" rid="ref_22">22</xref>]</p></td><td colspan="1" rowspan="1"><p>87.2</p></td><td colspan="1" rowspan="1"><p>85.6</p></td><td colspan="1" rowspan="1"><p>75.4</p></td><td colspan="1" rowspan="1"><p>82.73</p></td></tr><tr><td colspan="1" rowspan="1"><p>Faceness [<xref ref-type="bibr" rid="ref_35">35</xref>]</p></td><td colspan="1" rowspan="1"><p>71.3</p></td><td colspan="1" rowspan="1"><p>66.4</p></td><td colspan="1" rowspan="1"><p>42.4</p></td><td colspan="1" rowspan="1"><p>60.03</p></td></tr><tr><td colspan="1" rowspan="1"><p>MTCNN [<xref ref-type="bibr" rid="ref_36">36</xref>]</p></td><td colspan="1" rowspan="1"><p>85.1</p></td><td colspan="1" rowspan="1"><p>82.0</p></td><td colspan="1" rowspan="1"><p>60.7</p></td><td colspan="1" rowspan="1"><p>75.93</p></td></tr><tr><td colspan="1" rowspan="1"><p>RetinaFace [<xref ref-type="bibr" rid="ref_23">23</xref>]</p></td><td colspan="1" rowspan="1"><p>96.71</p></td><td colspan="1" rowspan="1"><p>96.08</p></td><td colspan="1" rowspan="1"><p>91.44</p></td><td colspan="1" rowspan="1"><p>94.74</p></td></tr><tr><td colspan="1" rowspan="1"><p>Li et al. [<xref ref-type="bibr" rid="ref_37">37</xref>]</p></td><td colspan="1" rowspan="1"><p>87.4</p></td><td colspan="1" rowspan="1"><p>84.1</p></td><td colspan="1" rowspan="1"><p>67.3</p></td><td colspan="1" rowspan="1"><p>79.60</p></td></tr></tbody></table>
        </table-wrap>
      
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>This study presented an enhanced face detection framework that integrates YOLOv11 as the primary detection backbone with Slicing Aided Hyper Inference (SAHI) to address the challenge of detecting small-scale faces in crowded scenes. The results demonstrate that the proposed method significantly improves the detection of small and crowded faces, achieving 96.33% AP on the Easy subset, 95.87% on Medium, and 90.81% on Hard. Compared with state-of-the-art detectors, YOLOv11 + SAHI performs competitively with RetinaFace and consistently outperforms YOLOv7, YOLOv5, and other lightweight architectures such as YOLOv3 Tiny and MTCNN, especially on the Hard subset, where small-scale faces, occlusion, and pose variations dominate.</p><p>The improvement is primarily attributed to SAHI’s ability to preserve the relative resolution of small faces by processing overlapping high-resolution slices, allowing YOLOv11 to capture fine-grained spatial details that are often lost in conventional full-image inference. Although the slicing process introduces additional computational overhead, the resulting trade-off between speed and accuracy remains acceptable for applications prioritizing detection robustness over real-time performance.</p><p>Nevertheless, performance declines persist under severe occlusion, heavy blur, and extreme pose variations, and fixed slicing introduces redundancy in sparse images. Future work should focus on pose-invariant representation learning, occlusion-aware modeling, and adaptive slicing strategies to further improve robustness and efficiency across diverse real-world conditions.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p>The research funding by National Grants for Applied Research from the Ministry of Education, Culture, Research, and Technology, Indonesia (Grant No.: 090/E5/PG.02.00.PL/2024).</p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>1188</page-range>
          <issue>8</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Adjabi</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Ouahabi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Benzaoui</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Taleb-Ahmed</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/electronics9081188</pub-id>
          <article-title>Past, present, and future of face recognition: A review</article-title>
          <source>Electronics</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>141-147</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Fatchan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Pulung  Andono</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Affandy</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Fanani</surname>
              <given-names>A. Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/ijcmem.130115</pub-id>
          <article-title>Hybrid deep autoencoder and AdaBoost for robust facial expression recognition</article-title>
          <source>International Journal of Computational Methods and Experimental Measurements</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="conf-paper">
          <page-range>I-511-I-518</page-range>
          <year>2001</year>
          <person-group person-group-type="author">
            <name>
              <surname>Viola</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Jones</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/cvpr.2001.990517</pub-id>
          <article-title>Rapid object detection using a boosted cascade of simple features</article-title>
          <source>Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="conf-paper">
          <volume>1</volume>
          <page-range>886-893</page-range>
          <year>2005</year>
          <person-group person-group-type="author">
            <name>
              <surname>Dalal</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Triggs</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/cvpr.2005.177</pub-id>
          <article-title>Histograms of oriented gradients for human detection</article-title>
          <source>2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="conf-paper">
          <page-range>5525–5533</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Loy</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>Wider face: A face detection benchmark,</article-title>
          <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Ji</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Face R-CNN,</article-title>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="conf-paper">
          <volume>2019</volume>
          <page-range>1-4</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cakiroglu</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Ozer</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Gunsel</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/siu.2019.8806447</pub-id>
          <article-title>Design of a deep face detector by mask R-CNN</article-title>
          <source>2019 27th Signal Processing and Communications Applications Conference (SIU)</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>49</volume>
          <page-range>4017-4028</page-range>
          <issue>11</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Yin</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/tcyb.2018.2859482</pub-id>
          <article-title>Face detection with different scales based on faster R-CNN</article-title>
          <source>IEEE Transactions on Cybernetics</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Zhu</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Luu</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Savvides</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>CMS-RCNN: Contextual multi-scale region-based CNN for unconstrained face detection,</article-title>
          <source>Deep Learning for Biometrics</source>
          <publisher-name>Springer</publisher-name>
          <year>2017</year>
          <page-range>57–79</page-range>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>299</volume>
          <page-range>42-50</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sun</surname>
              <given-names>Xudong</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Pengcheng</given-names>
            </name>
            <name>
              <surname>Hoi</surname>
              <given-names>Steven C.H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neucom.2018.03.030</pub-id>
          <article-title>Face detection using deep learning: An improved faster RCNN approach</article-title>
          <source>Neurocomputing</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="conf-paper">
          <volume>2017</volume>
          <page-range>650-657</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jiang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Learned-Miller</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/fg.2017.82</pub-id>
          <article-title>Face detection with the faster R-CNN</article-title>
          <source>2017 12th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2017)</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <volume>2018</volume>
          <page-range>4165–4170</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Ding</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Gong</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Tao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/smc.2018.00706</pub-id>
          <article-title>Face detection using R-FCN based deformable convolutional networks</article-title>
          <source>2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Ji</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <article-title>Detecting faces using region-based fully convolutional networks,</article-title>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="conf-paper">
          <volume>2021</volume>
          <page-range>8445-8450</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ye</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Tong</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.23919/ccc52363.2021.9550294</pub-id>
          <article-title>Face SSD: A real-time face detector based on SSD</article-title>
          <source>2021 40th Chinese Control Conference (CCC)</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="conf-paper">
          <page-range>951–959</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hu</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Ramanan</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Finding tiny faces,</article-title>
          <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1–9</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Lei</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Faceboxes: A cpu real-time face detector with high accuracy,</article-title>
          <source>Proceedings of the IEEE International Joint Conference on Biometrics (IJCB)</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="conf-paper">
          <page-range>192–201</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Lei</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>S3FD: Single shot scale-invariant face detector,</article-title>
          <source>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="conf-paper">
          <volume>2024</volume>
          <page-range>1-5</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>J</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>S</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>P</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>R V</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>RM</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/iconstem60960.2024.10568713</pub-id>
          <article-title>Deep learning based FPN and MT-CNN face mask detection system</article-title>
          <source>2024 Ninth International Conference on Science Technology Engineering and Mathematics (ICONSTEM)</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="conf-paper">
          <page-range>812–828</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Du</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-030-01240-3_49</pub-id>
          <article-title>Pyramidbox: A context-assisted single shot face detector,</article-title>
          <source>European Conference on Computer Vision (ECCV)</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>380</volume>
          <page-range>180-189</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Jialiang</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Xiongwei</given-names>
            </name>
            <name>
              <surname>Hoi</surname>
              <given-names>Steven C.H.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Jianke</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neucom.2019.10.087</pub-id>
          <article-title>Feature agglomeration networks for single stage face detection</article-title>
          <source>Neurocomputing</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="conf-paper">
          <volume>2015</volume>
          <page-range>5325–5334</page-range>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Brandt</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hua</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/cvpr.2015.7299170</pub-id>
          <article-title>A convolutional neural network cascade for face detection</article-title>
          <source>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>142423-142433</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hoang</surname>
              <given-names>T. M.</given-names>
            </name>
            <name>
              <surname>Nam</surname>
              <given-names>G. P.</given-names>
            </name>
            <name>
              <surname>Cho</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/access.2020.3012660</pub-id>
          <article-title>DEFace: Deep efficient face network for small scale variations</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="conf-paper">
          <volume>2020</volume>
          <page-range>5202–5211</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Deng</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Ververas</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Kotsia</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Zafeiriou</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/cvpr42600.2020.00525</pub-id>
          <article-title>RetinaFace: Single-shot multi-level face localisation in the wild</article-title>
          <source>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="conf-paper">
          <volume>2016</volume>
          <page-range>779-788</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Redmon</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Divvala</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Girshick</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Farhadi</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/cvpr.2016.91</pub-id>
          <article-title>You only look once: Unified, real-time object detection</article-title>
          <source>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jegham</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Koh</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Abdelatti</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hendawi</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>YOLO evolution: A comprehensive benchmark and architectural review of YOLOv12, YOLO11, and their previous versions,</article-title>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>840-855</page-range>
          <issue>3</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zheng</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Photong</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.55214/25768484.v9i3.5356</pub-id>
          <article-title>GCSEM-YOLO small scale enhanced face detector based on YOLO</article-title>
          <source>Edelweiss Applied Science and Technology</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="conf-paper">
          <volume>2022</volume>
          <page-range>333-338</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pebrianto</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Mudjirahardjo</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Pramono</surname>
              <given-names>S. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/eeccis54468.2022.9902919</pub-id>
          <article-title>YOLO method analysis and comparison for real-time human face detection</article-title>
          <source>2022 11th Electrical Power, Electronics, Communications, Controls and Informatics Seminar (EECCIS)</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="conf-paper">
          <volume>2024</volume>
          <page-range>105-109</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sufian Chan</surname>
              <given-names>A. A.</given-names>
            </name>
            <name>
              <surname>L Abdullah</surname>
              <given-names>M.F.</given-names>
            </name>
            <name>
              <surname>Mustam</surname>
              <given-names>S. M.</given-names>
            </name>
            <name>
              <surname>Poad</surname>
              <given-names>F. A.</given-names>
            </name>
            <name>
              <surname>Joret</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/gecost60902.2024.10475115</pub-id>
          <article-title>Face detection with YOLOv7: A comparative study of YOLO-Based face detection models</article-title>
          <source>2024 International Conference on Green Energy, Computing and Sustainable Technology (GECOST)</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="conf-paper">
          <volume>2023</volume>
          <page-range>1-5</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Vemulapalli</surname>
              <given-names>N. S.</given-names>
            </name>
            <name>
              <surname>Paladugula</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Prabhat</surname>
              <given-names>G. S.</given-names>
            </name>
            <name>
              <surname>Abhishek</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>T</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/icefeet59656.2023.10452204</pub-id>
          <article-title>Face detection with landmark using YOLOv8</article-title>
          <source>2023 3rd International Conference on Emerging Frontiers in Electrical and Electronic Technologies (ICEFEET)</source>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <volume>181</volume>
          <page-range>329-337</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gao</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.comcom.2021.10.023</pub-id>
          <article-title>Face detection algorithm based on improved TinyYOLOv3 and attention mechanism</article-title>
          <source>Computer Communications</source>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Qi</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Tan</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>YOLO5Face: Why reinventing a face detector</article-title>
          <source>Lecture Notes in Computer Science</source>
          <publisher-name>Springer Nature Switzerland</publisher-name>
          <year>2023</year>
          <volume>2022</volume>
          <page-range>228-244</page-range>
          <pub-id pub-id-type="doi">10.1007/978-3-031-25072-9_15</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>107-119</page-range>
          <issue>2</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Nashwan Saleh</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Alaa Fares</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Hiba Dhiya</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Mustafa Sabah</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/ijcmem.120201</pub-id>
          <article-title>An effective face detection and recognition model based on improved YOLO v3 and VGG 16 networks</article-title>
          <source>International Journal of Computational Methods and Experimental Measurements</source>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="conf-paper">
          <volume>2022</volume>
          <page-range>966-970</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Akyon</surname>
              <given-names>F. C.</given-names>
            </name>
            <name>
              <surname>Onur Altinuc</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Temizel</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/icip46576.2022.9897990</pub-id>
          <article-title>Slicing aided hyper inference and fine-tuning for small object detection</article-title>
          <source>2022 IEEE International Conference on Image Processing (ICIP)</source>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Al Amoudi</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Ramli</surname>
              <given-names>D. A.</given-names>
            </name>
          </person-group>
          <article-title>YOLOv7-Tiny and YOLOv8n evaluation for face detection</article-title>
          <source>Lecture Notes in Electrical Engineering</source>
          <publisher-name>Springer Nature Singapore</publisher-name>
          <year>2024</year>
          <volume>2021</volume>
          <page-range>477-483</page-range>
          <pub-id pub-id-type="doi">10.1007/978-981-99-9005-4_60</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_35">
        <label>35.</label>
        <element-citation publication-type="conf-paper">
          <volume>2015</volume>
          <page-range>3676–3684</page-range>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Loy</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/iccv.2015.419</pub-id>
          <article-title>From facial parts responses to face detection: A deep learning approach</article-title>
          <source>2015 IEEE International Conference on Computer Vision (ICCV)</source>
        </element-citation>
      </ref>
      <ref id="ref_36">
        <label>36.</label>
        <element-citation publication-type="conf-paper">
          <volume>2017</volume>
          <page-range>424-427</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Xiang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/icisce.2017.95</pub-id>
          <article-title>Joint face detection and facial expression recognition with MTCNN</article-title>
          <source>2017 4th International Conference on Information Science and Control Engineering (ICISCE)</source>
        </element-citation>
      </ref>
      <ref id="ref_37">
        <label>37.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>174922-174930</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/access.2020.3023782</pub-id>
          <article-title>Face detection based on receptive field enhanced multi-task cascaded convolutional neural networks</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>