<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-xE5P1LF1Ws5olss-JKA6mnjdfcubOprw</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml010104</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Enhancing Session-Based Recommendations with Popularity-Aware Graph Neural Networks</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8316-3328</contrib-id>
          <name>
            <surname>Sun</surname>
            <given-names>Qingbo</given-names>
          </name>
          <email>sunqb@sdupsl.edu.cn</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7104-0331</contrib-id>
          <name>
            <surname>Yuan</surname>
            <given-names>Weihua</given-names>
          </name>
          <email>huahua_qingdao@sdjzu.edu.cn</email>
          <xref ref-type="aff" rid="aff_2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-9431-2810</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Qi</given-names>
          </name>
          <email>zq@sdupsl.edu.cn</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9532-7158</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Zhijun</given-names>
          </name>
          <email>zhangzj@sdjzu.edu.cn</email>
          <xref ref-type="aff" rid="aff_2">2</xref>
        </contrib>
        <aff id="aff_1">School of Cyberspace Security, Shandong University of Political Science and Law, 250014 Jinan, China</aff>
        <aff id="aff_2">School of Computer Science and Technology, Shandong Jianzhu University, 250101 Jinan, China</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>19</day>
        <month>11</month>
        <year>2022</year>
      </pub-date>
      <volume>1</volume>
      <issue>1</issue>
      <fpage>22</fpage>
      <lpage>29</lpage>
      <page-range>22-29</page-range>
      <history>
        <date date-type="received">
          <day>22</day>
          <month>07</month>
          <year>2022</year>
        </date>
        <date date-type="accepted">
          <day>19</day>
          <month>09</month>
          <year>2022</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>Â©2022 by the author(s)</copyright-statement>
        <copyright-year>2022</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract>Real-time and reliable recommendations are essential for anonymous users in session-based recommendation systems. Graph neural network-based algorithms are attracting more researchers due to their simplicity and efficiency. However, current methods overlook the influence of edge frequency on feature aggregation in graph modeling and fail to account for the impact of item popularity on user interest. To address these issues, a novel approach called Popularity-Aware Graph Neural Networks for Session-based Recommendations is proposed. This study integrates both edge frequency and item popularity into the modeling process to enhance the learning of item features and user interests. A graph that includes the number of edge occurrences is constructed, and a graph neural network with an attention mechanism is utilized to learn user interests and item features by aggregating information from the graph. Finally, the session's final representation is learned based on the occurrence frequency of items. The proposed study evaluates the model on two classical e-commerce datasets and demonstrates its superiority over existing methods.</abstract>
      <kwd-group>
        <kwd>Session-based recommendation</kwd>
        <kwd>Graph neural networks</kwd>
        <kwd>Popularity-aware modeling</kwd>
        <kwd>Item popularity</kwd>
        <kwd>User interests</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="4"/>
        <fig-count count="2"/>
        <table-count count="2"/>
        <ref-count count="18"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>In the fast-paced digital economy era, finding useful information from massive and complex data is crucial. Recommendation systems have emerged as effective tools to overcome information overload and are widely used in online platforms. Traditional recommendation systems use collaborative filtering to mine users' interests from their historical behaviors and then predict the products they might like. However, long-term historical behaviors may overshadow users' current interests during a session, resulting in poor user experience. To address this issue, session-based recommendation systems have been developed, which aim to predict users' behavior during the current session. Session-based recommendation systems have been widely adopted by various e-commerce platforms, such as T-mall and Amazon.</p><p>In the field of recommendation systems, there are two main categories of traditional session-based recommendation algorithms: collaborative filtering and Markov chain-based algorithms. Collaborative filtering [<xref ref-type="bibr" rid="ref_1">1</xref>] typically utilizes user or item similarity to make recommendations. In contrast, the Markov chain-based recommendation algorithm [<xref ref-type="bibr" rid="ref_2">2</xref>] treats session-based recommendation as a sequence prediction task and predicts the next item based on the sequence relationship between items. However, it is important to note that the Markov chain-based approach faces the challenge of dimension explosion, which makes it difficult to apply in actual production scenarios. This limitation arises because the number of possible item combinations can increase exponentially as the sequence length grows, leading to a computational explosion. As a result, it becomes impractical to compute the probabilities for all possible combinations of items.</p><p>With the development of deep learning technology, many researchers have applied neural network technology in recommendation systems. Several session-based recommendation models based on deep learning have been proposed, such as GRU4REC [<xref ref-type="bibr" rid="ref_3">3</xref>], Tan et al. [<xref ref-type="bibr" rid="ref_4">4</xref>], NARM [<xref ref-type="bibr" rid="ref_5">5</xref>], STAMP [<xref ref-type="bibr" rid="ref_6">6</xref>], SR-GNN [<xref ref-type="bibr" rid="ref_7">7</xref>], HA-GNN [<xref ref-type="bibr" rid="ref_8">8</xref>], TPA-GNN [<xref ref-type="bibr" rid="ref_9">9</xref>], GCE-GNN [<xref ref-type="bibr" rid="ref_10">10</xref>], DIDN [<xref ref-type="bibr" rid="ref_11">11</xref>], and CORE [<xref ref-type="bibr" rid="ref_12">12</xref>]. These models have achieved good performance in recommendations; however, they have some limitations that can be improved.</p><p>GRU4REC uses multi-layer GRU to model the whole session and capture the continuous preferences of users. However, it only captures the session relationship between one-way adjacent items and may not capture long-term dependencies between items.</p><p>NARM combines the attention mechanism with RNN to capture users' interests. However, it also suffers from the limitation of capturing long-term dependencies between items.</p><p>STAMP captures users' long-term interests by using the multi-layer perceptron and the attention mechanism, and combines users' short-term preferences for recommendations. However, it still lacks the ability to capture the influence of popularity on user interest.</p><p>SR-GNN uses graph neural networks to learn the transformation relationship between items and capture the high-order relationship in graphs. However, it still uses the attention mechanism to filter out noise and may not effectively capture the influence of popularity on user interest.</p><p>HA-GNN captures the high-order relationship in graphs by using the attention mechanism. However, it does not explicitly consider the influence of popularity on user interest.</p><p>TPA-GNN combines time series with graph neural networks to capture users' interests. However, it still lacks the ability to effectively capture the influence of popularity on user interest.</p><p>GCE-GNN uses the features of the session graph and the global graph to learn items. However, it also suffers from the limitation of capturing long-term dependencies between items.</p><p>DIDN incorporates dynamic intent-aware and iterative denoising modules to learn dynamic item embeddings and filter out noisy clicks within sessions. However, it still lacks the ability to capture the influence of popularity on user interest.</p><p>CORE unifies the representation space for both the encoding and decoding processes in session-based recommendation to address the issue of inconsistent predictions. However, it also does not explicitly consider the influence of popularity on user interest.</p><p>To address the aforementioned issues, we propose a novel model called the Popularity-Aware Machine for Session-Based Recommendations (PASR). <xref ref-type="fig" rid="fig_1">Figure 1</xref> illustrates the workflow of the proposed PASR method. Firstly, PASR constructs a popularity-aware item graph, which effectively captures users' preferences for popular items. Secondly, PASR aggregates the features of neighboring nodes based on the type and frequency of edges in the graph, which enhances the model's ability to capture the dependencies between items. Finally, popularity embeddings are integrated into the attention mechanism to learn users' interests and improve the accuracy of recommendations. The main contributions of this work are as follows:</p><p>(1) The PASR model uses the number of edge occurrences to learn item features, which captures dependencies between items for the first time.</p><p>(2) The PASR model learns the user's interest by considering the popularity of items to reflect their importance in the session.</p><p>(3) Experiments were conducted on two widely-used datasets (Tmall dataset and Nowplaying dataset). The results of the experiments demonstrate that our model exhibits excellent performance.</p>
    </sec>
    <sec sec-type="">
      <title>2. Model</title>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Process of PASR</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/3/img_VTxk2PM--cJEteYG.png"/>
        </fig>
      
      <p>A. Problem description</p><p>In a session-based recommendation system, we define the item set <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>V</mi>
    <mo>=</mo>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">{</mo>
      <mo>,</mo>
      <mo>,</mo>
      <mo>â¦</mo>
      <mo>,</mo>
      <mo data-mjx-texclass="CLOSE">}</mo>
      <msub>
        <mi>v</mi>
        <mn>1</mn>
      </msub>
      <msub>
        <mi>v</mi>
        <mn>2</mn>
      </msub>
      <msub>
        <mi>v</mi>
        <mi>m</mi>
      </msub>
    </mrow>
  </math>
</inline-formula>, where m is the total number of items, and the session sequence <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>s</mi>
    <mo>=</mo>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">[</mo>
      <mo>,</mo>
      <mo>,</mo>
      <mo>â¦</mo>
      <mo>,</mo>
      <mo data-mjx-texclass="CLOSE">]</mo>
      <msub>
        <mi>v</mi>
        <mn>1</mn>
      </msub>
      <msub>
        <mi>v</mi>
        <mn>2</mn>
      </msub>
      <msub>
        <mi>v</mi>
        <mi>t</mi>
      </msub>
    </mrow>
  </math>
</inline-formula>, where t is the length of the session. The primary objective of a session-based recommendation system is to predict the user's next click behavior based on the item sequence. For example, the next clicked item <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>v</mi>
      <mrow data-mjx-texclass="ORD">
        <mi>t</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
  </math>
</inline-formula> is predicted according to <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>s</mi>
    <mo>=</mo>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">[</mo>
      <mo>,</mo>
      <mo>,</mo>
      <mo>â¦</mo>
      <mo>,</mo>
      <mo data-mjx-texclass="CLOSE">]</mo>
      <msub>
        <mi>v</mi>
        <mn>1</mn>
      </msub>
      <msub>
        <mi>v</mi>
        <mn>2</mn>
      </msub>
      <msub>
        <mi>v</mi>
        <mi>t</mi>
      </msub>
    </mrow>
  </math>
</inline-formula>, the session-based recommendation model outputs n candidate items that may interact according to the current interaction sequence.</p><p>B. Graph construction</p><p>To aggregate the representations of nodes in the graph, the session sequence is transformed into a graph using a graph neural network. For any given session sequence <italic>s</italic>, a directed graph can be constructed <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>G</mi>
      <mi>s</mi>
    </msub>
    <mo>=</mo>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">{</mo>
      <mo>,</mo>
      <mo data-mjx-texclass="CLOSE">}</mo>
      <msub>
        <mi>V</mi>
        <mi>s</mi>
      </msub>
      <msub>
        <mi>E</mi>
        <mi>s</mi>
      </msub>
    </mrow>
  </math>
</inline-formula>, where <italic>Vs </italic>represents the node set of the session, <italic>E<sub>s</sub></italic> represents the edge set of session <italic>s</italic> , which includes item <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>v</mi>
      <mrow data-mjx-texclass="ORD">
        <mi>n</mi>
        <mo>â</mo>
        <mn>1</mn>
      </mrow>
    </msub>
  </math>
</inline-formula> and item <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>v</mi>
      <mi>n</mi>
    </msub>
  </math>
</inline-formula>, and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">{</mo>
      <mo>,</mo>
      <mo data-mjx-texclass="CLOSE">}</mo>
      <msub>
        <mi>v</mi>
        <mrow data-mjx-texclass="ORD">
          <mi>n</mi>
          <mo>â</mo>
          <mn>1</mn>
        </mrow>
      </msub>
      <msub>
        <mi>v</mi>
        <mi>n</mi>
      </msub>
    </mrow>
    <mo>â</mo>
    <msub>
      <mi>E</mi>
      <mi>S</mi>
    </msub>
  </math>
</inline-formula>.</p><p>C. Graph aggregator</p><p>For session <italic>s</italic>, we defines the embedding of each item as <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>H</mi>
    <mo>=</mo>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">{</mo>
      <mo>,</mo>
      <mo>,</mo>
      <mo>â¦</mo>
      <mo>,</mo>
      <mo data-mjx-texclass="CLOSE">}</mo>
      <msub>
        <mi>h</mi>
        <mrow data-mjx-texclass="ORD">
          <msub>
            <mi>v</mi>
            <mn>1</mn>
          </msub>
        </mrow>
      </msub>
      <msub>
        <mi>h</mi>
        <mrow data-mjx-texclass="ORD">
          <msub>
            <mi>v</mi>
            <mn>2</mn>
          </msub>
        </mrow>
      </msub>
      <msub>
        <mi>h</mi>
        <mrow data-mjx-texclass="ORD">
          <msub>
            <mi>v</mi>
            <mi>m</mi>
          </msub>
        </mrow>
      </msub>
    </mrow>
  </math>
</inline-formula>, where <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>h</mi>
      <mrow data-mjx-texclass="ORD">
        <msub>
          <mi>v</mi>
          <mi>i</mi>
        </msub>
      </mrow>
    </msub>
  </math>
</inline-formula> refers to the unique hot code of the item <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>v</mi>
      <mi>i</mi>
    </msub>
    <mo stretchy="false">(</mo>
    <mo>â¤</mo>
    <mo>â¤</mo>
    <mo stretchy="false">)</mo>
    <mn>1</mn>
    <mi>i</mi>
    <mi>t</mi>
  </math>
</inline-formula>, <italic>m</italic> is the number of unique items in session <italic>s</italic>. Many works [<xref ref-type="bibr" rid="ref_13">13</xref>], [<xref ref-type="bibr" rid="ref_14">14</xref>] have proved that self-loops in graphs are beneficial to feature learning. We add self-connections for each node in the graph. For any item v<sub>n</sub>, there are 4 different types of edges:</p><p>e<sub>self</sub>: It represents the self-connection of the item.</p><p>e<sub>out</sub>: It represents the edge from item v<sub>n</sub> to item <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">V</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">n</mi>
        </mrow>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
  </math>
</inline-formula>.</p><p>e<sub>in</sub>: It represents the edge from item v<sub>n</sub> to item <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>V</mi>
      <mrow data-mjx-texclass="ORD">
        <mi>n</mi>
        <mo>â</mo>
        <mn>1</mn>
      </mrow>
    </msub>
  </math>
</inline-formula>.</p><p>e<sub>in-out</sub>: It indicates that there are edges from item <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">V</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">n</mi>
        </mrow>
        <mo>+</mo>
        <mn>1</mn>
      </mrow>
    </msub>
  </math>
</inline-formula> to item v<sub>n</sub> and from item v<sub>n</sub> to item <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>V</mi>
      <mrow data-mjx-texclass="ORD">
        <mi>n</mi>
        <mo>â</mo>
        <mn>1</mn>
      </mrow>
    </msub>
  </math>
</inline-formula>.</p><p>Constructing a graph-based model with these edges allows the model to capture both the local and global structure of the session and learn features that are specific to each item, as well as the relationships between items. This, in turn, can lead to more accurate and relevant recommendations for the user.</p><p>Moreover, different edges appear in the dataset with varying frequencies, and frequently occurring edges often indicate common browsing habits. By segmenting the appearance times of edges and training different weight vectors for each segment, we can effectively capture the influence of edge frequency on feature aggregation in graph modeling. Specifically, we divide the number of occurrences of edges by a multiple of 10, and consider edges with more than 100 occurrences as a rare interval. By incorporating edge frequency into the weight vector training process, we can improve the accuracy and effectiveness of the PASR model in session-based recommendation tasks.</p><p>In the graph, the importance of item neighbors is reflected in the weight of edges. We believe that the weight is influenced by the type and number of occurrences of edges. Therefore, we define the following function to aggregate the features of neighbors:</p>
      
        <disp-formula>
          <label>(1)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <msub>
              <mi>X</mi>
              <mrow data-mjx-texclass="ORD">
                <msub>
                  <mi>v</mi>
                  <mi>i</mi>
                </msub>
              </mrow>
            </msub>
            <mo>=</mo>
            <mi>Agg</mi>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">(</mo>
              <mo>,</mo>
              <mo>,</mo>
              <mo>,</mo>
              <mo data-mjx-texclass="CLOSE">)</mo>
              <msub>
                <mi>h</mi>
                <mrow data-mjx-texclass="ORD">
                  <msub>
                    <mi>v</mi>
                    <mi>i</mi>
                  </msub>
                </mrow>
              </msub>
              <msub>
                <mi>h</mi>
                <mrow data-mjx-texclass="ORD">
                  <msub>
                    <mi>v</mi>
                    <mi>i</mi>
                  </msub>
                </mrow>
              </msub>
              <msup>
                <mi>e</mi>
                <mrow data-mjx-texclass="ORD">
                  <mtext>typeÂ </mtext>
                </mrow>
              </msup>
              <msup>
                <mi>e</mi>
                <mrow data-mjx-texclass="ORD">
                  <mtext>numÂ </mtext>
                </mrow>
              </msup>
            </mrow>
          </math>
        </disp-formula>
      
      <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>h</mi>
      <mrow data-mjx-texclass="ORD">
        <msub>
          <mi>v</mi>
          <mi>i</mi>
        </msub>
      </mrow>
    </msub>
  </math>
</inline-formula> represents the features of the target node, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>h</mi>
      <mrow data-mjx-texclass="ORD">
        <msub>
          <mi>v</mi>
          <mi>j</mi>
        </msub>
      </mrow>
    </msub>
  </math>
</inline-formula> represents the features of the neighbor, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msup>
      <mi>e</mi>
      <mrow data-mjx-texclass="ORD">
        <mtext>typeÂ </mtext>
      </mrow>
    </msup>
  </math>
</inline-formula> indicates the type of edge, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msup>
      <mi>e</mi>
      <mrow data-mjx-texclass="ORD">
        <mtext>numÂ </mtext>
      </mrow>
    </msup>
  </math>
</inline-formula> represents the interval that the number of occurrences of the edge.</p><p>To capture the different types and frequencies of edges in the graph, we use a learnable embedding matrix <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msup>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">E</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mtext>typeÂ </mtext>
      </mrow>
    </msup>
    <mo>=</mo>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">[</mo>
      <mo>,</mo>
      <mo>,</mo>
      <mo>,</mo>
      <mo data-mjx-texclass="CLOSE">]</mo>
      <msubsup>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">a</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mtext>typeÂ </mtext>
        </mrow>
        <mn>1</mn>
      </msubsup>
      <msubsup>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">a</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mtext>typeÂ </mtext>
        </mrow>
        <mn>2</mn>
      </msubsup>
      <msubsup>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">a</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mtext>typeÂ </mtext>
        </mrow>
        <mn>3</mn>
      </msubsup>
      <msubsup>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">a</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mtext>typeÂ </mtext>
        </mrow>
        <mn>4</mn>
      </msubsup>
    </mrow>
  </math>
</inline-formula>, where each element represents the features of <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">e</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mtext>selfÂ </mtext>
      </mrow>
    </msub>
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">e</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mtext>outÂ </mtext>
      </mrow>
    </msub>
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">e</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mrow data-mjx-texclass="ORD">
          <mi data-mjx-auto-op="false">in</mi>
        </mrow>
      </mrow>
    </msub>
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">e</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mtext>in-outÂ </mtext>
      </mrow>
    </msub>
    <mo>,</mo>
    <mo>,</mo>
    <mo>,</mo>
  </math>
</inline-formula>. In addition, we use the learnable embedding matrix <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msup>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">E</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mtext>numÂ </mtext>
      </mrow>
    </msup>
    <mo>=</mo>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">[</mo>
      <mo>,</mo>
      <mo>,</mo>
      <mo>â¦</mo>
      <mo>,</mo>
      <mo data-mjx-texclass="CLOSE">]</mo>
      <msubsup>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">a</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mtext>numÂ </mtext>
        </mrow>
        <mn>1</mn>
      </msubsup>
      <msubsup>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">a</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mtext>numÂ </mtext>
        </mrow>
        <mn>2</mn>
      </msubsup>
      <msubsup>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">a</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mtext>numÂ </mtext>
        </mrow>
        <mi>n</mi>
      </msubsup>
    </mrow>
  </math>
</inline-formula>, where a<sub>i</sub> represents the features of the edge in partition i. To capture the dependencies between items and improve the accuracy of recommendations, the PASR model learns to assign appropriate weights to each edge based on its type and frequency, by utilizing the aforementioned embeddings. We use the following method to learn the edge weights:</p>
      
        <disp-formula>
          <label>(2)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <msub>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="normal">e</mi>
              </mrow>
              <mrow data-mjx-texclass="ORD">
                <mrow data-mjx-texclass="ORD">
                  <mi data-mjx-auto-op="false">ij</mi>
                </mrow>
              </mrow>
            </msub>
            <mo>=</mo>
            <mi>Î³</mi>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">(</mo>
              <mo>+</mo>
              <mo>+</mo>
              <mo data-mjx-texclass="CLOSE">)</mo>
              <msub>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">w</mi>
                </mrow>
                <mn>1</mn>
              </msub>
              <msub>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">b</mi>
                </mrow>
                <mn>1</mn>
              </msub>
              <msubsup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">a</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mrow data-mjx-texclass="ORD">
                    <mi data-mjx-auto-op="false">ij</mi>
                  </mrow>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mtext>typeÂ </mtext>
                </mrow>
              </msubsup>
              <msubsup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">a</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mrow data-mjx-texclass="ORD">
                    <mi data-mjx-auto-op="false">ij</mi>
                  </mrow>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mtext>numÂ </mtext>
                </mrow>
              </msubsup>
            </mrow>
          </math>
        </disp-formula>
      
      
        <disp-formula>
          <label>(3)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <msubsup>
              <mi>e</mi>
              <mrow data-mjx-texclass="ORD">
                <mi>i</mi>
                <mi>j</mi>
              </mrow>
              <mrow data-mjx-texclass="ORD">
                <mi data-mjx-alternate="1">â²</mi>
              </mrow>
            </msubsup>
            <mo>=</mo>
            <mfrac>
              <mrow>
                <mi>exp</mi>
                <mo data-mjx-texclass="NONE">â¡</mo>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <msub>
                    <mi>e</mi>
                    <mrow data-mjx-texclass="ORD">
                      <mi>i</mi>
                      <mi>j</mi>
                    </mrow>
                  </msub>
                </mrow>
              </mrow>
              <mrow>
                <munder>
                  <mo data-mjx-texclass="OP">â</mo>
                  <mrow data-mjx-texclass="ORD">
                    <msub>
                      <mi>v</mi>
                      <mi>k</mi>
                    </msub>
                    <mo>â</mo>
                    <msubsup>
                      <mi>N</mi>
                      <mi>s</mi>
                      <mrow data-mjx-texclass="ORD">
                        <msub>
                          <mi>v</mi>
                          <mi>i</mi>
                        </msub>
                      </mrow>
                    </msubsup>
                  </mrow>
                </munder>
                <mi>exp</mi>
                <mo data-mjx-texclass="NONE">â¡</mo>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <msub>
                    <mi>e</mi>
                    <mrow data-mjx-texclass="ORD">
                      <mi>i</mi>
                      <mi>k</mi>
                    </mrow>
                  </msub>
                </mrow>
              </mrow>
            </mfrac>
          </math>
        </disp-formula>
      
      <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">W</mi>
      </mrow>
      <mn>1</mn>
    </msub>
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mtext>Â </mtext>
        <mi mathvariant="normal">W</mi>
      </mrow>
      <mn>2</mn>
    </msub>
    <mo>,</mo>
    <mo>â</mo>
    <msup>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">R</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">d</mi>
        </mrow>
      </mrow>
    </msup>
  </math>
</inline-formula> are weight, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">b</mi>
      </mrow>
      <mn>1</mn>
    </msub>
    <mo>â</mo>
    <msup>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">R</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">d</mi>
        </mrow>
      </mrow>
    </msup>
  </math>
</inline-formula> is a bias, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msup>
      <mi>a</mi>
      <mrow data-mjx-texclass="ORD">
        <mtext>typeÂ </mtext>
      </mrow>
    </msup>
    <msup>
      <mi>a</mi>
      <mrow data-mjx-texclass="ORD">
        <mtext>numÂ </mtext>
      </mrow>
    </msup>
    <msup>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">R</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">d</mi>
        </mrow>
      </mrow>
    </msup>
    <mo>,</mo>
    <mo>â</mo>
  </math>
</inline-formula> are features of edges, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>Î³</mi>
  </math>
</inline-formula> is sigmoid or tanh activation function. <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">e</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mrow data-mjx-texclass="ORD">
          <mi data-mjx-auto-op="false">ij</mi>
        </mrow>
      </mrow>
    </msub>
  </math>
</inline-formula> is the weight of the learned edge. <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msubsup>
      <mi>N</mi>
      <mi>s</mi>
      <mrow data-mjx-texclass="ORD">
        <msub>
          <mi>v</mi>
          <mi>i</mi>
        </msub>
      </mrow>
    </msubsup>
    <mo>=</mo>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">{</mo>
      <mo>â£</mo>
      <mo>,</mo>
      <mo>â</mo>
      <mo>;</mo>
      <mo>=</mo>
      <mo>Â±</mo>
      <mo data-mjx-texclass="CLOSE">}</mo>
      <msub>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">v</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mrow data-mjx-texclass="ORD">
            <mi mathvariant="normal">j</mi>
          </mrow>
        </mrow>
      </msub>
      <msub>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">v</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mrow data-mjx-texclass="ORD">
            <mi mathvariant="normal">i</mi>
          </mrow>
        </mrow>
      </msub>
      <msub>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">v</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mrow data-mjx-texclass="ORD">
            <mi mathvariant="normal">j</mi>
          </mrow>
        </mrow>
      </msub>
      <msub>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">V</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mrow data-mjx-texclass="ORD">
            <mi mathvariant="normal">s</mi>
          </mrow>
        </mrow>
      </msub>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">j</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">i</mi>
      </mrow>
      <mn>1</mn>
    </mrow>
  </math>
</inline-formula> represents the neighbor set of item v<sub>i</sub>.</p><p>Finally, the features of each node can be updated to:</p>
      
        <disp-formula>
          <label>(4)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <msub>
              <mi>H</mi>
              <mi>i</mi>
            </msub>
            <msub>
              <mi>X</mi>
              <mrow data-mjx-texclass="ORD">
                <msub>
                  <mi>v</mi>
                  <mi>j</mi>
                </msub>
              </mrow>
            </msub>
            <mo>=</mo>
            <munder>
              <mo data-mjx-texclass="OP">â</mo>
              <mrow data-mjx-texclass="ORD">
                <msub>
                  <mi>v</mi>
                  <mi>j</mi>
                </msub>
                <mo>â</mo>
                <msubsup>
                  <mi>N</mi>
                  <mi>s</mi>
                  <mrow data-mjx-texclass="ORD">
                    <msub>
                      <mi>v</mi>
                      <mi>i</mi>
                    </msub>
                  </mrow>
                </msubsup>
              </mrow>
            </munder>
            <msubsup>
              <mi>e</mi>
              <mrow data-mjx-texclass="ORD">
                <mi>i</mi>
                <mi>j</mi>
              </mrow>
              <mrow data-mjx-texclass="ORD">
                <mi data-mjx-alternate="1">â²</mi>
              </mrow>
            </msubsup>
          </math>
        </disp-formula>
      
      <p>D. Interest encoder</p><p>To learn the user's interest based on the item representation learned by the graph aggregator, we use an attention mechanism. Unlike previous models, we focus on the popularity of each item to better understand the contribution of each item to the user's interest. To achieve this, we incorporate a learnable popularity embedding matrix <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>P</mi>
      <mi>e</mi>
    </msub>
    <mo>=</mo>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">[</mo>
      <mo>,</mo>
      <mo>,</mo>
      <mo>â¦</mo>
      <mo>,</mo>
      <mo data-mjx-texclass="CLOSE">]</mo>
      <msub>
        <mi>p</mi>
        <mn>1</mn>
      </msub>
      <msub>
        <mi>p</mi>
        <mn>2</mn>
      </msub>
      <msub>
        <mi>p</mi>
        <mi>n</mi>
      </msub>
    </mrow>
  </math>
</inline-formula>, where <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>e</mi>
      <mi>i</mi>
    </msub>
    <mo>â</mo>
    <msup>
      <mi>R</mi>
      <mi>d</mi>
    </msup>
  </math>
</inline-formula> represents the popularity embedding of item <italic>i</italic>, based on the number of occurrences.</p><p>By incorporating popularity into the attention mechanism, the model can better capture the diversity of user interests and make more accurate recommendations. We integrate the popularity vector into the calculation process of the attention mechanism, as follows:</p>
      
        <disp-formula>
          <label>(5)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <msub>
              <mi>Î±</mi>
              <mi>i</mi>
            </msub>
            <mo>=</mo>
            <msup>
              <mi>Q</mi>
              <mi>T</mi>
            </msup>
            <mi>Ï</mi>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">(</mo>
              <mo>+</mo>
              <mo>+</mo>
              <mo>+</mo>
              <mo data-mjx-texclass="CLOSE">)</mo>
              <mi>w</mi>
              <mi>w</mi>
              <mi>b</mi>
              <msub>
                <mi>X</mi>
                <mrow data-mjx-texclass="ORD">
                  <msub>
                    <mi>v</mi>
                    <mi>i</mi>
                  </msub>
                </mrow>
              </msub>
              <msub>
                <mi>X</mi>
                <mi>a</mi>
              </msub>
              <msub>
                <mi>p</mi>
                <mn>1</mn>
              </msub>
            </mrow>
          </math>
        </disp-formula>
      
      
        <disp-formula>
          <label>(6)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <msub>
              <mi>X</mi>
              <mi>a</mi>
            </msub>
            <msub>
              <mi>X</mi>
              <mrow data-mjx-texclass="ORD">
                <msub>
                  <mi>v</mi>
                  <mi>i</mi>
                </msub>
              </mrow>
            </msub>
            <mo>=</mo>
            <mfrac>
              <mn>1</mn>
              <mi>n</mi>
            </mfrac>
            <munderover>
              <mo data-mjx-texclass="OP">â</mo>
              <mrow data-mjx-texclass="ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>n</mi>
            </munderover>
          </math>
        </disp-formula>
      
      
        <disp-formula>
          <label>(7)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <msup>
              <mi>H</mi>
              <mi>s</mi>
            </msup>
            <mo>=</mo>
            <munderover>
              <mo data-mjx-texclass="OP">â</mo>
              <mrow data-mjx-texclass="ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>l</mi>
            </munderover>
            <msub>
              <mi>Î±</mi>
              <mi>i</mi>
            </msub>
            <msub>
              <mi>X</mi>
              <mrow data-mjx-texclass="ORD">
                <msub>
                  <mi>v</mi>
                  <mi>i</mi>
                </msub>
              </mrow>
            </msub>
          </math>
        </disp-formula>
      
      <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>Q</mi>
    <mo>â</mo>
    <msup>
      <mi>R</mi>
      <mi>d</mi>
    </msup>
  </math>
</inline-formula>, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>Ï</mi>
  </math>
</inline-formula> is activation function, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>W</mi>
      <mn>2</mn>
    </msub>
    <msub>
      <mi>W</mi>
      <mn>3</mn>
    </msub>
    <mo>â</mo>
    <mo>,</mo>
    <mo>â</mo>
    <msup>
      <mi>R</mi>
      <mrow data-mjx-texclass="ORD">
        <mi>d</mi>
        <mi>d</mi>
        <mo>Ã</mo>
      </mrow>
    </msup>
    <msup>
      <mi>R</mi>
      <mrow data-mjx-texclass="ORD">
        <mi>d</mi>
        <mi>d</mi>
        <mo>Ã</mo>
        <mn>2</mn>
      </mrow>
    </msup>
  </math>
</inline-formula> are weight matrix, <italic>h<sub>a </sub></italic>is the average embedded in the session.</p><p>E. Prediction layer</p><p>Once the user's interest <italic>H<sup>s</sup></italic> has been obtained, the model calculates the dot product of the embedding and the interest of each candidate item. Subsequently, the softmax normalization is applied to obtain the probability of the user clicking on each item next time.</p>
      
        <disp-formula>
          <label>(8)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <msub>
              <mrow data-mjx-texclass="ORD">
                <mover>
                  <mi>y</mi>
                  <mo stretchy="false">^</mo>
                </mover>
              </mrow>
              <mi>i</mi>
            </msub>
            <mo>=</mo>
            <mi>sofmax</mi>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">(</mo>
              <mo data-mjx-texclass="CLOSE">)</mo>
              <msubsup>
                <mi>h</mi>
                <mi>s</mi>
                <mi>T</mi>
              </msubsup>
              <msub>
                <mi>h</mi>
                <mrow data-mjx-texclass="ORD">
                  <msub>
                    <mi>v</mi>
                    <mi>i</mi>
                  </msub>
                </mrow>
              </msub>
            </mrow>
          </math>
        </disp-formula>
      
      <p>To learn the parameters of the model, the cross-entropy loss function is used, and the backpropagation algorithm is applied to train the model.</p>
      
        <disp-formula>
          <label>(9)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>L</mi>
            <mi>log</mi>
            <mi>log</mi>
            <mo stretchy="false">(</mo>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mo>â</mo>
            <mo data-mjx-texclass="NONE">â¡</mo>
            <mo>+</mo>
            <mo data-mjx-texclass="NONE">â¡</mo>
            <mrow data-mjx-texclass="ORD">
              <mover>
                <mi>y</mi>
                <mo stretchy="false">^</mo>
              </mover>
            </mrow>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">(</mo>
              <mo data-mjx-texclass="CLOSE">)</mo>
              <msub>
                <mrow data-mjx-texclass="ORD">
                  <mover>
                    <mi>y</mi>
                    <mo stretchy="false">^</mo>
                  </mover>
                </mrow>
                <mi>i</mi>
              </msub>
            </mrow>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">(</mo>
              <mo>â</mo>
              <mo data-mjx-texclass="CLOSE">)</mo>
              <mn>1</mn>
              <msub>
                <mi>y</mi>
                <mi>i</mi>
              </msub>
            </mrow>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">(</mo>
              <mo>â</mo>
              <mo data-mjx-texclass="CLOSE">)</mo>
              <mn>1</mn>
              <msub>
                <mrow data-mjx-texclass="ORD">
                  <mover>
                    <mi>y</mi>
                    <mo stretchy="false">^</mo>
                  </mover>
                </mrow>
                <mi>i</mi>
              </msub>
            </mrow>
            <munderover>
              <mo data-mjx-texclass="OP">â</mo>
              <mrow data-mjx-texclass="ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>n</mi>
            </munderover>
            <msub>
              <mi>y</mi>
              <mi>i</mi>
            </msub>
          </math>
        </disp-formula>
      
      <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>y</mi>
      <mi>i</mi>
    </msub>
  </math>
</inline-formula> is the unique hot code of the session tag item.</p>
    </sec>
    <sec sec-type="">
      <title>3. Experiments</title>
      <p>The experiment aims to demonstrate the effectiveness of the PASR model by addressing the following two questions:</p><p>Q1: Does the PASR model outperform the latest baselines?</p><p>Q2: Is the popularity-aware mechanism of the PASR model effective?</p><p>A. Experimental setup</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Statistics of the datasets</title>
          </caption>
          <table><tbody><tr><td><p>Dataset</p></td><td><p>Tmall</p></td><td><p>Nowplaying</p></td></tr><tr><td><p># click</p></td><td><p>818,479</p></td><td><p>1,367,963</p></td></tr><tr><td><p># train</p></td><td><p>351,268</p></td><td><p>825,304</p></td></tr><tr><td><p># test</p></td><td><p>25,898</p></td><td><p>89,824</p></td></tr><tr><td><p># items</p></td><td><p>40,728</p></td><td><p>60,417</p></td></tr><tr><td><p>avg. len.</p></td><td><p>6.69</p></td><td><p>7.42</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>Dataset: We employ two classic e-commerce datasets, namely the Tmall (https://tianchi.aliyun.com/dataset/dataDetail?dataId=42) and Nowplaying (http://dbis-nowplaying.uibk.ac.at/#nowplaying) datasets. To ensure the validity of the model, we adopt the same processing method as [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>]. Prior to the experiment, we filter out sessions with a length of 2, or items that appear less than 5 times. We also filter out sessions with more than 20 reverse positions, as long-distance projects have little impact on the prediction results of the model but can slow down the running speed of the model.</p><p>For the Tmall dataset, we use click data from the last 100 seconds as the test set, and the remaining data as the training set. For the Nowplaying dataset, we use data from the last two months as the test set, and the remaining data as the training set. Additionally, we employ data augmentation techniques by generating a series of sessions and corresponding tags for the session <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>s</mi>
    <mo>=</mo>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">[</mo>
      <mo>,</mo>
      <mo>,</mo>
      <mo>â¦</mo>
      <mo data-mjx-texclass="CLOSE">]</mo>
      <msub>
        <mi>v</mi>
        <mn>1</mn>
      </msub>
      <msub>
        <mi>v</mi>
        <mn>2</mn>
      </msub>
      <msub>
        <mi>v</mi>
        <mi>l</mi>
      </msub>
    </mrow>
  </math>
</inline-formula>. These include <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">(</mo>
      <mo>,</mo>
      <mo data-mjx-texclass="CLOSE">)</mo>
      <mrow data-mjx-texclass="INNER">
        <mo data-mjx-texclass="OPEN">[</mo>
        <mo>,</mo>
        <mo>,</mo>
        <mo>â¦</mo>
        <mo>,</mo>
        <mo data-mjx-texclass="CLOSE">]</mo>
        <msub>
          <mi>v</mi>
          <mn>1</mn>
        </msub>
        <msub>
          <mi>v</mi>
          <mn>2</mn>
        </msub>
        <msub>
          <mi>v</mi>
          <mrow data-mjx-texclass="ORD">
            <mi>l</mi>
            <mo>â</mo>
            <mn>1</mn>
          </mrow>
        </msub>
      </mrow>
      <msub>
        <mi>v</mi>
        <mi>l</mi>
      </msub>
    </mrow>
  </math>
</inline-formula>, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">(</mo>
      <mo>,</mo>
      <mo>,</mo>
      <mo>â¦</mo>
      <mo>,</mo>
      <mo data-mjx-texclass="CLOSE" fence="true" stretchy="true" symmetric="true"/>
      <mrow data-mjx-texclass="INNER">
        <mo data-mjx-texclass="OPEN">[</mo>
        <mo>,</mo>
        <mo>,</mo>
        <mo>â¦</mo>
        <mo>,</mo>
        <mo data-mjx-texclass="CLOSE">]</mo>
        <msub>
          <mi>v</mi>
          <mn>1</mn>
        </msub>
        <msub>
          <mi>v</mi>
          <mn>2</mn>
        </msub>
        <msub>
          <mi>v</mi>
          <mrow data-mjx-texclass="ORD">
            <mi>l</mi>
            <mo>â</mo>
            <mn>2</mn>
          </mrow>
        </msub>
      </mrow>
      <mrow data-mjx-texclass="INNER">
        <mo data-mjx-texclass="OPEN">(</mo>
        <mo>,</mo>
        <mo data-mjx-texclass="CLOSE">)</mo>
        <mrow data-mjx-texclass="INNER">
          <mo data-mjx-texclass="OPEN">[</mo>
          <mo data-mjx-texclass="CLOSE">]</mo>
          <msub>
            <mi>v</mi>
            <mn>1</mn>
          </msub>
        </mrow>
        <msub>
          <mi>v</mi>
          <mn>2</mn>
        </msub>
      </mrow>
      <msub>
        <mi>v</mi>
        <mrow data-mjx-texclass="ORD">
          <mi>l</mi>
          <mo>â</mo>
          <mn>1</mn>
        </mrow>
      </msub>
    </mrow>
  </math>
</inline-formula>. Such methods expand the training data and ensure the model's parameters are sufficiently trained.</p><p>The statistics of the datasets after preprocessing are summarized in <xref ref-type="table" rid="table_1">Table 1</xref>.</p><p>Consistent with previous work, we adopt two commonly used evaluation indicators: P@20 and MRR@20.</p><p>P@20 measures the proportion of recommended items that are predicted correctly among the top 20 recommendation results. Its calculation formula is:</p>
      
        <disp-formula>
          <label>(10)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>P</mi>
            <mrow data-mjx-texclass="ORD">
              <mo>@</mo>
            </mrow>
            <mn>20</mn>
            <mo>=</mo>
            <mfrac>
              <mn>1</mn>
              <mi>M</mi>
            </mfrac>
            <munderover>
              <mo data-mjx-texclass="OP">â</mo>
              <mrow data-mjx-texclass="ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>M</mi>
            </munderover>
            <msub>
              <mi>y</mi>
              <mi>i</mi>
            </msub>
          </math>
        </disp-formula>
      
      <p>The evaluation metric MRR@20 is defined as the reciprocal of the position of the highest ranked correct recommended item in the top 20 recommendation results.</p><p>where, <italic>M</italic> is the number of items that the user is actually interested in among the top 20 recommended items.</p><p>MRR@20 is calculated using the following formula:</p>
      
        <disp-formula>
          <label>(11)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mtable displaystyle="true" columnspacing="1em" rowspacing="3pt">
              <mtr>
                <mtd>
                  <mi>M</mi>
                  <mi>R</mi>
                  <mi>R</mi>
                  <mi>Reci</mi>
                  <mi>i</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mo>@</mo>
                  </mrow>
                  <mn>20</mn>
                  <mo>=</mo>
                  <mo stretchy="false">(</mo>
                  <mo stretchy="false">)</mo>
                  <mfrac>
                    <mn>1</mn>
                    <mi>M</mi>
                  </mfrac>
                  <munderover>
                    <mo data-mjx-texclass="OP">â</mo>
                    <mrow data-mjx-texclass="ORD">
                      <mi>i</mi>
                      <mo>=</mo>
                      <mn>1</mn>
                    </mrow>
                    <mi>M</mi>
                  </munderover>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mi>Reci</mi>
                  <mi>i</mi>
                  <mo stretchy="false">(</mo>
                  <mo stretchy="false">)</mo>
                  <mo>=</mo>
                  <mrow data-mjx-texclass="INNER">
                    <mo data-mjx-texclass="OPEN">{</mo>
                    <mo data-mjx-texclass="CLOSE" fence="true" stretchy="true" symmetric="true"/>
                    <mtable columnspacing="1em" rowspacing="4pt">
                      <mtr>
                        <mtd>
                          <mfrac>
                            <mn>1</mn>
                            <mrow>
                              <mi>Rank</mi>
                              <mi>i</mi>
                              <mo stretchy="false">(</mo>
                              <mo stretchy="false">)</mo>
                            </mrow>
                          </mfrac>
                          <mo>,</mo>
                          <mo stretchy="false">(</mo>
                          <mo stretchy="false">)</mo>
                          <mo>â¤</mo>
                          <mi>Rank</mi>
                          <mi>i</mi>
                          <mn>20</mn>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd>
                          <mn>0</mn>
                          <mn>20</mn>
                          <mo>,</mo>
                          <mo stretchy="false">(</mo>
                          <mo stretchy="false">)</mo>
                          <mo>&gt;</mo>
                          <mi>Rank</mi>
                          <mi>i</mi>
                        </mtd>
                      </mtr>
                    </mtable>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
          </math>
        </disp-formula>
      
      <p>where, Rank(i) represents the order of labels in session i, and Reci(i) represents the reciprocal of the rank. Reci(i) is assigned a value of 0 if the rank is greater than 20.</p><p>These two indicators respectively reflect the accuracy of the model and the ranking of the tags in the candidate items. The larger the value of two indicators, the better the recommendation performance.</p><p>Parameters: We set the dimension of the hidden layer to 100 and the number of multiple attention heads to 5. The minimum batch size is set to 100. All weight matrices and embedding layers are initialized with a Gaussian distribution with a mean of 0 and a variance of 0.01. The initial value of all biases is set to 0.</p><p>To optimize the model, we use the Adam optimizer with an initial learning rate of 0.001 and an attenuation value of 1 every three epochs. The L2 penalty item is set to 10<sup>-5</sup>. These parameters are chosen based on previous studies and empirical experiments to achieve the best performance of the PASR model on the given datasets.</p><p>B. Baselines model</p><p>To evaluate the performance of the proposed PASR model, we compare it with the following latest baseline models:</p><p>(1) POP: This model recommends the most popular items in the training set.</p><p>(2) Item-KNN [<xref ref-type="bibr" rid="ref_15">15</xref>]: This model uses cosine similarity between items for recommendation.</p><p>(3) FMPC [<xref ref-type="bibr" rid="ref_16">16</xref>]: In this model, the Markov chain is used for recommendation.</p><p>(4) GRU4REC [<xref ref-type="bibr" rid="ref_17">17</xref>]: This model uses GRU to learn the user's final interest.</p><p>(5) NARM [<xref ref-type="bibr" rid="ref_18">18</xref>]: This model combines GRU and attention mechanism for recommendations.</p><p>(6) STAMP [<xref ref-type="bibr" rid="ref_6">6</xref>]: This model uses the soft attention mechanism to learn the user's long-term interest and then combines the user's short-term interest for recommendations.</p><p>(7) SR-GNN [<xref ref-type="bibr" rid="ref_7">7</xref>]: This model uses a graph neural network to capture the transformation relationship between items and then uses the soft attention mechanism to learn the user's interest.</p><p>(8) GCE-GNN [<xref ref-type="bibr" rid="ref_10">10</xref>]: This model is an improvement of SR-GNN. It uses session graphs and a global graph to learn the representation of items.</p><p>(9) DIDN [<xref ref-type="bibr" rid="ref_11">11</xref>]: This model incorporates user behavior patterns hidden behind items in the click process.</p><p>(10) CORE [<xref ref-type="bibr" rid="ref_12">12</xref>]: This model unifies the representation space for both encoding and decoding processes to address the inconsistent prediction issue while recommending items. This algorithm can be used as the latest baseline.</p><p>These baseline models are chosen based on their popularity and effectiveness in previous studies.</p><p>C. Comparison with baselines</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>Comparison with baselines</title>
          </caption>
          <table><tbody><tr><td rowspan="2"><p>Method</p></td><td colspan="2"><p>Tmall</p></td><td colspan="2"><p>Nowplaying</p></td></tr><tr><td><p>P@20</p></td><td><p>MRR@20</p></td><td><p>P@20</p></td><td><p>MRR@20</p></td></tr><tr><td><p>POP</p></td><td><p>2.00</p></td><td><p>0.90</p></td><td><p>2.28</p></td><td><p>0.86</p></td></tr><tr><td><p>Item-KNN</p></td><td><p>9.15</p></td><td><p>3.31</p></td><td><p>15.94</p></td><td><p>4.91</p></td></tr><tr><td><p>FPMC</p></td><td><p>16.06</p></td><td><p>7.32</p></td><td><p>7.36</p></td><td><p>2.82</p></td></tr><tr><td><p>GRU4REC</p></td><td><p>10.93</p></td><td><p>5.89</p></td><td><p>7.92</p></td><td><p>4.48</p></td></tr><tr><td><p>NARM</p></td><td><p>23.30</p></td><td><p>10.70</p></td><td><p>18.59</p></td><td><p>6.93</p></td></tr><tr><td><p>STAMP</p></td><td><p>26.47</p></td><td><p>13.36</p></td><td><p>17.66</p></td><td><p>6.88</p></td></tr><tr><td><p>SR-GNN</p></td><td><p>27.57</p></td><td><p>13.72</p></td><td><p>18.87</p></td><td><p>7.47</p></td></tr><tr><td><p>GCE-GNN</p></td><td><p>33.42</p></td><td><p>15.42</p></td><td><p>23.11</p></td><td><p>7.55</p></td></tr><tr><td><p>DIDN</p></td><td><p>34.25</p></td><td><p>15.01</p></td><td><p>23.16</p></td><td><p>7.59</p></td></tr><tr><td><p>CORE</p></td><td><p>34.67</p></td><td><p>15.56</p></td><td><p>22.09</p></td><td><p>7.41</p></td></tr><tr><td><p>PASR</p></td><td><p>35.91</p></td><td><p>15.83</p></td><td><p>23.32</p></td><td><p>7.83</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>To address Q1, we compared the performance of the proposed PASR model with that of the commonly used baseline models listed in Section 3.B. In the experiment, we conducted 10 runs with different random seeds and recorded the average results. The experimental results are presented in <xref ref-type="table" rid="table_2">Table 2</xref>, where the best-performing baseline and PASR model in each column are underlined and boldfaced, respectively. It can be observed from <xref ref-type="table" rid="table_2">Table 2</xref> that the PASR model achieves the best prediction performance on both datasets, outperforming the baseline models.</p><p>Among the traditional recommendation algorithms, POP is a very simple method that ignores the differences among users and has poor recommendation performance. Item-KNN and FPMC have improved to some extent, but they still have limitations. Item-KNN does not consider the sequence information of items, while the dimension explosion problem of FPMC makes it difficult to apply in actual production.</p><p>Recommendation algorithms based on deep learning, such as GRU4REC, NARM, and STAMP, have achieved better performance than traditional recommendation algorithms. However, GRU4REC and NARM based on GRU still suffer from the problem of gradient disappearance. STAMP only relies on the attention mechanism to learn the user's interest, but ignores the dependency between items.</p><p>Recommendation algorithms based on graph neural networks have further improved the performance of session recommendation because they can capture the transformation relationship between long-distance items. SR-GNN constructs the session into a graph, uses the gating graph neural network to learn the representation of the item, and then uses the soft attention mechanism to learn the user's interest. GCE-GNN uses the attention mechanism to evaluate and consider the importance of each item and generates the final item representation. DIDN incorporates item-aware, user-aware, and temporal-aware information to learn dynamic item embeddings and filter out noisy clicks within sessions. CORE applies a weighted sum for item embeddings to encode sessions and robust distance measuring techniques to prevent overfitting.</p><p>Although these models have achieved good recommendation performance, the PASR model still outperforms them. Unlike previous models, the PASR model constructs the item graph based on popularity, effectively capturing the preference of users for popular items. This ensures that the recommendations are tailored to the interests of the majority of users. The use of learnable embedding matrices E<sup>type</sup> and E<sup>num</sup> enables PASR to capture the different types and frequencies of edges in the graph. This allows the model to assign appropriate weights to each edge based on its type and frequency, which can better capture the dependencies between items and improve the accuracy of recommendations. Finally, the integration of popularity embeddings into the attention mechanism helps to better learn the user's interest and the contribution of each item to it. All of these factors together make PASR a powerful model for session-based recommendation, outperforming other state-of-the-art models.</p><p>D. Influence of popularity aware mechanism on recommendation performance</p><p>To address Q2 and demonstrate the effectiveness of the popularity-aware mechanism, we conducted the following comparative experiments:</p><p>(1) PASR-NP1: The number of occurrences of edges in the graph aggregator is not considered.</p><p>(2) PASR-NP2: The number of occurrences of items in the interest encoder is not considered.</p><p>(3) PASR-NP12: Neither the number of occurrences of edges in the graph aggregator nor the number of occurrences of items in the interest encoder is considered.</p><p>In this study, we fully tested the above comparison models and the PASR model on two datasets, and the experimental results are shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>. The results demonstrate that the PASR model outperforms the other models, proving the importance of the popularity-aware mechanism. This is because the PASR model considers both edge occurrences in graph aggregators and item popularity in the interest encoder.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Influence of popularity aware on recommendation performance</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/3/img_fuMBFrbCyrKDeEsl.png"/>
        </fig>
      
      <p>The performance of PASR-NP1 and PASR-NP2 is lower than that of the PASR model under normal conditions. This is because the former models ignore the number of occurrences of edges in the graph aggregator, making it difficult to accurately learn the weight of edges. The latter model does not consider the number of occurrences of items in the interest encoder, leading to interest bias.</p><p>The PASR-NP12 model has the worst performance because it ignores both the number of edges in the graph aggregator and the number of items in the interest encoder. This makes the model unable to capture the impact of item popularity on user interests.</p><p>Overall, these comparative experiments demonstrate the effectiveness of the popularity-aware mechanism in the PASR model, which can accurately learn the contribution of each item and improve the accuracy of recommendations.</p>
    </sec>
    <sec sec-type="">
      <title>4. Conclusion</title>
      <p>In this study, we proposed a popularity-aware graph neural network model for session-based recommendation systems. Our model investigates the impact of the number of edges and items in the graph neural network on recommendation performance. Through comprehensive experiments on two different datasets, we demonstrated that our proposed PASR model outperforms ten baseline schemes.</p><p>In future work, we plan to investigate the applicability of the PASR model in other recommendation systems, such as conversational recommendation systems. We also aim to explore the integration of other factors such as novelty and diversity into our model to further improve the quality of recommendations.</p><p>In conclusion, our proposed PASR model provides a powerful solution for session-based recommendation systems, which effectively captures the dependencies between items and improves the accuracy of recommendations. The success of our model also suggests that incorporating popularity-aware mechanisms can be a promising direction for improving recommendation performance in various domains.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p>This paper is partially supported by the Natural Science Foundation of Shandong Province (Grant No.: ZR2021MF099, ZR2022MF334)</p><p> Undergraduate Education Reform Project of Shandong Province (Grant No.: M2021130, M2022245, Z2022202)</p><p> Construction Project of High Quality Professional Degree Teaching Casebase in Shandong Province (Grant No.: SDYAL2022155)</p><p> Shandong Province Key R&amp;amp</p><p>D Program (Soft Science Project) (Grant No.: 2021RKY03056).</p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>2009</volume>
          <page-range>1-19</page-range>
          <year>2009</year>
          <person-group person-group-type="author">
            <name>
              <surname>Su</surname>
              <given-names>Xiaoyuan</given-names>
            </name>
            <name>
              <surname>Khoshgoftaar</surname>
              <given-names>Taghi M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2009/421425</pub-id>
          <article-title>A Survey of Collaborative Filtering Techniques</article-title>
          <source>Advances in Artificial Intelligence</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="conf-paper">
          <year>2012</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Shuo</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>Josh L.</given-names>
              <surname>Moore</surname>
            </name>
            <name>
              <given-names>Douglas</given-names>
              <surname>Turnbull</surname>
            </name>
            <name>
              <given-names>Thorsten</given-names>
              <surname>Joachims</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/2339530.2339643</pub-id>
          <article-title>Playlist prediction via metric embedding</article-title>
          <source>Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>2016</volume>
          <page-range>1-10</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hidasi</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Karatzoglou</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Baltrunas</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Tikk</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Session-based recommendations with recurrent neural networks</article-title>
          <source>In 4th International Conference on Learning Representations</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="conf-paper">
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Yong Kiam</given-names>
              <surname>Tan</surname>
            </name>
            <name>
              <given-names>Xinxing</given-names>
              <surname>Xu</surname>
            </name>
            <name>
              <given-names>Yong</given-names>
              <surname>Liu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/2988450.2988452</pub-id>
          <article-title>Improved Recurrent Neural Networks for Session-based Recommendations</article-title>
          <source>Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="conf-paper">
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Jing</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>Pengjie</given-names>
              <surname>Ren</surname>
            </name>
            <name>
              <given-names>Zhumin</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>Zhaochun</given-names>
              <surname>Ren</surname>
            </name>
            <name>
              <given-names>Tao</given-names>
              <surname>Lian</surname>
            </name>
            <name>
              <given-names>Jun</given-names>
              <surname>Ma</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3132847.3132926</pub-id>
          <article-title>Neural Attentive Session-based Recommendation</article-title>
          <source>Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="conf-paper">
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Qiao</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>Yifu</given-names>
              <surname>Zeng</surname>
            </name>
            <name>
              <given-names>Refuoe</given-names>
              <surname>Mokhosi</surname>
            </name>
            <name>
              <given-names>Haibin</given-names>
              <surname>Zhang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3219819.3219950</pub-id>
          <article-title>STAMP</article-title>
          <source>Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp;amp; Data Mining</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>33</volume>
          <page-range>346-353</page-range>
          <issue>01</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>Shu</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>Yuyuan</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Yanqiao</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Liang</given-names>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names>Xing</given-names>
            </name>
            <name>
              <surname>Tan</surname>
              <given-names>Tieniu</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1609/aaai.v33i01.3301346</pub-id>
          <article-title>Session-Based Recommendation with Graph Neural Networks</article-title>
          <source>Proceedings of the AAAI Conference on Artificial Intelligence</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>52</volume>
          <page-range>16975-16989</page-range>
          <issue>14</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sang</surname>
              <given-names>Sheng</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Nan</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Wenxuan</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Zhijun</given-names>
            </name>
            <name>
              <surname>Qin</surname>
              <given-names>Qianqian</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>Weihua</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10489-022-03170-7</pub-id>
          <article-title>High-order attentive graph neural network for session-based recommendation</article-title>
          <source>Applied Intelligence</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="conf-paper">
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Qingbo</given-names>
              <surname>Sun</surname>
            </name>
            <name>
              <given-names>Zhijun</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>Sheng</given-names>
              <surname>Sang</surname>
            </name>
            <name>
              <given-names>Fang</given-names>
              <surname>Dong</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/iccc54389.2021.9674354</pub-id>
          <article-title>Time and Position Aware Graph Neural Networks for Session-based Recommendation</article-title>
          <source>2021 7th International Conference on Computer and Communications (ICCC)</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="conf-paper">
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Ziyang</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>Wei</given-names>
              <surname>Wei</surname>
            </name>
            <name>
              <given-names>Gao</given-names>
              <surname>Cong</surname>
            </name>
            <name>
              <given-names>Xiao-Li</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>Xian-Ling</given-names>
              <surname>Mao</surname>
            </name>
            <name>
              <given-names>Minghui</given-names>
              <surname>Qiu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3397271.3401142</pub-id>
          <article-title>Global Context Enhanced Graph Neural Networks for Session-based Recommendation</article-title>
          <source>Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>59</volume>
          <page-range>102936</page-range>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Xiaokun</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Hongfei</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Bo</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Chenliang</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Yuan</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Haifeng</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>Fenglong</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ipm.2022.102936</pub-id>
          <article-title>Dynamic intent-aware iterative denoising network for session-based recommendation</article-title>
          <source>Information Processing &amp;amp; Management</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Yupeng</given-names>
              <surname>Hou</surname>
            </name>
            <name>
              <given-names>Binbin</given-names>
              <surname>Hu</surname>
            </name>
            <name>
              <given-names>Zhiqiang</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>Wayne Xin</given-names>
              <surname>Zhao</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3477495.3531955</pub-id>
          <article-title>CORE: Simple and Effective Session-based Recommendation within Consistent Representation Space</article-title>
          <source>Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>1-14</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kipf</surname>
              <given-names>T. N.</given-names>
            </name>
            <name>
              <surname>Welling</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Semi-supervised classification with graph convolutional networks</article-title>
          <source>In Proceedings of the International Conference on Learning Representations</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>35</volume>
          <page-range>4573-4581</page-range>
          <issue>5</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>Han</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>Kaili</given-names>
            </name>
            <name>
              <surname>Cheng</surname>
              <given-names>James</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1609/aaai.v35i5.16586</pub-id>
          <article-title>Rethinking Graph Regularization for Graph Neural Networks</article-title>
          <source>Proceedings of the AAAI Conference on Artificial Intelligence</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="conf-paper">
          <year>2001</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Badrul</given-names>
              <surname>Sarwar</surname>
            </name>
            <name>
              <given-names>George</given-names>
              <surname>Karypis</surname>
            </name>
            <name>
              <given-names>Joseph</given-names>
              <surname>Konstan</surname>
            </name>
            <name>
              <given-names>John</given-names>
              <surname>Riedl</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/371920.372071</pub-id>
          <article-title>Item-based collaborative filtering recommendation algorithms</article-title>
          <source>Proceedings of the 10th international conference on World Wide Web</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="conf-paper">
          <year>2010</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Steffen</given-names>
              <surname>Rendle</surname>
            </name>
            <name>
              <given-names>Christoph</given-names>
              <surname>Freudenthaler</surname>
            </name>
            <name>
              <given-names>Lars</given-names>
              <surname>Schmidt-Thieme</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/1772690.1772773</pub-id>
          <article-title>Factorizing personalized Markov chains for next-basket recommendation</article-title>
          <source>Proceedings of the 19th international conference on World wide web</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation/>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>2016</volume>
          <page-range>366-381</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ko</surname>
              <given-names>Y. J.</given-names>
            </name>
            <name>
              <surname>Maystre</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Grossglauser</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Collaborative recurrent neural networks for dynamic recommender systems</article-title>
          <source>In Proceedings of Machine Learning Research (PMLR)</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>