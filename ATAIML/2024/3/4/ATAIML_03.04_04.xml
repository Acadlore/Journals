<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-UfJ7z1NHwDqG1htJPtSqttbgz50OpGZp</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml030404</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Innovative Hybrid Deep Learning Models for Financial Sentiment Analysis</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1,2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1071-3771</contrib-id>
          <name>
            <surname>Marqas</surname>
            <given-names>Ridwan B.</given-names>
          </name>
          <email>ridwanmarqas@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1,3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1590-1875</contrib-id>
          <name>
            <surname>Mousa</surname>
            <given-names>Abdulazeez</given-names>
          </name>
          <email>abdulazizmoosa93@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8154-6691</contrib-id>
          <name>
            <surname>öZyurt</surname>
            <given-names>Fatih</given-names>
          </name>
          <email>fatih.ozyurt@firat.edu.tr</email>
        </contrib>
        <aff id="aff_1">Department of Software Engineering, Firat University, 23119 Elazig, Turkey</aff>
        <aff id="aff_2">Department of Computer Science, Knowledge University, 44001 Erbil, Iraq</aff>
        <aff id="aff_3">Department of Computer Science, Nawroz University, 42001 Duhok, Iraq</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>23</day>
        <month>12</month>
        <year>2024</year>
      </pub-date>
      <volume>3</volume>
      <issue>4</issue>
      <fpage>225</fpage>
      <lpage>236</lpage>
      <page-range>225-236</page-range>
      <history>
        <date date-type="received">
          <day>24</day>
          <month>10</month>
          <year>2024</year>
        </date>
        <date date-type="accepted">
          <day>18</day>
          <month>12</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>This study explores hybrid deep learning architectures for the classification of financial sentiment, focusing on the integration of the Convolutional Neural Network (CNN) with the Support Vector Machine (SVM) and the Random Forest (RF). CNN, with its powerful feature extraction capabilities, was combined with SVM’s ability to handle non-linear decision boundaries, while RF enhanced model generalization through ensemble learning. The proposed hybrid frameworks addressed two fundamental challenges in sentiment analysis: overfitting and class imbalance. These challenges were mitigated, resulting in improved model accuracy and reliability compared to standalone methods. Empirical evaluations demonstrated that the CNN-SVM model achieved competitive or superior validation accuracy and loss, indicating its suitability for precise financial sentiment classification. By enabling more accurate sentiment categorization, the model provides actionable insights for financial analysts and investors, thereby supporting better market assessment and investment decision-making. Future work is suggested to incorporate advanced techniques such as adversarial training and domain-specific pre-trained models to further enhance model performance.</p></abstract>
      <kwd-group>
        <kwd>Finance</kwd>
        <kwd>Deep learning</kwd>
        <kwd>Convolutional Neural Network</kwd>
        <kwd>Hybrid models</kwd>
        <kwd>Sentiment analysis</kwd>
        <kwd>Support Vector Machine</kwd>
        <kwd>Random Forest</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="3"/>
        <fig-count count="8"/>
        <table-count count="3"/>
        <ref-count count="27"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>The process of analyzing financial feelings with deep learning involves the application of deep learning methodologies to understand and assess the emotions and sentiments expressed by people and the general public on financial and economic topics. This research involves the use of financial and economic knowledge information such as financial reports, market information, and economic signals. Machine learning, particularly in the kind known as deep learning, was employed for the training of the computer models for the purpose of extractive processing of the stuff and financial data in particular. Such models include, for instance, the use of deep neural networks, which consist of complex structures designed to carry out highly detailed data computation and to recognize the existence of certain patterns or trends. The objective of this technology is to improve the decision-making process in investment by providing investors and financial analysts with better evaluations of the financial and economic environment. In addition, it can be used as a tool for market research to monitor the trends of financial markets as well as to predict the tendencies in the near future. Using deep learning approaches to analyze feelings about money, there is an opportunity for different parties, including persons, businesses, and organizations, to obtain a better understanding of financial markets and their behavior. Due to this improved perception, they can be in a better position to make better monetary decisions towards investments.</p><p>CNN integration with SVM and RF as a tool for financial sentiment classification represents a major leap forward in the use of deep learning applications in this area. SVM and RF are highly capable at classification, whereas CNN can extract in-depth textual data features. This study proposes models that combine the strengths of these approaches to tackle issues of sentiment analysis in finance.</p><p>The CNN-SVM hybrid model proposed in this study employs CNN to extract features and to discover semantic and syntactic patterns from financial texts such as articles, reports, and social media posts. SVM, being robust to complex decision boundaries and its ability to curb overfitting while working in the multi-class scenario, classifies the extracted features. In addition, CNN‑RF uses CNN to learn features and uses RF as an ensemble classifier to deal with high-dimensional feature space and noisy datasets to increase generalizability and robustness.</p><p>In contrast to regular deep learning methods that depend solely on neural net structure, these hybrid models prioritize interpretabilility and efficiency. The models leverage the strengths of traditional classifiers with CNN to resolve common problems, like class imbalance and overfitting, found in financial sentiment datasets. This integration increases both classification accuracy and reliance on prediction and gives a more extensive understanding of financial sentiments.</p><p>What these advances mean in practice, however, is profound. For investors and financial analysts, correctly labeling sentiments as positive, negative, or neutral helps them analyze market and economic trends in a better way. The proposed hybrid models have the ability to improve the decision-making processes, and their utilization in market research and investment strategies can substantially improve actionable insights extracted from many sources of financial data.</p><p>The selection between these two hybrid models depends on the nature and size of the dataset, the requirement for a specific sentiment analysis task, and computational capacities. Both of the hybrid models have the aim of increasing the accuracy and the reliability of sentiment analysis in financial contexts by combining outstanding deep learning feature extraction methods, namely CNN, with stable classification algorithms such as SVM or RF. In an attempt to compare the performance of both methodologies, researchers and analysts often conduct experimentation with one or the other and sometimes both.</p>
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p>Sentiment analysis has turned out to be one of the fundamental tools for financial analysis that helps stakeholders to extract actionable insights from unstructured big textual data sources such as news articles, social media, financial reports, etc. Following recent breakthroughs in machine learning and deep learning techniques, sentiment classification is becoming both more accurate and more efficient. This observation is especially notable as hybrid deep learning models have demonstrated an exceptional capability to learn and capture the most nuanced of all financial sentiments. This section synthesizes existing studies to offer a broad overview of the field.</p><p>Recently, CNN-based techniques have been shown to be an extremely powerful method for extracting high-level features from raw text data. Although Kim [<xref ref-type="bibr" rid="ref_1">1</xref>] demonstrated CNN’s utility for sentence classification, it has not yet been applied to financial sentiment analysis. Following that, Zhang and Wallace [<xref ref-type="bibr" rid="ref_2">2</xref>] showed how sensitive CNN architectures are to different hyperparameters and the critical need to tune these hyperparameters. When these models are combined with cutting-edge classification algorithms, better sentiment analysis scores can be consistently obtained compared to traditional approaches. There are numerous studies that have applied deep learning to the problems of stock market predictions. Akita et al. [<xref ref-type="bibr" rid="ref_3">3</xref>] combined deep learning models to predict stock trends using numerical and textual data. Hu et al. [<xref ref-type="bibr" rid="ref_4">4</xref>] also proposed a framework with news data to predict trends, which involves textual analysis to examine market movements. Ding et al. [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>] further extended deep learning in financial contexts by introducing structured events with which they were able to predict stock price movements using structured financial data.</p><p>A lot of work has been done to explore how effective CNN in combination with SVM or RF would be for sentiment classification. Using topic-based sentiment analysis on Twitter data, Si et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] revealed that hybrid approaches are indispensable where social media dynamics are concerned. Nguyen et al. [<xref ref-type="bibr" rid="ref_8">8</xref>] extended this work by applying lexicon-based approaches to sentiment analysis using StockTwits data, and found the shortcomings of only using lexicon-based approaches and hybrid approaches. The CNN-SVM model uses CNN for feature extraction and uses SVM for classification. Cortes and Vapnik [<xref ref-type="bibr" rid="ref_9">9</xref>] and Joachims [<xref ref-type="bibr" rid="ref_10">10</xref>] pointed out the good behavior of SVM for multi-classification problems and how it is immune to the order of some variables. The methods integrating CNN with RF have also been explored. For example, Breiman [<xref ref-type="bibr" rid="ref_11">11</xref>] discussed the ensemble capability of RF on handling large and noisy datasets.</p><p>Deep learning has recently brought forth novel techniques for performing sentiment analysis. For the case of financial time series forecasting, Widiputra et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] used a CNN‐LSTM framework as an appreciation of both the CNN for feature extraction and the LSTM for temporal modeling. Liu et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] included adversarial training in their models and the authors were able to show how it can be beneficial in the quest to improve robustness by minimizing the effects that come with noisy or even adversarial inputs. These progress in turn stresses the usefulness of hybrid convolutional and recurrent structures in financial analysis, specifically in applications that demand spatial structuring as well as sequential features learning, Nelson et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] employed the Long Short-Term Memory (LSTM) networks for stock movement prediction.</p><p>The development of FinancialBERT and more domain-specific models like it has been revolutionary for financial natural language processing (NLP). FinancialBERT, a pre-trained model for financial text mining, was introduced by Yang et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] and is already achieving state-of-the-art results on sentiment classification tasks. All of this highlights that domain-specific adaptation contributes greatly to the performance of models.</p><p>Over the last couple of years, the use of the market variables with textual data has been a common feature in sentiment analysis. Wen et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] showed that trends can be forecasted by means of high-order time series, providing means for integrating quantitative time series data with textual data for improved prediction. Some work that is similar to this has been done by Li et al. [<xref ref-type="bibr" rid="ref_17">17</xref>]. The elements of market fundamental and textual analysis have not been worked through as comprehensively as they are in this paper. By using this approach of integrating structured financial data indicators with unstructured textual sentiment, then the market behavior assessment can be analyzed from a more rounded perspective.</p><p>However, in spite of excellent progress made in the field of financial sentiment analysis, there are still many issues that deserve attention. As overfitting becomes an innate problem, as Nelson et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] stated, regularization techniques and hyper parameter optimization have to be employed. In financial datasets, one of the issues is class imbalance, which can be addressed by weighted loss functions and advanced sampling techniques. According to Ding et al. [<xref ref-type="bibr" rid="ref_6">6</xref>], future research should address the interpretation of hybrid models so that they can enhance decision-making. Moreover, sentiment classification accuracy can be improved by integrating diverse data sources, such as the integration of structured financial reports with unstructured social media data. Feng et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] reported emerging techniques, such as Generative Adversarial Network (GAN) and reinforcement learning, as interesting areas for future research.</p><p>The discussion has been further substantiated with the additional references which provide insights into distinct methodologies. The fundamentals of gradient-based learning lay the foundations for what modern CNN is built on today, as emphasized by LeCun et al. [<xref ref-type="bibr" rid="ref_19">19</xref>]. Peng and Jiang [<xref ref-type="bibr" rid="ref_20">20</xref>] and Liu et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] revealed that loading word embedding with deep neural networks improves the accuracy of prediction in financial environments. This combination improves semantic and syntactic interpretability for real textual data to provide more accurate contextually-driven financial forecasts to the model.</p><p>Fine-grained opinion extraction is an important problem, and as a result, there have been great strides in the methods for aspect-based sentiment analysis (ABSA). Wankhade et al. [<xref ref-type="bibr" rid="ref_21">21</xref>] provided a survey of ABSA techniques for financial domains and found that they can be adapted. This landscape has been further enriched with the advent of multi-modal sentiment analysis. Pandey and Vishwakarma [<xref ref-type="bibr" rid="ref_22">22</xref>] conducted a review of applying deep learning on multi-modal sentiment analysis, particularly with regard to the integration of textual, visual and audio data to better interpret sentiments. Combining financial time series analysis with deep learning methods has seen interest as an ability to capture temporal dynamics. Chen et al. [<xref ref-type="bibr" rid="ref_23">23</xref>] surveyed deep learning models for standalone and hybrid models for financial time series prediction, and concluded that integrated approaches for sentiment-driven financial forecasting are more effective in financial forecasting. In addition, as for this utility of hybrid feature extraction techniques, such as Word2Vec-TFIDF, which combines Word2Vec with Term Frequency-Inverse Document Frequency (TFIDF), George and Murugesan [<xref ref-type="bibr" rid="ref_24">24</xref>] showed their utility in enhancing the analysis of sentiment in financial news headlines.</p><p>As such, Mohamed et al. [<xref ref-type="bibr" rid="ref_25">25</xref>] developed a hybrid lexicon and deep learning model called LexDeep to analyze unemployment-related Twitter discourse on COVID-19. Finally, this model serves to demonstrate that hybrid methods are adaptable to domain-specific applications such as financial crises. In sector-level sentiment analysis, Almalis et al. [<xref ref-type="bibr" rid="ref_26">26</xref>] extended use cases of deep learning to financial markets and showed how granular insights in the financial markets may be obtained. Furthermore, Ewald and Li [<xref ref-type="bibr" rid="ref_27">27</xref>] also studied how news sentiment contributes to forecasting commodity prices, thus filling the gap between financial forecasting and sentiment analysis.</p>
    </sec>
    <sec sec-type="">
      <title>3. Methodology</title>
      <p>The method used in this study is centered around a mechanical framework designed to investigate and analyze the research question.</p>
      
        <sec>
          
            <title>3.1. Cnn</title>
          
          <p>CNN is a type of neural network that is specifically designed to cater to the format of feed data, images in the form of photographs or any format that is arranged in a matrix [<xref ref-type="bibr" rid="ref_19">19</xref>]. CNN uses convolutional filters to obtain feature hierarchies from the input it receives. In the studies relying on the sentiment analysis of financial language, CNN has been applied to extract all semantic and syntactic features from the input texts [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_20">20</xref>]. The convolutional layer develops its own filters to build up feature maps which are further passed on to the pooling layer to get the important features. Several convolutional-pooling layers allow the extraction of the features up to alternative abstraction levels. It has been observed that many features generated out of CNN contain a lot of useful discriminative information that would help in achieving a very accurate sentiment classification.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Svm</title>
          
          <p>SVM is one of the most commonly used machine learning algorithms that is employed in different fields. It is a kind of supervised technique for learning by training that is applicable in cases of classification as well as regression. SVM is the type of supervised machine learning model commonly used for the purpose of classification tasks [<xref ref-type="bibr" rid="ref_9">9</xref>]. It is a type of classification algorithm with the objective of looking for the so-called hyperplane to fully and representatively separate the diverse classes. Each hyperplane is placed in such a way that maximizes the distance between the support vectors – the points which are most distant from the separating boundary. SVM has known capacity to respond to intricate decision boundaries and multi-class conditions appropriately [<xref ref-type="bibr" rid="ref_10">10</xref>]. When it comes to sentiment analysis, SVM can be used for performing the multi-class classification. This classification process involves employing features extracted from CNN to categorize financial text into three sentiment polarities. It is either positive, negative or neutral [<xref ref-type="bibr" rid="ref_4">4</xref>]. Optimization of the SVM, together with the concept of large margin separation, leads to improved generalization performance and the ability to classify new unseen data samples.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Rf</title>
          
          <p>RF, also referred to as random decision forest, is a machine learning algorithm that is made up of many decision trees.</p><p>RF is another kind of classification model based on the family of ensemble models constructed with a number of decision trees [<xref ref-type="bibr" rid="ref_11">11</xref>]. In each decision tree, the features of the learning dataset are randomly selected for training the tree. Invoking the case of sentiment classification, the features that are extracted from CNN are used as features by multiple decision trees. They also do independent multi-class sentiment classification with each tree. The macro-level sentiment polarity is then determined through voting or averaging of the forecasts from all trees [<xref ref-type="bibr" rid="ref_13">13</xref>]. In fact, RF employs many uncorrelated models in order to strengthen the accuracy and robustness of the forecast, which is better than that of an independent decision tree. The application of ensemble methods allows for the idea of gaining stability when there is noise inside complex textual data.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Hybrid models</title>
          
          <p>In this concept, hybrid models are described as a type of computational model that consists of many techniques or methods that are used in solving a specific problem or task.</p><p>(a) CNN-RF hybrid model</p><p>CNN-RF is a synergistic model between CNN and RF. The hybrid model combines CNN and RF classifiers [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_14">14</xref>]. Firstly, for the CNN, convolutions and pooling methods are used to obtain HFRs by entering other financial text data. RF classifier inputs are characteristics that incorporate semantic knowledge. RF is a set of decision trees that work independently, but with a goal of multi-class sentiment classification. For the rating-sentiment, the overall polarity of the sentiment is computed from the individual trees by voting or averaging [<xref ref-type="bibr" rid="ref_14">14</xref>]. The hybrid CNN-RF model integrates the feature learning technique of CNN and the ensemble technique of RF to enhance the performance of sentiment analysis.</p><p>(b) CNN-SVM hybrid model</p><p>SVM-CNN is one of the frequently used deep learning models, which includes CNN with SVM in its framework [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_20">20</xref>]. In the CNN module, textual features are obtained by convolving the input text. The capturing characteristics of semantic relationship properties are then forwarded to an SVM classifier. In this case, SVM is applied to the task of multiple-class classification. It does this by using trained decision hyperplanes to identify the separation of sentiment into the positive, negative and neutral categories. This model combines the feature-extracting skills of CNN with the general skills of SVM for sentiment classification [<xref ref-type="bibr" rid="ref_20">20</xref>].</p>
        </sec>
      
      
        <sec>
          
            <title>3.5. Model parameter settings</title>
          
          <p>(a) CNN architecture</p><p><inline-formula>
  <mml:math id="mhvg22uz5l">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Embedding layer: The vocabulary size (vocab_size) was set to 10,000 words, the embedding dimension was set to 128, and the input length (max_length) was fixed at 200.</p><p><inline-formula>
  <mml:math id="mba3002jeo">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Convolutional layer: The number of filters was set to 64, the kernel size was set to 5, and ReLU was used as the activation function.</p><p><inline-formula>
  <mml:math id="mw94flrvwn">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Pooling layer: Global Max Pooling was employed as the pooling type.</p><p><inline-formula>
  <mml:math id="mnga7xg47w">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Dense layers: The first dense layer consists of 64 neurons with ReLU activation. The output layer is composed of three neurons, corresponding to the sentiment classes, with the use of a softmax activation function.</p><p>(b) Optimizer and loss function</p><p><inline-formula>
  <mml:math id="mbhu0iyl9p">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Optimizer: Adam was employed as the optimizer.</p><p><inline-formula>
  <mml:math id="mj99no3qet">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Learning rate: The default learning rate from the Keras Adam optimizer was used.</p><p><inline-formula>
  <mml:math id="mcsbqkokhv">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Loss function: Sparse Categorical Crossentropy was used as the loss function for multi-class classification.</p><p><inline-formula>
  <mml:math id="m5amifp2cz">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Metrics: Accuracy was used as the evaluation metric.</p><p>(c) Training settings</p><p><inline-formula>
  <mml:math id="m51acv5wlr">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Epochs: The model was trained for 100 epochs.</p><p><inline-formula>
  <mml:math id="mhljrkmkli">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Batch size: A batch size of 64 was used.</p><p><inline-formula>
  <mml:math id="m69utmo9vn">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Validation split: 20% of the dataset was allocated for validation.</p>
        </sec>
      
      
        <sec>
          
            <title>3.6. Network architecture</title>
          
          <p>The architecture of the network was defined as follows: input → embedding layer → Conv1D → global max pooling → dense layer (ReLU) → dense output layer (Softmax).</p><p>CNN is used both as a classifier and a feature extractor. For feature extraction, all layers except the final dense layer are retained.</p>
        </sec>
      
      
        <sec>
          
            <title>3.7. Optimization strategies</title>
          
          <p>(a) CNN optimization</p><p>Early stopping can be implemented to prevent overfitting. In addition, hyperparameter tuning can be considered, such as the learning rate, filter size, and kernel size, via grid search or random search.</p><p>(b) Feature-based models</p><p><inline-formula>
  <mml:math id="m3n00qrxq6">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> RF: The number of trees (n_estimators) was set to 100, and the random state was fixed at 42 for reproducibility.</p><p><inline-formula>
  <mml:math id="mre1qxfm0x">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> SVM: The kernel was set to linear, and the regularization parameter (C) was set to 1.0.</p>
        </sec>
      
      
        <sec>
          
            <title>3.8. Data preprocessing</title>
          
          <p>(a) Text data preprocessing</p><p><inline-formula>
  <mml:math id="myxuc1v8jo">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Tokenization: Text was converted into integer sequences using the Tokenizer from Keras.</p><p><inline-formula>
  <mml:math id="m3xu4cumgo">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Padding: Padding was applied to ensure uniform sequence length, with a maximum length (max_length) of 200.</p><p><inline-formula>
  <mml:math id="mesbwmsucu">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Label encoding: String labels were converted into integers using LabelEncoder.</p><p>(b) Dataset splitting</p><p><inline-formula>
  <mml:math id="m9phahdvct">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Train-test split: The dataset was split into 80% for training and 20% for testing.</p><p><inline-formula>
  <mml:math id="mi6vhdtxbb">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Random state: A random state of 42 was used.</p>
        </sec>
      
      
        <sec>
          
            <title>3.9. Evaluation metrics</title>
          
          <p><inline-formula>
  <mml:math id="mw25wpe9pt">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Accuracy: Accuracy was used to evaluate the overall performance.</p><p><inline-formula>
  <mml:math id="mvc4syznrv">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Classification report: The classification report included precision, recall, and F1-score for each class.</p><p><inline-formula>
  <mml:math id="mg1buyl1hc">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Confusion matrix: The confusion matrix was used to visualize prediction errors.</p><p><inline-formula>
  <mml:math id="mlhyg5lnpu">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Plots: Plots of accuracy/loss over epochs and sentiment distributions were generated to provide insight into model behavior.</p>
        </sec>
      
    </sec>
    <sec sec-type="results">
      <title>4. Results</title>
      <p>In this study, two hybrid models, CNN + RF and CNN + SVM, were developed for financial sentiment analysis and compared for their performance. The analysis is based upon examining validation accuracy and validation loss with respect to varied training configurations in order to highlight the strengths and weaknesses of each model.</p>
      
        <sec>
          
            <title>4.1. Performance of the cnn + rf model</title>
          
          <p>(a) Model configuration and results</p><p> <xref ref-type="table" rid="table_1">Table 1</xref> shows the configuration and performance results of the CNN + RF model.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Configurations of the CNN + RF model</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Configuration</p></td><td colspan="1" rowspan="1"><p>Epochs</p></td><td colspan="1" rowspan="1"><p>Batch Size</p></td><td colspan="1" rowspan="1"><p>Validation Accuracy</p></td><td colspan="1" rowspan="1"><p>Validation Loss</p></td></tr><tr><td colspan="1" rowspan="1"><p>Configuration 1</p></td><td colspan="1" rowspan="1"><p>150</p></td><td colspan="1" rowspan="1"><p>64</p></td><td colspan="1" rowspan="1"><p>0.6801</p></td><td colspan="1" rowspan="1"><p>2.2488</p></td></tr><tr><td colspan="1" rowspan="1"><p>Configuration 2</p></td><td colspan="1" rowspan="1"><p>100</p></td><td colspan="1" rowspan="1"><p>64</p></td><td colspan="1" rowspan="1"><p>0.6869</p></td><td colspan="1" rowspan="1"><p>2.0131</p></td></tr><tr><td colspan="1" rowspan="1"><p>Configuration 3</p></td><td colspan="1" rowspan="1"><p>50</p></td><td colspan="1" rowspan="1"><p>64</p></td><td colspan="1" rowspan="1"><p>0.6818</p></td><td colspan="1" rowspan="1"><p>1.9538</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>(b) Performance summary</p><p>The highest validation accuracy (0.6869) was achieved by Configuration 2. The lowest validation loss (1.9538) occurred in Configuration 3.</p>
        </sec>
      
      
        <sec>
          
            <title>4.2. Performance of the cnn + svm model</title>
          
          <p>(a) Model configuration and results</p><p> <xref ref-type="table" rid="table_2">Table 2</xref> shows the configuration and performance results of the CNN + SVM model.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Configurations of the CNN + SVM model</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Configuration</p></td><td colspan="1" rowspan="1"><p>Epochs</p></td><td colspan="1" rowspan="1"><p>Batch Size</p></td><td colspan="1" rowspan="1"><p>Validation Accuracy</p></td><td colspan="1" rowspan="1"><p>Validation Loss</p></td></tr><tr><td colspan="1" rowspan="1"><p>Configuration 1</p></td><td colspan="1" rowspan="1"><p>150</p></td><td colspan="1" rowspan="1"><p>128</p></td><td colspan="1" rowspan="1"><p>0.6801</p></td><td colspan="1" rowspan="1"><p>2.3228</p></td></tr><tr><td colspan="1" rowspan="1"><p>Configuration 2</p></td><td colspan="1" rowspan="1"><p>100</p></td><td colspan="1" rowspan="1"><p>64</p></td><td colspan="1" rowspan="1"><p>0.6938</p></td><td colspan="1" rowspan="1"><p>1.7226</p></td></tr><tr><td colspan="1" rowspan="1"><p>Configuration 3</p></td><td colspan="1" rowspan="1"><p>50</p></td><td colspan="1" rowspan="1"><p>64</p></td><td colspan="1" rowspan="1"><p>0.6903</p></td><td colspan="1" rowspan="1"><p>1.8402</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>(b) Performance summary</p><p>The highest validation accuracy (0.6938) was achieved by Configuration 2. The lowest validation loss (1.7226) also occurred in Configuration 2, outperforming all CNN + RF configurations.</p>
        </sec>
      
      
        <sec>
          
            <title>4.3. Key observations</title>
          
          <p>(a) Comparison of models</p><p>CNN + SVM consistently outperformed CNN + RF in both validation accuracy and validation loss. The highest validation accuracy of CNN + SVM (0.6938) exceeded that of CNN + RF (0.6869). The lowest validation loss of CNN + SVM (1.7226) was significantly better than CNN + RF (1.9538).</p><p>(b) Overfitting</p><p>Training accuracy for CNN peaked at 0.92 after 100 epochs, with a corresponding loss of 0.10. However, validation accuracy decreased from 0.69 to 0.67, and validation loss increased to 2.37, indicating overfitting.</p>
        </sec>
      
      
        <sec>
          
            <title>4.4. Model superiority</title>
          
          <p>CNN + SVM (Configuration 2) outperformed other configurations, yielding the highest accuracy and the lowest loss. As seen in <xref ref-type="table" rid="table_1">Table 1</xref>, the proposed CNN + RF method has a slightly higher validation loss compared to the results of the SVM method, meaning that SVM is more appropriate in this case.</p>
        </sec>
      
      
        <sec>
          
            <title>4.5. Insights on hybrid models</title>
          
          <p>Combining the deep learning model (CNN) with conventional machine learning models (RF and SVM) has proved to be more efficient. The combination of CNN and SVM turned out to be relatively easy to implement and highly effective for the task of financial sentiment analysis.</p><p>The final validation accuracy achieved was 0.666, which is pretty close to the test accuracy of 0.666. The other method is alignment, which, in addition to this performance graph, further supports the presence of overfitting. The model performed so much better on the training data versus unseen data. The performance was evaluated against various metrics, including precision, recall, and F1-score. The overall performance was found satisfactory, but areas for improvement remain. The class 0 has the worst performance of the classes in the confusion matrix by frequently misclassifying. This can be provoked by imbalance between classes. CNN + RF model displays an obvious sign of overfitting on the whole since it has a lackluster metric difference from training metrics to validation as well as test metrics. The performance of the RF classifier is also not good enough to compensate for the overly fitted features of the CNN which would lead the model to be very hard to generalize.</p><p>The figures can help learn how the training and evaluation of the model. <xref ref-type="fig" rid="fig_1">Figure 1</xref> and <xref ref-type="fig" rid="fig_2">Figure 2</xref> show that the dataset has a large class imbalance, with positive sentiment being the minority class (30.8%) and neutral sentiment being the majority class (53.6%). Careful data preprocessing is required due to this imbalance since the model is likely to end up predicting an unbalanced dataset of ratios.</p><p>The confusion matrix of CNN + RF is shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref> and has better performance in identifying neutral sentiment than negative sentiment. This discrepancy indicates that the model needs to be refined to give better minority class performance.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>Sentiment distribution (CNN+RF)</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/11/img_uHeYBL532s0z5qro.png"/>
            </fig>
          
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Pie chart sentiment distribution (CNN+RF)</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/11/img_3JhGlGzKVCxfdRVW.png"/>
            </fig>
          
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Confusion matrix (CNN+RF)</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/11/img_tVQ9SA0Pde6QEBEX.png"/>
            </fig>
          
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Model accuracy (CNN+RF)</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/11/img_p89eicZcViDrzXZG.png"/>
            </fig>
          
          <p>The challenge of overfitting with the CNN + RF model is also demonstrated in <xref ref-type="fig" rid="fig_4">Figure 4</xref>. Finally, training accuracy and loss were steadily improved, but validation metrics revealed a large discrepancy. Such overfitting points to the value of using strategies like regularization, dropout, or reducing model complexity to improve generalization.</p><p>In order to improve the performance of CNN, advanced regularization techniques, such as increased L2 regularization and dropout layers, were suggested. The result showed that these methods can solve the issue of overfitting, i.e., poor generalization on unseen data when the model becomes too tailored to the training data.</p><p>Other potential improvements are as follows:</p><p>a) Addressing class imbalance: To ensure a better class distribution for the classes inside the dataset to improve the model performance for all classes.</p><p>b) Hyperparameter optimization for RF: The performance of the RF model can be fine-tuned once by passing parameters like n_estimators (number of trees) and max_depth.</p><p>c) Use of CNN as a feature extractor: Using the CNN to obtain features and then feeding them to a linear model like SVM instead of RF for better results.</p><p>Finally, overfitting was proven to still be the major source that limits the performance in this research. The sentiment analysis was performed in the financial domain using a hybrid CNN-SVM model. The approach suggests a need for a systematic improvement in feature extraction, hyperparameter tuning, and class handling for better results.</p><p>For classification imbalance in the dataset used, <xref ref-type="fig" rid="fig_5">Figure 5</xref> and <xref ref-type="fig" rid="fig_6">Figure 6</xref> show the effect of class imbalance on the model classifying minority classes (negative sentiments).</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Sentiment distribution (CNN + SVM)</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/11/img_CSP_bdaZ-KxO0AMH.png"/>
            </fig>
          
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Pie chart sentiment distribution (CNN + SVM)</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/11/img_qQB5t7wpF87kdDe8.png"/>
            </fig>
          
          <p>As shown in <xref ref-type="fig" rid="fig_7">Figure 7</xref>, the CNN-SVM model does relatively better on neutral and positive classes. However, there is still room to improve for the negative sentiments.</p><p>As illustrated by <xref ref-type="fig" rid="fig_8">Figure 8</xref>, overfitting is a major challenge, and the only few ways to correct it for the model to generalize properly are regularization, dropout, or introducing more data in the dataset.</p>
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>CNN+SVM confusion matrix</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/11/img_fYyeYtvhzU1H8dDw.png"/>
            </fig>
          
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>CNN+SVM model accuracy</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/11/img_Xov91LhX_2Ply5yJ.png"/>
            </fig>
          
          <p>According to <xref ref-type="table" rid="table_3">Table 3</xref>, the key observations are as follows:</p><p>a) Validation accuracy</p><p>CNN + SVM always was better than CNN + RF in validation accuracy and had greater accuracy compared with CNN + RF (0.6938 vs. 0.6869).</p><p>b) Validation loss</p><p>CNN + SVM achieved lower validation loss (1.7226) compared to CNN + RF (1.9538), indicating better generalization.</p><p>c) Class-wise performance</p><p>The performance of both models was comparable with classification for the neutral class, but struggled in terms of negative sentiments. Classification of the minority classes was marginally better with CNN + SVM.</p><p>d) Overfitting</p><p>Both models were overfitting, but CNN + SVM showed slightly better validation performance (i.e., better overfitting resistance).</p><p>e) Robustness</p>
          <p>In all, CNN + SVM turned out to be the overall more robust model because of higher validation accuracy and lower loss.</p><p>The CNN model achieved test accuracy of 0.686, which matches well to the validation accuracy and is lower than training accuracy. It reinforces that there is overfitting present in the model. Furthermore, further insight on the classification report was presented with metrics, i.e., precision, recall, and F1-score, for each class. The overall performance is adequate, but still a great deal of improvement is possible. It can be observed from the confusion matrix that the most problematic thing is with predicting class 0. It seems that many samples belonging to that class are misclassified as the other classes, which is probably due to the poor class imbalance characteristics of the dataset. To overcome these difficulties, several strategies were suggested, such as applying more robust regularization methods, coping better with class imbalance, as well as further tuning the hyperparameters, for better overall and generalization performance. Although improvements would not likely outperform, it would likely produce better predictions on minority classes and reduce the disparity of training and validation metrics observed currently.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Comparison of both models</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Aspect</p></td><td colspan="1" rowspan="1"><p>CNN + RF</p></td><td colspan="1" rowspan="1"><p>CNN + SVM</p></td></tr><tr><td colspan="1" rowspan="1"><p>Dataset distribution</p></td><td colspan="1" rowspan="1"><p>Both models faced the same class imbalance, with neutral sentiments dominating (53.6%).</p></td><td colspan="1" rowspan="1"><p>The same dataset distribution challenge, with similar performance variations by sentiment class.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Best validation accuracy</p></td><td colspan="1" rowspan="1"><p>0.6869 (Configuration 2 with 100 epochs and a batch size of 64).</p></td><td colspan="1" rowspan="1"><p>0.6938 (Configuration 2 with 100 epochs and a batch size of 64).</p></td></tr><tr><td colspan="1" rowspan="1"><p>Best validation loss</p></td><td colspan="1" rowspan="1"><p>1.9538 (Configuration 3 with 50 epochs and a batch size of 64).</p></td><td colspan="1" rowspan="1"><p>1.7226 (Configuration 2 with 100 epochs and a batch size of 64).</p></td></tr><tr><td colspan="1" rowspan="1"><p>Confusion matrix insights</p></td><td colspan="1" rowspan="1"><p>Better performance on the neutral class but struggled with minority classes (negative sentiment).</p></td><td colspan="1" rowspan="1"><p>Slightly better performance on minority classes (negative sentiments) than CNN + RF.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Training accuracy</p></td><td colspan="1" rowspan="1"><p>Reached about 92% during training.</p></td><td colspan="1" rowspan="1"><p>Similar, reaching about 92% during training.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Overfitting behavior</p></td><td colspan="1" rowspan="1"><p>Overfitting observed, validation accuracy plateaued, and validation loss increased over epochs.</p></td><td colspan="1" rowspan="1"><p>Overfitting observed but slightly better validation performance compared to CNN + RF.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Performance trends</p></td><td colspan="1" rowspan="1"><p>Stronger on neutral class; higher validation loss indicates less robust generalization.</p></td><td colspan="1" rowspan="1"><p>Higher validation accuracy and lower validation loss indicate better generalization; better at minority class handling.</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>The results indicate that the proposed CNN-SVM and CNN-RF hybrid models are capable of having significantly better generalization capabilities than standalone approaches. The CNN–SVM model showed consistent improvements over the traditional methods in multi-class sentiment classification tasks by using CNN for robust feature extraction and SVM for handling complex decision boundaries. Likewise, the CNN-RF hybrid model was able to tackle high-dimensional and noisy datasets by utilizing the ensemble presented by RF, thereby improving in general the reliability of sentiment predictions. In this study, two key innovations were introduced, i.e., relying on the accuracy in extracting intricate semantic patterns of CNN coupled with the classification robustness of SVM and RF. These approaches can successfully deal with long-standing issues in financial sentiment analysis, including overfitting and class imbalance. The presented hybrid frameworks outperform existing models that often fail on these tasks with respect to both accuracy and interpretability.</p><p>These hybrid architectures were analyzed comparatively with previously published research, highlighting the gains achieved. In addition, even though standalone CNN, amongst other deep learning models, can provide very powerful feature extractors, it lacks the precision and stability of using traditional classifiers. This gap was filled by the proposed methods that achieve improved generalization and robustness and remediate these critical limitations of existing sentiment analysis techniques. The broader implications of this study are in using the developed features to advance sentiment analysis techniques in financial contexts and to understand the behavior of financial asset data. The hybrid models help investors, analysts and organizations make better decisions by allowing the sentiment classification to be more reliable. In future work, adversarial training might be added to further improve model robustness or possibly pre-trained domain-specific models. For instance, FinancialBERT could yield good performance in more niche financial datasets. The applicability of these directions can also extend hybrid approaches to dynamic financial environments and further refine such hybrid approaches.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p>This work is supported by the Scientific Research Project Fund of FIRAT ÜNİVERSİTESİ (Grant No.: MF.24.46).</p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p><span style="font-family: Times New Roman, serif">The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kim</surname>
              <given-names>Yoon</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1408.5882</pub-id>
          <article-title>Convolutional neural networks for sentence classification</article-title>
          <source>arXiv preprint arXiv:1408.5882</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Yue</given-names>
            </name>
            <name>
              <surname>Wallace</surname>
              <given-names>Byron</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1510.03820</pub-id>
          <article-title>A sensitivity analysis of (and practitioners' guide to) convolutional neural networks for sentence classification</article-title>
          <source>arXiv preprint arXiv:1510.03820</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-6</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Akita</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Yoshihara</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Matsubara</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Uehara</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICIS.2016.7550882</pub-id>
          <article-title>Deep learning for stock prediction using numerical and textual information</article-title>
          <source>2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS), Okayama, Japan</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="conf-paper">
          <page-range>261-269</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Bian</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>T. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3159652.3159690</pub-id>
          <article-title>Listening to chaotic whispers: A deep learning framework for news-oriented stock trend prediction</article-title>
          <source>Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, New York, NY, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1415-1425</page-range>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ding</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Duan</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3115/v1/D14-1148</pub-id>
          <article-title>Using structured events to predict stock price movement: An empirical investigation</article-title>
          <source>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Doha, Qatar</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="conf-paper">
          <page-range>2327-2333</page-range>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ding</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Duan</surname>
              <given-names>J. W.</given-names>
            </name>
          </person-group>
          <article-title>Deep learning for event-driven stock prediction</article-title>
          <source>Proceedings of the 24th International Conference on Artificial Intelligence, Buenos Aires, Argentina</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="conf-paper">
          <page-range>24-29</page-range>
          <year>2013</year>
          <person-group person-group-type="author">
            <name>
              <surname>Si</surname>
              <given-names>J. F.</given-names>
            </name>
            <name>
              <surname>Mukherjee</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>H. Y.</given-names>
            </name>
            <name>
              <surname>Deng</surname>
              <given-names>X. T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.13140/2.1.3604.7043</pub-id>
          <article-title>Exploiting topic-based Twitter sentiment for stock prediction</article-title>
          <source>Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>42</volume>
          <page-range>9603-9611</page-range>
          <issue>24</issue>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Nguyen</surname>
              <given-names>Thien Hai</given-names>
            </name>
            <name>
              <surname>Shirai</surname>
              <given-names>Kiyoaki</given-names>
            </name>
            <name>
              <surname>Velcin</surname>
              <given-names>Julien</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2015.07.052</pub-id>
          <article-title>Sentiment analysis on social media for stock movement prediction</article-title>
          <source>Expert Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>20</volume>
          <page-range>273-297</page-range>
          <year>1995</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cortes</surname>
              <given-names>Corinna</given-names>
            </name>
            <name>
              <surname>Vapnik</surname>
              <given-names>Vladimir</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/BF00994018</pub-id>
          <article-title>Support-vector networks</article-title>
          <source>Mach. Learn.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="conf-paper">
          <page-range>137-142</page-range>
          <year>1998</year>
          <person-group person-group-type="author">
            <name>
              <surname>Joachims</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/BFb0026683</pub-id>
          <article-title>Text categorization with support vector machines: Learning with many relevant features</article-title>
          <source>Proceedings of 10th European Conference on Machine Learning, Chemnitz, Germany</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>45</volume>
          <page-range>5-32</page-range>
          <year>2001</year>
          <person-group person-group-type="author">
            <name>
              <surname>Breiman</surname>
              <given-names>Leo</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
          <article-title>Random forests</article-title>
          <source>Mach. Learn.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>2021</volume>
          <page-range>9903518</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Widiputra</surname>
              <given-names>H</given-names>
            </name>
            <name>
              <surname>Mailangkay</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Gautama</surname>
              <given-names>E</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2021/9903518</pub-id>
          <article-title>Multivariate CNN-LSTM model for multiple parallel financial time-series prediction</article-title>
          <source>Complexity</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>204</volume>
          <page-range>117604</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2022.117604</pub-id>
          <article-title>Prediction of stock market index based on ISSA-BP neural network</article-title>
          <source>Expert Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1419-1426</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Nelson</surname>
              <given-names>D. M.</given-names>
            </name>
            <name>
              <surname>Pereira</surname>
              <given-names>A. C.</given-names>
            </name>
            <name>
              <surname>De Oliveira</surname>
              <given-names>R. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/IJCNN.2017.7966019</pub-id>
          <article-title>Stock market's price movement prediction with LSTM neural networks</article-title>
          <source>2017 International Joint Conference on Neural Networks (IJCNN), Anchorage, AK, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>Yi</given-names>
            </name>
            <name>
              <surname>UY</surname>
              <given-names>Mark Christopher Siy</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Allen</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2006.08097</pub-id>
          <article-title>FinancialBERT: A pre-trained language model for financial text mining</article-title>
          <source>arXiv preprint arXiv:2006.08097</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>28299-28308</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wen</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>L. F.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2019.2901842</pub-id>
          <article-title>Stock market trend prediction using high-order information of time series</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>69</volume>
          <page-range>14-23</page-range>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>X. D.</given-names>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names>H. R.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J. P.</given-names>
            </name>
            <name>
              <surname>Deng</surname>
              <given-names>X. T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.knosys.2014.04.022</pub-id>
          <article-title>News impact on stock price return via sentiment analysis</article-title>
          <source>Knowl.-Based Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Feng</surname>
              <given-names>F. L.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>H. M.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>X. N.</given-names>
            </name>
            <name>
              <surname>Ding</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>M. S.</given-names>
            </name>
            <name>
              <surname>Chua</surname>
              <given-names>T. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1810.09936</pub-id>
          <article-title>Enhancing stock movement prediction with adversarial training</article-title>
          <source>arXiv preprint arXiv:1810.09936</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>86</volume>
          <page-range>2278-2324</page-range>
          <issue>11</issue>
          <year>1998</year>
          <person-group person-group-type="author">
            <name>
              <surname>LeCun</surname>
              <given-names>Yann</given-names>
            </name>
            <name>
              <surname>Bottou</surname>
              <given-names>Léon</given-names>
            </name>
            <name>
              <surname>Bengio</surname>
              <given-names>Yoshua</given-names>
            </name>
            <name>
              <surname>Haffner</surname>
              <given-names>Patrick</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/5.726791</pub-id>
          <article-title>Gradient-based learning applied to document recognition</article-title>
          <source>Proc. IEEE</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="conf-paper">
          <page-range>374-378</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Peng</surname>
              <given-names>Y. T.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18653/v1/N16-1041</pub-id>
          <article-title>Leverage financial news to predict stock price movements using word embeddings and deep neural networks</article-title>
          <source>Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego, California</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>167</volume>
          <page-range>112249</page-range>
          <issue>Part A</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wankhade</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kulkarni</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Rao</surname>
              <given-names>A. C. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.asoc.2024.112249</pub-id>
          <article-title>A survey on aspect base sentiment analysis methods and challenges</article-title>
          <source>Appl. Soft Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>152</volume>
          <page-range>111206</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pandey</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Vishwakarma</surname>
              <given-names>D. K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.asoc.2023.111206</pub-id>
          <article-title>Progress, achievements, and challenges in multimodal sentiment analysis using deep learning: A survey</article-title>
          <source>Appl. Soft Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>139</volume>
          <page-range>187-224</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>W. S.</given-names>
            </name>
            <name>
              <surname>Hussain</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Cauteruccio</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.32604/cmes.2023.031388</pub-id>
          <article-title>Deep learning for financial time series prediction: A state-of-the-art review of standalone and hybrid models</article-title>
          <source>Comput. Model. Eng. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>244</volume>
          <page-range>1-8</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>George</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Murugesan</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.procs.2024.10.172</pub-id>
          <article-title>Improving sentiment analysis of financial news headlines using hybrid Word2Vec-TFIDF feature extraction technique</article-title>
          <source>Procedia Comput. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>75</volume>
          <page-range>1577-1601</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mohamed</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Zain</surname>
              <given-names>Z. M.</given-names>
            </name>
            <name>
              <surname>Shaiba</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Alturki</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Aldehim</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Sakri</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Yatin</surname>
              <given-names>S. F. M.</given-names>
            </name>
            <name>
              <surname>Zain</surname>
              <given-names>J. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.32604/cmc.2023.034746</pub-id>
          <article-title>LexDeep: Hybrid lexicon and deep learning sentiment analysis using Twitter for unemployment-related discussions during COVID-19</article-title>
          <source>Comput. Mater. Contin.</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>258</volume>
          <page-range>109954</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Almalis</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Kouloumpris</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Vlahavas</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.knosys.2022.109954</pub-id>
          <article-title>Sector-level sentiment analysis with deep learning</article-title>
          <source>Knowl.-Based Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>36</volume>
          <page-range>100438</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ewald</surname>
              <given-names>C. O.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.jcomm.2024.100438</pub-id>
          <article-title>The role of news sentiment in salmon price prediction using deep learning</article-title>
          <source>J. Commodity Markets</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>