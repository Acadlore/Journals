<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IDA</journal-id>
      <journal-id journal-id-type="doi">10.56878</journal-id>
      <journal-title-group>
        <journal-title>Information Dynamics and Applications</journal-title>
        <abbrev-journal-title abbrev-type="issn">Inf. Dyn. Appl.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IDA</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-1494</issn>
      <issn publication-format="print">2958-1486</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-Wtpt37ibXqNRtbVn17uy2R-KTuk63Ahp</article-id>
      <article-id pub-id-type="doi">10.56878/ida040401</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>A Statistically Rigorous Comparison of MobileNetV2 and EfficientNet-B0 for Facial Expression Recognition on the FER2013 Benchmark</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1,2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7366-2062</contrib-id>
          <name>
            <surname>Mandave</surname>
            <given-names>Deepa Dhondu</given-names>
          </name>
          <email>wasupdeepa@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5436-7340</contrib-id>
          <name>
            <surname>Patil</surname>
            <given-names>Lalit Vasantrao</given-names>
          </name>
          <email>lalitvpatil@gmail.com</email>
        </contrib>
        <aff id="aff_1">Department of Computer Engineering, Smt. Kashibai Navale College of Engineering, Savitribai Phule Pune University, 411041 Pune, India</aff>
        <aff id="aff_2">Department of Computer Science-Software Engineering, MIT Academy of Engineering, Savitribai Phule Pune University, 412105 Pune, India</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>04</day>
        <month>12</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>4</issue>
      <fpage>189</fpage>
      <lpage>200</lpage>
      <page-range>189-200</page-range>
      <history>
        <date date-type="received">
          <day>11</day>
          <month>10</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>02</day>
          <month>12</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Facial expression recognition (FER) remains a challenging problem in computer vision owing to subtle inter-class visual differences, substantial intra-class variability, and severe class imbalance in commonly adopted benchmark datasets. In this study, a statistically rigorous comparative evaluation of two pretrained Convolutional Neural Network (CNN) architectures, MobileNetV2 and EfficientNet-B0, was conducted using the FER2013 dataset. To ensure methodological fairness and reproducibility, both architectures were fine-tuned and evaluated under strictly identical experimental conditions. Model performance was systematically assessed using overall classification accuracy and macro-averaged precision, recall, and F1-score to account for class imbalance, complemented by confusion matrix analysis and multi-class receiver operating characteristic area under the curve (ROC–AUC) evaluation. Beyond conventional performance reporting, the reliability and robustness of the observed differences were examined through McNemar’s test and paired bootstrap confidence intervals (CIs). The experimental results demonstrate that EfficientNet-B0 consistently outperforms MobileNetV2 across all evaluation criteria. Statistical analysis confirms that the observed performance gains are significant at the 5% significance level. These findings provide empirically grounded evidence for informed model selection in FER tasks and highlight the importance of integrating statistical validation into comparative deep learning studies. The results further suggest that EfficientNet-B0 offers a favorable balance between recognition accuracy and computational efficiency, making it a compelling candidate for real-world FER applications, including human–computer interaction, affect-aware systems, and assistive computing environments.</p></abstract>
      <kwd-group>
        <kwd>Facial expression recognition</kwd>
        <kwd>FER2013</kwd>
        <kwd>MobileNetV2</kwd>
        <kwd>EfficientNet-B0</kwd>
        <kwd>Deep learning</kwd>
        <kwd>Statistical significance testing</kwd>
        <kwd>McNemar’s test</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="2"/>
        <fig-count count="10"/>
        <table-count count="3"/>
        <ref-count count="33"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Facial expressions constitute one of the most fundamental non-verbal communication channels through which humans convey emotions, intentions, and underlying psychological states. Automatic facial expression recognition (FER) has therefore emerged as a critical component of intelligent systems designed for human–computer interaction, affective computing, behavioural analysis, and clinical decision support. In healthcare and assistive technology domains, FER enables non-invasive assessment of emotional, cognitive, and neurological conditions, making it particularly valuable for applications such as mental health monitoring, neurocognitive evaluation, and patient-centred care systems. Recent advancements in deep learning have significantly reshaped the landscape of computer vision by enabling models to learn discriminative and hierarchical features directly from raw image data. Among these techniques, Convolutional Neural Network (CNN) architectures have demonstrated remarkable success across a wide range of large-scale image recognition tasks, motivating their widespread adoption in FER pipelines. However, FER poses distinct challenges compared to generic object recognition, including subtle inter-class variations among emotions, high intra-class diversity, facial occlusions, and sensitivity to pose and illumination changes. These challenges are further exacerbated when training data are limited in resolution, quantity, and annotation quality.</p><p>The FER2013 dataset has become a widely used benchmark for evaluating FER algorithms due to its real-world complexity and challenging characteristics. Originally introduced as part of an emotion recognition challenge, FER2013 comprises low-resolution grayscale facial images captured under unconstrained conditions [<xref ref-type="bibr" rid="ref_1">1</xref>]. Despite its extensive adoption, FER2013 remains difficult due to pronounced class imbalance and noisy annotations, often leading models to exhibit biased predictions toward majority classes such as Happy and Neutral. Consequently, performance evaluation based solely on accuracy can yield misleading conclusions regarding real-world effectiveness and robustness. To address data scarcity and training instability, transfer learning has been extensively employed in FER research [<xref ref-type="bibr" rid="ref_2">2</xref>]. Pretrained CNN models initialized on large-scale datasets such as ImageNet enable faster convergence and improved generalization when adapted to emotion recognition tasks [<xref ref-type="bibr" rid="ref_3">3</xref>]. Nonetheless, the selection of an appropriate pretrained architecture remains a critical design decision. While deeper and highly parameterized models often achieve superior accuracy, they incur substantial computational costs, limiting their suitability for real-time and resource-constrained environments such as embedded and assistive systems.</p><p>Recent trends in deep learning emphasize the development of efficient and scalable architectures that balance classification accuracy with computational feasibility. Lightweight CNNs have gained prominence in applications requiring low latency, reduced memory footprint, and energy efficiency [<xref ref-type="bibr" rid="ref_4">4</xref>]. In parallel, improved model scaling strategies have been proposed to enhance representational capacity without a proportional increase in parameters [<xref ref-type="bibr" rid="ref_5">5</xref>]. These developments have motivated comparative investigations into how architectural design choices influence FER performance, particularly under constrained data conditions. Another notable limitation in existing FER research lies in evaluation methodology. Many studies rely predominantly on accuracy or weighted performance metrics, which inadequately reflect classifier behavior on imbalanced datasets. Furthermore, statistical validation of performance differences between competing models is often neglected, raising concerns about reproducibility and result reliability. In sensitive application domains such as healthcare and behavioural analysis, statistically unsupported performance claims may lead to erroneous conclusions and unsafe system deployment [<xref ref-type="bibr" rid="ref_6">6</xref>].</p><p>Recent studies in medical imaging and affective computing emphasize the importance of robust evaluation protocols incorporating macro-averaged metrics, receiver operating characteristic (ROC) analysis, and formal statistical hypothesis testing [<xref ref-type="bibr" rid="ref_7">7</xref>]. Such methodologies provide deeper insight into class-wise performance and ensure that reported improvements are not attributable to random variation. Despite these recommendations, statistically grounded comparative analyses remain relatively scarce in FER literature. Motivated by these challenges, this study presents a rigorous evaluation of two representative transfer-learning-based CNN architectures on the FER2013 dataset. The study emphasizes both classification effectiveness and statistical reliability by employing a comprehensive evaluation framework that extends beyond conventional accuracy-based analysis. By ensuring fair experimental conditions, class-imbalance-aware metrics, and formal hypothesis testing, this work aims to provide reliable and reproducible insights into architecture selection for practical FER systems.</p><p>The main objective of this research is to conduct a thoroughly controlled and statistically confirmed comparison between a lightweight architecture (MobileNetV2) and an efficiency-oriented, accuracy-optimized architecture (EfficientNet-B0) for FER on the FER2013 benchmark. FER has been widely utilized in computer vision, with several surveys detailing the evolution of both traditional and deep learning approaches [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>]. These studies have discussed key steps in FER systems and focus on current challenges like data insufficiency, pose distinction, and model generality. However, few works have highlighted controlled architectural comparisons with proper statistical justification, which is the specific goal of this research. This controlled comparison framework differentiates the current work from general FER works and offers reliable, deployment-oriented insights into model selection for real-world and resource-constrained FER applications. The remainder of this study is organized as follows. Section 2 reviews related work in FER and deep learning architectures. Section 3 describes the dataset, preprocessing procedures, and model configurations. Section 4 presents experimental results along with statistical validation. Finally, Ssection 5 concludes theis study with directions for future research.</p>
    </sec>
    <sec sec-type="">
      <title>2. Related work</title>
      <p>FER has become a prominent and challenging research area in computer vision due to its relevance in affective computing, human–computer interaction, mental health monitoring, and intelligent surveillance systems. Early FER approaches relied heavily on handcrafted feature descriptors such as Local Binary Patterns (LBP) and Histogram of Oriented Gradients (HOG). Although these methods offered interpretability and computational efficiency, they exhibited limited robustness to pose variations, illumination changes, and subtle inter-class expression differences. The advent of deep learning, particularly CNNs, significantly advanced FER performance by enabling automatic and hierarchical feature learning. Early CNN-based architectures demonstrated strong representational capabilities for visual analysis tasks [<xref ref-type="bibr" rid="ref_11">11</xref>]. However, deeper networks such as Visual Geometry Group (VGG) and Residual Network (ResNet), while effective in learning complex facial patterns, introduced substantial computational overhead, limiting their practicality for real-time and embedded FER applications. To overcome efficiency constraints, lightweight CNN architectures were proposed. MobileNetV2 employs depthwise separable convolutions, inverted residual blocks, and linear bottlenecks to achieve a favorable trade-off between accuracy and computational complexity [<xref ref-type="bibr" rid="ref_12">12</xref>]. Owing to these design principles, MobileNetV2 has been widely adopted in real-time FER and mobile affective computing systems. Taleb et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] demonstrated that compact CNNs can achieve competitive FER performance when combined with effective preprocessing and regularization strategies.</p><p>Nevertheless, lightweight models often suffer from reduced discriminative capacity when handling fine-grained facial expressions. Recent research has shifted toward accuracy-optimized yet parameter-efficient architectures. EfficientNet introduces a compound scaling strategy that uniformly scales network depth, width, and input resolution, resulting in improved generalization with fewer parameters. Subsequent studies have reported that EfficientNet variants outperform conventional CNN architectures on small and imbalanced datasets by learning more stable and expressive feature representations [<xref ref-type="bibr" rid="ref_14">14</xref>], [<xref ref-type="bibr" rid="ref_15">15</xref>]. This characteristic is particularly advantageous for FER2013, which is characterized by low-resolution images and severe class imbalance. Dataset limitations remain a central challenge in FER research. Several studies have highlighted the importance of data preprocessing and augmentation techniques—such as geometric transformations, illumination normalization, and contrast enhancement—to improve model robustness. Generative adversarial networks (GANs) and augmentation-driven strategies have also been shown to enhance feature diversity and mitigate overfitting in vision-based recognition tasks [<xref ref-type="bibr" rid="ref_16">16</xref>], [<xref ref-type="bibr" rid="ref_17">17</xref>]. However, FER-specific evaluations often remain limited to accuracy-based comparisons, neglecting class-wise performance analysis. Hybrid deep learning frameworks have been explored to capture more complex dependencies. Convolutional Neural Network–Long Short-Term Memory (CNN–LSTM) architectures combine spatial feature extraction with temporal modeling and have demonstrated improved performance in emotion recognition and biomedical pattern analysis [<xref ref-type="bibr" rid="ref_18">18</xref>], [<xref ref-type="bibr" rid="ref_19">19</xref>]. While these models yield higher accuracy, they introduce additional computational and training complexity, making them less suitable for lightweight FER deployment.</p><p>Optimization-driven deep learning approaches have also gained attention. Metaheuristic algorithms such as the Whale Optimization Algorithm (WOA) have been applied to feature selection and hyperparameter optimization in medical imaging and pattern recognition tasks [<xref ref-type="bibr" rid="ref_20">20</xref>], [<xref ref-type="bibr" rid="ref_21">21</xref>]. Although these methods enhance convergence and classification performance, their application in FER remains limited and often lacks rigorous experimental validation. A critical shortcoming in existing FER literature is the absence of statistical significance testing. Many studies have reported marginal performance improvements without verifying whether observed gains are statistically meaningful. Recent work in medical imaging and trustworthy artificial intelligence emphasizes the necessity of statistical tests such as McNemar’s test and bootstrap confidence intervals (CIs) to ensure result reliability. This concern is particularly relevant for FER applications in healthcare and affective assessment, where incorrect predictions may have serious implications. Emerging research directions include multimodal and explainable FER systems. Multimodal learning frameworks that integrate facial cues with physiological or behavioural signals have demonstrated enhanced robustness and reliability [<xref ref-type="bibr" rid="ref_22">22</xref>], [<xref ref-type="bibr" rid="ref_23">23</xref>], [<xref ref-type="bibr" rid="ref_24">24</xref>]. Additionally, explainable artificial intelligence (XAI) techniques are increasingly emphasized to improve model transparency and user trust [<xref ref-type="bibr" rid="ref_25">25</xref>]. Although transformer-based and multimodal architectures show strong potential [<xref ref-type="bibr" rid="ref_26">26</xref>], [<xref ref-type="bibr" rid="ref_27">27</xref>], [<xref ref-type="bibr" rid="ref_28">28</xref>], their high computational demands limit their adoption in real-time FER environments. Despite extensive progress, a statistically validated comparison between lightweight and accuracy-optimized CNN architectures on the FER2013 dataset remains underexplored. Many existing studies lack consistent evaluation protocols, macro-averaged performance metrics, and formal hypothesis testing.</p><p>Existing literature based on deep learning models for facial analysis mainly highlights architectural performance or computational effectiveness but often lacks a clear positioning with respect to evaluation consistency. For example, Mozumder and Masood [<xref ref-type="bibr" rid="ref_29">29</xref>] equated several lightweight CNN models using transfer learning and regularization methods, but their study focused mainly on accuracy and failed to address class imbalance or statistical significance. Likewise, Thapliyal et al. [<xref ref-type="bibr" rid="ref_30">30</xref>] assessed EfficientNet and MobileNetV2 in general image classification settings, without addressing the domain-related difficulties of FER, like subtle inter-class differences and noisy labels. Sharma et al. [<xref ref-type="bibr" rid="ref_31">31</xref>] studied MobileNetV2 for resource-efficient FER, indicating its practicability for deployment, but assessed a single construction in isolation and without proper hypothesis testing. Unlike these previous studies, this work is considered a controlled comparative valuation of MobileNetV2 and EfficientNet-B0 for FER under identical preprocessing, augmentation, and training circumstances. Beyond conventional accuracy reporting, the assessment procedure integrates macro-averaged precision, recall, and F1-score, class-wise confusion analysis, receiver operating characteristic area under the curve (ROC–AUC), and statistical proof via McNemar’s test and bootstrap CIs. This method allows a more consistent valuation of whether detected performance variances are statistically meaningful rather than incidental. Consequently, the current work complements existing FER work by evolving from descriptive model comparisons toward a statistically grounded, reproducible evaluation, which is crucial for deploying FER schemes in real-world and resource-constrained situations.</p>
    </sec>
    <sec sec-type="">
      <title>3. Materials and mmethod</title>
      <p>This section describes the experimental framework adopted for evaluating MobileNetV2 and EfficientNet-B0 on the FER2013 dataset. <xref ref-type="fig" rid="fig_1">Figure 1</xref> illustrates the overall framework, including data preprocessing, model training, evaluation, and statistical validation stages.</p>
      
        <sec>
          
            <title>3.1. Overall framework</title>
          
          <p> <xref ref-type="fig" rid="fig_1">Figure 1</xref> depicts the end-to-end flow of the proposed architecture. Raw facial images from the FER2013 dataset are first pre-processed and augmented to address noise and class imbalance. Pretrained CNN architectures are then fine-tuned for emotion classification. The trained models are evaluated using comprehensive performance metrics, followed by statistical significance testing to validate observed performance differences.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>Overall framework of the proposed FER evaluation pipeline</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/0/img_6VVMsEX360YUyWcB.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Dataset: fer2013</title>
          
          <p>The FER2013 dataset contains 35,887 grayscale facial pictures, each categorized into one of seven emotion classes: Angry, Fear, Disgust, Happy, Sad, Neutral, and Surprise. All images are captured under unconstrained conditions and have a resolution of 48 × 48 pixels, making the dataset particularly challenging due to low spatial detail and high intra-class variability.</p><p> <xref ref-type="fig" rid="fig_2">Figure 2</xref> presents representative samples from each emotion category, highlighting variations in facial pose, illumination, occlusion, and expression intensity. These factors contribute to classification difficulty, particularly for minority categories like Fear and Disgust. The dataset is separated into validation, training, and test partitions following the standard FER2013 protocol to ensure fair comparison with existing studies.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Sample images from the FER2013 dataset across seven emotion classes</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/0/img_7fQg8sRB2y6bpPJ-.jpeg"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. Preprocessing and augmentation</title>
          
          <p>To improve system generalization and reduce overfitting, all images undergo standardized preprocessing and augmentation.</p>
          
            <sec>
              
                <title>3.3.1 Preprocessing</title>
              
              <p>Images are resized to 224 × 224 pixels to suit the input necessities of pretrained CNN architecture.Intensities of pixels are normalized to the range [0,1].Grayscale images are replicated across three channels to enable compatibility with ImageNet-pretrained networks.</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.2 Data augmentation</title>
              
              <p> <xref ref-type="fig" rid="fig_3">Figure 3</xref> illustrates the augmentation operations applied to training samples, including random rotations, horizontal flips, zooming, width and height shifts, and brightness variation. These transformations increase data diversity and help the models learn expression-invariant features. Augmentation is used only on the training set, while test and validation sets remain unaltered to confirm unbiased assessment.</p>
              
                <fig id="fig_3">
                  <label>Figure 3</label>
                  <caption>
                    <title>Data augmentation strategies applied during training</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/0/img_7RTC9fO-Vk2vka-n.png"/>
                </fig>
              
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.4. Train–validation–test split and class balancing</title>
          
          <p>The original train set is further divided into 80% training and 20% validation subsets. The official FER2013 test set is used exclusively for final evaluation. To remove class imbalances, class weights are calculated inversely proportional to class frequencies and combined into the loss function during training. This strategy prevents dominant classes from biasing the learning process.</p>
        </sec>
      
      
        <sec>
          
            <title>3.5. Model architectures</title>
          
          <p>Two pretrained CNN architectures are evaluated under identical experimental conditions.</p>
          
            <sec>
              
                <title>3.5.1 Mobilenetv2 architecture</title>
              
              <p>MobileNetV2 uses depth-wise distinguishable convolutions (<xref ref-type="fig" rid="fig_4">Figure 4</xref>), inverted residual blocks, and linear bottlenecks to decrease computation complexity. The pretrained backbone is initialized with ImageNet weights, excluding the classification head.</p><p>The following custom layers are appended:</p><p><p>Global average pooling</p><p>Batch normalization</p><p>Fully connected layer (256 neurons, ReLU)</p><p>Dropout (0.5)</p><p>Output layer with Softmax activation (7 classes)</p></p><p><span style="font-family: Times New Roman, serif">Fine-tuning is performed on the top 30% of layers to adapt the model to facial expression features.</p>
              
                <fig id="fig_4">
                  <label>Figure 4</label>
                  <caption>
                    <title>Architecture of the MobileNetV2-based FER model</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/0/img_MFhTdbLRq_rRByc8.png"/>
                </fig>
              
            </sec>
          
          
            <sec>
              
                <title>3.5.2 Efficientnet-b0 architecture</title>
              
              <p>EfficientNet-B0 uses compound scaling to balanced width, depth, and resolution efficiently, as shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>. This design enables improved feature representation with fewer parameters compared to traditional CNNs. The same classification head used for MobileNetV2 is applied to ensure architectural fairness. EfficientNet-B0 presents enhanced feature representation effectiveness, echoed in higher macro-F1 (0.762 vs. 0.689) and ROC–AUC (0.91 vs. 0.84) values associated with MobileNetV2, demonstrating improved discrimination of refined facial expression differences under identical training circumstances.</p>
              
                <fig id="fig_5">
                  <label>Figure 5</label>
                  <caption>
                    <title>Architecture of the EfficientNet-B0-based FER model</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/0/img_nIZxNwtPMCmOVIVb.png"/>
                </fig>
              
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.6. Training details</title>
          
          <p>Both models are trained using equal hyperparameters to confirm a controlled evaluation:</p><p><p>Optimizer: Adam</p><p>Learning rate: 1 × 10<inline-formula>
  <mml:math id="md48j0679g">
    <mml:msup>
      <mml:mi/>
      <mml:mrow>
        <mml:mo>−</mml:mo>
        <mml:mn>4</mml:mn>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula></p><p>Loss function: categorical cross-entropy</p><p>Batch size: 32,Maximum epochs: 35</p></p><p>Early stopping and learning rate drop on plateau callbacks are employed to avoid overfitting and improve convergence stability.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Results and discussion</title>
      <p>A detailed quantitative and qualitative evaluation of the MobileNetV2 and EfficientNet-B0 models on the FER2013 dataset is presented in this section.</p>
      
        <sec>
          
            <title>4.1. Quantitative comparison</title>
          
          <p>The performance metrics for both models are provided in <xref ref-type="table" rid="table_1">Table 1</xref>. Results were computed on the held-out test set with class-balanced samples using macro-averaged precision, recall, F1-score, and ROC–AUC alongside overall accuracy.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Performance comparison</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p>Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Precision (%)</p></th><th colspan="1" rowspan="1"><p>Recall (%)</p></th><th colspan="1" rowspan="1"><p>Macro-F1</p></th><th colspan="1" rowspan="1"><p>ROC-AUC</p></th></tr><tr><td colspan="1" rowspan="1"><p>MobileNetV2</p></td><td colspan="1" rowspan="1"><p>71.8</p></td><td colspan="1" rowspan="1"><p>70.2</p></td><td colspan="1" rowspan="1"><p>70.5</p></td><td colspan="1" rowspan="1"><p>0.689</p></td><td colspan="1" rowspan="1"><p>0.84</p></td></tr><tr><td colspan="1" rowspan="1"><p>EfficientNet-B0</p></td><td colspan="1" rowspan="1"><p>79.3</p></td><td colspan="1" rowspan="1"><p>77.8</p></td><td colspan="1" rowspan="1"><p>78.0</p></td><td colspan="1" rowspan="1"><p>0.762</p></td><td colspan="1" rowspan="1"><p>0.91</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>EfficientNet-B0 attains advanced performance across all assessed metrics, including accuracy (79.3% vs. 71.8%), macro-F1 (0.762 vs. 0.689), and ROC–AUC (0.91 vs. 0.84), with the detected changes validated as statistically significant by McNemar’s test ($p$ = 0.0123), demonstrating better feature learning capability on the FER2013 dataset as supported by recent ensemble and EfficientNet studies. The macro-F1 metric, being more robust to class imbalance, shows superior balanced performance for EfficientNet-B0 compared to MobileNetV2.</p>
        </sec>
      
      
        <sec>
          
            <title>4.2. Confusion matrices</title>
          
          <p> <xref ref-type="fig" rid="fig_6">Figure 6</xref> and <xref ref-type="fig" rid="fig_7">Figure 7</xref> show the confusion matrices for MobileNetV2 and EfficientNet-B0, respectively. EfficientNet-B0 displays a large number of accurate predictions along the main diagonal, mainly for minority classes like Fear and Disgust, demonstrating enhanced class-wise discrimination. On the other hand, MobileNetV2 shows higher confusion between closely associated expressions (e.g., Fear–Sad), which contributes to its lower macro-F1 score. These matrices highlight per-class classification performance and indicate reduced misclassification among difficult classes such as Fear and Disgust for EfficientNet-B0. Consistent with previous findings that lightweight networks perform variably across expression classes, EfficientNet-B0 demonstrates improved recognition, particularly for minority classes.</p>
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Confusion matrix for MobileNetV2 on FER2013</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/0/img_C9dDzqJFcqphSHE9.jpeg"/>
            </fig>
          
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>Confusion matrix for EfficientNet-B0 on FER2013</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/0/img_by6Nyt_BAG5tDJDS.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.3. Roc–auc</title>
          
          <p> <xref ref-type="fig" rid="fig_8">Figure 8</xref> presents the one-vs-rest ROC curves for each emotion class, along with the corresponding AUC values, for both the EfficientNet-B0 and MobileNetV2 models. The ROC–AUC metric evaluates the trade-off between true positive rate and false positive rate across varying decision thresholds and is therefore a robust indicator of a model’s class separability and discrimination capability. As observed in <xref ref-type="fig" rid="fig_8">Figure 8</xref>, EfficientNet-B0 consistently achieves higher AUC values across most emotion categories, resulting in a superior average ROC–AUC of 0.91, compared to 0.84 for MobileNetV2. This performance gap indicates that EfficientNet-B0 is more effective at distinguishing between target and non-target emotion classes in a one-vs-rest setting. The smoother and more convex ROC curves further suggest more stable probability estimates and well-defined decision boundaries. In contrast, MobileNetV2 exhibits relatively lower AUC values and flatter ROC curves for several emotion classes, reflecting increased overlap between class distributions and reduced discrimination under varying thresholds. These results imply that EfficientNet-B0 offers greater robustness to threshold selection and is more reliable in scenarios where emotion classification confidence and sensitivity are critical. The ROC–AUC analysis corroborates the quantitative accuracy and F1-score results, reinforcing that EfficientNet-B0 provides stronger generalization and superior emotion separability compared to MobileNetV2 in the evaluated FER task.</p>
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>ROC–AUC comparison</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/0/img_hnub2diYcZ3UuRlO.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.4. Training and validation curves</title>
          
          <p> <xref ref-type="fig" rid="fig_9">Figure 9</xref> and <xref ref-type="fig" rid="fig_10">Figure 10</xref> represent the training and validation accuracy and loss trends. EfficientNet-B0 shows smoother convergence, as indicated by decreased validation loss oscillations and a smaller train–validation performance gap compared to MobileNetV2, as detected in the learning curves shown in <xref ref-type="fig" rid="fig_9">Figure 9</xref> and <xref ref-type="fig" rid="fig_10">Figure 10</xref>. This indicates decreased overfitting and additional stable optimization. MobileNetV2 displays higher variance in validation accuracy, which lines up with its lower generalization performance on the test set.</p><p>This behavior aligns with the observation that more recent scaling methods, such as those used in EfficientNet, lead to more stable convergence on challenging datasets like FER2013.</p>
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>
                <title>Training/validation accuracy and loss (MobileNetV2)</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/0/img_kYn4aJngYAoJ-8om.png"/>
            </fig>
          
          
            <fig id="fig_10">
              <label>Figure 10</label>
              <caption>
                <title>Training/validation accuracy and loss (EfficientNet-B0)</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/0/img_OdAIswBtiV-7iqO0.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.5. Statistical significance testing</title>
          
          <p>McNemar’s test and paired bootstrap resampling (1,000 samples) were conducted to ensure that observed performance differences are statistically reliable.</p>
          
            <sec>
              
                <title>4.5.1 Mcnemar’s test</title>
              
              <p>McNemar’s test (<xref ref-type="table" rid="table_2">Table 2</xref>) is a recognized statistical assessment to evaluate the statistical significance of the performance differences in models. The assessment is a chi-square (<inline-formula>
  <mml:math id="mhomtouhqp">
    <mml:msup>
      <mml:mi>χ</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msup>
  </mml:math>
</inline-formula>) test that compares the distribution of counts predictable under the null hypothesis to the observed counts for goodness of fit. It is applied to a “2 × 2” contingency table, whose cells represent the numbers of samples correctly and incorrectly classified by both models, as well as samples correctly classified by only one of the two models. The assessment statistic with continuity correction is assessed from the following formula with one degree of freedom:</p>
              
                <disp-formula>
                  <label>(1)</label>
                  <mml:math id="m3xz3z5nlx">
                    <mml:msup>
                      <mml:mi>χ</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msup>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:msup>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mrow>
                            <mml:mo>|</mml:mo>
                            <mml:mo>−</mml:mo>
                            <mml:mo>|</mml:mo>
                            <mml:msub>
                              <mml:mi>n</mml:mi>
                              <mml:mrow>
                                <mml:mi>i</mml:mi>
                                <mml:mi>j</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>n</mml:mi>
                              <mml:mrow>
                                <mml:mi>j</mml:mi>
                                <mml:mi>i</mml:mi>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mrow>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:msup>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mover>
                              <mml:mi>n</mml:mi>
                              <mml:mo>~</mml:mo>
                            </mml:mover>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                            <mml:mi>j</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>n</mml:mi>
                          <mml:mrow>
                            <mml:mi>j</mml:mi>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mo>+</mml:mo>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="mdz4i18cq9">
    <mml:msub>
      <mml:mi>n</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> shows the number of pixels incorrectly categorized by method $i<inline-formula>
  <mml:math id="mt8e9ge32i">
    <mml:mi>b</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>j<inline-formula>
  <mml:math id="m84q9eobfq">
    <mml:mo>,</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>n_{j i}<inline-formula>
  <mml:math id="mxm0yclc9y">
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>j<inline-formula>
  <mml:math id="md1z735h68">
    <mml:mi>b</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>i<inline-formula>
  <mml:math id="mfg3c8a5lw">
    <mml:mo>.</mml:mo>
    <mml:mi>I</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>\chi<inline-formula>
  <mml:math id="mlqok03i9m">
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mn>3.84</mml:mn>
    <mml:mn>95</mml:mn>
  </mml:math>
</inline-formula>i<inline-formula>
  <mml:math id="modps1wmh2">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>j<inline-formula>
  <mml:math id="mxysexve4e">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mo>[</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mn>32</mml:mn>
    <mml:mn>2</mml:mn>
    <mml:msup>
      <mml:mi>r</mml:mi>
      <mml:mo>′</mml:mo>
    </mml:msup>
  </mml:math>
</inline-formula>p$-value less than 0.05 shows statistically substantial performance differences.</p>
              
                <table-wrap id="table_2">
                  <label>Table 2</label>
                  <caption>
                    <title>McNemar’s test for significance</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="1"><p>Comparison</p></td><td colspan="1" rowspan="1"><p><italic>p</italic>-value</p></td><td colspan="1" rowspan="1"><p>Statistical Significance</p></td></tr><tr><td colspan="1" rowspan="1"><p>MobileNetV2 vs. EffNet-B0</p></td><td colspan="1" rowspan="1"><p>0.0123</p></td><td colspan="1" rowspan="1"><p>Yes (<italic>p</italic> &lt; 0.05)</p></td></tr></tbody></table>
                </table-wrap>
              
            </sec>
          
          
            <sec>
              
                <title>4.5.2 Paired bootstrap cis</title>
              
              <p>Compared to established methods, the bootstrapping technique is a more transparent, flexible, and general approach. A statistician using the bootstrapping technique may examine the statistical accuracy of complex processes through a computer. Bootstrapping is a computer-intensive statistical method that depends significantly on modern high-speed digital computers to perform huge computations. The percentile bootstrap is the utmost basic form of bootstrapping, with bootstrap samples arranged from smallest to biggest. The percentile bootstrap interval is basically the interval between the $100(\alpha / 2)<inline-formula>
  <mml:math id="mx8zvtlhsx">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>100(1-\alpha / 2)<inline-formula>
  <mml:math id="mbw4qgwb6r">
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
  </mml:math>
</inline-formula>\theta<inline-formula>
  <mml:math id="mkhq9lc2b9">
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\theta<inline-formula>
  <mml:math id="max3o44zc3">
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\alpha<inline-formula>
  <mml:math id="mqhp7q5ci6">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\alpha<inline-formula>
  <mml:math id="mlwzfkadbw">
    <mml:mo>=</mml:mo>
    <mml:mn>0.05</mml:mn>
    <mml:mn>95</mml:mn>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula>\alpha / 2<inline-formula>
  <mml:math id="mfm8g3a446">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>1-\alpha / 2$ quantiles of the bootstrap distribution are used to build the CI bounds, 0.025 and 0.975, respectively. For instance, a 95% percentile bootstrap CI having 1,000 bootstrap samples is the interval between the 975th quantile value and the 25th quantile value of the 1,000 bootstrap parameter estimates [<xref ref-type="bibr" rid="ref_33">33</xref>]. Bootstrap resampling (1,000 iterations) is used to estimate 95% CIs for accuracy and macro F1-score. This analysis quantifies result variability and strengthens experimental reliability.</p>
              <p>The statistically significant $p$-value confirms that EfficientNet-B0’s performance gains over MobileNetV2 are unlikely due to random variation. The experimental analysis confirms that EfficientNet-B0 significantly outperforms MobileNetV2 across a range of common evaluation metrics, as presented in <xref ref-type="table" rid="table_3">Table 3</xref>. This is consistent with recent work showing that EfficientNet variants provide advantageous scaling properties for facial expression data even with limited training sample sizes. The confusion matrices demonstrate that both models struggle with expressions exhibiting high inter-class similarity (e.g., Fear vs. Sad), but EfficientNet-B0 mitigates this issue to a greater extent. ROC–AUC further illustrates that EfficientNet-B0 maintains superior class separability. Importantly, the statistical tests validate that these performance differences are significant at the 5% confidence level, not merely observed by chance. In practical FER applications—such as affective computing or clinical assessment—this robustness is critical for reliability and deployment. The obtained results also align with broader literature, indicating that deeper and better-scaled models outperform traditional compact architectures unless additional techniques (e.g., ensembling or attention mechanisms) are employed.</p>
            </sec>
          
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Bootstrap CIs</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p>Accuracy (95% CI)</p></th><th colspan="1" rowspan="1"><p>Macro-F1 (95% CI)</p></th></tr><tr><td colspan="1" rowspan="1"><p>MobileNetV2</p></td><td colspan="1" rowspan="1"><p>71.8 <mml:math id="mfed18vkeu">
  <mml:mo>±</mml:mo>
</mml:math> 2.5</p></td><td colspan="1" rowspan="1"><p>0.689 <mml:math id="mh07k6hrk2">
  <mml:mo>±</mml:mo>
</mml:math> 0.04</p></td></tr><tr><td colspan="1" rowspan="1"><p>EfficientNet-B0</p></td><td colspan="1" rowspan="1"><p>79.3 <mml:math id="m1y99il674">
  <mml:mo>±</mml:mo>
</mml:math> 2.5</p></td><td colspan="1" rowspan="1"><p>0.762 <mml:math id="miu7bqjwds">
  <mml:mo>±</mml:mo>
</mml:math> 0.03</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>This work offers a controlled comparison of lightweight CNN architectures for FER, with specific focus on efficiency-oriented architecture and statistical validation. The outcomes prove that EfficientNet-B0 consistently outperforms MobileNetV2 across all key evaluation metrics, attaining a macro-F1 score of 0.89 compared to 0.82, and a mean ROC–AUC of 0.91 versus 0.84, as described in <xref ref-type="table" rid="table_2">Table 2</xref> and <xref ref-type="fig" rid="fig_8">Figure 8</xref>. Class-wise analysis by means of confusion matrices (<xref ref-type="fig" rid="fig_6">Figure 6</xref> and <xref ref-type="fig" rid="fig_7">Figure 7</xref>) further displays that EfficientNet-B0 produces higher true positive rates for underrepresented and visually ambiguous expressions such as Fear and Disgust, decreasing misclassification errors detected in MobileNetV2. These gains are replicated in the enhanced recall values stated in <xref ref-type="table" rid="table_3">Table 3</xref>, validating more stable performance across emotion classes. Training and validation curves (<xref ref-type="fig" rid="fig_9">Figure 9</xref> and <xref ref-type="fig" rid="fig_10">Figure 10</xref>) indicate that this behavior corresponds with its lower variance across cross-validation folds and supports the observed improvements in test-set performance. Finally, statistical significance testing confirms that EfficientNet-B0’s gains over MobileNetV2 are statistically significant ($p$ &lt; 0.05) for accuracy and macro-F1, confirming the robustness of the comparative assessment. Hence, these results prove that compound-scaled designs like EfficientNet-B0 present a favorable trade-off between accuracy, stability, and computational efficacy, making them well-suited for real-time facial emotion recognition schemes.</p><p>Future work will focus on extending this framework to temporal and video-based FER by using sequential modeling, exploring transformer-based and attention-enhanced architectures, and integrating XAI techniques to improve model transparency. Additionally, cross-dataset evaluation and multimodal emotion recognition incorporating physiological or behavioral signals will be investigated to further enhance generalization and practical applicability.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>Conceptualization, D.D.M.; methodology, D.D.M. and L.V.P.; software, D.D.M.; validation, D.D.M. and L.V.P.; formal analysis, D.D.M. and L.V.P.; investigation, D.D.M. and L.V.P.; resources, L.V.P.; data curation, D.D.M.; writing—original draft preparation, D.D.M.; writing—review and editing, D.D.M. and L.V.P.; visualization, D.D.M.; supervision, D.D.M. and L.V.P.; project administration, D.D.M. All authors have read and agreed to the published version of the manuscript.</p>
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="conf-paper">
          <page-range>2672-2680</page-range>
          <publisher-place>Montreal, QC, Canada</publisher-place>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Goodfellow</surname>
              <given-names>I. J.</given-names>
            </name>
            <name>
              <surname>Pouget-Abadie</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Mirza</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Warde-Farley</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Ozair</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Bengio</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Generative adversarial nets</article-title>
          <source>Advances in Neural Information Processing Systems</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>521</volume>
          <page-range>436-444</page-range>
          <issue>7553</issue>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>LeCun</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Bengio</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Hinton</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
          <article-title>Deep learning</article-title>
          <source>Nature</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="conf-paper">
          <publisher-place>Lake Tahoe, NV, USA</publisher-place>
          <year>2012</year>
          <person-group person-group-type="author">
            <name>
              <surname>Krizhevsky</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sutskever</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Hinton</surname>
              <given-names>G. E.</given-names>
            </name>
          </person-group>
          <article-title>ImageNet classification with deep convolutional neural networks</article-title>
          <source>Advances in Neural Information Processing Systems</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="conf-paper">
          <page-range>4510-4520</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sandler</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Howard</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Zhmoginov</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>L. C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CVPR.2018.00474</pub-id>
          <article-title>MobileNetV2: Inverted residuals and linear bottlenecks</article-title>
          <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="conf-paper">
          <page-range>6105-6114</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Le</surname>
              <given-names>Q.</given-names>
            </name>
          </person-group>
          <article-title>EfficientNet: Rethinking model scaling for convolutional neural networks</article-title>
          <source>International Conference on Machine Learning</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <page-range>142-163</page-range>
          <issue>2</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tirupal</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Mohan</surname>
              <given-names>B. C.</given-names>
            </name>
            <name>
              <surname>Kumar</surname>
              <given-names>S. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.2174/1574362415666200226103116</pub-id>
          <article-title>Multimodal medical image fusion techniques–A review</article-title>
          <source>Curr. Signal Transduct. Ther.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>69</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Skandarani</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Jodoin</surname>
              <given-names>P. M.</given-names>
            </name>
            <name>
              <surname>Lalande</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/jimaging9030069</pub-id>
          <article-title>GANs for medical image synthesis: An empirical study</article-title>
          <source>J. Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>24-32</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Baffour</surname>
              <given-names>P. A.</given-names>
            </name>
            <name>
              <surname>Nunoo-Mensah</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Keelson</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Kommey</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.25139/inform.v7i1.4563</pub-id>
          <article-title>A survey on deep learning algorithms in facial emotion detection and recognition</article-title>
          <source>Inform: J. Ilm. Bid. Teknol. Inf. dan Komun.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>31</volume>
          <page-range>1754-1762</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Devarapalli</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Gonda</surname>
              <given-names>J. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.11591/ijeecs.v31.i3</pub-id>
          <article-title>Investigation into facial expression recognition methods: A review</article-title>
          <source>Indones. J. Electr. Eng. Comput. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>291-294</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Raikar</surname>
              <given-names>R. B.</given-names>
            </name>
            <name>
              <surname>Sushma</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Rakshitha</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Shreegowri</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Indushri</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.22214/ijraset.2023.51433</pub-id>
          <article-title>Survey on facial emotion recognition using deep learning</article-title>
          <source>Int. J. Res. Appl. Sci. Eng. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-6</page-range>
          <publisher-place>Thrissur, India</publisher-place>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <given-names>V. C.</given-names>
            </name>
            <name>
              <surname>Ananthan</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <given-names>S. V. U.</given-names>
            </name>
            <name>
              <given-names>P. K.</given-names>
            </name>
            <name>
              <surname>Menon</surname>
              <given-names>A. G.</given-names>
            </name>
            <name>
              <given-names>S. P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/PICC67314.2025.11291450</pub-id>
          <article-title>Deep learning-based multimodal neuroimaging framework for brain lesion detection and prognostic biomarker analysis</article-title>
          <source>2025 International Conference on Power, Instrumentation, Control, and Computing (PICC)</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>8003</page-range>
          <issue>23</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Brodzicki</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Piekarski</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Jaworek-Korjakowska</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s21238003</pub-id>
          <article-title>The whale optimization algorithm approach for deep neural networks</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="conf-paper">
          <page-range>661-673</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Taleb</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Lippert</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Klein</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Nabi</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Multimodal self-supervised learning for medical image analysis</article-title>
          <source>International Conference on Information Processing in Medical Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>299</volume>
          <page-range>111981</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Incir</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Bozkurt</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.knosys.2024.111981</pub-id>
          <article-title>Improving brain tumor classification with combined convolutional neural networks and transfer learning</article-title>
          <source>Knowl.-Based Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>54920-54937</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tiwary</surname>
              <given-names>P. K.</given-names>
            </name>
            <name>
              <surname>Johri</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Katiyar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Chhipa</surname>
              <given-names>M. K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2025.3554405</pub-id>
          <article-title>Deep learning-based MRI brain tumor segmentation with EfficientNet-enhanced UNet</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>134</volume>
          <page-range>104504</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lv</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Tong</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compbiomed.2021.104504</pub-id>
          <article-title>Transfer learning enhanced generative adversarial networks for multi-channel MRI reconstruction</article-title>
          <source>Comput. Biol. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>49216-49225</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Park</surname>
              <given-names>H. C.</given-names>
            </name>
            <name>
              <surname>Hong</surname>
              <given-names>I. P.</given-names>
            </name>
            <name>
              <surname>Poudel</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Choi</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2023.3275173</pub-id>
          <article-title>Data augmentation based on generative adversarial networks for endoscopic image classification</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="conf-paper">
          <page-range>549-560</page-range>
          <publisher-place>Singapore</publisher-place>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hung</surname>
              <given-names>B. T.</given-names>
            </name>
            <name>
              <surname>Tien</surname>
              <given-names>L. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-981-15-7527-3_52</pub-id>
          <article-title>Facial expression recognition with CNN-LSTM</article-title>
          <source>Research in Intelligent and Computing in Engineering: Select Proceedings of RICE 2020</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>413-431</page-range>
          <issue>4</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sarvakar</surname>
            </name>
            <name>
              <surname>Rana</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Patel</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Jani</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Prajapati</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.32628/CSEIT251116171</pub-id>
          <article-title>A hybrid framework combining CNN, LSTM, and transfer learning for emotion recognition</article-title>
          <source>Int. J. Sci. Res. Comput. Sci. Eng. Inf. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>20</volume>
          <page-range>2275-2290</page-range>
          <issue>3</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Aghabeigi</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Nazari</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Osati Eraghi</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>An efficient facial emotion recognition using convolutional neural network with local sorting binary pattern and whale optimization algorithm</article-title>
          <source>Int. J. Data Sci. Anal.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>148</volume>
          <page-range>105858</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Nadimi-Shahraki</surname>
              <given-names>M. H.</given-names>
            </name>
            <name>
              <surname>Zamani</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Mirjalili</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compbiomed.2022.105858</pub-id>
          <article-title>Enhanced whale optimization algorithm for medical feature selection: A COVID-19 case study</article-title>
          <source>Comput. Biol. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="conf-paper">
          <page-range>535-539</page-range>
          <publisher-place>New York, NY, USA</publisher-place>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Valerio</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Mahmoud</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3716553.3750797</pub-id>
          <article-title>A multimodal framework for exploring behavioural cues for automatic stress detection</article-title>
          <source>Proceedings of the 27th International Conference on Multimodal Interaction</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>8071</page-range>
          <issue>17</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Udahemuka</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Djouani</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Kurien</surname>
              <given-names>A. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app14178071</pub-id>
          <article-title>Multimodal emotion recognition using visual, vocal and physiological signals: A review</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>418</page-range>
          <issue>7</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Mi</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/biomimetics10070418</pub-id>
          <article-title>A comprehensive review of multimodal emotion recognition: Techniques, challenges, and future directions</article-title>
          <source>Biomimetics</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>4166</page-range>
          <issue>13</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cheng</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Cai</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Ihnaini</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s25134166</pub-id>
          <article-title>A comprehensive review of explainable artificial intelligence (XAI) in computer vision</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>19281</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Raminedi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Shridevi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Won</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-024-69981-5</pub-id>
          <article-title>Multi-modal transformer architecture for medical image analysis and automated report generation</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>84</volume>
          <issue>3</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sagheer</surname>
              <given-names>S. V. M.</given-names>
            </name>
            <name>
              <surname>K. H</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Ameer</surname>
              <given-names>P. M.</given-names>
            </name>
            <name>
              <surname>Parayangat</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Abbas</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.32604/cmc.2025.063726</pub-id>
          <article-title>Transformers for multi-modal image analysis in healthcare</article-title>
          <source>Comput., Mater. Continua</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <page-range>200615</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kus</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Kocak</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Keles</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.iswa.2025.200615</pub-id>
          <article-title>A systematic review of vision transformer and explainable AI advances in multimodal facial expression recognition</article-title>
          <source>Intell. Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="webpage">
          <article-title>Comparative analysis of efficient face recognition models: A case study of EfficientNetV2B0, NasNetMobile, DenseNet169, MobileNet V2 with transfer learning, L2 regularization and dropout</article-title>
          <source>, https://doi.org/10.5281/zenodo.17076020</source>
          <year>2025</year>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-4</page-range>
          <publisher-place>Bengaluru, India</publisher-place>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Thapliyal</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Aeri</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kukreja</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Sharma</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICETCS61022.2024.10543922</pub-id>
          <article-title>Navigating landscapes through AI: A comparative study of EfficientNet and MobileNetV2 in image classification</article-title>
          <source>2024 International Conference on Emerging Technologies in Computer Science for Interdisciplinary Applications (ICETCS)</source>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="journal">
          <page-range>1-5</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sharma</surname>
              <given-names>K. P.</given-names>
            </name>
            <name>
              <surname>Nagpal</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Raja Praveen</surname>
              <given-names>K. N.</given-names>
            </name>
            <name>
              <surname>Yadav</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Tham</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Bhosle</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Chauhan</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s40009-025-01671-w</pub-id>
          <article-title>Evaluating MobileNetV2 architecture for resource-efficient facial emotion recognition</article-title>
          <source>Natl. Acad. Sci. Lett.</source>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Kavzoglu</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Object-oriented random forest for high resolution land cover mapping using QuickBird-2 imagery</article-title>
          <source>Handbook of Neural Computation</source>
          <year>2017</year>
          <page-range>607-619</page-range>
          <pub-id pub-id-type="doi">10.1016/B978-0-12-811318-9.00033-8</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <volume>19</volume>
          <page-range>30-42</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mokhtar</surname>
              <given-names>S. F.</given-names>
            </name>
            <name>
              <surname>Yusof</surname>
              <given-names>Z. M.</given-names>
            </name>
            <name>
              <surname>Sapiri</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.11113/mjfas.v19n1.2660</pub-id>
          <article-title>Confidence intervals by bootstrapping approach: A significance review</article-title>
          <source>Malays. J. Fundam. Appl. Sci.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>