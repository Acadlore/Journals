<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-fYyUg5hmIv0gaHBuleaw3_mWkdM4fA5u</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml040201</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>A Hybrid Graph-Attention and Contextual Sentiment Embedding Model for Sentiment Analysis in Legal Documents</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2500-6030</contrib-id>
          <name>
            <surname>Jadhav</surname>
            <given-names>Sulaxan</given-names>
          </name>
          <email>sulaxan.jadhav@sse.ac.in</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1447-228X</contrib-id>
          <name>
            <surname>Shende</surname>
            <given-names>Ashvini Pradeep</given-names>
          </name>
          <email>ashvini.shende@sse.ac.in</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-9473-8670</contrib-id>
          <name>
            <surname>Sapkal</surname>
            <given-names>Samruddhi</given-names>
          </name>
          <email>samruddhisapkal26@gmail.com</email>
        </contrib>
        <aff id="aff_1">Symbiosis School of Economics, Symbiosis International (Deemed University), 411004 Pune, India</aff>
        <aff id="aff_2">Department Computer Engineering, ISBM College of Engineering, 412115 Pune, India</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>27</day>
        <month>03</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>2</issue>
      <fpage>62</fpage>
      <lpage>81</lpage>
      <page-range>62-81</page-range>
      <history>
        <date date-type="received">
          <day>05</day>
          <month>02</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>22</day>
          <month>03</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Sentiment analysis in legal documents presents significant challenges due to the intricate structure, domain-specific terminology, and strong contextual dependencies inherent in legal texts. In this study, a novel hybrid framework is proposed, integrating Graph Attention Networks (GATs) with domain-specific embeddings, i.e., Legal Bidirectional Encoder Representations from Transformers (LegalBERT) and an aspect-oriented sentiment classification approach to improve both predictive accuracy and interpretability. Unlike conventional deep learning models, the proposed method explicitly captures hierarchical relationships within legal texts through GATs while leveraging LegalBERT to enhance domain-specific semantic representation. Additionally, auxiliary features, including positional information and topic relevance, were incorporated to refine sentiment predictions. A comprehensive evaluation conducted on diverse legal datasets demonstrates that the proposed model achieves state-of-the-art performance, attaining an accuracy of 93.1% and surpassing existing benchmarks by a significant margin. Model interpretability was further enhanced through SHapley Additive exPlanations (SHAP) and Legal Context Attribution Score (LCAS) techniques, which provide transparency into decision-making processes. An ablation study confirms the critical contribution of each model component, while scalability experiments validate the model’s efficiency across datasets ranging from 10,000 to 200,000 sentences. Despite increased computational demands, strong robustness and scalability are exhibited, making this framework suitable for large-scale legal applications. Future research will focus on multilingual adaptation, computational optimization, and broader applications within the field of legal analytics.</p></abstract>
      <kwd-group>
        <kwd>Sentiment analysis</kwd>
        <kwd>GATs</kwd>
        <kwd>Domain-specific embeddings</kwd>
        <kwd>BERT</kwd>
        <kwd>Legal document analysis</kwd>
        <kwd>Legal BERT</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="3"/>
        <fig-count count="10"/>
        <table-count count="8"/>
        <ref-count count="24"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Sentiment analysis, a fundamental task in natural language processing (NLP), has been widely explored in domains such as social media analytics, product reviews, and consumer feedback [<xref ref-type="bibr" rid="ref_1">1</xref>]. However, its application to legal documents introduces unique challenges due to the technical, nuanced, and highly structured nature of legal texts [<xref ref-type="bibr" rid="ref_2">2</xref>]. Unlike conventional texts, legal documents encapsulate sentiments that are deeply intertwined with logical arguments, hierarchical structures, and domain-specific language, making standard sentiment analysis techniques insufficient in capturing the underlying intent and emotion [<xref ref-type="bibr" rid="ref_3">3</xref>]. This limitation is particularly critical as sentiment analysis in the legal domain has significant implications, including contract review, judicial decision-making, and case law analysis. The primary challenge lies in the complexity of legal documents, which exhibit characteristics such as (i) lengthy and intricate argumentation structures, (ii) reliance on domain-specific terminology and jurisprudential syntax, (iii) context-dependent sentiments that span across multiple sentences or paragraphs, and (iv) logical relationships between clauses, arguments, and judicial decisions [<xref ref-type="bibr" rid="ref_4">4</xref>], [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>]. Existing sentiment analysis models struggle to effectively process these intricacies, as traditional machine learning and deep learning methods primarily focus on token- or sentence-level analysis, failing to capture the broader relational and structural dependencies inherent in legal texts [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>]. Even transformer-based models such as BERT and its domain-specific variants like LegalBERT, while advancing NLP for legal language, are limited in their ability to incorporate hierarchical and contextual structures essential for sentiment interpretation in legal settings [<xref ref-type="bibr" rid="ref_9">9</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>]. To address these critical gaps, a novel hybrid approach that integrates GATs with domain-specific contextual embeddings was proposed, enabling a structured and context-aware sentiment analysis framework tailored for legal documents. The key innovation of the proposed approach is its ability to represent legal documents as directed graphs, where nodes correspond to sentences and edges represent argumentative and logical relationships derived through dependency parsing. By incorporating graph attention mechanisms, the proposed model effectively captures inter-sentence dependencies, hierarchical structures, and the overarching legal context, which are often overlooked in traditional NLP models. Additionally, fine-tuned legal language models, trained on sentiment-annotated legal corpora, ensure that embeddings accurately reflect domain-specific sentiment expressions. A further enhancement of the proposed methodology lies in its aspect-oriented sentiment analysis module, which enables a more granular understanding of sentiments associated with distinct legal constructs, such as judgments, evidence, and arguments. This disentangled sentiment analysis not only improves classification accuracy but also provides more actionable insights for legal practitioners. Furthermore, by incorporating explainable artificial intelligence (AI) techniques, the proposed model ensures interpretability and transparency, addressing the critical need for accountability in high-stakes legal applications.</p><p>By effectively tackling the inherent complexities of legal texts and integrating innovative techniques for sentiment modeling, the proposed methodology sets a new benchmark for sentiment analysis in the legal domain. Through extensive experimentation and comparative analysis, the method demonstrates its superiority over existing approaches in terms of accuracy, scalability, and interpretability. This research contributes a robust, structured, and explainable framework that advances the field of legal NLP and unlocks valuable applications in legal analytics, judicial decision support, and contract analysis. The remainder of this study is organized as follows: Section 2 presents a review of related work, Section 3 details the proposed methodology, Section 4 describes the experimental setup and results, Section 5 provides a comparative analysis with existing methods, Section 6 discusses limitations and future work, and Section 7 concludes the study.</p>
    </sec>
    <sec sec-type="">
      <title>2. Related works</title>
      <p>The application of sentiment analysis in the legal domain has garnered increasing attention due to its potential to enhance judicial efficiency and accessibility. Numerous studies have explored various methodologies, ranging from traditional machine learning approaches to cutting-edge deep learning techniques, to tackle the unique challenges posed by legal texts. This section discusses relevant works that form the foundation of sentiment analysis in legal documents and highlights the gaps addressed by this research. Early works in sentiment analysis focused on applying deep learning techniques to specific legal sub-domains. For example, Abimbola et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] proposed a framework combining Long Short-Term Memory (LSTM) and Convolutional Neural Networks (CNNs) for maritime judiciary records. The model successfully extracted critical sentiment-laden statements from distributed legal repositories, aiding judges in making informed decisions. While effective, this approach primarily focuses on feature extraction without addressing the relational structure of arguments in legal documents, a key challenge that the framework proposed in this current study aims to overcome. Building on the trend of multi-label classification in legal contexts, Sengupta [<xref ref-type="bibr" rid="ref_12">12</xref>] developed a pipeline combining supervised machine learning and NLP techniques to classify legal cases under the Indian Income Tax Act. By employing transformer-based techniques and traditional machine learning models like Support Vector Machines (SVMs), the applicability of such hybrid methods for legislative text analysis was demonstrated. However, the reliance on structured datasets and limited application scope underscores the need for more generalized frameworks, a gap that this current research seeks to address by leveraging graph-based techniques.</p><p>The limitations of manual judgment and inefficiencies in official document sentiment recognition were highlighted in the study by Hao et al. [<xref ref-type="bibr" rid="ref_13">13</xref>], where a BERT-SVM hybrid model achieved an impressive 95.12% accuracy in sentiment classification. This work demonstrates the potential of transformer-based architectures for legal sentiment analysis but lacks a focus on domain-specific nuances and argumentative structures critical for legal contexts. Similarly, Dey and Das [<xref ref-type="bibr" rid="ref_14">14</xref>] introduced a modified term frequency-inverse document frequency (TF-IDF) method combined with neural networks to enhance sentiment classification. The innovative use of global weighting factors and k-best selection highlights the importance of preprocessing for improved feature representation. Beyond sentiment analysis, related studies have explored complementary NLP tasks in the legal domain. For instance, the development of legal-specific Named Entity Recognition (NER) datasets in the study by Naik et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] showcases the need for domain-specific training resources. Additionally, Jain et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] and Gupta et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] proposed novel approaches for summarization and topic modelling of legal texts, emphasizing the significance of understanding document-level structures for downstream applications.</p><p>While these studies represent significant advancements, their limitations highlight the need for a framework tailored to the intricacies of legal documents. The model proposed in this current study bridges this gap by integrating graph-based relational reasoning with domain-specific embeddings, enabling a deeper understanding of the hierarchical and argumentative structures in legal texts.</p>
    </sec>
    <sec sec-type="">
      <title>3. Proposed methodology</title>
      <p>The proposed methodology involves designing and implementing a hybrid framework for sentiment analysis in legal documents. The approach combines graph-based neural networks with transformer-based language models to capture the hierarchical and relational structure of legal texts. This framework integrates three core components: a graph representation of legal documents, domain-specific embeddings for contextual understanding, and an aspect-oriented sentiment classification module. These components aim to ensure that the analysis is both precise and interpretable, addressing the complex nature of legal sentiment extraction. The flowchart of the proposed algorithm is shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>. The proposed methodology is designed to address the unique challenges of sentiment analysis in legal documents, which include the complexity of their hierarchical structures, the intricate relationships between clauses and arguments, and the specialized vocabulary and syntax inherent to the legal domain. Traditional sentiment analysis methods fail to capture these nuances, leading to a lack of contextual accuracy and interpretability. To overcome these challenges, the methodology integrates three core components:</p><p>a) Graph representation of legal texts: Legal documents are represented as directed graphs where nodes correspond to sentences or clauses, and edges capture logical and argumentative relationships derived from dependency parsing. This enables the model to learn hierarchical and relational structures effectively.</p><p>b) Domain-specific embeddings: Pre-trained legal language models, such as LegalBERT, are fine-tuned on sentiment-annotated legal corpora to generate embeddings that reflect the nuanced semantics and sentiment expressions unique to legal texts.</p><p>c) Aspect-oriented sentiment classification: Sentiments are analysed and classified based on specific legal constructs (e.g., judgments, evidence, or arguments), ensuring a deeper understanding of the document’s sentiment distribution and its interpretability for legal practitioners.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Schematic representation for the flow diagram of the proposed methodology</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_c07Uywa3JAX6Tutd.png"/>
        </fig>
      
      <p>These components work together to ensure precise, context-aware, and interpretable sentiment analysis tailored to the complex nature of legal documents.</p><p>The rationale behind selecting these techniques lies in their ability to capture intricate relationships, contextual dependencies, and domain-specific linguistic structures. GATs were chosen over traditional graph-based models like Graph Convolutional Networks (GCNs) due to their ability to dynamically assign varying importance to textual components, making them more effective for modeling legal texts, which often contain complex interdependencies between clauses, arguments, and case law references. LegalBERT was selected over general-purpose language models such as Robustly Optimized BERT Pretraining Approach (RoBERTa) and T5 [<xref ref-type="bibr" rid="ref_18">18</xref>], as it is specifically pre-trained on legal corpora, ensuring better representation of legal terminology and syntax. Aspect-oriented classification was integrated to enhance interpretability and fine-grained sentiment classification, which is crucial in legal applications where sentiment polarity often depends on specific case details. While alternative methods such as LSTM or CNN could have been considered, they lack the ability to capture long-range dependencies as effectively as transformer-based architectures. The comparison criteria for these models were based on accuracy, F1-score, interpretability, and computational efficiency, ensuring a robust evaluation. By leveraging these techniques, the proposed model achieves a balance between domain specificity, contextual understanding, and computational feasibility, making it well-suited for legal sentiment analysis.</p><p>Let <inline-formula>
  <mml:math id="mgib8719hh">
    <mml:mi>D</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mrow>
      <mml:mo>{</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>…</mml:mo>
      <mml:mo>…</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>}</mml:mo>
      <mml:msub>
        <mml:mi>s</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>s</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>s</mml:mi>
        <mml:mn>3</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>s</mml:mi>
        <mml:mi>n</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> represent a legal document with $n<inline-formula>
  <mml:math id="mlvp31p0a9">
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>s_j<inline-formula>
  <mml:math id="md3gqakwkb">
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>i<inline-formula>
  <mml:math id="m9gz1wyjcu">
    <mml:mo>−</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
  </mml:math>
</inline-formula>G=(V, E)<inline-formula>
  <mml:math id="mdjcoh3v6f">
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>V<inline-formula>
  <mml:math id="mhbm3f7zhj">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>s_1, s_2, s_3 \ldots \ldots, s_n<inline-formula>
  <mml:math id="mrlkaalfvd">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>E<inline-formula>
  <mml:math id="m6js35s9ce">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>\left(e_{i j} \in E\right)<inline-formula>
  <mml:math id="mos0cjorlr">
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>(</mml:mo>
  </mml:math>
</inline-formula>v_i \in V<inline-formula>
  <mml:math id="m68un7lmjz">
    <mml:mo>)</mml:mo>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula>\left(h_i \in R^d\right)<inline-formula>
  <mml:math id="mx0od7r49i">
    <mml:mo>,</mml:mo>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>d<inline-formula>
  <mml:math id="mmfiynk3zn">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>B</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>[</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>[</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>9</mml:mn>
    <mml:mn>10</mml:mn>
  </mml:math>
</inline-formula>h_i^{\prime}$ for each node by aggregating information from its neighbours and this is represented as:</p>
      
        <disp-formula>
          <label>(1)</label>
          <mml:math id="mnd8w1ffq3">
            <mml:msubsup>
              <mml:mi>h</mml:mi>
              <mml:mi>i</mml:mi>
              <mml:mrow>
                <mml:mi>′</mml:mi>
              </mml:mrow>
            </mml:msubsup>
            <mml:mo>=</mml:mo>
            <mml:mi>σ</mml:mi>
            <mml:mrow>
              <mml:mo>(</mml:mo>
              <mml:mo>)</mml:mo>
              <mml:munder>
                <mml:mo>∑</mml:mo>
                <mml:mrow>
                  <mml:mi>j</mml:mi>
                  <mml:mi>i</mml:mi>
                  <mml:mo>∈</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mrow>
                    <mml:mi data-mjx-variant="-tex-calligraphic">N</mml:mi>
                  </mml:mrow>
                </mml:mrow>
              </mml:munder>
              <mml:msub>
                <mml:mi>α</mml:mi>
                <mml:mrow>
                  <mml:mi>i</mml:mi>
                  <mml:mi>j</mml:mi>
                </mml:mrow>
              </mml:msub>
              <mml:msub>
                <mml:mi>h</mml:mi>
                <mml:mi>j</mml:mi>
              </mml:msub>
              <mml:mi>W</mml:mi>
            </mml:mrow>
          </mml:math>
        </disp-formula>
      
      <p>where, <inline-formula>
  <mml:math id="mfe6qc4xix">
    <mml:mrow>
      <mml:mi data-mjx-variant="-tex-calligraphic">N</mml:mi>
    </mml:mrow>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>i</mml:mi>
  </mml:math>
</inline-formula> represents the set of neighbouring nodes of <inline-formula>
  <mml:math id="mdzfmtcxm3">
    <mml:msub>
      <mml:mi>v</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="m94wktxhoh">
    <mml:msub>
      <mml:mi>α</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> is the attention weight for the edge from the nodes <inline-formula>
  <mml:math id="ma0xuobe3n">
    <mml:msub>
      <mml:mi>v</mml:mi>
      <mml:mi>j</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> to <inline-formula>
  <mml:math id="myye6mnx79">
    <mml:msub>
      <mml:mi>v</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>, given by Eq. (2).</p>
      
        <disp-formula>
          <label>(2)</label>
          <mml:math id="ma8pwcze8q">
            <mml:msub>
              <mml:mi>α</mml:mi>
              <mml:mrow>
                <mml:mi>i</mml:mi>
                <mml:mi>j</mml:mi>
              </mml:mrow>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>exp</mml:mi>
                <mml:mo>⁡</mml:mo>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mi>LeakyReLU</mml:mi>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msup>
                      <mml:mi>a</mml:mi>
                      <mml:mrow>
                        <mml:mi>⊤</mml:mi>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mrow>
                      <mml:mo>[</mml:mo>
                      <mml:mo fence="false">‖</mml:mo>
                      <mml:mo>]</mml:mo>
                      <mml:mi>W</mml:mi>
                      <mml:mi>W</mml:mi>
                      <mml:msub>
                        <mml:mi>h</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>h</mml:mi>
                        <mml:mi>j</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mrow>
              </mml:mrow>
              <mml:mrow>
                <mml:munder>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>k</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mo>∈</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mrow>
                      <mml:mi data-mjx-variant="-tex-calligraphic">N</mml:mi>
                    </mml:mrow>
                  </mml:mrow>
                </mml:munder>
                <mml:mi>exp</mml:mi>
                <mml:mo>⁡</mml:mo>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mi>LeakyReLU</mml:mi>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msup>
                      <mml:mi>a</mml:mi>
                      <mml:mrow>
                        <mml:mi>⊤</mml:mi>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mrow>
                      <mml:mo>[</mml:mo>
                      <mml:mo fence="false">‖</mml:mo>
                      <mml:mo>]</mml:mo>
                      <mml:mi>W</mml:mi>
                      <mml:mi>W</mml:mi>
                      <mml:msub>
                        <mml:mi>h</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>h</mml:mi>
                        <mml:mi>k</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mrow>
              </mml:mrow>
            </mml:mfrac>
          </mml:math>
        </disp-formula>
      
      <p>where, $W<inline-formula>
  <mml:math id="my1bf4s3no">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>a<inline-formula>
  <mml:math id="my5574qj3c">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\|<inline-formula>
  <mml:math id="m0miwxgvyq">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>h_i^{\prime}$, which encode both the semantic information of each sentence and its relationships with other sentences in the document. These embeddings were then used as input for downstream sentiment classification tasks. The graph-based representation ensures that the model captures both the fine-grained and holistic context of legal texts, essential for accurate sentiment analysis.</p><p>Algorithm 1 constructs a graph-based representation of legal documents and updates the embeddings of sentences using a GAT to capture the relationships between sentences. The process begins by treating each sentence in the document as a node and using a pre-trained language model (e.g., LegalBERT) to generate an initial semantic embedding for each node. These embeddings represent the individual meanings of the sentences.</p>
      <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Algorithm 1 Graph-based representation for legal texts</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">1:<span style="font-family: Times New Roman"> Input: Legal documents <inline-formula>
  <mml:math id="mwpj7noq0k">
    <mml:mi>D</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mo fence="false">{</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>…</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo fence="false">}</mml:mo>
    <mml:msub>
      <mml:mi>s</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>s</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>s</mml:mi>
      <mml:mn>3</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>s</mml:mi>
      <mml:mi>n</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>, pre-trained embedding model $M<inline-formula>
  <mml:math id="mszld9zprv">
    <mml:mo>,</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula>P<inline-formula>
  <mml:math id="mp8njisvin">
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>O</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mn>2</mml:mn>
  </mml:math>
</inline-formula>\{h_1', h_2', h_3', \ldots, h_n'\}<inline-formula>
  <mml:math id="msy04v49du">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>D</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>P</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mn>3</mml:mn>
    <mml:mn>4</mml:mn>
  </mml:math>
</inline-formula>\quad$ <inline-formula>
  <mml:math id="mfa6b7ydaj">
    <mml:mi>V</mml:mi>
    <mml:mi>∅</mml:mi>
    <mml:mo>←</mml:mo>
  </mml:math>
</inline-formula> (Initialize empty set of nodes)</p><p style="text-align: justify">5:<span style="font-family: Times New Roman"> <inline-formula>
  <mml:math id="mve1wq94am">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> <inline-formula>
  <mml:math id="m47lciv5a8">
    <mml:mi>E</mml:mi>
    <mml:mi>∅</mml:mi>
    <mml:mo>←</mml:mo>
  </mml:math>
</inline-formula> (Initialize empty set of edges)</p><p style="text-align: justify">6:<span style="font-family: Times New Roman"> <inline-formula>
  <mml:math id="mpzw0pm5mk">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> for each sentence <italic>s<sub>i</sub></italic> in <italic>D</italic> do</p><p style="text-align: justify">7: <inline-formula>
  <mml:math id="mhqiw4cqn4">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> <inline-formula>
  <mml:math id="ma13gbptzm">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> <inline-formula>
  <mml:math id="mj04zz25z7">
    <mml:msub>
      <mml:mi>v</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>s</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:mo>←</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>M</mml:mi>
  </mml:math>
</inline-formula></p><p style="text-align: justify">8: <inline-formula>
  <mml:math id="mfj32d7824">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> <inline-formula>
  <mml:math id="m2qm4jk2j8">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> Add <inline-formula>
  <mml:math id="m8ldu33ayo">
    <mml:msub>
      <mml:mi>v</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> to $V<inline-formula>
  <mml:math id="m9x15rku4q">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mn>9</mml:mn>
  </mml:math>
</inline-formula>\quad<inline-formula>
  <mml:math id="mwsdob4yit">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mn>10</mml:mn>
  </mml:math>
</inline-formula>\quad<inline-formula>
  <mml:math id="mr9ndshbys">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>(s_i, s_j)<inline-formula>
  <mml:math id="md8a5hk6jd">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>D<inline-formula>
  <mml:math id="m3t0uzu924">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mn>11</mml:mn>
  </mml:math>
</inline-formula>\quad$ <inline-formula>
  <mml:math id="m0dtg95ncn">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> if <inline-formula>
  <mml:math id="mcoa8tep1o">
    <mml:mi>P</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:msub>
      <mml:mi>s</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>s</mml:mi>
      <mml:mi>j</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> indicates a relationship then</p><p style="text-align: justify">12:<span style="font-family: Times New Roman"> <inline-formula>
  <mml:math id="mn08i1vwql">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> <inline-formula>
  <mml:math id="m2nnz7zb2o">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> <inline-formula>
  <mml:math id="mem98t8ieh">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> Add edge <inline-formula>
  <mml:math id="mmmf88d0xv">
    <mml:msub>
      <mml:mi>e</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mi>v</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>v</mml:mi>
      <mml:mi>j</mml:mi>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo accent="false">→</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> to $E<inline-formula>
  <mml:math id="mo63v58wp3">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mn>13</mml:mn>
  </mml:math>
</inline-formula>\quad$ <inline-formula>
  <mml:math id="meuvq4m656">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> end if</p><p style="text-align: justify">14:<span style="font-family: Times New Roman"> <inline-formula>
  <mml:math id="m9hse1klqv">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> end for</p><p style="text-align: justify">15:<inline-formula>
  <mml:math id="ma1pe9u2ny">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> for each node <inline-formula>
  <mml:math id="m1b5gxmgpt">
    <mml:msub>
      <mml:mi>v</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> in $V<inline-formula>
  <mml:math id="mifzcpz6f6">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mn>16</mml:mn>
  </mml:math>
</inline-formula>\quad$ <inline-formula>
  <mml:math id="mx2nkou96j">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> <inline-formula>
  <mml:math id="m3nky84zq5">
    <mml:msubsup>
      <mml:mi>h</mml:mi>
      <mml:mi>i</mml:mi>
      <mml:mo>′</mml:mo>
    </mml:msubsup>
    <mml:mo>←</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo fence="false">{</mml:mo>
    <mml:mo>∣</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo accent="false">→</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>∈</mml:mo>
    <mml:mo fence="false">}</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mrow>
      <mml:mi>Update</mml:mi>
      <mml:mi>Embedding</mml:mi>
      <mml:mstyle scriptlevel="0">
        <mml:mspace width="0.167em"/>
      </mml:mstyle>
    </mml:mrow>
    <mml:msub>
      <mml:mi>v</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>v</mml:mi>
      <mml:mi>j</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>v</mml:mi>
      <mml:mi>j</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>v</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:mi>E</mml:mi>
  </mml:math>
</inline-formula></p><p style="text-align: justify">17:<span style="font-family: Times New Roman"> <inline-formula>
  <mml:math id="mnb41tm3da">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> end for</p><p style="text-align: justify">18:<span style="font-family: Times New Roman"> <inline-formula>
  <mml:math id="mgduey6z10">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula>return <inline-formula>
  <mml:math id="m2vq76bz4f">
    <mml:mo fence="false">{</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>…</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo fence="false">}</mml:mo>
    <mml:msubsup>
      <mml:mi>h</mml:mi>
      <mml:mn>1</mml:mn>
      <mml:mo>′</mml:mo>
    </mml:msubsup>
    <mml:msubsup>
      <mml:mi>h</mml:mi>
      <mml:mn>2</mml:mn>
      <mml:mo>′</mml:mo>
    </mml:msubsup>
    <mml:msubsup>
      <mml:mi>h</mml:mi>
      <mml:mn>3</mml:mn>
      <mml:mo>′</mml:mo>
    </mml:msubsup>
    <mml:msubsup>
      <mml:mi>h</mml:mi>
      <mml:mi>n</mml:mi>
      <mml:mo>′</mml:mo>
    </mml:msubsup>
  </mml:math>
</inline-formula></p><p style="text-align: justify">19:<span style="font-family: Times New Roman"> end function</p></td></tr></tbody></table><p>Next, the relationships between sentences, such as logical connections (e.g., causality, contrast, or elaboration), were identified using a dependency parser. These relationships form the edges in the graph, connecting related nodes and encoding the flow of arguments in the document. The resulting graph structure <inline-formula>
  <mml:math id="m8silc21ts">
    <mml:mi>G</mml:mi>
    <mml:mi>V</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula>, with $V<inline-formula>
  <mml:math id="mvir39ayvu">
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula>E$ as edges (relationships), represents the document holistically. The GAT was then applied to this graph to update the sentence embeddings. At each node, the GAT aggregates information from its neighboring nodes by assigning importance scores (attention) to each neighbor. This allows the model to focus more on relevant sentences while minimizing the influence of less critical ones. The updated embeddings at each node thus incorporate both the semantic meaning of the sentence and the contextual information derived from its relationships in the graph. By iterating through all nodes in the graph and updating their embeddings, the algorithm creates a rich, context-aware representation of the document. These updated embeddings were then used in downstream tasks, such as sentiment analysis, where understanding the interdependencies between sentences is crucial for accurate predictions. This graph-based approach ensures that the model captures not just individual sentence meanings but also the broader argumentative and relational structure of legal documents, making it highly effective for analysing complex legal texts.</p>
      
        <sec>
          
            <title>3.1. Domain-specific embeddings</title>
          
          <p>Legal texts are characterized by highly specialized language, complex terminology, and intricate syntactical structures, making general-purpose language models inadequate for capturing their semantics. To address this, the second stage of the proposed methodology involves generating domain-specific embeddings using a pre-trained legal language model, such as LegalBERT, fine-tuned on sentiment-annotated legal corpora. These embeddings are designed to capture the contextual nuances of legal language and align them with sentiment-based representations. Given a sentence $s<inline-formula>
  <mml:math id="mpmtn766mk">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>E_s$ is obtained as Eq. (3).</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="mz72kukf65">
                <mml:msub>
                  <mml:mi>E</mml:mi>
                  <mml:mi>s</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mi>LegalBERT</mml:mi>
                <mml:mi>s</mml:mi>
                <mml:mi>θ</mml:mi>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mskj7cbett">
    <mml:mi>θ</mml:mi>
  </mml:math>
</inline-formula> represents the parameters of the LegalBERT model, which have been pre-trained on a large corpus of legal texts and fine-tuned on sentiment-specific tasks. Fine-tuning involves minimizing a loss function, such as cross-entropy loss for classification tasks, over a training set <inline-formula>
  <mml:math id="mbfgbb317j">
    <mml:mrow>
      <mml:mi data-mjx-variant="-tex-calligraphic">D</mml:mi>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>s</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:msub>
    </mml:mrow>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula>, where, <inline-formula>
  <mml:math id="m2j7c0s2fi">
    <mml:msub>
      <mml:mi>s</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is a sentence and <inline-formula>
  <mml:math id="mwloqc8v9o">
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is its corresponding sentiment label. To enrich the embeddings, additional domainspecific information, such as topic relevance or clause type, was integrated. This was achieved by extending the embeddings with auxiliary features <inline-formula>
  <mml:math id="mp1h0kb9op">
    <mml:msub>
      <mml:mi>F</mml:mi>
      <mml:mi>s</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> as described in <xref ref-type="table" rid="table_1">Table 1</xref>, yielding a final embedding as follows:</p>
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="m7c0weqh1z">
                <mml:msubsup>
                  <mml:mi>E</mml:mi>
                  <mml:mi>s</mml:mi>
                  <mml:mrow>
                    <mml:mtext>final </mml:mtext>
                  </mml:mrow>
                </mml:msubsup>
                <mml:mo>=</mml:mo>
                <mml:mrow>
                  <mml:mo>[</mml:mo>
                  <mml:mo fence="false">‖</mml:mo>
                  <mml:mo>]</mml:mo>
                  <mml:msub>
                    <mml:mi>E</mml:mi>
                    <mml:mi>s</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>F</mml:mi>
                    <mml:mi>s</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="miy7gswal5">
    <mml:msub>
      <mml:mi>F</mml:mi>
      <mml:mi>s</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> includes attributes such as the topic distribution from Latent Dirichlet Allocation (LDA) or positional information within the document hierarchy. These embeddings were then normalized to ensure numerical stability and compatibility for downstream sentiment classification as described in Algorithm 2.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Auxiliary features for domain-specific embeddings</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Feature Name</p></td><td colspan="1" rowspan="1"><p>Description</p></td><td colspan="1" rowspan="1"><p>Calculation Method</p></td><td colspan="1" rowspan="1"><p>Range</p></td></tr><tr><td colspan="1" rowspan="1"><p>Topic distribution</p></td><td colspan="1" rowspan="1"><p>Relevance of the sentence to predefined legal topics</p></td><td colspan="1" rowspan="1"><p>Generated via LDA (topic modelling)</p></td><td colspan="1" rowspan="1"><p>[0,1]</p></td></tr><tr><td colspan="1" rowspan="1"><p>Positional information</p></td><td colspan="1" rowspan="1"><p>Relative position of the sentence in the document</p></td><td colspan="1" rowspan="1"><p>Normalized sentence index</p></td><td colspan="1" rowspan="1"><p>[0,1]</p></td></tr><tr><td colspan="1" rowspan="1"><p>Clause type indicator</p></td><td colspan="1" rowspan="1"><p>Type of clause (e.g., judgment, evidence, and argument)</p></td><td colspan="1" rowspan="1"><p>Rule-based NLP classification</p></td><td colspan="1" rowspan="1"><p>Binary (0, 1)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Sentence length</p></td><td colspan="1" rowspan="1"><p>Length of the sentence in terms of token count</p></td><td colspan="1" rowspan="1"><p>Direct token count</p></td><td colspan="1" rowspan="1"><p>Positive integer</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>The embedding generation pipeline is computationally efficient and scalable, allowing the integration of legal domain-specific features while preserving the rich contextual representations provided by the transformer architecture.</p><table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Algorithm 2 Domain-specific embedding generation</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">1: Input: Legal document <inline-formula>
  <mml:math id="mx5w7f3gt1">
    <mml:mi>D</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mo fence="false">{</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>…</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo fence="false">}</mml:mo>
    <mml:msub>
      <mml:mi>s</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>s</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>s</mml:mi>
      <mml:mi>n</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>, pre-trained LegalBERT model $M<inline-formula>
  <mml:math id="mr43abdrzh">
    <mml:mo>,</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>F<inline-formula>
  <mml:math id="mkfb9ruej8">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>O</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mn>2</mml:mn>
  </mml:math>
</inline-formula>E_{\text{final}} = E_{s1}^{\text{final}}, \ldots, E_{sn}^{\text{final}}<inline-formula>
  <mml:math id="mnxxb3hzly">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mn>3</mml:mn>
  </mml:math>
</inline-formula>D, M, F<inline-formula>
  <mml:math id="mii2qkeihe">
    <mml:mo>)</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mn>4</mml:mn>
  </mml:math>
</inline-formula>\quad$ <inline-formula>
  <mml:math id="mcg7nnjav2">
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mrow>
        <mml:mtext>final</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mo>←</mml:mo>
    <mml:mi>∅</mml:mi>
  </mml:math>
</inline-formula> (Initialize empty set for final embeddings)</p><p style="text-align: justify">5: <inline-formula>
  <mml:math id="mep198ue0n">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> for each sentence <inline-formula>
  <mml:math id="mk5s08moxj">
    <mml:msub>
      <mml:mi>s</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> in $D<inline-formula>
  <mml:math id="m9cq8xoodk">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mn>6</mml:mn>
  </mml:math>
</inline-formula>\quad<inline-formula>
  <mml:math id="m3pdax29pp">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula><inline-formula>
  <mml:math id="ma2j2ce4ch">
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mrow>
        <mml:mi>s</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mi>s</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:mo>←</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>M</mml:mi>
  </mml:math>
</inline-formula></p><p style="text-align: justify">7: <inline-formula>
  <mml:math id="mjx659s2lx">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula><inline-formula>
  <mml:math id="m9jyiojycg">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula><inline-formula>
  <mml:math id="m81oh85s6r">
    <mml:msub>
      <mml:mi>F</mml:mi>
      <mml:mrow>
        <mml:mi>s</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mi>s</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:mo>←</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mtext>Extract Auxiliary Features</mml:mtext>
    <mml:mi>F</mml:mi>
  </mml:math>
</inline-formula></p><p style="text-align: justify">8: <inline-formula>
  <mml:math id="mq6qrsajq9">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula><inline-formula>
  <mml:math id="mfuwjhw103">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula><inline-formula>
  <mml:math id="mcykzxowfz">
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mrow>
        <mml:mi>s</mml:mi>
        <mml:mi>i</mml:mi>
        <mml:mtext>final</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mrow>
        <mml:mi>s</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mi>F</mml:mi>
      <mml:mrow>
        <mml:mi>s</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>←</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mtext>Concatenate</mml:mtext>
  </mml:math>
</inline-formula></p><p style="text-align: justify">9: <inline-formula>
  <mml:math id="m41y2kky6s">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula><inline-formula>
  <mml:math id="mbo8ah7q8c">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula><inline-formula>
  <mml:math id="mubtrvgdtu">
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mrow>
        <mml:mi>s</mml:mi>
        <mml:mi>i</mml:mi>
        <mml:mtext>final</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mrow>
        <mml:mi>s</mml:mi>
        <mml:mi>i</mml:mi>
        <mml:mtext>final</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mo>←</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mtext>Normalize</mml:mtext>
  </mml:math>
</inline-formula></p><p style="text-align: justify">10: <inline-formula>
  <mml:math id="mapzhk9qyz">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula><inline-formula>
  <mml:math id="ms0sgg7g1d">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula>Add <inline-formula>
  <mml:math id="mfm4huootw">
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mrow>
        <mml:mi>s</mml:mi>
        <mml:mi>i</mml:mi>
        <mml:mtext>final</mml:mtext>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> to <inline-formula>
  <mml:math id="m940n96823">
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mrow>
        <mml:mtext>final</mml:mtext>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula></p><p style="text-align: justify">11: <inline-formula>
  <mml:math id="mvoeqqahn2">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> end for</p><p style="text-align: justify">12: <inline-formula>
  <mml:math id="m5w5eeuahz">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> return <inline-formula>
  <mml:math id="m7i5oamztb">
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mrow>
        <mml:mtext>final</mml:mtext>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula></p><p style="text-align: justify">13: End function</p></td></tr></tbody></table><p>The auxiliary features are domain-specific enhancements incorporated into the LegalBERT-generated embeddings to improve their alignment with the unique requirements of legal texts. These features include the topic distribution, which provides probabilistic scores for the relevance of a sentence to predefined legal topics, such as evidence or arguments, using topic modelling techniques like LDA. Positional information captures the relative position of a sentence within the document, a critical factor in legal reasoning where the placement of a sentence often influences its significance. The clause type indicator assigns binary labels to clauses, identifying whether they belong to key legal constructs such as judgments or arguments, ensuring that embeddings are aware of sentence-level legal functions. Lastly, sentence length introduces a structural attribute by incorporating the token count of a sentence, as longer sentences frequently carry more complex arguments in legal texts. Together, these features fine-tune the embeddings, enabling them to address the complexities of legal texts and preparing them for downstream tasks such as aspect-oriented sentiment classification.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Aspect-oriented sentiment classification</title>
          
          <p>The aspect-oriented sentiment classification module focuses on breaking down the sentiment analysis into specific legal constructs or “aspects,” such as judgments, evidence, arguments, and legal clauses. This enables a granular understanding of how sentiment varies across different sections of legal documents, ensuring that each construct is evaluated independently while contributing to the overall sentiment of the document. Each aspect is treated as a distinct classification task, where embeddings generated from the previous stages are input into specialized classifiers for each aspect. This process is explained by Algorithm 3.</p><table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Algorithm 3 Aspect-oriented sentiment classification</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">1: Input: Final embedding set <inline-formula>
  <mml:math id="mqlvtaih2s">
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mrow>
        <mml:mtext>final</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>…</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:msubsup>
      <mml:mi>E</mml:mi>
      <mml:mrow>
        <mml:mi>s</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:mrow>
      <mml:mrow>
        <mml:mtext>final</mml:mtext>
      </mml:mrow>
    </mml:msubsup>
    <mml:msubsup>
      <mml:mi>E</mml:mi>
      <mml:mrow>
        <mml:mi>s</mml:mi>
        <mml:mi>n</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mtext>final</mml:mtext>
      </mml:mrow>
    </mml:msubsup>
  </mml:math>
</inline-formula>, aspect set <inline-formula>
  <mml:math id="m931ieizfb">
    <mml:mi>A</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mo fence="false">{</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>…</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo fence="false">}</mml:mo>
    <mml:mn>1</mml:mn>
    <mml:mn>2</mml:mn>
  </mml:math>
</inline-formula></p><p style="text-align: justify">2: Output: Predicted sentiments <inline-formula>
  <mml:math id="m19have2da">
    <mml:mi>Y</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mo fence="false">{</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>…</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo fence="false">}</mml:mo>
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mi>m</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> for all aspects</p><p style="text-align: justify">3: function: Aspect classification (<inline-formula>
  <mml:math id="mpxza7ntvh">
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mrow>
        <mml:mtext>final</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mo>,</mml:mo>
    <mml:mi>A</mml:mi>
  </mml:math>
</inline-formula>)</p><p style="text-align: justify">4: <inline-formula>
  <mml:math id="ms9ntu8yoz">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> <inline-formula>
  <mml:math id="mw68yrg5qf">
    <mml:mi>Y</mml:mi>
    <mml:mi>∅</mml:mi>
    <mml:mo>←</mml:mo>
  </mml:math>
</inline-formula> (Initialize empty set for predictions)</p><p style="text-align: justify">5: <inline-formula>
  <mml:math id="mykocqx572">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> for each aspect <inline-formula>
  <mml:math id="mg9bcg5luh">
    <mml:msub>
      <mml:mi>A</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> in $A<inline-formula>
  <mml:math id="m3a64ugwxa">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mn>6</mml:mn>
  </mml:math>
</inline-formula>\quad$ <inline-formula>
  <mml:math id="mp6307r23g">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> <inline-formula>
  <mml:math id="mfoql933j9">
    <mml:mi>C</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>←</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mtext>Initialize classifier</mml:mtext>
    <mml:msub>
      <mml:mi>A</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula></p><p style="text-align: justify">7: <inline-formula>
  <mml:math id="m4wb9x6l40">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> <inline-formula>
  <mml:math id="me3ujkgu9s">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> for each embedding <inline-formula>
  <mml:math id="m8s7b3i92k">
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mi>s</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> in <inline-formula>
  <mml:math id="mmbh90sea7">
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mrow>
        <mml:mtext>final</mml:mtext>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> do</p><p style="text-align: justify">8: <inline-formula>
  <mml:math id="mwpr1lx9fx">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> <inline-formula>
  <mml:math id="mhxhabo5sm">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> <inline-formula>
  <mml:math id="mwgxo04nrp">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> <inline-formula>
  <mml:math id="mjfjc42yip">
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mi>s</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mi>s</mml:mi>
    </mml:msub>
    <mml:mo>←</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mtext>Classifier</mml:mtext>
  </mml:math>
</inline-formula></p><p style="text-align: justify">9: <inline-formula>
  <mml:math id="mn9qppirwx">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> <inline-formula>
  <mml:math id="mf7affw58x">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> <inline-formula>
  <mml:math id="mfmnk6ek14">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> Add <inline-formula>
  <mml:math id="m6ocy6b1nx">
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mi>s</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> to $Y<inline-formula>
  <mml:math id="mgjhy3koxt">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mn>10</mml:mn>
  </mml:math>
</inline-formula>\quad$ <inline-formula>
  <mml:math id="mx6vaptpdj">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> end for</p><p style="text-align: justify">11: <inline-formula>
  <mml:math id="mmg3xua8j6">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> end for</p><p style="text-align: justify">12: <inline-formula>
  <mml:math id="m2fuihipll">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula> return $Y<inline-formula>
  <mml:math id="mth4d58oa3">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mn>13</mml:mn>
  </mml:math>
</inline-formula>E_s^{\text {final}}<inline-formula>
  <mml:math id="mbcg35fzzv">
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>A_i<inline-formula>
  <mml:math id="mu92me94xz">
    <mml:mo>,</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>f$ as follows:</p>
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="m6lirbng8r">
                <mml:mrow>
                  <mml:mover>
                    <mml:msub>
                      <mml:mi>y</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                    <mml:mo>^</mml:mo>
                  </mml:mover>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>;</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msubsup>
                    <mml:mi>E</mml:mi>
                    <mml:mi>s</mml:mi>
                    <mml:mrow>
                      <mml:mtext>final </mml:mtext>
                    </mml:mrow>
                  </mml:msubsup>
                  <mml:msub>
                    <mml:mi>A</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>Θ</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mi>f</mml:mi>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="m6clb3sw21">
    <mml:mrow>
      <mml:mover>
        <mml:msub>
          <mml:mi>y</mml:mi>
          <mml:mi>i</mml:mi>
        </mml:msub>
        <mml:mo>^</mml:mo>
      </mml:mover>
    </mml:mrow>
  </mml:math>
</inline-formula> is the predicted sentiment label for aspect <inline-formula>
  <mml:math id="m1gsqzg7ow">
    <mml:msub>
      <mml:mi>A</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>Θ</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula> represents the learnable parameters for the classifier corresponding to <inline-formula>
  <mml:math id="mzz8o9ciqg">
    <mml:msub>
      <mml:mi>A</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>; and $f$ is feed-forward neural network or transformer-based layer. To manage multi-aspect classification, the framework utilizes a shared embedding layer to ensure efficient parameter sharing while maintaining separate classifiers for each aspect, as given by <xref ref-type="fig" rid="fig_2">Figure 2</xref>. This modular design allows the addition of new aspects without retraining the entire pipeline, enhancing scalability.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Schematic representation of the layered architecture of the aspect classification network</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_tTd1zXwAtOAXfiMs.png"/>
            </fig>
          
          <p>In <xref ref-type="table" rid="table_2">Table 2</xref>, judgment sentiment evaluates the sentiment polarity in judicial decisions, such as whether the judgment leans toward a favourable or unfavourable outcome. It uses a feed-forward neural network for simplicity, as judgments are often explicit. Evidence sentiment supports evidence, which is more nuanced and often relies on auxiliary features, such as clause type or topic distribution, making an MLP a better fit due to its capacity for multi-feature integration. Argument sentiment is defined when arguments are often complex, requiring the model to assess their strength or persuasiveness. A transformer-based layer is used to handle these complexities effectively by capturing contextual dependencies. Finally, sentiments at the clause level, such as agreement or disagreement, are simpler and can be modelled effectively using logistic regression <inline-formula>
  <mml:math id="m6415sunak">
    <mml:msubsup>
      <mml:mi>E</mml:mi>
      <mml:mi>s</mml:mi>
      <mml:mrow>
        <mml:mtext>final </mml:mtext>
      </mml:mrow>
    </mml:msubsup>
    <mml:mo>+</mml:mo>
    <mml:mi>F</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Aspects and classification complexity</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Aspect Name</p></td><td colspan="1" rowspan="1"><p>Description</p></td><td colspan="1" rowspan="1"><p>Sentiment Categories</p></td><td colspan="1" rowspan="1"><p>Classifier Type</p></td><td colspan="1" rowspan="1"><p>Input Features</p></td></tr><tr><td colspan="1" rowspan="1"><p>Judgment</p></td><td colspan="1" rowspan="1"><p>Sentiment in final judicial decisions</p></td><td colspan="1" rowspan="1"><p>Positive, negative, and neutral</p></td><td colspan="1" rowspan="1"><p>Feed-forward neural network</p></td><td colspan="1" rowspan="1"><p><mml:math id="mggu93iwak">
  <mml:msubsup>
    <mml:mi>E</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mrow>
      <mml:mtext>final </mml:mtext>
    </mml:mrow>
  </mml:msubsup>
</mml:math></p></td></tr><tr><td colspan="1" rowspan="1"><p>Evidence</p></td><td colspan="1" rowspan="1"><p>Sentiment in supporting evidence</p></td><td colspan="1" rowspan="1"><p>Positive, negative, and contradictory</p></td><td colspan="1" rowspan="1"><p>Multi-Layer Perceptron (MLP)</p></td><td colspan="1" rowspan="1"><p><mml:math id="mvnyxh5c4t">
  <mml:msubsup>
    <mml:mi>E</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mrow>
      <mml:mtext>final </mml:mtext>
    </mml:mrow>
  </mml:msubsup>
</mml:math>+ <italic>Fs</italic></p></td></tr><tr><td colspan="1" rowspan="1"><p>Argument</p></td><td colspan="1" rowspan="1"><p>Sentiment in legal arguments presented</p></td><td colspan="1" rowspan="1"><p>Strong and weak</p></td><td colspan="1" rowspan="1"><p>Transformer-based layer</p></td><td colspan="1" rowspan="1"><p><mml:math id="myjqu3e6vn">
  <mml:msubsup>
    <mml:mi>E</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mrow>
      <mml:mtext>final </mml:mtext>
    </mml:mrow>
  </mml:msubsup>
</mml:math></p></td></tr><tr><td colspan="1" rowspan="1"><p>Clause</p></td><td colspan="1" rowspan="1"><p>Sentiment in specific legal clauses</p></td><td colspan="1" rowspan="1"><p>Agreement, disagreement, and neutral</p></td><td colspan="1" rowspan="1"><p>Logistic regression</p></td><td colspan="1" rowspan="1"><p><mml:math id="mddwvvvyh7">
  <mml:msubsup>
    <mml:mi>E</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mrow>
      <mml:mtext>final </mml:mtext>
    </mml:mrow>
  </mml:msubsup>
</mml:math>+ <italic>Fs</italic></p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. Explainability and interpretability evaluation</title>
          
          <p>In high-stakes domains like law, explainability is critical to ensure trust and accountability in AI-driven sentiment analysis systems. The proposed methodology incorporates three key techniques: Layer-wise Relevance Propagation (LRP), SHAP values, and a novel metric (LCAS). LRP assigns relevance scores to input features (e.g., words or phrases) by propagating the model's prediction back to the input, revealing the contribution of each feature to the sentiment prediction. SHAP quantifies the marginal impact of each input feature by treating it as a cooperative game and using SHAP values to measure its importance in the final prediction. LCAS, the novel contribution of this study, extends these methods by introducing a domain-specific measure that evaluates the coherence between the sentiment prediction and the logical structure of legal documents. It assigns a score based on the consistency of the prediction with hierarchical legal constructs, such as judgments, arguments, and evidence, ensuring that the model aligns with legal reasoning.</p><p><inline-formula>
  <mml:math id="mz7geqm25g">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> LRP: This computes the relevance <inline-formula>
  <mml:math id="m3qx2kb9st">
    <mml:msub>
      <mml:mi>R</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> of each input feature <inline-formula>
  <mml:math id="mdshuuelxt">
    <mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> based on the model's output $y$. The relevance is propagated backward through the layers of the network and is given by Eq. (6).</p>
          
            <disp-formula>
              <label>(6)</label>
              <mml:math id="mzczya4hje">
                <mml:msub>
                  <mml:mi>R</mml:mi>
                  <mml:mi>k</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>R</mml:mi>
                  <mml:mi>j</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:munder>
                  <mml:mo>∑</mml:mo>
                  <mml:mi>j</mml:mi>
                </mml:munder>
                <mml:mfrac>
                  <mml:msub>
                    <mml:mi>z</mml:mi>
                    <mml:mrow>
                      <mml:mi>k</mml:mi>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mrow>
                    <mml:munder>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:msup>
                          <mml:mi>k</mml:mi>
                          <mml:mrow>
                            <mml:mi>′</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                    </mml:munder>
                    <mml:msub>
                      <mml:mi>z</mml:mi>
                      <mml:mrow>
                        <mml:msup>
                          <mml:mi>k</mml:mi>
                          <mml:mrow>
                            <mml:mi>′</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                        <mml:mi>j</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="m334wrebhb">
    <mml:msub>
      <mml:mi>z</mml:mi>
      <mml:mrow>
        <mml:mi>k</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>w</mml:mi>
      <mml:mrow>
        <mml:mi>k</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula> is the weighted contribution of input <inline-formula>
  <mml:math id="mwu0isgcq4">
    <mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> to neuron <inline-formula>
  <mml:math id="mvgsyohgat">
    <mml:mi>j</mml:mi>
    <mml:mo>;</mml:mo>
    <mml:msub>
      <mml:mi>R</mml:mi>
      <mml:mi>j</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is the relevance of neuron $j<inline-formula>
  <mml:math id="m1evltf1l5">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>\sum_{k^{\prime}} z_{k^{\prime} j}<inline-formula>
  <mml:math id="mgynwylajz">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>j<inline-formula>
  <mml:math id="me1uv8mlky">
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
  </mml:math>
</inline-formula>\bullet<inline-formula>
  <mml:math id="m5p61511ec">
    <mml:mi>S</mml:mi>
    <mml:mi>H</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>P</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>H</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>P</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>:</mml:mo>
  </mml:math>
</inline-formula>\phi_k<inline-formula>
  <mml:math id="mst60rgnke">
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>x_k$ is computed using Eq. (7).</p>
          
            <disp-formula>
              <label>(7)</label>
              <mml:math id="mdkses3x3b">
                <mml:msub>
                  <mml:mi>ϕ</mml:mi>
                  <mml:mi>k</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:munder>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>S</mml:mi>
                    <mml:mi>∖</mml:mi>
                    <mml:mo>⊆</mml:mo>
                    <mml:mrow>
                      <mml:mo>{</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>…</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>}</mml:mo>
                      <mml:msub>
                        <mml:mi>x</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>x</mml:mi>
                        <mml:mi>m</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>{</mml:mo>
                      <mml:mo>}</mml:mo>
                      <mml:msub>
                        <mml:mi>x</mml:mi>
                        <mml:mi>k</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                  </mml:mrow>
                </mml:munder>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mo>|</mml:mo>
                    <mml:mo>!</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>!</mml:mo>
                    <mml:mi>S</mml:mi>
                    <mml:mi>m</mml:mi>
                    <mml:mi>S</mml:mi>
                    <mml:mrow>
                      <mml:mo>|</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>|</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>|</mml:mo>
                    </mml:mrow>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>m</mml:mi>
                    <mml:mo>!</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mrow>
                  <mml:mo>[</mml:mo>
                  <mml:mo>−</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mo>]</mml:mo>
                  <mml:mi>f</mml:mi>
                  <mml:mi>f</mml:mi>
                  <mml:mi>S</mml:mi>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>∪</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>S</mml:mi>
                    <mml:mrow>
                      <mml:mo>{</mml:mo>
                      <mml:mo>}</mml:mo>
                      <mml:msub>
                        <mml:mi>x</mml:mi>
                        <mml:mi>k</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, $S<inline-formula>
  <mml:math id="mqutcu3pmv">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
  </mml:math>
</inline-formula>x_k ; m<inline-formula>
  <mml:math id="ml21baw9x9">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>f(S)<inline-formula>
  <mml:math id="mai3cquu84">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:msup>
      <mml:mi>l</mml:mi>
      <mml:mo>′</mml:mo>
    </mml:msup>
  </mml:math>
</inline-formula>S<inline-formula>
  <mml:math id="mftwvlfseq">
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>\bullet<inline-formula>
  <mml:math id="m24r0tvrcj">
    <mml:mi>L</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>:</mml:mo>
  </mml:math>
</inline-formula>\widehat{y}<inline-formula>
  <mml:math id="mnb0fs0ldu">
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
  </mml:math>
</inline-formula>C$. It is defined as follows:</p>
          
            <disp-formula>
              <label>(8)</label>
              <mml:math id="mpmv9elg5j">
                <mml:mrow>
                  <mml:mi data-mjx-auto-op="false">LCAS</mml:mi>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:munderover>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>n</mml:mi>
                    </mml:munderover>
                    <mml:msub>
                      <mml:mi>w</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                    <mml:mo>⋅</mml:mo>
                    <mml:mi>δ</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mrow>
                        <mml:mover>
                          <mml:mi>y</mml:mi>
                          <mml:mo>^</mml:mo>
                        </mml:mover>
                      </mml:mrow>
                      <mml:msub>
                        <mml:mi>y</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:munderover>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>n</mml:mi>
                    </mml:munderover>
                    <mml:msub>
                      <mml:mi>w</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mm66a0wx5p">
    <mml:msub>
      <mml:mi>w</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is the weight assigned to the $i<inline-formula>
  <mml:math id="mynqcm9dpw">
    <mml:mo>−</mml:mo>
    <mml:mo>;</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>\delta\left(\hat{y}, y_i\right)<inline-formula>
  <mml:math id="mqngb0z3l5">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula>y_i<inline-formula>
  <mml:math id="m20v6d4ncm">
    <mml:mo>,</mml:mo>
    <mml:mo>;</mml:mo>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mn>0</mml:mn>
  </mml:math>
</inline-formula>n$ is the total number of constructs in the document.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Experiments and results</title>
      
        <sec>
          
            <title>4.1. Data collection and preprocessing</title>
          
          <p>The dataset used for this research includes Indian Supreme Court judgments collected in PDF format, starting from 1950, and is organized in CSV files with details such as diary numbers, judgment types, case numbers, petitioner and respondent details, advocates, judgment dates, and download links to the judgment PDFs and is available at the website [<xref ref-type="bibr" rid="ref_19">19</xref>]. This dataset provides a rich resource for analysing legal texts and their corresponding sentiments. For this implementation, a Windows 11 machine equipped with an NVIDIA RTX 3060 GPU, 16GB RAM, and an Intel Core i7 processor was used. The system setup allowed for efficient processing of large datasets and computationally intensive models. Python was the primary programming language, with libraries such as TensorFlow and PyTorch utilized for deep learning implementations. Data analysis and preprocessing were supported by pandas and NumPy, while NLP tasks such as tokenization and dependency parsing were performed using spaCy. For constructing and analysing graph structures, NetworkX was employed. High-definition graphs and visualizations were generated using Matplotlib, Seaborn, and Plotly, ensuring clarity and precision in presenting the results. The GPU setup significantly reduced the training time for complex models like GATs and transformer-based architectures, which are integral to the proposed methodology. Overall, the system configuration provided a robust environment for experimenting with legal document processing at scale.</p><p>To prepare the dataset of Supreme Court judgments for analysis, several preprocessing steps were applied to ensure the data was structured and ready for input into the models. First, the text content was extracted from the PDF files using the pdfplumber library, which allowed for precise parsing of text blocks, even in legal documents with varying formats. The extracted text was then tokenized into sentences and words using spaCy, which ensured the preservation of sentence structure and legal terminology. Following tokenization, lemmatization was performed to reduce words to their base forms, facilitating consistent representation of terms while ensuring domain-specific legal terms were not altered. Dependency parsing was also carried out using spaCy, capturing the relationships between clauses and sentences, which are crucial for constructing the graph representation of legal texts. Key features, such as clause types, positional information, and topic relevance, were extracted and stored in a structured format for downstream tasks. For example, topic relevance was derived using LDA, which assigned probabilistic scores for each sentence's relevance to predefined legal topics like evidence or judgments. The dataset was then split into training, validation, and test sets in an 80-10-10 ratio to ensure robust evaluation and prevent data leakage. The legal document dataset used in this study comprises real-world legal texts, sourced from publicly available court rulings, case law repositories, and government legal archives. To ensure data authenticity and legal compliance, strict data acquisition protocols were followed, anonymizing any sensitive information while preserving the structural and linguistic integrity of the documents. Additionally, a subset of data was manually annotated by legal experts to validate the sentiment classification labels. This ensures the dataset's reliability for training and evaluation purposes. The data preprocessing steps, including text normalization, tokenization, and noise removal, were applied consistently across all documents to maintain uniformity.</p>
        </sec>
      
      
        <sec>
          
            <title>4.2. Experimental setup</title>
          
          <p>The sentiment analysis pipeline integrates three key models for specific purposes. The GAT is used to capture hierarchical relationships in legal documents by representing sentences as nodes and their dependencies as edges, ensuring context-aware representations. LegalBERT, a transformer model fine-tuned on legal corpora, provides domain-specific embeddings to handle the complex semantics of legal text. Finally, aspect-oriented classifiers are modular neural networks designed for sentiment classification tailored to specific legal constructs like judgments, evidence, and arguments, enabling fine-grained sentiment analysis. This architecture is shown in <xref ref-type="table" rid="table_3">Table 3</xref>.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Training configuration and network architecture</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Component</p></td><td colspan="1" rowspan="1"><p>Architecture Details</p></td><td colspan="1" rowspan="1"><p>Hyperparameters</p></td><td colspan="1" rowspan="1"><p>Purpose</p></td></tr><tr><td colspan="1" rowspan="1"><p>GAT</p></td><td colspan="1" rowspan="1"><p>Input: Sentence embeddings <mml:math id="mfqxzourjq">
  <mml:mo>∈</mml:mo>
  <mml:msup>
    <mml:mrow>
      <mml:mi>R</mml:mi>
    </mml:mrow>
    <mml:mrow>
      <mml:mrow>
        <mml:mn>7</mml:mn>
      </mml:mrow>
      <mml:mrow>
        <mml:mn>6</mml:mn>
      </mml:mrow>
      <mml:mrow>
        <mml:mn>8</mml:mn>
      </mml:mrow>
    </mml:mrow>
  </mml:msup>
</mml:math>Layers: Two attention layers Output: Node embeddings <mml:math id="mv44a99i1a">
  <mml:mo>∈</mml:mo>
  <mml:msup>
    <mml:mrow>
      <mml:mi>R</mml:mi>
    </mml:mrow>
    <mml:mrow>
      <mml:mrow>
        <mml:mn>5</mml:mn>
      </mml:mrow>
      <mml:mrow>
        <mml:mn>1</mml:mn>
      </mml:mrow>
      <mml:mrow>
        <mml:mn>2</mml:mn>
      </mml:mrow>
    </mml:mrow>
  </mml:msup>
</mml:math></p></td><td colspan="1" rowspan="1"><p>Learning rate: 0.001 Batch size: 32 Optimizer: Adam</p></td><td colspan="1" rowspan="1"><p>Captures relationships between sentences and clauses through dependency parsing</p></td></tr><tr><td colspan="1" rowspan="1"><p>LegalBERT</p></td><td colspan="1" rowspan="1"><p>Input: Tokenized legal sentences Layers: 12 transformer layers Embedding size: <mml:math id="mn5kk2kyf3">
  <mml:mo>∈</mml:mo>
  <mml:msup>
    <mml:mrow>
      <mml:mi>R</mml:mi>
    </mml:mrow>
    <mml:mrow>
      <mml:mrow>
        <mml:mn>7</mml:mn>
      </mml:mrow>
      <mml:mrow>
        <mml:mn>6</mml:mn>
      </mml:mrow>
      <mml:mrow>
        <mml:mn>8</mml:mn>
      </mml:mrow>
    </mml:mrow>
  </mml:msup>
</mml:math></p></td><td colspan="1" rowspan="1"><p>Pre-trained weights (fine-tuned) Learning rate: 0.00002 Batch size: 16</p></td><td colspan="1" rowspan="1"><p>Generates contextual embeddings tailored to legal texts</p></td></tr><tr><td colspan="1" rowspan="1"><p>Aspect-oriented classifiers</p></td><td colspan="1" rowspan="1"><p>Input: Node embeddings from GAT Hidden layers: Two dense layers Activation: ReLU Output: Sentiment labels</p></td><td colspan="1" rowspan="1"><p>Learning rate: 0.001 Loss function: Cross-entropy Early stopping: Patience five epochs</p></td><td colspan="1" rowspan="1"><p>Predicts sentiment for legal constructs (e.g., judgments, evidence, and arguments).</p></td></tr><tr><td colspan="1" rowspan="1"><p>General training</p></td><td colspan="1" rowspan="1"><p>Dataset split: 80% training, 10% validation, 10% testing Epochs: 20 Regularization: Dropout (0.5)</p></td><td colspan="1" rowspan="1"><p>Validation strategy: Early stopping GPU utilization: NVIDIA RTX 3060</p></td><td colspan="1" rowspan="1"><p>Prevents overfitting and ensures model generalization</p></td></tr></tbody></table>
            </table-wrap>
          
          <p> <xref ref-type="table" rid="table_3">Table 3</xref> highlights how each component of the pipeline is designed to handle specific challenges in legal sentiment analysis. GAT captures hierarchical relationships, LegalBERT ensures semantic precision for legal terminology, and aspect-oriented classifiers enable targeted sentiment predictions. The parameters, including learning rates, batch sizes, and regularization techniques, were carefully chosen to optimize the model’s performance while maintaining efficiency. This structured and modular design ensures scalability and adaptability for legal NLP tasks.</p>
        </sec>
      
      
        <sec>
          
            <title>4.3. Experiment 1 (performance vs. baseline models across legal constructs)</title>
          
          <p>To evaluate the performance of the proposed approach, it was compared against baseline models, including LegalBERT, BERT, logistic regression, and SVM, across three key legal aspects: judgment, evidence, and arguments. Each model was trained and tested on the same dataset split (80% training, 10% validation, and 10% testing) to ensure consistency. Performance was measured using precision, recall, F1-score, and the novel LCAS to assess alignment with legal reasoning. This comparison highlights the strengths of the proposed model in capturing domain-specific semantics and hierarchical structures.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Comparative analysis of precision and accuracy across models</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_P6lYN_Y6U6TOcpCt.png"/>
            </fig>
          
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Comparative analysis of recall across models</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_ekPmWASb3V-Mzxix.png"/>
            </fig>
          
          <p><xref ref-type="fig" rid="fig_3">Figure 3</xref> illustrates the comparative performance of precision and accuracy for the proposed model, LegalBERT, BERT, logistic regression, and SVM over 100 epochs. The proposed model consistently outperforms all baselines, achieving the highest precision and accuracy values throughout the training process. LegalBERT follows as the second-best performer, with BERT trailing behind due to its lack of domain-specific fine-tuning. Logistic regression and SVM exhibit slower improvements, plateauing at significantly lower values, reflecting their limitations in handling the complexity of legal texts. This figure highlights the proposed model's robustness in balancing precision and accuracy effectively. As shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>, the proposed model leads in recall performance across all epochs, demonstrating its superior ability to identify relevant instances in the dataset. LegalBERT and BERT show competitive trends but fail to match the consistent improvement of the proposed model. Logistic regression and SVM once again plateau at lower recall values, indicating their limited capacity to generalize across diverse legal constructs. This result emphasizes the advantage of incorporating graph-based relationships and domain-specific embeddings in the proposed model. One of the critical challenges in sentiment analysis of legal documents is the high incidence of false sentiment detection in conventional models. Legal texts often contain complex linguistic structures, implicit sentiment cues, and domain-specific terminology that can lead traditional sentiment analysis models to misclassify neutral or objective statements as strongly positive or negative. The proposed model addresses this issue through the integration of GATs, LegalBERT embeddings, and aspect-oriented classifiers, which collectively improve the model’s ability to differentiate subtle sentiment variations.</p><p>As shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref> and <xref ref-type="fig" rid="fig_4">Figure 4</xref>, the proposed model achieves higher precision and recall compared to conventional models, indicating that it not only correctly classifies positive and negative sentiments but also minimizes misclassifications. A significant increase in F1-score (<xref ref-type="fig" rid="fig_5">Figure 5</xref>) further supports this claim, as it represents a balanced improvement in precision and recall, reducing both false positives and false negatives.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Comparative analysis of F1-score across models</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_QE3xTrBfpnAvRXRp.png"/>
            </fig>
          
          <p> <xref ref-type="fig" rid="fig_5">Figure 5</xref> provides a comparative view of F1-scores for all models. The proposed model exhibits the highest F1-scores, showcasing its balanced approach to precision and recall. LegalBERT performs moderately well, with BERT slightly behind, reflecting their respective capabilities. The classical models, logistic regression and SVM fail to maintain competitive F1-scores, further underlining their lack of adaptability for legal sentiment analysis. This analysis reinforces the efficacy of the proposed model in delivering holistic performance across key metrics. <xref ref-type="fig" rid="fig_6">Figure 6</xref> presents the LCAS for each model, a critical metric for assessing alignment with legal reasoning. The proposed model achieves the highest LCAS values, demonstrating its ability to capture hierarchical relationships and context-specific semantics effectively. LegalBERT and BERT show reasonable improvements, but their domain limitations hinder performance. Logistic regression and SVM lag significantly, reflecting their inability to model the complexity of legal constructs. This figure underscores the proposed model's strength in producing contextually coherent and legally aligned predictions.</p><p>These results collectively demonstrate the proposed model's superiority in capturing legal text nuances, balancing precision, recall, F1-score, and LCAS, and outperforming traditional and state-of-the-art baselines in all aspects.</p>
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Comparative analysis of LCAS across models</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_xhGVesNT8iireKY2.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.4. Experiment 2 (ablation study on the proposed model’s components)</title>
          
          <p>To evaluate the contribution of each component in the proposed model, an ablation study was conducted by systematically removing key components: GAT, LegalBERT embeddings, and auxiliary features like topic distribution and positional information. When GAT was removed, only LegalBERT embeddings with classifiers were used; replacing LegalBERT with standard BERT highlighted the importance of domain-specific embeddings. Additionally, removing auxiliary features revealed their impact on performance. The results of these variations are summarized in <xref ref-type="table" rid="table_4">Table 4</xref>, while the performance degradation trends across metrics are visualized in <xref ref-type="fig" rid="fig_7">Figure 7</xref>.</p>
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Performance comparison with component removal</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Component Removed</p></td><td colspan="1" rowspan="1"><p>Precision (Judgment)</p></td><td colspan="1" rowspan="1"><p>Recall (Judgment)</p></td><td colspan="1" rowspan="1"><p>F1-score (Judgment)</p></td><td colspan="1" rowspan="1"><p>Precision (Evidence)</p></td><td colspan="1" rowspan="1"><p>Recall (Evidence)</p></td><td colspan="1" rowspan="1"><p>F1-score (Evidence)</p></td><td colspan="1" rowspan="1"><p>Precision (Argument)</p></td><td colspan="1" rowspan="1"><p>Recall (Argument)</p></td><td colspan="1" rowspan="1"><p>F1-score (Argument)</p></td><td colspan="1" rowspan="1"><p>LCAS</p></td></tr><tr><td colspan="1" rowspan="1"><p>None (full model)</p></td><td colspan="1" rowspan="1"><p>0.92</p></td><td colspan="1" rowspan="1"><p>0.90</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>0.88</p></td><td colspan="1" rowspan="1"><p>0.88</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>0.90</p></td><td colspan="1" rowspan="1"><p>0.94</p></td></tr><tr><td colspan="1" rowspan="1"><p>GAT removed</p></td><td colspan="1" rowspan="1"><p>0.88</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>0.84</p></td><td colspan="1" rowspan="1"><p>0.82</p></td><td colspan="1" rowspan="1"><p>0.83</p></td><td colspan="1" rowspan="1"><p>0.87</p></td><td colspan="1" rowspan="1"><p>0.84</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>0.89</p></td></tr><tr><td colspan="1" rowspan="1"><p>LegalBERT replaced with BERT</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>0.84</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>0.83</p></td><td colspan="1" rowspan="1"><p>0.80</p></td><td colspan="1" rowspan="1"><p>0.82</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>0.83</p></td><td colspan="1" rowspan="1"><p>0.84</p></td><td colspan="1" rowspan="1"><p>0.87</p></td></tr><tr><td colspan="1" rowspan="1"><p>Auxiliary features removed</p></td><td colspan="1" rowspan="1"><p>0.87</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>0.84</p></td><td colspan="1" rowspan="1"><p>0.83</p></td><td colspan="1" rowspan="1"><p>0.83</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>0.84</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>0.88</p></td></tr><tr><td colspan="1" rowspan="1"><p>GAT + auxiliary features removed</p></td><td colspan="1" rowspan="1"><p>0.84</p></td><td colspan="1" rowspan="1"><p>0.82</p></td><td colspan="1" rowspan="1"><p>0.83</p></td><td colspan="1" rowspan="1"><p>0.82</p></td><td colspan="1" rowspan="1"><p>0.80</p></td><td colspan="1" rowspan="1"><p>0.81</p></td><td colspan="1" rowspan="1"><p>0.83</p></td><td colspan="1" rowspan="1"><p>0.81</p></td><td colspan="1" rowspan="1"><p>0.82</p></td><td colspan="1" rowspan="1"><p>0.86</p></td></tr><tr><td colspan="1" rowspan="1"><p>LegalBERT + auxiliary features removed</p></td><td colspan="1" rowspan="1"><p>0.83</p></td><td colspan="1" rowspan="1"><p>0.81</p></td><td colspan="1" rowspan="1"><p>0.82</p></td><td colspan="1" rowspan="1"><p>0.80</p></td><td colspan="1" rowspan="1"><p>0.78</p></td><td colspan="1" rowspan="1"><p>0.79</p></td><td colspan="1" rowspan="1"><p>0.82</p></td><td colspan="1" rowspan="1"><p>0.80</p></td><td colspan="1" rowspan="1"><p>0.81</p></td><td colspan="1" rowspan="1"><p>0.85</p></td></tr><tr><td colspan="1" rowspan="1"><p>All components except classifiers</p></td><td colspan="1" rowspan="1"><p>0.80</p></td><td colspan="1" rowspan="1"><p>0.78</p></td><td colspan="1" rowspan="1"><p>0.79</p></td><td colspan="1" rowspan="1"><p>0.78</p></td><td colspan="1" rowspan="1"><p>0.76</p></td><td colspan="1" rowspan="1"><p>0.77</p></td><td colspan="1" rowspan="1"><p>0.79</p></td><td colspan="1" rowspan="1"><p>0.77</p></td><td colspan="1" rowspan="1"><p>0.78</p></td><td colspan="1" rowspan="1"><p>0.82</p></td></tr></tbody></table>
            </table-wrap>
          
          <p><xref ref-type="table" rid="table_4">Table 4</xref> provides a comprehensive performance comparison of the proposed model under different component removal scenarios. The full model, incorporating all components, achieves the highest scores across precision, recall, F1-score, and LCAS for all legal aspects (judgment, evidence, and arguments), emphasizing the synergistic importance of GAT, LegalBERT, and auxiliary features. Removing GAT results in a notable drop in performance, particularly in LCAS, as the model loses its ability to capture hierarchical relationships. Replacing LegalBERT with standard BERT reduces domain-specific semantic understanding, leading to lower scores in all metrics. Similarly, removing auxiliary features such as topic distribution and positional information degrades performance slightly, particularly in precision and F1-score. The worst performance is observed when all components except the classifiers are removed, indicating that the classifiers alone cannot effectively handle the complexity of legal texts. The table highlights that each component contributes uniquely to the overall performance, with the combination of GAT, LegalBERT, and auxiliary features being critical for achieving optimal results. This analysis underscores the robustness and necessity of the modular design of the proposed model.</p>
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>Analysis plot for performance drops upon removal of the components</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_lTjgUeD50qM7M2tD.png"/>
            </fig>
          
          <p> <xref ref-type="fig" rid="fig_7">Figure 7</xref> illustrates the performance drop in key metrics (precision, recall, F1-score, and LCAS) when components of the proposed model are removed. The full model achieves the highest scores across all metrics, demonstrating the combined effectiveness of GAT, LegalBERT, and auxiliary features. Removing GAT leads to a noticeable decline in LCAS, emphasizing its role in capturing hierarchical relationships. Replacing LegalBERT with BERT reduces the model's ability to handle domain-specific semantics, affecting precision and recall significantly. Similarly, removing auxiliary features results in moderate degradation across metrics, highlighting their importance in fine-tuning predictions. This graph underscores the critical contribution of each component to the model’s overall performance.</p>
        </sec>
      
      
        <sec>
          
            <title>4.5. Analysis of explainability metrics</title>
          
          <p>The objective in this section is to analyse the interpretability of the proposed model using LCAS, SHAP, and LRP methods. LCAS is computed for key legal aspects (judgment, evidence, and arguments) across documents. SHAP values are used to identify the top contributing features for sentiment predictions.</p><p> <xref ref-type="fig" rid="fig_8">Figure 8</xref> illustrates the LCAS for different documents across the three legal aspects: judgment, evidence, and arguments. Higher LCAS values, represented by darker shades in the heatmap, indicate stronger alignment with the legal reasoning and context. The proposed model consistently achieves high LCAS across all aspects and documents, showcasing its ability to capture hierarchical and contextual relationships effectively. <xref ref-type="fig" rid="fig_9">Figure 9</xref> presents the SHAP feature importance for the top contributing features in sentiment predictions. Features with higher SHAP values, such as Feature 1 and Feature 2, have the most significant impact on the model's predictions. This visualization highlights the explainability of the proposed model, providing insights into how individual features influence the sentiment classification process. Together, these figures demonstrate the model's robustness and transparency in analysing legal texts.</p>
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>LCAS across documents and aspects</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_nLxXNS-HEluB1sMZ.png"/>
            </fig>
          
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>
                <title>SHAP feature importance</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_PyR8fGycz0j8vbQp.png"/>
            </fig>
          
          <p> <xref ref-type="table" rid="table_5">Table 5</xref> provides a detailed overview of the LCAS averages and SHAP feature rankings across the three key legal aspects: judgment, evidence, and arguments. The LCAS averages highlight the model's alignment with legal reasoning, with the highest scores observed in the judgment aspect (0.92), followed by arguments (0.89) and evidence (0.85). This indicates the proposed model's robustness in capturing contextual relationships for complex legal constructs. The top SHAP features rank the most influential features contributing to sentiment predictions for each aspect. For instance, Feature 1 and Feature 2 have the highest contributions to judgment predictions, while Feature 3 and Feature 5 dominate in the arguments aspect. The SHAP contribution column reflects the cumulative impact of the top-ranked features, demonstrating their critical role in shaping the model's predictions. This comprehensive table underscores the proposed model's ability to align sentiment predictions with legal context while providing transparency through feature attribution, further enhancing its interpretability for high-stakes legal applications.</p>
          
            <table-wrap id="table_5">
              <label>Table 5</label>
              <caption>
                <title>LCAS averages and SHAP feature rankings for each aspect</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Aspect</p></td><td colspan="1" rowspan="1"><p>LCAS Average (Judgment)</p></td><td colspan="1" rowspan="1"><p>LCAS Average (Evidence)</p></td><td colspan="1" rowspan="1"><p>LCAS Average (Arguments)</p></td><td colspan="1" rowspan="1"><p>Top SHAP Features (Rank 1 to 5)</p></td><td colspan="1" rowspan="1"><p>SHAP Contribution (Cumulative)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Judgment</p></td><td colspan="1" rowspan="1"><p>0.92</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>Feature 1, </p><p>Feature 2, </p><p>Feature 3, </p><p>Feature 4,</p><p>Feature 5</p></td><td colspan="1" rowspan="1"><p>82%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Evidence</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>Feature 2,</p><p>Feature 1,</p><p>Feature 3,</p><p>Feature 5,</p><p>Feature 4</p></td><td colspan="1" rowspan="1"><p>78%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Arguments</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>Feature 3,</p><p>Feature 5,</p><p>Feature 1,</p><p>Feature 2,</p><p>Feature 4</p></td><td colspan="1" rowspan="1"><p>80%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Combined Avg.</p></td><td colspan="1" rowspan="1"><p>0.92</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>Feature 1,</p><p>Feature 2,</p><p>Feature 3,</p><p>Feature 5,</p><p>Feature 4</p></td><td colspan="1" rowspan="1"><p>80%</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>4.6. Experiment 3 (efficiency and scalability analysis)</title>
          
          <p>The aim of this experiment is to evaluate the efficiency and scalability of the proposed model by measuring its training time, GPU memory usage, and inference speed across datasets of varying sizes. By running the model on small (10,000 sentences), medium (50,000 sentences), and large (100,000 sentences) and extra-large datasets, this analysis provides insights into how the model performs under different computational loads. The results are summarized in <xref ref-type="table" rid="table_6">Table 6</xref>, which highlights key metrics and their variations across dataset sizes.</p>
          
            <table-wrap id="table_6">
              <label>Table 6</label>
              <caption>
                <title>LCAS averages and SHAP feature rankings for each aspect</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Dataset Size</p></td><td colspan="1" rowspan="1"><p>Training Time (hour)</p></td><td colspan="1" rowspan="1"><p>Peak GPU Memory Usage (GB)</p></td><td colspan="1" rowspan="1"><p>Average GPU Utilization (%)</p></td><td colspan="1" rowspan="1"><p>Inference Speed (Sentences/Second)</p></td><td colspan="1" rowspan="1"><p>Training Epochs</p></td><td colspan="1" rowspan="1"><p>Validation Accuracy (%)</p></td><td colspan="1" rowspan="1"><p>Validation Loss</p></td><td colspan="1" rowspan="1"><p>Number of Parameters (million)</p></td><td colspan="1" rowspan="1"><p>Average CPU Utilization (%)</p></td><td colspan="1" rowspan="1"><p>Disk I/O (MB/s)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Small (10,000 sentences)</p></td><td colspan="1" rowspan="1"><p>2.1</p></td><td colspan="1" rowspan="1"><p>4.2</p></td><td colspan="1" rowspan="1"><p>65</p></td><td colspan="1" rowspan="1"><p>120</p></td><td colspan="1" rowspan="1"><p>10</p></td><td colspan="1" rowspan="1"><p>88.5</p></td><td colspan="1" rowspan="1"><p>0.34</p></td><td colspan="1" rowspan="1"><p>110</p></td><td colspan="1" rowspan="1"><p>30</p></td><td colspan="1" rowspan="1"><p>25</p></td></tr><tr><td colspan="1" rowspan="1"><p>Medium (50,000 sentences)</p></td><td colspan="1" rowspan="1"><p>10.8</p></td><td colspan="1" rowspan="1"><p>9.5</p></td><td colspan="1" rowspan="1"><p>78</p></td><td colspan="1" rowspan="1"><p>95</p></td><td colspan="1" rowspan="1"><p>15</p></td><td colspan="1" rowspan="1"><p>90.3</p></td><td colspan="1" rowspan="1"><p>0.29</p></td><td colspan="1" rowspan="1"><p>110</p></td><td colspan="1" rowspan="1"><p>50</p></td><td colspan="1" rowspan="1"><p>45</p></td></tr><tr><td colspan="1" rowspan="1"><p>Large (100,000 sentences)</p></td><td colspan="1" rowspan="1"><p>21.6</p></td><td colspan="1" rowspan="1"><p>16.8</p></td><td colspan="1" rowspan="1"><p>85</p></td><td colspan="1" rowspan="1"><p>70</p></td><td colspan="1" rowspan="1"><p>20</p></td><td colspan="1" rowspan="1"><p>91.7</p></td><td colspan="1" rowspan="1"><p>0.24</p></td><td colspan="1" rowspan="1"><p>110</p></td><td colspan="1" rowspan="1"><p>65</p></td><td colspan="1" rowspan="1"><p>70</p></td></tr><tr><td colspan="1" rowspan="1"><p>Extra large (200,000 sentences)</p></td><td colspan="1" rowspan="1"><p>45.2</p></td><td colspan="1" rowspan="1"><p>24.5</p></td><td colspan="1" rowspan="1"><p>92</p></td><td colspan="1" rowspan="1"><p>50</p></td><td colspan="1" rowspan="1"><p>25</p></td><td colspan="1" rowspan="1"><p>93.1</p></td><td colspan="1" rowspan="1"><p>0.20</p></td><td colspan="1" rowspan="1"><p>110</p></td><td colspan="1" rowspan="1"><p>75</p></td><td colspan="1" rowspan="1"><p>120</p></td></tr></tbody></table>
            </table-wrap>
          
          <p><xref ref-type="table" rid="table_6">Table 6</xref> provides a detailed analysis of the proposed model's performance across varying dataset sizes, including small (10,000 sentences), medium (50,000 sentences), large (100,000 sentences), and extra large (200,000 sentences). The metrics cover training time, GPU memory usage, inference speed, validation performance, and system-level resource utilization to offer a granular understanding of the model's scalability and efficiency. Training time increases significantly as the dataset size grows, from 2.1 hours for the small dataset to 45.2 hours for the extra-large dataset, with peak GPU memory usage scaling proportionally to 24.5 GB for the largest dataset. Inference speed decreases with larger datasets, dropping from 120 sentences/second for the small dataset to 50 sentences/second for the extra-large dataset, reflecting the computational overhead of processing larger inputs. Validation accuracy shows steady improvement with dataset size, reaching 93.1% for the largest dataset, indicating better generalization with more data. Validation loss consistently decreases across dataset sizes, reflecting enhanced optimization and performance. System utilization metrics reveal increasing CPU utilization, peaking at 75% for the extra-large dataset, while disk input/output (I/O) grows from 25 MB/s for the small dataset to 120 MB/s for the largest dataset, highlighting the significant processing demands as data scales. Despite these variations, the number of parameters remains constant at 110 million, reflecting the stable architecture of the proposed model. A crucial factor contributing to this improvement is the aspect-oriented classification, which allows the model to focus on specific legal aspects within the text rather than making generalized sentiment assumptions. This is evident in <xref ref-type="table" rid="table_5">Table 5</xref>, where LCAS shows that the proposed model assigns more accurate sentiment ratings across various legal document aspects, particularly in judgment, evidence, and argumentation-based texts. The use of SHAP feature rankings further demonstrates how the model identifies the most critical features influencing sentiment classification, leading to higher interpretability and reduced false sentiment predictions. Furthermore, the component ablation study (<xref ref-type="table" rid="table_4">Table 4</xref> and <xref ref-type="fig" rid="fig_7">Figure 7</xref>) provides clear evidence that removing GATs, LegalBERT, or auxiliary features significantly deteriorates performance, particularly in recall and F1-score. This indicates that these components play a crucial role in distinguishing genuine sentiment expressions from legal rhetoric and procedural language, which often confound conventional sentiment analysis models. Additionally, the efficiency analysis in <xref ref-type="table" rid="table_6">Table 6</xref> reveals that despite the incorporation of advanced deep learning techniques, the model maintains scalability across different dataset sizes without a disproportionate increase in computational cost. This ensures that the improvements in false sentiment detection do not come at the expense of practical feasibility in real-world legal applications. In conclusion, the results substantiate that the proposed model significantly outperforms existing approaches in handling false sentiment detection, demonstrating its robustness across various legal document types. The combination of domain-specific embeddings, structured attention mechanisms, and aspect-driven classification ensures more reliable sentiment classification, addressing a major shortcoming of traditional sentiment analysis models in the legal domain. This comprehensive evaluation underscores the scalability and efficiency of the model, demonstrating its adaptability to varying computational loads and dataset sizes.</p>
        </sec>
      
      
        <sec>
          
            <title>4.7. Real-time performance analysis</title>
          
          <p>To evaluate the scalability and applicability of the proposed model in real-time legal environments, an experiment was conducted, measuring inference speed and system latency under varying workload conditions. A subset of 10,000 legal documents, each averaging 500 tokens, was used for evaluation. The model's performance was tested under three concurrency levels: 1, 5, and 10 simultaneous inference requests. Key performance metrics recorded included average inference time (ms/document), which measures the time taken to generate sentiment predictions, throughput (documents/second), indicating the number of documents processed per second, and GPU utilization (%), reflecting computational resource consumption during inference. The results, illustrated in <xref ref-type="fig" rid="fig_10">Figure 10</xref>, provide insights into the model’s efficiency and real-time adaptability.</p>
          
            <fig id="fig_10">
              <label>Figure 10</label>
              <caption>
                <title>Scalability analysis – inference performance vs. concurrency levels</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_WWAY1HDJNu7f_7tj.png"/>
            </fig>
          
          <p><xref ref-type="fig" rid="fig_10">Figure 10</xref> presents the scalability analysis of inference performance under varying concurrency levels, demonstrating the trade-offs between efficiency and computational resource utilization. The x-axis represents the concurrency level (number of simultaneous inference requests), while the y-axes denote throughput (documents per second), inference time (ms/document), CPU utilization (%), GPU utilization (%), and memory usage (GB). The results indicate that as concurrency increases, throughput (blue bars) initially scales well but plateaus at higher levels due to resource constraints. CPU utilization (red bars) rises significantly with increasing concurrency, peaking at higher levels, reflecting the computational burden on system resources. Memory usage (green bars) follows a steady increase, ensuring stable model performance. Inference time (black line) exhibits a rising trend, indicating that response times lengthen as the workload grows. However, GPU utilization (dashed purple line) remains relatively stable, suggesting that the model is more CPU-intensive for inference tasks. These results demonstrate that while the proposed model effectively scales to moderate concurrency levels beyond a certain threshold, system performance is affected due to resource saturation. This analysis highlights the importance of efficient resource allocation when deploying the model in real-time legal applications.</p>
        </sec>
      
      
        <sec>
          
            <title>4.8. Measurement of cpu consumption and memory usage</title>
          
          <p>To further evaluate the practical feasibility of the proposed model, an experiment was conducted, measuring execution time and memory usage across different dataset sizes. This assessment provides insights into the computational efficiency of the model, particularly in real-world applications where scalability is a key concern. <xref ref-type="table" rid="table_7">Table 7</xref> presents a detailed breakdown of execution time (both training and inference) and memory consumption at varying dataset scales.</p>
          
            <table-wrap id="table_7">
              <label>Table 7</label>
              <caption>
                <title>Execution time and memory usage analysis across dataset sizes</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Dataset Size (Sentences)</p></td><td colspan="1" rowspan="1"><p>Training Time (h)</p></td><td colspan="1" rowspan="1"><p>Inference Time (ms/doc)</p></td><td colspan="1" rowspan="1"><p>Peak GPU Memory (GB)</p></td><td colspan="1" rowspan="1"><p>CPU Utilization (%)</p></td><td colspan="1" rowspan="1"><p>GPU Utilization (%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>10 K</p></td><td colspan="1" rowspan="1"><p>1.2</p></td><td colspan="1" rowspan="1"><p>120</p></td><td colspan="1" rowspan="1"><p>4.5</p></td><td colspan="1" rowspan="1"><p>35</p></td><td colspan="1" rowspan="1"><p>20</p></td></tr><tr><td colspan="1" rowspan="1"><p>50 K</p></td><td colspan="1" rowspan="1"><p>4.8</p></td><td colspan="1" rowspan="1"><p>150</p></td><td colspan="1" rowspan="1"><p>7.8</p></td><td colspan="1" rowspan="1"><p>50</p></td><td colspan="1" rowspan="1"><p>40</p></td></tr><tr><td colspan="1" rowspan="1"><p>100 K</p></td><td colspan="1" rowspan="1"><p>9.3</p></td><td colspan="1" rowspan="1"><p>185</p></td><td colspan="1" rowspan="1"><p>12.2</p></td><td colspan="1" rowspan="1"><p>65</p></td><td colspan="1" rowspan="1"><p>55</p></td></tr><tr><td colspan="1" rowspan="1"><p>200 K</p></td><td colspan="1" rowspan="1"><p>18.7</p></td><td colspan="1" rowspan="1"><p>230</p></td><td colspan="1" rowspan="1"><p>18.5</p></td><td colspan="1" rowspan="1"><p>80</p></td><td colspan="1" rowspan="1"><p>70</p></td></tr><tr><td colspan="1" rowspan="1"><p>500 K</p></td><td colspan="1" rowspan="1"><p>42.1</p></td><td colspan="1" rowspan="1"><p>315</p></td><td colspan="1" rowspan="1"><p>28.4</p></td><td colspan="1" rowspan="1"><p>95</p></td><td colspan="1" rowspan="1"><p>85</p></td></tr></tbody></table>
            </table-wrap>
          
          <p><xref ref-type="table" rid="table_7">Table 7</xref> provides a comprehensive analysis of the execution time and memory usage of the proposed model across different dataset sizes. As the dataset size increases from 10,000 to 500,000 sentences, the training time scales significantly from 1.2 hours to 42.1 hours, highlighting the computational intensity of the model. Similarly, inference time per document rises from 120 ms to 315 ms, indicating a gradual increase in processing latency. GPU memory consumption follows a similar trend, peaking at 28.4 GB for the largest dataset, which reflects the resource-intensive nature of handling large-scale legal documents. Additionally, CPU utilization increases from 35% to 95%, while GPU utilization rises from 20% to 85%, demonstrating the scalability challenges associated with high-concurrency processing. These results emphasize the need for optimizing resource efficiency and inference speed for real-time legal applications.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>5. Comparative analysis</title>
      <p>This section provides a detailed comparison of the proposed model with existing baseline models and methodologies in the field of legal sentiment analysis. The analysis highlights the advancements made by the model in terms of performance metrics, scalability, and interpretability, showcasing its superiority in addressing the complexities of legal texts.</p>
      
        <table-wrap id="table_8">
          <label>Table 8</label>
          <caption>
            <title>Comparative performance analysis across approaches</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Study</p></td><td colspan="1" rowspan="1"><p>Methodology</p></td><td colspan="1" rowspan="1"><p>Model/Algorithm</p></td><td colspan="1" rowspan="1"><p>Dataset</p></td><td colspan="1" rowspan="1"><p>Accuracy (%)</p></td><td colspan="1" rowspan="1"><p>Legal Context Handling</p></td><td colspan="1" rowspan="1"><p>Explainability (SHAP/LCAS)</p></td><td colspan="1" rowspan="1"><p>Scalability (Large Data)</p></td><td colspan="1" rowspan="1"><p>Hierarchical Analysis</p></td><td colspan="1" rowspan="1"><p>Domain-Specific Embedding</p></td><td colspan="1" rowspan="1"><p>Auxiliary Features</p></td></tr><tr><td colspan="1" rowspan="1"><p>Proposed model</p></td><td colspan="1" rowspan="1"><p>Hybrid (graph + Transformer + auxiliary features)</p></td><td colspan="1" rowspan="1"><p>GAT + LegalBERT + aspect classifiers</p></td><td colspan="1" rowspan="1"><p>Indian Legal Judgments</p></td><td colspan="1" rowspan="1"><p>93.1</p></td><td colspan="1" rowspan="1"><p>✓</p></td><td colspan="1" rowspan="1"><p>✓</p></td><td colspan="1" rowspan="1"><p>✓</p></td><td colspan="1" rowspan="1"><p>✓</p></td><td colspan="1" rowspan="1"><p>✓</p></td><td colspan="1" rowspan="1"><p>✓</p></td></tr><tr><td colspan="1" rowspan="1"><p>[<xref ref-type="bibr" rid="ref_20">20</xref>]</p></td><td colspan="1" rowspan="1"><p>Deep learning</p></td><td colspan="1" rowspan="1"><p>CNN + LSTM</p></td><td colspan="1" rowspan="1"><p>Canadian Maritime Case Law</p></td><td colspan="1" rowspan="1"><p>98.05</p></td><td colspan="1" rowspan="1"><p>✕</p></td><td colspan="1" rowspan="1"><p>✕</p></td><td colspan="1" rowspan="1"><p>✕</p></td><td colspan="1" rowspan="1"><p>✕</p></td><td colspan="1" rowspan="1"><p>✕</p></td><td colspan="1" rowspan="1"><p>✕</p></td></tr><tr><td colspan="1" rowspan="1"><p>[<xref ref-type="bibr" rid="ref_21">21</xref>]</p></td><td colspan="1" rowspan="1"><p>Machine learning</p></td><td colspan="1" rowspan="1"><p>SVM with N-grams</p></td><td colspan="1" rowspan="1"><p>Twitter Data</p></td><td colspan="1" rowspan="1"><p>81</p></td><td colspan="1" rowspan="1"><p>✕</p></td><td colspan="1" rowspan="1"><p>✕</p></td><td colspan="1" rowspan="1"><p>✓</p></td><td colspan="1" rowspan="1"><p>✕</p></td><td colspan="1" rowspan="1"><p>✕</p></td><td colspan="1" rowspan="1"><p>✕</p></td></tr><tr><td colspan="1" rowspan="1"><p>[<xref ref-type="bibr" rid="ref_22">22</xref>]</p></td><td colspan="1" rowspan="1"><p>Machine learning + embeddings</p></td><td colspan="1" rowspan="1"><p>Random forest + LegalBERT, T5, RoBERTa</p></td><td colspan="1" rowspan="1"><p>Legal Judgments</p></td><td colspan="1" rowspan="1"><p>67.5</p></td><td colspan="1" rowspan="1"><p>✓</p></td><td colspan="1" rowspan="1"><p>✕</p></td><td colspan="1" rowspan="1"><p>✓</p></td><td colspan="1" rowspan="1"><p>✕</p></td><td colspan="1" rowspan="1"><p>✓</p></td><td colspan="1" rowspan="1"><p>✕</p></td></tr><tr><td colspan="1" rowspan="1"><p>[<xref ref-type="bibr" rid="ref_23">23</xref>]</p></td><td colspan="1" rowspan="1"><p>Neural network</p></td><td colspan="1" rowspan="1"><p>MLP + TF-IDF</p></td><td colspan="1" rowspan="1"><p>Twitter Data</p></td><td colspan="1" rowspan="1"><p>88</p></td><td colspan="1" rowspan="1"><p>✕</p></td><td colspan="1" rowspan="1"><p>✕</p></td><td colspan="1" rowspan="1"><p>✕</p></td><td colspan="1" rowspan="1"><p>✕</p></td><td colspan="1" rowspan="1"><p>✕</p></td><td colspan="1" rowspan="1"><p>✕</p></td></tr><tr><td colspan="1" rowspan="1"><p>[<xref ref-type="bibr" rid="ref_24">24</xref>]</p></td><td colspan="1" rowspan="1"><p>Machine learning + philosophy</p></td><td colspan="1" rowspan="1"><p>Sentence-BERT</p></td><td colspan="1" rowspan="1"><p>Religious Discrimination</p></td><td colspan="1" rowspan="1"><p>92.5</p></td><td colspan="1" rowspan="1"><p>✓</p></td><td colspan="1" rowspan="1"><p>✓</p></td><td colspan="1" rowspan="1"><p>✓</p></td><td colspan="1" rowspan="1"><p>✓</p></td><td colspan="1" rowspan="1"><p>✓</p></td><td colspan="1" rowspan="1"><p>✕</p></td></tr></tbody></table>
        </table-wrap>
      
      <p><xref ref-type="table" rid="table_8">Table 8</xref> provides a detailed comparative analysis of the proposed model against other state-of-the-art approaches across multiple dimensions. The proposed model achieves a high accuracy of 93.1%, showcasing its robust performance in legal sentiment analysis. Unlike other models, the proposed model effectively handles legal context through its graph-based hierarchical analysis and domain-specific embeddings (LegalBERT), setting it apart from approaches like CNN-LSTM or MLP, which lack these capabilities. Furthermore, the proposed model uniquely incorporates auxiliary features such as positional information and topic distribution, enhancing its predictive power. In terms of explainability, the proposed model stands out by integrating SHAP and LCAS, providing transparency in its predictions, which is unmatched by most baseline approaches except for the approach proposed by Izzidien [<xref ref-type="bibr" rid="ref_24">24</xref>], which uses a heuristic-based method. The model also demonstrates superior scalability, maintaining high performance even with large datasets, unlike traditional machine learning methods. Additionally, its ability to perform hierarchical analysis through GAT ensures that the complex relationships within legal documents are captured effectively. Overall, the proposed model not only outperforms existing approaches in terms of accuracy and scalability but also excels in critical aspects like interpretability and legal context awareness, making it a comprehensive and advanced solution for legal sentiment analysis.</p>
    </sec>
    <sec sec-type="">
      <title>6. Limitations and future work</title>
      <p>While the proposed model demonstrates high accuracy, scalability, and interpretability in legal sentiment analysis, it has certain limitations. The reliance on pre-trained embeddings like LegalBERT means the model's performance may be constrained by the quality and coverage of the underlying training data, particularly for underrepresented legal domains. Additionally, the computational requirements, especially for large datasets, can be resource-intensive, posing challenges for deployment in low-resource environments. Another limitation is the absence of additional evaluation metrics such as robustness and a dedicated scalability analysis beyond accuracy-based assessments. While <xref ref-type="fig" rid="fig_3">Figure 3</xref>, <xref ref-type="fig" rid="fig_4">Figure 4</xref>, <xref ref-type="fig" rid="fig_5">Figure 5</xref>, <xref ref-type="fig" rid="fig_6">Figure 6</xref> and <xref ref-type="table" rid="table_6">Table 6</xref> provide evidence of the model’s effectiveness in handling large datasets and maintaining performance consistency, a more comprehensive robustness framework would further solidify these findings. Future work will aim to incorporate robustness-specific metrics and explore a broader evaluation framework tailored to high-stakes legal decision-making. Furthermore, the proposed approach makes certain assumptions about the complexity of legal documents, particularly their hierarchical structures and domain-specific language. While <xref ref-type="table" rid="table_5">Table 5</xref> demonstrates the model’s ability to capture key legal aspects using LCAS and SHAP feature rankings, legal texts vary significantly across jurisdictions and case types. More complex legal environments, such as multi-layered regulatory frameworks or multilingual legal systems, may require additional adaptations. Future work will explore methods to enhance the model’s ability to generalize across diverse legal structures by integrating hierarchical document representation techniques, multi-domain embeddings, and transfer learning strategies. Moreover, the model currently focuses on sentiment classification within legal texts. Expanding its capabilities to include legal document summarization, argument extraction, and case law similarity analysis would enhance its utility. Incorporating multilingual capabilities to handle diverse legal systems and optimizing computational efficiency for real-time applications will also be key areas of future exploration. By addressing these aspects, the model can further enhance its adaptability and effectiveness in real-world legal scenarios.</p>
    </sec>
    <sec sec-type="">
      <title>7. Conclusion</title>
      <p>This study presents a comprehensive approach to sentiment analysis in legal documents through a novel hybrid framework that integrates graph-based reasoning, domain-specific embeddings, and aspect-oriented sentiment classification. The proposed model demonstrates its ability to address the complexities of legal texts, including their hierarchical structure, domain-specific semantics, and context-dependent sentiments. Extensive experiments validate the effectiveness of the proposed model, achieving superior performance compared to baseline methods across key metrics such as precision, recall, F1-score, and LCAS. The ablation study highlights the critical contributions of each component, such as GAT, LegalBERT, and auxiliary features, in enhancing the model's predictive accuracy and interpretability. Furthermore, scalability analysis reveals the model's efficiency in handling datasets of varying sizes, demonstrating its robustness for large-scale applications. By incorporating advanced explainability techniques such as SHAP and LCAS, the model ensures transparency, making it suitable for high-stakes legal environments where interpretability is essential. Despite these achievements, the model has limitations, such as high computational requirements and reliance on pre-trained embeddings. Future work can focus on optimizing the model for efficiency, extending its capabilities to multilingual and cross-jurisdictional legal systems, and exploring its application in related tasks like legal document summarization and argument extraction. This research provides a significant step forward in legal NLP, paving the way for more intelligent, interpretable, and scalable AI solutions in the legal domain.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>19</volume>
          <page-range>165-184</page-range>
          <issue>10</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pichardo-Lagunas</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Martinez-Seis</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Hidalgo-Reyes</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Miranda</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Automatic detection of opposition relations in legal texts using sentiment analysis techniques: A case study</article-title>
          <source>Acta Polytech. Hung.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sil</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">https://doi.org/10.2139/ssrn.4145582</pub-id>
          <article-title>Sentiment analysis-based legal case prediction system</article-title>
          <source>Preprint at SSRN</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>101-115</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Rajapaksha</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Mudalige</surname>
              <given-names>C. R.</given-names>
            </name>
            <name>
              <surname>Karunarathna</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>de Silva</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Ratnayaka</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>a.  Perera</surname>
              <given-names>A. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.26421/jdi3.1-1</pub-id>
          <article-title>Sigmalaw PBSA-A deep learning approach for aspect based sentiment analysis in legal opinion texts</article-title>
          <source>J. Data Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Vaissnave</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Deepalakshmi</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Comparative analysis: Sentiment analysis for legal judgment text in India’s supreme court based on GLoVe pretrained word embedding and deep learning models</article-title>
          <source>Soft Computing: Theories and Applications: Proceedings of SoCTA 2021</source>
          <publisher-name>Singapore: Springer Nature Singapore</publisher-name>
          <year>2022</year>
          <page-range>33-44</page-range>
          <pub-id pub-id-type="doi">10.1007/978-981-19-0707-4_4</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>1-15</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Krishnan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Shashidhar</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Varol</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Islam</surname>
              <given-names>A. R.</given-names>
            </name>
          </person-group>
          <article-title>Sentiment analysis of case suspects in digital forensics and legal analytics</article-title>
          <source>Int. J. Secur.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>35</volume>
          <page-range>24621-24632</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hao</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00521-023-08226-4</pub-id>
          <article-title>Sentiment recognition and analysis method of official document text based on BERT–SVM model</article-title>
          <source>Neural Computing and Applications</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>204-222</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Farhadishad</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kazemifard</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Rezaei</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.22059/jitm.2023.350464.3206</pub-id>
          <article-title>Predicting court judgment in criminal cases by text mining techniques</article-title>
          <source>J. Inf. Technol. Manag.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>e0000063</page-range>
          <issue>7</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ramjee</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Louisa  Smith</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Doanvo</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Charpignon</surname>
              <given-names>M. L.</given-names>
            </name>
            <name>
              <surname>McNulty-Nebel</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Lett</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Angel  Desai</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Maimuna  Majumder</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1371/journal.pdig.0000063</pub-id>
          <article-title>Evaluating criminal justice reform during COVID-19: The need for a novel sentiment analysis package</article-title>
          <source>PLOS Digital Health</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="conf-paper">
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Licari</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Comandè</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>ITALIAN-LEGAL-BERT: A pre-trained transformer language model for Italian law</article-title>
          <source>EKAW’22: Companion Proceedings of the 23rd International Conference on Knowledge Engineering and Knowledge Management, Bozen-Bolzano, Italy</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="conf-paper">
          <page-range>148-156</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Licari</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Bushipaka</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Marino</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Comandè</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Cucinotta</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3594536.3595177</pub-id>
          <article-title>Legal holding extraction from italian case documents using Italian-legal-BERT text summarization</article-title>
          <source>Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law, Braga, Portugal</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <page-range>3401-3409</page-range>
          <issue>6</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Abimbola</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Tan</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Enrique  De La Cal Marín</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s41870-024-01820-2</pub-id>
          <article-title>Sentiment analysis of Canadian maritime case law: A sentiment case law and deep learning approach</article-title>
          <source>International Journal of Information Technology</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>443</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sengupta</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s42979-024-02836-y</pub-id>
          <article-title>Legislative text analysis from judicial case reports using machine learning</article-title>
          <source>SN Comput. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>35</volume>
          <page-range>24621-24632</page-range>
          <issue>35</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hao</surname>
              <given-names>Shule</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Peng</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Sen</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Yuhang</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00521-023-08226-4</pub-id>
          <article-title>Sentiment recognition and analysis method of official document text based on BERT–SVM model</article-title>
          <source>Neural Comput. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>82</volume>
          <page-range>32967-32990</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Dey</surname>
              <given-names>Ranit Kumar</given-names>
            </name>
            <name>
              <surname>Das</surname>
              <given-names>Asit Kumar</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-023-14653-1</pub-id>
          <article-title>Modified term frequency-inverse document frequency based deep hybrid framework for sentiment analysis</article-title>
          <source>Multimed. Tools Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Naik</surname>
              <given-names>Varsha</given-names>
            </name>
            <name>
              <surname>Patel</surname>
              <given-names>Purvang</given-names>
            </name>
            <name>
              <surname>Kannan</surname>
              <given-names>Rajeswari</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.14569/IJACSA.2023.0140389</pub-id>
          <article-title>Legal entity extraction: An experimental study of NER approach for legal documents</article-title>
          <source>Int. J. Adv. Comput. Sci. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>32</volume>
          <page-range>165-200</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jain</surname>
              <given-names>Deepali</given-names>
            </name>
            <name>
              <surname>Borah</surname>
              <given-names>Malaya Dutta</given-names>
            </name>
            <name>
              <surname>Biswas</surname>
              <given-names>Anupam</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10506-023-09345-y</pub-id>
          <article-title>A sentence is known by the company it keeps: Improving legal document summarization using deep clustering</article-title>
          <source>Artif. Intell. Law</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>2273-2282</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gupta</surname>
              <given-names>Isha</given-names>
            </name>
            <name>
              <surname>Chatterjee</surname>
              <given-names>Indranath</given-names>
            </name>
            <name>
              <surname>Gupta</surname>
              <given-names>Neha</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s41870-023-01273-z</pub-id>
          <article-title>A two-staged NLP-based framework for assessing the sentiments on Indian supreme court judgments</article-title>
          <source>Int. J. Inf. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mengi</surname>
              <given-names>Rucha</given-names>
            </name>
            <name>
              <surname>Ghorpade</surname>
              <given-names>Harshad</given-names>
            </name>
            <name>
              <surname>Kakade</surname>
              <given-names>Ashwini</given-names>
            </name>
          </person-group>
          <article-title>Fine-tuning T5 and RoBERTa models for enhanced text summarization and sentiment analysis</article-title>
          <source>Great Lakes Bot.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="webpage">
          <article-title>Indian Supreme Court Judgments</article-title>
          <source>, https://www.kaggle.com/datasets/vangap/indian-supreme-court-judgments</source>
          <year>2025</year>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <page-range>877-897</page-range>
          <issue>2</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Abimbola</surname>
              <given-names>Bolanle</given-names>
            </name>
            <name>
              <surname>La Cal Marin</surname>
              <given-names>Enrique</given-names>
            </name>
            <name>
              <surname>Tan</surname>
              <given-names>Qing</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/make6020041</pub-id>
          <article-title>Enhancing legal sentiment analysis: A convolutional neural network–long short-term memory document-level model</article-title>
          <source>Mach. Learn. Knowl. Extr.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Irawan</surname>
              <given-names>Deny</given-names>
            </name>
            <name>
              <surname>Sensuse</surname>
              <given-names>Dana Indra</given-names>
            </name>
            <name>
              <surname>Putro</surname>
              <given-names>Prasetyo Adi Wibowo</given-names>
            </name>
            <name>
              <surname>Prasetyo</surname>
              <given-names>Aji</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.14569/IJACSA.2023.0140236</pub-id>
          <article-title>Public response to the legalization of the criminal code bill with twitter data sentiment analysis</article-title>
          <source>Int. J. Adv. Comput. Sci. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-5</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pavani</surname>
              <given-names>B. V.</given-names>
            </name>
            <name>
              <surname>Mahitha</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Prabhakar</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Pati</surname>
              <given-names>P. B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/i2ct61223.2024.10544116</pub-id>
          <article-title>Identifying sentiment in legal case judgments using random forest classifier</article-title>
          <source>2024 IEEE 9th International Conference for Convergence in Technology (I2CT), Pune, India</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>11-20</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zamani</surname>
              <given-names>Fathy El</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.60005/coreid.v1i1.4</pub-id>
          <article-title>Sentiment analysis and twitter social media visualization regarding the omnibus law draft</article-title>
          <source>CoreID J.</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>251</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Izzidien</surname>
              <given-names>Ahmed</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1057/s41599-023-01693-z</pub-id>
          <article-title>Using the interest theory of rights and Hohfeldian taxonomy to address a gap in machine learning methods for legal document analysis</article-title>
          <source>Humanit. Soc. Sci. Commun.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>