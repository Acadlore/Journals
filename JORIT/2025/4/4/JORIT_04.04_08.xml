<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">JORIT</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Journal of Research, Innovation and Technologies</journal-title>
        <abbrev-journal-title abbrev-type="issn">J. Res. Innov. Technol.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">JORIT</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2971-8317</issn>
      <issn publication-format="print"/>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-LR442AGbS4fHxFn4nYMxb6RBpaXIr9Z0</article-id>
      <article-id pub-id-type="doi">10.56578/jorit040408</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Using Artificial Intelligence to Manage Visual AR/VR Scenarios in Media Branding of Cultural Institutions</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9629-9865</contrib-id>
          <name>
            <surname>Kyianytsia</surname>
            <given-names>Ievgeniia</given-names>
          </name>
          <email>Kyianytsia.ievgenia1@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2165-7760</contrib-id>
          <name>
            <surname>Yatsiuk</surname>
            <given-names>Dmytro</given-names>
          </name>
          <email>yatsiukk12@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1962-4409</contrib-id>
          <name>
            <surname>Aldankova</surname>
            <given-names>Halyna</given-names>
          </name>
          <email>kondratskah@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-8672-5469</contrib-id>
          <name>
            <surname>Horobets</surname>
            <given-names>Oleksii</given-names>
          </name>
          <email>horobets.oleksiy3@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-9749-6755</contrib-id>
          <name>
            <surname>Dobrovolskyi</surname>
            <given-names>Viktor</given-names>
          </name>
          <email>victor.dobrovol9@ukr.net</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-8134-021X</contrib-id>
          <name>
            <surname>Slipchenko</surname>
            <given-names>Vladyslav</given-names>
          </name>
          <email>vlad.slipchenkoo1985@ukr.net</email>
        </contrib>
        <aff id="aff_1">Department of Journalism and Advertising, State University of Trade and Economics, 02156 Kyiv, Ukraine</aff>
        <aff id="aff_2">Department of Marketing, State University of Trade and Economics, 02156 Kyiv, Ukraine</aff>
        <aff id="aff_3">Department of Theater Arts Organization named after I. D. Bezgin, Kyiv National I. K. Karpenko-Kary Theatre, Cinema and Television University, 02156 Kyiv, Ukraine</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>26</day>
        <month>12</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>4</issue>
      <fpage>422</fpage>
      <lpage>431</lpage>
      <page-range>422-431</page-range>
      <history>
        <date date-type="received">
          <day>06</day>
          <month>10</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>14</day>
          <month>12</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>In the context of the digital transformation of cultural institutions, there exists an imperative for interactive AR/VR systems that leverage the capabilities of artificial intelligence for the personalized presentation of cultural content and the management of cognitive interaction scenarios. In light of the above, the purpose of this study was to develop and empirically evaluate the efficacy of an interactive AI model (CurioMind) for AR/VR experiences, which tailors information input to align with user requests and behaviors. Within the framework of the study, an experiment was conducted with two groups of participants (<italic>n</italic> = 60), which involved comparing the measures of attraction, memorization and subjective evaluation of the experience. Furthermore, NASA-TLX cognitive load assessments were conducted alongside semi-structured interviews to qualitatively evaluate interface perception and content. Participants engaging with the CurioMind model demonstrated significantly higher levels of information retention (mean score of 9.1 compared to 7.2 in the control group) and longer exposure time. Moreover, their ratings on the parameters of emotional engagement, personalization, and interface attractiveness were markedly higher. The findings substantiate the study’s hypothesis: the integration of an adaptive AI agent within AR/VR experiences augments the efficacy of informal cultural learning and elevates the quality of the user experience. The CurioMind model introduces a novel approach to digital museum storytelling based on behavioral personalization. Future research may encompass the scaling of the system to authentic museum environments, adding multimodal input (gestures, gaze).</p></abstract>
      <kwd-group>
        <kwd>Artificial intelligence</kwd>
        <kwd>AR/VR</kwd>
        <kwd>Cultural heritage</kwd>
        <kwd>Personalization</kwd>
        <kwd>Script management</kwd>
        <kwd>Digital museum</kwd>
        <kwd>Narrative agent</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="6"/>
        <fig-count count="1"/>
        <table-count count="4"/>
        <ref-count count="38"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Modern changes in social communications are strengthening the role of adaptive digital media. They are shaping new approaches to user interaction with the information environment. Communication interactions within the context of digital media necessitate a profound reevaluation of the conceptual underpinnings. This evolution is intrinsically linked to the advancement of artificial intelligence (AI) models that tailor the experience of engaging with the informational landscape. Augmented reality (AR) and virtual reality (VR) unveil new horizons for creating immersive experiences (<xref ref-type="bibr" rid="ref_12">Graeske &amp;amp; Sjöberg, 2021</xref>; <xref ref-type="bibr" rid="ref_34">Syed et al., 2022</xref>). Such technologies not only facilitate the visualization of objects but also actively engage users in interaction with the contextual framework. At the same time, the proliferation of large language models (LLM) and other AI tools engenders the prerequisites for the evolution of personalized communication systems within the cultural space (<xref ref-type="bibr" rid="ref_7">Chu et al., 2025</xref>).</p><p>Despite the availability of technological solutions, many AR/VR applications in the cultural sphere remain linear and insufficiently adaptive. They rarely take into account individual interests and cognitive characteristics of users and do not use the potential of conversational AI models (<xref ref-type="bibr" rid="ref_15">Huq et al., 2024</xref>). Consequently, there exists a pressing need to develop models that amalgamate a flexible presentation of information with adaptation to the behaviors and inquiries of specific visitors.</p><p>The relevance of this study is underscored by the imperative to integrate AI into cultural practices, particularly in the creation of next-generation virtual guides. Such systems possess the capacity to enhance knowledge accessibility and foster effective informal learning. The hypothesis underpinning this study posits that the deployment of an interactive AI model. It combines the functions of a dialog agent, an adaptive scripting module, and a user model, increasing the level of cognitive engagement. It is also expected to improve information retention and the overall quality of interaction compared to traditional methods of presenting museum content. The scientific novelty of this study resides in the elaborating and validating the architecture for an adaptive AR/VR guide. The proposed system concurrently fulfills three essential functions: it responds to user inquiries in real time, personalizes the narrative trajectory based on behavioral models, and modulates the depth and format of content delivery in accordance with the cognitive load level. This paradigm allows perceiving the system not merely as an “information guide”, but as a dynamic interlocutor that supports the cultural cognition process.</p><p>The purpose of this study was to devise and experimentally validate the efficacy of a prototype interactive AI model for AR/VR exhibitions within the field of cultural heritage, which offers personalized dialogue support to users. To achieve the purpose, the following tasks were set:</p><p>1. To formulate an adaptive model architecture that integrates a language model, a script control system, and a user simulation module.</p><p>2. To implement a functional prototype in the form of a historical and cultural pavilion.</p><p>3. To design and conduct an experiment comparing interaction efficacy between the CurioMind model and traditional museum tools.</p><p>4. To analyze the obtained data employing both quantitative and qualitative methodologies, drawing conclusions regarding the effectiveness of the proposed approach.</p>
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p>In contemporary digital environment, cultural institutions are confronted with the imperative to update their strategies for engaging with audiences. The advent of augmented and virtual reality technologies has opened up novel avenues for immersive experiences within cultural content, alongside the integration of artificial intelligence for the personalization and managing of these visual narratives. Nonetheless, there is still no holistic approach that harmonizes the potential of AI with the branding aspirations of cultural institutions.</p><p>Several studies concentrate on the architectural and technological realization of AI in cultural heritage initiatives. Notably, <xref ref-type="bibr" rid="ref_24">Martusciello et al. (2025)</xref> delineate a modular architecture employing generative AI for AR applications with gamification elements. However, despite the technological innovation, the authors insufficiently address critical issues pertaining to branding and user behavior patterns. A comparable scenario is evident in the development of InheritAge, a mobile application that combines AR/VR with gamification (<xref ref-type="bibr" rid="ref_31">Srdanović et al., 2025</xref>). This application showcases a successful adaptation of content to digital realms but is deficient in performance analytics regarding marketing strategies.</p><p>Individual scholarly works examine the enhancement of interactive experiences through tactile feedback, the integration of virtual agents, or advancements in computer vision (<xref ref-type="bibr" rid="ref_10">Dahaghin et al., 2024</xref>; <xref ref-type="bibr" rid="ref_20">Krumpen et al., 2021</xref>; <xref ref-type="bibr" rid="ref_30">Spyrou et al., 2025</xref>). Still, their emphasis predominantly lies on technical dimensions, often leaving aside the communicative elements and the strategic considerations in terms of branding.</p><p>A number of papers delve into the role of generative AI within storytelling contexts. For instance, <xref ref-type="bibr" rid="ref_21">Lau et al. (2025a)</xref> elucidate how AI can tailor virtual narratives by engaging users in the co-creation of cultural content. This paradigm opens new vistas for emotional engagement but fails to materialize into robust models of branding communication. Yet other literary sources, such as <xref ref-type="bibr" rid="ref_22">Lau et al. (2025b)</xref> and <xref ref-type="bibr" rid="ref_11">Doh et al. (2025)</xref>, approach the integration of user experience, AI, and personalization, yet they fail to establish a systemic connection to cultural institutions’ objectives.</p><p>In a series of review publications <xref ref-type="bibr" rid="ref_4">Boboc et al. (2022)</xref> and <xref ref-type="bibr" rid="ref_27">Ribeiro et al. (2024)</xref>, the scholars systematically catalog the utilization of AR/VR and AI within the field of heritage. While contextualizing the discourse, they typically do not proffer specific models for script management or branding strategies. Innovative methodologies such as Teachable Reality (enabling interaction with AR sans coding) or VRCoPilot (automated creation of VR scenes leveraging AI) hold significant promise for developers. However, they necessitate adaptation to the unique characteristics of the cultural landscape. At the same time, practical solutions (such as open-source AR applications, generative AI restoration, as well as audio-AR) exhibit considerable technological promise, yet infrequently integrate into a cohesive media branding framework (<xref ref-type="bibr" rid="ref_8">Cliffe et al., 2019</xref>; <xref ref-type="bibr" rid="ref_13">Green, 2023</xref>; <xref ref-type="bibr" rid="ref_32">Stoean et al., 2024</xref>).</p><p>At the same time, the range of works devoted to museum branding, institutional communications and the study of the audience experience is important for the context of the study. In the modern cultural sphere, museum branding is considered a tool for building trust, increasing recognition and creating sustainable emotional value for visitors (<xref ref-type="bibr" rid="ref_38">Zheng et al., 2021</xref>). Studies emphasize that an effective communication strategy of cultural institutions is based on a combination of visual identity, digital narratives and inclusive formats of interaction with the audience (<xref ref-type="bibr" rid="ref_2">Arsenijević &amp;amp; Arsenijević, 2022</xref>).</p><p>The experience of interaction with a museum is shaped not only by the content of the exhibition, but also by the quality of communication, the level of personalization and emotional involvement (<xref ref-type="bibr" rid="ref_5">Chang &amp;amp; Suh, 2025</xref>). It is these factors that determine whether the user will return to the museum and whether he will form a holistic idea of the institution’s brand. In this context, the integration of adaptive digital technologies—including AI models and AR/VR—is seen as a tool for strengthening the brand through the creation of clear, recognizable and “human-centered” interaction formats (<xref ref-type="bibr" rid="ref_16">Hutson &amp;amp; Hutson, 2024</xref>).</p><p>That being said, the prevailing body of research evidences the active implementation of AI, AR, and VR within the cultural heritage sector. On the other hand, a substantial part thereof overlooks the strategic positioning needs of cultural institutions and fails to provide solutions that systematically intertwine visual scenarios with branding objectives. A pronounced gap persists in the integration of user experience design, generative logic, behavioral analytics, and narrative management into a single architecture that could underpin institutional media branding. Given the identified limitations within the available literature, this work seeks to bridge the existing gaps between technical implementation and the strategic branding imperatives of the cultural domain.</p>
    </sec>
    <sec sec-type="">
      <title>3. Materials and methods</title>
      
        <sec>
          
            <title>3.1. Research procedure</title>
          
          <p>The first stage entailed the direct advancement of CurioMind’s AI model, an interactive dialogue system that intricately weaves personalized presentations of museum content with engaging narrative structures. This model is constructed upon a sophisticated language model (LLM) tailored for educational interactions through a process of instruction tuning. The content utilized for training the model encompassed a diverse array of museum-themed texts, including accessible scientific explanations, hypothetical dialogues with visitors, excursion scenarios, and expert commentary. Furthermore, the model was engineered to detect the user’s prior knowledge level through its questions and responses.</p><p>In the second stage, a network of narrative branches was devised, facilitating a story-driven interaction between the user and the exhibition material. Each branch comprised logically interconnected nodes of knowledge, enabling the user to delve into the subject matter from multiple perspectives. Special emphasis was placed on ensuring that the system could sustain user engagement, pose clarifying questions, and present varying depths of immersion in the topic. The next step involved the creation of an experimental environment. In the final stage of the study, an empirical evaluation of the system was conducted in the form of a comparative experiment. Two groups participated: the experimental group, which engaged with CurioMind, and the control group, which explored the same topic through static text without dialogue support.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Formation of a research sample</title>
          
          <p>For the purpose of conducting experiments, the pavilion “Medieval Kiev” was designed with principal structures: the Golden Gate, Tithing Church, Shopping Square, and Artisan’s Workshop. Each structure is accompanied by four distinct narrative trajectories: historical, socio-cultural, architectural, and quotidian. The information is systematically arranged as a modular knowledge base linked to the user model. The investigation was carried out over a span of ten days within the confines of the laboratory. A total of sixty participants, aged between 18 and 45, were randomly assigned into two equal groups.</p><p>The experimental group (<italic>n</italic> = 30) engaged with the CurioMind model, whereas the control group (<italic>n</italic> = 30) was acquainted with the same content without the CHI component—utilizing traditional navigation methods with a text guide and lacking dialogical support. Participants in the experimental group were provided with comprehensive instructions on utilizing the AR/VR interface and were afforded the opportunity to interact with a virtual guide through either microphone or text box. Upon completion of the virtual route, which did not exceed thirty minutes, each participant undertook a knowledge retention assessment comprising twelve multiple-choice questions.</p><p>Additionally, they evaluated their experiences utilizing the User Experience Questionnaire (UEQ-S) (<xref ref-type="bibr" rid="ref_29">Schrepp &amp;amp; Thomaschewski, 2023</xref>) and completed the NASA-TLX questionnaire (<xref ref-type="bibr" rid="ref_28">Said et al., 2020</xref>) to measure cognitive load. Testing took place in an experimental pavilion, with all study participants providing written informed consent following thorough familiarization with the study’s purpose, objectives, and participation conditions. The right to withdraw voluntarily from participation at any stage without incurring negative consequences was ensured.</p><p>Rationale for Control Condition. Static text was chosen as the control because it reflects a typical, traditional way of presenting museum content. This allows us to compare CurioMind’s work with real-world museum communications practice and ensures proper ecological validity of the experiment.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Methods and models of research</title>
          
          <p>As part of the study, an adaptive model of a virtual guide, provisionally named CurioMind, was designed and implemented. In essence, CurioMind represents a cognitive-oriented augmented and virtual reality system founded upon the architectural principle of multi-tiered adaptive interaction. It combines a comprehensive Large Language Model (LLM) core, a dynamic Narrative Graph Engine, User Modeling Engine, and a Content Adapter to convey information within an AR/VR environment. This modular architecture facilitates reactive, personalized, and semantically relevant communication that dynamically adjusts to the user’s cognitive style in real time. The architecture of the model is depicted in <xref ref-type="fig" rid="fig_1">Figure 1</xref> as follows.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>The flowchart of the model under study</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/11/img_6scbaLPLxeFn32fP.png"/>
            </fig>
          
          <p>The model comprises the following components:</p><p>1. LLM Core (<xref ref-type="bibr" rid="ref_35">Topsakal &amp;amp; Akinci, 2023</xref>). A central computing module responsible for processing language queries, generating a response and dynamically shaping the representation of knowledge. The system implements its own advanced transformer model based on GPT-4 (context length: 8k tokens, 6.1b parameters). It is further structured using reinforcement learning from human feedback (RLHF) in a specialized domain.</p><p>Model parameters:</p><p>- Architecture: Decoder-only Transformer</p><p>- Number of layers: 48</p><p>- Attention heads: 96</p><p>- Precision: mixed (FP16/INT8 for inference)</p><p>Integrated multilingualism (English, Ukrainian). <xref ref-type="table" rid="table_1">Table 1</xref> shows the performance parameters of this model.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Performance (on Nvidia A100 inference cluster)</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Parameter</p></td><td colspan="1" rowspan="1"><p>Value</p></td></tr><tr><td colspan="1" rowspan="1"><p>Latency (average response)</p></td><td colspan="1" rowspan="1"><p>210 ms</p></td></tr><tr><td colspan="1" rowspan="1"><p>Throughput</p></td><td colspan="1" rowspan="1"><p>45 req/sec</p></td></tr><tr><td colspan="1" rowspan="1"><p>Token generation speed</p></td><td colspan="1" rowspan="1"><p>~38 tokens/sec (per GPU)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Percentage of relevant responses</p></td><td colspan="1" rowspan="1"><p>94.1% (at top-<italic>k </italic>= 40)</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>2. User Modeling Engine (<xref ref-type="bibr" rid="ref_9">Conrardy et al., 2024</xref>). This module is responsible for creating a personalized cognitive model of the user. Based on the input data (dialogue history, attention intensity, scenario selection), the system forms a dynamic user profile:</p><p>- Current level of knowledge (domain-specific ontology embedding)</p><p>- Learning style benefits (e.g. visual, narrative)</p><p>- Speed of response/cognitive load</p><p>- Motivational profile.</p><p>Saving and updating the profile takes place in an ontologically oriented knowledge base (OWL DL) with support for semantic query via SPARQL.</p><p>3. Narrative Graph Engine (<xref ref-type="bibr" rid="ref_37">Yan &amp;amp; Tang, 2023</xref>). This engine generates an adaptive narrative framework that aligns with the subject of exposure and the user’s proficiency level. Scenarios are presented through an oriented acyclic knowledge graph (DAG), wherein nodes represent significant entities (artefacts, historical events, elucidations) and edges denote semantic transitions.</p><p>To tailor the above graph to the individual user, a dual-component methodology encompassing graph heuristics and contextual weighting is employed. Graph heuristic traversal constitutes a navigation procedure that identifies the most pertinent pathway to the subsequent node based on its interconnections (input/output edges), semantic relevance weight, and script architecture. The algorithm considers: the frequency of engagement with specific nodes, the current narrative depth (depth-level balancing), as well as the presence of the key objects/topics pertinent to preceding dialogues. In contextual weighting, the significance of each node is further refined according to the personalized user profile, which is constructed based on factors such as learning modality (visual, auditory, textual) and cognitive capacity, ascertained through brief assessments or passive behavioral analysis. This process also accounts for the user’s prior requests and responses, particularly their emotional reactions, depth of interest, and the duration spent assimilating information. Weighting is executed through a modification of the heuristic function.</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="mosd4q9wgs">
                <mml:mi>f</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mi>h</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
              </mml:math>
            </disp-formula>
          
          <p>where, <italic>f</italic>(<italic>n</italic>) is the “cost” of the transition to the node, taking into account the content load; <italic>h(n)</italic>—predictive evaluation of the node’s utility for the current user.</p><p>Support for multiple narrative branches and real-time script reconfiguration. CurioMind employs an adaptive narrative framework that transcends conventional linear delivery paradigms. The entire system is based on an oriented graph of scenarios, which facilitates the following: the graph offers numerous alternative branches that diverge from a singular node. Each branch may encapsulate distinct subject areas (technological, cultural, historical), presentation modalities (scientific, metaphorical, interactive), or levels of complexity (basic, advanced). The user is not confined to a predetermined script and can fluidly navigate between branches at will through voice commands, gestures, or selections within AR/VR environments. Following each interaction with the user, the system enumerates and updates the narrative graph. The above is accomplished by dynamically disconnecting or integrating nodes and edges, thereby enabling the construction of a new narrative pathway without necessitating a system restart.</p><p>4. Content Adapter (Knowledge Base). The content adapter is an intermediary between logical levels and the visual interface. It performs the function of semantic normalization, transforming abstract objects into visual or linguistic inferences. In its base is a semiotically labelled content base that allows mapping the objects to AR/VR models, text fragments, audio tracks, etc. Formats: glTF 2.0, FBX, SVG, WebM, WebVTT. Unified API on GraphQL.</p><p>5. AR/VR Interface. This component is implemented as a WebXR interface compatible with Oculus Quest, HoloLens 2 and desktop browsers (Chrome + WebXR polyfill). It provides:</p><p>- virtual space rendering (based on Babylon.js and Unity WebGL);</p><p>- built-in interactivity (selection of objects, movement in space);</p><p>- contextual appearance of clues, visual effects.</p><p>The AR/VR environment is connected to AI through a WebSocket bus that transmits requests in JSON-RPC format with a latency of less than 50 ms.</p><p>AI adaptation features:</p><p>- Few-shot prompting is used with contextual memory.</p><p>- The model holds the contextual content up to 8 previous dialogue steps.</p><p>- Multimodal embedding alignment is used.</p><p>- Language responses are synchronized with visual AR/VR content.</p><p>- Attention-weighted personalization is carried out.</p><p>- The output of information is modulated according to the user’s attention to previous topics.</p><p>Technical environment:</p><p>- Server OS: Ubuntu 22.04 LTS.</p><p>- The AI model runs in a Docker container based on Nvidia Triton Inference Server.</p><p>- The requests are handled by FastAPI with caching via Redis.</p><p>- Narrative columns are stored in Neo4j.</p><p>- User profiles are in PostgreSQL.</p><p>- The interaction logs are in MongoDB.</p><p>- CI/CD implemented via GitHub Actions.</p><p>- System monitoring is realized through Prometheus and Grafana.</p><p>Statistical Analysis. Descriptive statistics (mean, median, standard deviation) and comparative tests were used to process the data. An independent <italic>t</italic>-test was used to assess the differences between the experimental and control groups, since the data met the requirements of normal distribution. Additionally, the effect size Cohen’s <italic>d</italic> was calculated as an indicator of the practical significance of the differences.</p><p>Effect values were interpreted according to the established scale:</p><p>0.2—small effect; 0.5—medium; 0.8 and above—large effect.</p><p>All statistical procedures were performed in SPSS 29.0. The level of statistical significance was taken at <italic>p</italic> &lt; 0.05.</p>
        </sec>
      
    </sec>
    <sec sec-type="results">
      <title>4. Results</title>
      <p>In the course of the experimental study, the impact of CurioMind interactive AI model on users’ cognitive, behavioral, and emotional performance within the established study environment was examined. The analysis of the findings is based on three main criteria: knowledge retention, engagement metrics, and subjective evaluations of experience. Knowledge Retention Test was administered, aimed at assessing the efficacy with which participants retained essential information from the exposition. Participants in both cohorts undertook the same test (<xref ref-type="table" rid="table_2">Table 2</xref>) after their familiarization with the material. </p><p>Statistical analysis of between-group differences was performed using an independent <italic>t</italic>-test with effect size calculation (Cohen’s <italic>d</italic>) to assess the practical significance of the results.</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>Comparison of knowledge retention test</title>
          </caption>
          <table><tbody><tr><td><p>Group</p></td><td><p>Average Score (Max. 12)</p></td><td><p>Standard Deviation</p></td><td><p>Median</p></td></tr><tr><td><p>Experimental (AI)</p></td><td><p>9.1</p></td><td><p>1.6</p></td><td><p>9</p></td></tr><tr><td><p>Control (without AI)</p></td><td><p>7.2</p></td><td><p>1.9</p></td><td><p>7</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>The findings presented in <xref ref-type="table" rid="table_2">Table 2</xref> reveal a markedly superior success rate for the experimental group across all statistical metrics. Specifically, the mean score of participants who engaged with the AI agent was 1.9 points higher than that of the control group. The median further corroborates this trend, illustrating that the majority of participants in the AI group achieved scores approaching the upper limits. These outcomes underscore the enhanced efficacy of personalized, dialogical information delivery that adapts to the unique level of each user. The second evaluative criterion, user engagement (<xref ref-type="table" rid="table_3">Table 3</xref>), reflecting activity, duration of interaction, and interest in the content. These parameters were measured using behavioral analytics within the study environment.</p>
      
        <table-wrap id="table_3">
          <label>Table 3</label>
          <caption>
            <title>Engagement metrics</title>
          </caption>
          <table><tbody><tr><td><p>Indicator</p></td><td><p>Experimental Group</p></td><td><p>Control Group</p></td></tr><tr><td><p>Average number of questions</p></td><td><p>7.8</p></td><td><p>–</p></td></tr><tr><td><p>Average stay time (min.)</p></td><td><p>23.5</p></td><td><p>17.2</p></td></tr><tr><td><p>Share of users who have completed ≥ 3 narrative branches</p></td><td><p>83%</p></td><td><p>42%</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>As can be seen from <xref ref-type="table" rid="table_3">Table 3</xref>, participants in the experimental group showed significantly higher levels of involvement. Not only did they spend 36% more time in the environment, but they also explored different narrative branches significantly more often, suggesting deeper engagement with the content. What is more, the number of questions that users have asked the system indicates a high level of interest and an active cognitive position. The third group of metrics covers the emotional-semantic experiences of users assessed using a short version of the Standardized User Experience Questionnaire (UEQ-S). This tool allows assessing the user perceptions of the product based on critical parameters (<xref ref-type="table" rid="table_4">Table 4</xref>).</p>
      
        <table-wrap id="table_4">
          <label>Table 4</label>
          <caption>
            <title>Subjective evaluations of experience (user experience questionnaire—UEQ-S)</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Parameter</p></td><td colspan="1" rowspan="1"><p>Experimental Group</p></td><td colspan="1" rowspan="1"><p>Control Group</p></td></tr><tr><td colspan="1" rowspan="1"><p>Attractiveness</p></td><td colspan="1" rowspan="1"><p>5.9/7</p></td><td colspan="1" rowspan="1"><p>4.8/7</p></td></tr><tr><td colspan="1" rowspan="1"><p>Clarity of information</p></td><td colspan="1" rowspan="1"><p>5.7/7</p></td><td colspan="1" rowspan="1"><p>5.3/7</p></td></tr><tr><td colspan="1" rowspan="1"><p>Interactivity</p></td><td colspan="1" rowspan="1"><p>6.2/7</p></td><td colspan="1" rowspan="1"><p>3.2/7</p></td></tr><tr><td colspan="1" rowspan="1"><p>Personalization</p></td><td colspan="1" rowspan="1"><p>5.8/7</p></td><td colspan="1" rowspan="1"><p>3.9/7</p></td></tr><tr><td colspan="1" rowspan="1"><p>Emotional involvement</p></td><td colspan="1" rowspan="1"><p>6.1/7</p></td><td colspan="1" rowspan="1"><p>4.5/7</p></td></tr><tr><td colspan="1" rowspan="1"><p>Intention to return again</p></td><td colspan="1" rowspan="1"><p>6.0/7</p></td><td colspan="1" rowspan="1"><p>4.6/7</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>Expectedly, all parameters documented in <xref ref-type="table" rid="table_4">Table 4</xref> exhibit elevated values in the group that engaged with the AI model, indicating the beneficial impact of intelligent accompaniment on the quality of the user experience. Notably significant is the disparity in the scores for interactivity (<italic>a</italic> 3-point difference) and personalization (<italic>a</italic> 1.9-point difference), which corroborates the relevance of the adaptive dialogue approach.</p>
    </sec>
    <sec sec-type="discussion">
      <title>5. Discussion</title>
      <p>Compared to previous studies, CurioMind offers a number of advantages. For example, unlike <xref ref-type="bibr" rid="ref_24">Martusciello et al. (2025)</xref>, the model includes not only an architectural description, but also a full empirical validation. InheritAge emphasizes visualization and gamification but lacks adaptive dialogue (<xref ref-type="bibr" rid="ref_31">Srdanović et al., 2025</xref>). This study precisely incorporates that essential personification.</p><p><xref ref-type="bibr" rid="ref_20">Krumpen et al. (2021)</xref> direct the focus towards tactile feedback, whereas the CurioMind model adeptly processes cognitive-communicative feedback. <xref ref-type="bibr" rid="ref_30">Spyrou et al. (2025)</xref> describe virtual agents, yet they lack behavioral adaptation; CurioMind, in contrast, implements such adaptation in real time. Furthermore, in <xref ref-type="bibr" rid="ref_4">Boboc et al. (2022)</xref>’s research, there is no emphasis on AI, a dimension that this study precisely contributes.</p><p>A study of <xref ref-type="bibr" rid="ref_6">Chen et al. (2025)</xref> elucidating the potential for emotional involvement showcases quantitative indicators of this attraction. In study of <xref ref-type="bibr" rid="ref_27">Ribeiro et al. (2024)</xref>, an overview approach is proposed, while CurioMind concretizes the suggested directions through empirical verification. The paper of <xref ref-type="bibr" rid="ref_21">Lau et al. (2025a)</xref> concentrates on storytelling, adapting it specifically to the framework of a cultural institution.</p><p>Introducing the groundbreaking application of eye-tracking in <xref ref-type="bibr" rid="ref_22">Lau et al. (2025b)</xref>, CurioMind facilitates personalization through interactive scenarios devoid of specialized equipment. The study of <xref ref-type="bibr" rid="ref_8">Cliffe et al. (2019)</xref> delves into the audio component but lacks AI scenario modulation; the solution presented in this paper offers precisely such control. The work of <xref ref-type="bibr" rid="ref_17">Jaramillo &amp;amp; Sipiran (2024)</xref> examines high-quality graphics but without interactivity; the proposed study makes a focus on interactions. While work of <xref ref-type="bibr" rid="ref_13">Green (2023)</xref> holds merit in practical solutions, it lacks adaptive content, a gap that CurioMind precisely fills.</p><p>In <xref ref-type="bibr" rid="ref_10">Dahaghin et al. (2024)</xref>’s study, an effective segmentation of objects is proposed, and the suggested solution extends this towards semantic accompaniment. The study of <xref ref-type="bibr" rid="ref_1">Altaweel &amp;amp; Khelifi (2024)</xref> is valuable for its reconstructions, yet it lacks scenario control; CurioMind introduces this control via a narrative graph. The findings of study <xref ref-type="bibr" rid="ref_14">He et al. (2025)</xref> are significant for collective storytelling, though they lack a branding focus. In this work, storytelling is tailored to institutional identity. An extensive review is provided by <xref ref-type="bibr" rid="ref_18">Kiourexidou &amp;amp; Stamou (2025)</xref>, which CurioMind continues, elaborating on the proposed concepts. The development in work of <xref ref-type="bibr" rid="ref_19">Kontogiorgakis et al. (2024)</xref> is oriented towards tourism; however, it lacks script control and personalization; these elements are adeptly implemented in CurioMind.</p><p>Scenario modelling is not considered in <xref ref-type="bibr" rid="ref_23">Marto et al. (2022)</xref>’s study; however, CurioMind incorporates this vital feature. The study of <xref ref-type="bibr" rid="ref_3">Behravan et al. (2025)</xref> dedicated to real-time 3D generation, merging this with behavioral logic. The study of <xref ref-type="bibr" rid="ref_11">Doh et al. (2025)</xref> harbors potential for storytelling, which is further developed with adaptive content management in mind. Developments in <xref ref-type="bibr" rid="ref_36">Wang (2025)</xref>’s study aim at inclusivity, yet not content; CurioMind adeptly combines inclusivity with intelligent adaptation.</p><p>Created in study of <xref ref-type="bibr" rid="ref_25">Monteiro et al. (2023)</xref>, Teachable Reality empowers users to train the system. In contrast, CurioMind autonomously adapts itself, negating the need for manual training. Collaboration with autonomous agents is presented in <xref ref-type="bibr" rid="ref_26">Paradise et al. (2023)</xref>’s work. However, CurioMind emphasizes human-centered interface interaction. The work of <xref ref-type="bibr" rid="ref_33">Suzuki et al. (2023)</xref> presents generalizing material, lacking a practical case; CurioMind serves as a quintessential example of the application of these technologies within a specific environment.</p><p>In essence, the CurioMind study illustrates that the confluence of AI, scenario management, and personalization technologies enables a transition from passive to dialogical interaction models. This engenders a novel paradigm for the digital museum—not merely as a display, but as an active interlocutor and cognitive partner. The results obtained during the experiment substantiate the articulated hypothesis. CurioMind’s adaptive AI model amalgamates the functionalities of a dialogue agent, a narrative constructor, and a personalized user profile. The implementation of this model significantly enhances cognitive engagement, information retention, and overall user satisfaction compared to conventional forms of museum exposition.</p><p>Notably, the experimental group exhibited superior performance on knowledge retention assessments and spent an increased duration in the AR/VR environment. They were also more proactive in dialogue with the system and rendered more favorable evaluations of their experience. It is crucial to note that the efficacy of the model is not a product of a technical breakthrough, but rather the judicious integration of existing technologies, in particular the large language model, the knowledge base, and the mechanisms of scenario personalization. The practical implications lie in the potential to scale the proposed approach for diverse types of cultural institutions – from state museums to virtual galleries. Furthermore, it can be seamlessly integrated into mobile applications for informal education or tourism.</p>
      
        <sec>
          
            <title>5.1. Limitations of the study</title>
          
          <p>Despite the promising results, the study is not without its limitations that warrant consideration: </p><p>- The experiment was conducted on a limited sample of 60 participants under controlled laboratory conditions. Although the sample was stratified, it does not fully reflect the broader population or the visitors to cultural institutions.</p><p>- The testing of the model took place in an experimental pavilion, rather than in a real museum setting. Real-world scenarios may present additional distractions, variable navigation challenges, as well as unforeseen technical difficulties.</p><p>- The CurioMind model was constrained by the limited scope of its knowledge base and relied solely on in-session context preservation. While the incorporation of cloud models or multimodal input/output could enhance its functionality, such considerations were beyond the scope of this study.</p><p>- Systems employing language models can occasionally produce incorrect, imprecise, or overly simplistic responses. This model implements restrictions regarding sensitive content; however, a comprehensive solution to this issue necessitates further development.</p>
        </sec>
      
      
        <sec>
          
            <title>5.2. Recommendations</title>
          
          <p>Based on the results obtained, a series of overarching recommendations have been articulated that can be implemented across cultural, educational, research, and technological domains. It is advisable to deploy interactive AR/VR experiences that combine adaptive user interaction scenarios with personalized information presentation tailored to the visitor’s cognitive behaviors. The utilization of dialogue modules featuring artificial intelligence should be contemplated not merely as navigational tools, but also as instruments of cognitive support, capable of elucidating complex phenomena, providing comparative insights, and fostering reflective engagement.</p><p>The development of such systems necessitates the establishment of a flexible software architecture that facilitates a synthesis of narrative structures alongside multi-level adaptation of informational content. Equally significant is the incorporation of multimodal interaction channels, encompassing text, voice, and tactile elements. Particular emphasis should be placed on the implementation of contextual memory mechanisms within user sessions, as well as the system’s responsiveness to behavioral and emotional cues that reflect the interaction style.</p>
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>6. Conclusions</title>
      <p>In the context of the digital transformation of cultural institutions, the advancement of innovative tools for engaging with cultural heritage holds particular significance. Given the growing demand for personalized, flexible, and interactive experiences, the development of intelligent models to guide users through virtual and augmented expositions emerges as an urgent scientific and practical endeavor. This research is aimed at modeling and evaluating an intelligent system for dialogue-based interaction with users within a cultural environment, specifically within the framework of an experimental historical pavilion.</p><p>Within the scope of the study, a conceptual framework was proposed, realized in the implementation of a functional prototype known as the CurioMind model. This model combines the capacities of artificial intelligence, conversational agents, semantic memory, and multimodal interaction to enhance visitor experiences in the AR/VR domain. The model emphasizes the adaptive construction of narrative scenarios and context-sensitive accompaniment, tailored to reflect the cognitive requirements and the user’s behavioral traits. By leveraging LLM modules, localized knowledge bases, and script management, the system facilitates cognitively relevant, dynamic, and natural interactions with cultural content.</p><p>During the experimental stage, a historical AR/VR test environment, integrated with an intelligent agent, was established, and a methodology for the empirical assessment of the model’s performance was developed. The analysis of the findings indicated that users exhibited increased cognitive engagement and demonstrated an improved retention of fundamental information. Furthermore, they showed greater interest in the contextual aspects of the exhibition compared to scenarios where traditional means of accompaniment were employed. There was also a favorable reception of dialogue interaction as an element of personal engagement in the exhibition experience.</p><p>The study substantiated that the confluence of artificial intelligence, script design, and adaptive interface represents a promising avenue for the evolution of intellectual cultural and educational systems. The model possesses the potential for scalability across various sectors, including education, tourism, and the virtual reconstruction of cultural heritage. Notably, the integration of the system into offline environments via mobile devices, along with the enhancement of the agent’s semantic memory, taking into account the long user experience, emerge as a particularly promising prospect.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>Conceptualization, I.K., O.H., V.D., and D.Y.; methodology, I.K., V.D., and D.Y.; software, H.A., V.S., and V.D.; validation, O.H., D.Y., and H.A.; formal analysis, O.H.; investigation, D.Y.; resources, I.K., V.S., and V.D.; data curation, I.K.; writing—original draft preparation, H.A.; writing—review and editing, I.K., V.S., and V.D.; visualization, H.A.; supervision, D.Y., V.S., and O.H.; project administration, H.A. and O.H. All authors have read and agreed to the published version of the manuscript.</p>
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>301-315</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Altaweel</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Khelifi</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.5334/jcaa.146</pub-id>
          <article-title>Using generative AI for reconstructing cultural artifacts: Examples using roman coins</article-title>
          <source>J. Comput. Appl. Archaeol.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>32</volume>
          <page-range>241-255</page-range>
          <issue>56</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Arsenijević</surname>
              <given-names>J. M.</given-names>
            </name>
            <name>
              <surname>Arsenijević</surname>
              <given-names>O. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.5937/bastina32-36183</pub-id>
          <article-title>Changing the cultural paradigm in the digital age</article-title>
          <source>Bastina</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Behravan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Haghani</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Gračanin</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Transcending dimensions using generative AI: Real-time 3D model generation in augmented reality</article-title>
          <source>International Conference on Human-Computer Interaction</source>
          <publisher-name>Springer Nature Switzerland</publisher-name>
          <year>2025</year>
          <page-range>13-32</page-range>
          <pub-id pub-id-type="doi">10.1007/978-3-031-93700-2_2</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>9859</page-range>
          <issue>19</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Boboc</surname>
              <given-names>R. G.</given-names>
            </name>
            <name>
              <surname>Băutu</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Gîrbacia</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Popovici</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Popovici</surname>
              <given-names>D. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app12199859</pub-id>
          <article-title>Augmented reality in cultural heritage: An overview of the last decade of applications</article-title>
          <source>Applied Sciences</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>2914</page-range>
          <issue>9</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Suh</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s25092914</pub-id>
          <article-title>The impact of digital storytelling on presence, immersion, enjoyment, and continued usage intention in VR-based museum exhibitions</article-title>
          <source>Sensors.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>89</page-range>
          <issue>3</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Jesus</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Vilarigues</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/jimaging11030089</pub-id>
          <article-title>Synergy of art, science, and technology: A case study of augmented reality and artificial intelligence in enhancing cultural heritage engagement</article-title>
          <source>Journal of Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <page-range>arXiv:2503.11733</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Ye</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhong</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>J. et al.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2503.11733</pub-id>
          <article-title>LLM agents for education: Advances and applications</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>176-182</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cliffe</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Mansell</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Cormac</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Greenhalgh</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Hazzard</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3356590.3356617</pub-id>
          <article-title>The audible artifact: Promoting cultural exploration and engagement with audio augmented reality</article-title>
          <source>, http://dx.doi.org/10.1145/3356590.3356617</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <page-range>arXiv:2412.15871</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Conrardy</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Capozucca</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Cabot</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2412.15871</pub-id>
          <article-title>User modeling in model-driven engineering: A systematic literature review</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <page-range>arXiv:2409.19039</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Dahaghin</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Castillo</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Riahidehkordi</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Toso</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Del Bue</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2409.19039</pub-id>
          <article-title>Gaussian heritage: 3D digitization of cultural heritage with integrated object segmentation</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <page-range>arXiv:2505.15973</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Doh</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Jain</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Ramani</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2505.15973</pub-id>
          <article-title>An exploratory study on multi-modal generative AI in AR storytelling</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>76-83</page-range>
          <issue>8</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Graeske</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Sjöberg</surname>
              <given-names>S. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.5539/ies.v14n8p76</pub-id>
          <article-title>VR-technology in teaching: Opportunities and challenges</article-title>
          <source>International Education Studies</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <page-range>arXiv:2310.13700</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Green</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2310.13700</pub-id>
          <article-title>Augmenting heritage: An open-source multiplatform AR application</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>1-30</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>He</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Su</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Lc</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3711006</pub-id>
          <article-title>'I recall the past': Exploring how people collaborate with generative AI to create cultural heritage narratives</article-title>
          <source>, https://doi.org/10.1145/3711006</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>19</volume>
          <page-range>1059-1078</page-range>
          <issue>3</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Huq</surname>
              <given-names>S. M.</given-names>
            </name>
            <name>
              <surname>Maskeliūnas</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Damaševičius</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/17483107.2022.2146768</pub-id>
          <article-title>Dialogue agents for artificial intelligence-based conversational systems for cognitively disabled: A systematic review</article-title>
          <source>Disability and Rehabilitation: Assistive Technology</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Hutson</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hutson</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Immersive technologies</article-title>
          <source>Inclusive Smart Museums: Engaging Neurodiverse Audiences and Enhancing Cultural Heritage</source>
          <publisher-name>Springer Nature Switzerland</publisher-name>
          <year>2024</year>
          <page-range>153-228</page-range>
          <pub-id pub-id-type="doi">10.1007/978-3-031-43615-4_5</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>104-117</page-range>
          <year>2024</year>
          <publisher-name>Springer, Cham</publisher-name>
          <person-group person-group-type="author">
            <name>
              <surname>Jaramillo</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Sipiran</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-031-91572-7_7</pub-id>
          <article-title>Cultural heritage 3D reconstruction with diffusion networks</article-title>
          <source>, https://doi.org/10.1007/978-3-031-91572-7_7</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>1884</page-range>
          <issue>9</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kiourexidou</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Stamou</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/electronics14091884</pub-id>
          <article-title>Interactive heritage: The role of artificial intelligence in digital museums</article-title>
          <source>Electronics</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>73</page-range>
          <issue>6</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kontogiorgakis</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Zidianakis</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Kontaki</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Partarakis</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Manoli</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Ntoa</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Stephanidis</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/technologies12060073</pub-id>
          <article-title>Gamified VR storytelling for cultural tourism using 3D reconstructions, virtual humans, and 360° videos</article-title>
          <source>Technologies</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>1-17</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Krumpen</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Klein</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Weinmann</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3470470</pub-id>
          <article-title>Toward tangible cultural heritage experiences—Enriching VR-based object inspection with haptic feedback</article-title>
          <source>Journal on Computing and Cultural Heritage</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>305-316</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lau</surname>
              <given-names>K. H. C.</given-names>
            </name>
            <name>
              <surname>Sen</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Stark</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Bozkir</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Kasneci</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3716553.3750760</pub-id>
          <article-title>Adaptive Gen-AI guidance in virtual reality: A multimodal exploration of engagement in Neapolitan pizza-making</article-title>
          <source>, https://doi.org/10.1145/3716553.3750760</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>312-332</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lau</surname>
              <given-names>K. H. C.</given-names>
            </name>
            <name>
              <surname>Yun</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Saruba</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Bozkir</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Kasneci</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3745900.3746103</pub-id>
          <article-title>Wrapped in anansi's web: Unweaving the impacts of generative-AI personalization and VR immersion in oral storytelling</article-title>
          <source>, https://doi.org/10.1145/3745900.3746103</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>102</volume>
          <page-range>426-440</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Marto</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Gonçalves</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Melo</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Bessa</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.cag.2021.10.001</pub-id>
          <article-title>A survey of multisensory VR and AR applications for cultural heritage</article-title>
          <source>Computers &amp;amp; Graphics</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <page-range>arXiv:2506.04090</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Martusciello</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Muccini</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Bucchiarone</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2506.04090</pub-id>
          <article-title>A reference architecture for gamified cultural heritage applications leveraging generative AI and augmented reality</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>1-15</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Monteiro</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Vatsal</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Chulpongsatorn</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Parnami</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Suzuki</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3544548.3581449</pub-id>
          <article-title>Teachable reality: Prototyping tangible augmented reality with everyday objects by leveraging interactive machine teaching</article-title>
          <source>, http://dx.doi.org/10.1145/3544548.3581449</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>4</volume>
          <page-range>1210211</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Paradise</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Surve</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Menezes</surname>
              <given-names>Jovan C.</given-names>
            </name>
            <name>
              <surname>Gupta</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Bisht</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Jang</surname>
              <given-names>K. R.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Qiu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>J. et al.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/frvir.2023.1210211</pub-id>
          <article-title>RealTHASC—A cyber-physical XR testbed for AI-supported real-time human autonomous systems collaborations</article-title>
          <source>Frontiers in Virtual Reality</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>1-10</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ribeiro</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Santos</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lobo</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Araújo</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Magalhães</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Adão</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3665318.3677172</pub-id>
          <article-title>VR, AR, gamification and AI towards the next generation of systems supporting cultural heritage: Addressing challenges of a museum context</article-title>
          <source>, http://dx.doi.org/10.1145/3665318.3677172</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>e19472</page-range>
          <issue>9</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Said</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Gozdzik</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Roche</surname>
              <given-names>T. R.</given-names>
            </name>
            <name>
              <surname>Braun</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Rössler</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Kaserer</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Spahn</surname>
              <given-names>D. R.</given-names>
            </name>
            <name>
              <surname>Nöthiger</surname>
              <given-names>C. B.</given-names>
            </name>
            <name>
              <surname>Tscholl</surname>
              <given-names>D. W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.2196/19472</pub-id>
          <article-title>Validation of the raw national aeronautics and space administration task load index (NASA-TLX) questionnaire to assess perceived workload in patient monitoring tasks: Pooled analysis study using mixed models</article-title>
          <source>Journal of Medical Internet Research</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <volume>18</volume>
          <page-range>86-104</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Schrepp</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Thomaschewski</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>A comparison of SUS, UMUX-LITE, and UEQ-S</article-title>
          <source>J. User Exp.</source>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>36</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Spyrou</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Hurst</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Krampe</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/fi17010036</pub-id>
          <article-title>A reference architecture for virtual human integration in the metaverse: Enhancing the galleries, libraries, archives, and museums (GLAM) sector with AI-driven experiences</article-title>
          <source>Future Internet</source>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>257</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Srdanović</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Skala</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Maričević</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app15010257</pub-id>
          <article-title>Inheritage—A gamified mobile application with AR and VR for cultural heritage preservation in the metaverse</article-title>
          <source>Applied Sciences</source>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <volume>69</volume>
          <page-range>18-26</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Stoean</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Bacanin</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Stoean</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Ionescu</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.culher.2024.07.008</pub-id>
          <article-title>Bridging the past and present: AI-driven 3D restoration of degraded artefacts for museum digital display</article-title>
          <source>Journal of Cultural Heritage</source>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>1-3</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Suzuki</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Gonzalez-Franco</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Sra</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Lindlbauer</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3586182.3617432</pub-id>
          <article-title>XR and AI: AI-enabled virtual, augmented, and mixed reality</article-title>
          <source>, http://dx.doi.org/10.1145/3586182.3617432</source>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="journal">
          <volume>23</volume>
          <page-range>146</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Syed</surname>
              <given-names>T. A.</given-names>
            </name>
            <name>
              <surname>Siddiqui</surname>
              <given-names>M. S.</given-names>
            </name>
            <name>
              <surname>Abdullah</surname>
              <given-names>H. B.</given-names>
            </name>
            <name>
              <surname>Jan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Namoun</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Alzahrani</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Nadeem</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Alkhodre</surname>
              <given-names>Ahmad B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s23010146</pub-id>
          <article-title>In-depth review of augmented reality: Tracking technologies, development tools, AR displays, collaborative AR, and security concerns</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_35">
        <label>35.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>1050-1056</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Topsakal</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Akinci</surname>
              <given-names>T. C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.59287/icaens.1127</pub-id>
          <article-title>Creating large language model applications utilizing a long chain: A primer on developing LLM apps fast</article-title>
          <source>International Conference on Applied Engineering and Natural Sciences</source>
        </element-citation>
      </ref>
      <ref id="ref_36">
        <label>36.</label>
        <element-citation publication-type="journal">
          <page-range>arXiv:2506.15189</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2506.15189</pub-id>
          <article-title>Accessible gesture-driven augmented reality interaction system</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_37">
        <label>37.</label>
        <element-citation publication-type="journal">
          <volume>32</volume>
          <page-range>206-221</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yan</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11518-023-5561-0</pub-id>
          <article-title>Narrative graph: Telling evolving stories based on event-centric temporal knowledge graph</article-title>
          <source>Journal of Systems Science and Systems Engineering</source>
        </element-citation>
      </ref>
      <ref id="ref_38">
        <label>38.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>29-40</page-range>
          <issue>2</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zheng</surname>
              <given-names>J. J.</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>H. W.</given-names>
            </name>
            <name>
              <surname>Chau</surname>
              <given-names>K. Y.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>E. L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18089/tms.2021.170203</pub-id>
          <article-title>Factors influencing conative loyalty in anthropology museum tourism</article-title>
          <source>Tour. Manag. Stud.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>