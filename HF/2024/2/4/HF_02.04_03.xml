<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">HF</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Healthcraft Frontiers</journal-title>
        <abbrev-journal-title abbrev-type="issn">Healthcraft. Front.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">HF</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">3005-799X</issn>
      <issn publication-format="print">3005-7981</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-WU2LpripUVjcMu4GjMFI_gaygJsz6uK0</article-id>
      <article-id pub-id-type="doi">10.56578/hf020403</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Deep Learning-Based MRI Classification for Early Diagnosis of Alzheimer’s Disease</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9349-5695</contrib-id>
          <name>
            <surname>Edalatpanah</surname>
            <given-names>Seyyed Ahmad</given-names>
          </name>
          <email>s.a.edalatpanah@aihe.ac.ir</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-1011-2014</contrib-id>
          <name>
            <surname>Saeedi</surname>
            <given-names>Shamila</given-names>
          </name>
          <email>shamila.saeedi@aihe.ac.ir</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-0943-1993</contrib-id>
          <name>
            <surname>Ghasemabadi</surname>
            <given-names>Nadia</given-names>
          </name>
          <email>ghasemabadinadia@gmail.com</email>
        </contrib>
        <aff id="aff_1">Department of Applied Mathematics, Ayandegan Institute of Higher Education, 4681853617 Tonekabon, Iran</aff>
        <aff id="aff_2">Department of Computer Engineering, Ayandegan Institute of Higher Education, 4681853617 Tonkabon, Iran</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>30</day>
        <month>12</month>
        <year>2024</year>
      </pub-date>
      <volume>2</volume>
      <issue>4</issue>
      <fpage>203</fpage>
      <lpage>216</lpage>
      <page-range>203-216</page-range>
      <history>
        <date date-type="received">
          <day>14</day>
          <month>10</month>
          <year>2024</year>
        </date>
        <date date-type="accepted">
          <day>15</day>
          <month>12</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Alzheimer’s disease (AD), a progressive neurodegenerative disorder characterized by severe cognitive decline, necessitates early and accurate diagnosis to improve patient outcomes. Recent advancements in deep learning (DL), particularly convolutional neural networks (CNNs), have demonstrated significant potential in medical image analysis (MIA). This study presents a robust CNN-based framework for the classification of AD using magnetic resonance imaging (MRI) data. The proposed methodology incorporates contrast stretching for image preprocessing, followed by principal component analysis (PCA) and recursive feature elimination (RFE) for feature selection, to enhance the discriminative power of the model. The framework is designed to classify MRI into four distinct categories: non-demented, very mildly demented, mildly demented, and moderately demented. Experimental validation on a comprehensive dataset reveals that the proposed approach achieves exceptional performance, with a validation accuracy of 97% and a training accuracy of 100%, alongside reduced loss and improved sensitivity. The integration of PCA and RFE is shown to effectively reduce dimensionality while retaining diagnostically critical features, thereby optimizing the model’s efficiency and interpretability. These findings underscore the potential of DL techniques to revolutionize the early detection and diagnosis of AD, offering a powerful tool for clinical decision-making and advancing the field of neuroimaging analysis. The proposed framework not only addresses the challenges of high-dimensional data but also provides a scalable and generalizable solution for the classification of neurodegenerative disorders.</p></abstract>
      <kwd-group>
        <kwd>Alzheimer’s disease (AD)</kwd>
        <kwd>Convolutional neural networks (CNNs)</kwd>
        <kwd>Medical image classification</kwd>
        <kwd>Recursive feature elimination (RFE)</kwd>
        <kwd>Magnetic resonance imaging (MRI)</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="3"/>
        <fig-count count="16"/>
        <table-count count="0"/>
        <ref-count count="30"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Doctors can use artificial intelligence (AI) to help them diagnose patients more quickly and accurately. It may predict a disease's risk beforehand, enabling its prevention. To analyze medical data and treat diseases, researchers can employ deep learning (DL) (<xref ref-type="bibr" rid="ref_9">Helaly et al., 2022</xref>). In contrast, medical image analysis (MIA) can be a laborious and complex process. To identify Alzheimer’s disease (AD) early, a DL model was used in this study. </p><p>In order to process data and generate patterns for utilization in decision-making, DL (sometimes referred to as deep structured learning or hierarchical learning) is an AI function that mimics how the human brain functions (<xref ref-type="bibr" rid="ref_24">So et al., 2019</xref>). In AI, DL is a subset of machine learning (ML) that includes networks that can learn unsupervisedly from unstructured or unlabelled data. Deep neural learning and deep neural network (DNN) are some names for it. </p><p>Working memory comes initially. Maintaining focus and attention while receiving data is its responsibility. Short-term memory (STM) comes next. STM oversees the preservation of information for slightly more than a 24-hour period. At last, for durations longer than a day, all of the events observed are recorded and stored in long-term memory (LTM) (<xref ref-type="bibr" rid="ref_12">Jiang et al., 2022</xref>). Memory, reasoning, and the ability to perform even the most basic tasks have all been severely affected by AD, a degenerative brain disease. </p>
      <p>AD is a degenerative disease of the brain and neurological system that worsens over time (<xref ref-type="bibr" rid="ref_28">Wen et al., 2020</xref>). The incidence of AD is rising annually as a result of the global acceleration of the aging process. As the condition worsens, older adults with AD will face a number of brain damage symptoms, including progressive memory loss, mobility issues, a decline in language expression, and cognitive challenges (<xref ref-type="bibr" rid="ref_17">Logan et al., 2021</xref>). Magnetic resonance imaging (MRI) of the brain is typically used to assess this stage of neuropsychiatric symptom development. The progression of AD is seen in <xref ref-type="fig" rid="fig_1">Figure 1</xref>.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Progression of AD</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_bPQVk5Qg78O3cyHV.jpeg"/>
        </fig>
      
      <p>In recent years, research has focused on utilizing ML to diagnose AD from data like MRI. This technology has expedited the medical process and made the task of medical experts easier. The objective in this study is to use a convolutional neural network (CNN) to classify AD images (<xref ref-type="bibr" rid="ref_5">Ben Ahmed et al., 2015</xref>). The most traditional and widely used DL framework is CNN, which is a multilayer neural network. The state of the art (SOTA) method for image classification is CNN with DL (<xref ref-type="bibr" rid="ref_20">Poloni et al., 2021</xref>).</p><p>The study aims to address the problem of model accuracy and data sensitivity. Additionally, it uses CNN to categorize four classes of AD, as shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>. In that instance, the recursive feature elimination (RFE) approach was applied and principal component analysis (PCA) was incorporated into CNN. In a model, PCA is seen as a commonly utilized technique, whose aim is to reduce dataset dimensions (<xref ref-type="bibr" rid="ref_1">AbdulAzeem et al., 2021</xref>). It enables the model to visualize the data simply and train itself more quickly. In this work, a PCA was used to confirm whether or not the features are independent of one another. The least features were eliminated and new independent features were produced from the previous ones with the support of PCA.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Types of classes in AD</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_KMpapkESNXviz-X4.png"/>
        </fig>
      
      <p>Exploring the best feature in a database is the goal of RFE, which is similar to a greedy optimization technique (<xref ref-type="bibr" rid="ref_7">Fu’adah et al., 2021</xref>). To do this, the model's basic DL algorithm must be fitted, the features must be ranked according to importance, the least important features must be removed, and the model must then be re-fitted.</p>
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p><xref ref-type="bibr" rid="ref_10">Islam &amp;amp; Zhang (2018)</xref> proposed applied ensemble DL models combining CNNs with other architectures. It enhances the performance in complex tasks like image classification, MIA, and more. MRI data was used to diagnose AD. MRI scans from relevant datasets were obtained, providing various imaging modalities such as T1-weighted images, which are crucial for AD diagnosis. High-level features were automatically extracted from the MRI data using a CNN. By recognizing patterns like shrinkage in particular brain regions, the CNN layers were able to recognize AD. To guarantee uniformity throughout the dataset, one of the preprocessing stages involved normalizing the MRI intensity values. PCA helps in reducing the dimensionality of the extracted features. The computational cost was also reduced via PCA; it also emphasized the most relevant features. For classification tasks, the more comprehensive and effective solution was offered by these models, because these models utilize the robustness of several methods.</p><p><xref ref-type="bibr" rid="ref_4">Basaia et al. (2019)</xref> proposed a 3D CNN model trained on structural MRI data. It is considered to be an effective method for evaluating volumetric medical images. Comprehensive data regarding the brain structure were offered by the following activities: disease detection, brain segmentation, and structural MRI scans. The 3D CNN model lowered the computational cost and maintained the spatial data via reducing the dimensionality of the feature maps. To perform tasks like regression or classification, 3D CNNs usually have fully connected layers that use the features after feature extraction. The entire volume of MRI data was analyzed by the 3D CNN model, effectively capturing the spatial relationship among various brain regions. However, the 2D CNN model fails to capture that. Compared to 2D models, the 3D CNN models are effective and attain better classification accuracy in tasks like AD detection, brain tumor classification, and brain segmentation. The problems related to computational demands and data requirements must be managed carefully.</p><p><xref ref-type="bibr" rid="ref_19">Ortiz et al. (2016)</xref> proposed a 3D CNN model with transfer learning (TL) applied to brain MRI data. Through task adaptation, TL uses a model that has already been trained on a large dataset. This leverages the learned features and weights from the pre-trained model, reducing the need for extensive training on the new dataset. While modifying and retraining parts of the pre-trained model on the target dataset (brain MRI), other parts remain fixed. This adapts the model to the specific features of MRI data. For 3D CNNs, TL might involve pre-trained models on large 3D datasets. In practice, pre-trained models for medical images are less common than 2D models, but there are specialized models available for tasks such as brain segmentation. TL in 3D CNNs typically involves extracting features from pre-trained models and fine-tuning the network on MRI data to improve performance for specific tasks. TL helps in achieving higher accuracy by leveraging pre-trained features that capture general image patterns, which are fine-tuned to the specific characteristics of MRI data. It results in improved accuracy, reduced training time, and better feature extraction, though it comes with challenges related to data compatibility, computational demands, model interpretability and robustness in classification.</p><p>To classify AD using MRI data, a deep CNN model was created by <xref ref-type="bibr" rid="ref_25">Tariq et al. (2019)</xref>. The goal is to create a CNN model capable of accurately classifying MRI into different AD-related classes. In order to build a classifier that can differentiate between different disease states, this usually entails extracting features from MRI scans. The MRI is accepted by the input layer. While 3D CNNs deal with volumetric data, 2D CNNs often handle images as slices or projections. These layers use either 2D or 3D convolutions to extract hierarchical features from MRI. The use of many convolutional layers with varied filter sizes was considered to collect various levels of information. After feature extraction, the gathered features were combined using fully connected layers to determine the final classification. The output layer uses an activation function (AF), such as softmax for multi-class classification (MCC) or sigmoid for binary classification, to produce class probabilities. The objective of this strategy is to give accurate and dependable MRI classification for the diagnosis and comprehension of AD by utilizing cutting-edge approaches and ongoing improvement.</p><p><xref ref-type="bibr" rid="ref_15">Kong et al. (2022)</xref> proposed an applied 3D CNN model on functional magnetic resonance imaging (fMRI) data for AD detection. This method involves leveraging the spatiotemporal information within fMRI volumes to identify patterns associated with the disease. By identifying variations in blood flow, which represent neural activity in various brain areas, fMRI measures brain activity. Unlike structural MRI, which captures the anatomy of the brain, fMRI provides insight into brain function and connectivity. fMRI data consists of 3D volumes over time, making it a 4D dataset. Each voxel represents a small cube of brain tissue, and the signal intensity indicates the level of neural activity. The brain function gets affected by AD. Disruption in neural activity and connectivity are the consequences of AD. The fMRI is employed for detecting these variations. For early diagnosis, this fMRI is considered to be an effective tool. Typically, a softmax AF is used in the final layer to produce class probabilities. The application of this technique has demonstrated significant potential in clinical settings, particularly for early diagnosis and monitoring of disease progression, considering the fact that it requires careful attention to data preparation, model construction, and the challenges of working with high-dimensional, temporal data.</p><p>By combining CNN with a recurrent neural network (RNN), <xref ref-type="bibr" rid="ref_16">Li et al. (2019)</xref> showed how to diagnose AD by utilizing the advantages of both architectures for the analysis of neuroimaging data, such as MRI or fMRI. While RNNs, especially long short-term memory (LSTM) networks or gated recurrent units (GRUs), are effective at capturing temporal dependencies, CNNs are good at extracting spatial features from images. The purpose of CNNs is to automatically and adaptively learn feature spatial hierarchies from input images. CNNs are especially good in finding patterns in brain scans in neuroimaging, such as fMRI activation patterns or structural abnormalities in MRI. RNNs are made to identify previous inputs in order to identify patterns in data sequences. In the context of neuroimaging, RNNs can be used to analyze sequences of brain scans over time, capturing temporal dynamics that may indicate the progression of AD. It provides a powerful tool for diagnosing AD by leveraging the spatial and temporal characteristics of brain scans. Improved clinical diabetes mellitus, improved monitoring of disease progression, and earlier and more accurate diagnosis are all possible outcomes of this hybrid approach.</p><p>To enhance CNN performance on MRI data, the data augmentation approach was used by <xref ref-type="bibr" rid="ref_23">Singh et al. (2022)</xref>. This method decreases overfitting, increases the robustness of the model, and improves CNNs' capacity for generalization. Medical datasets, including MRI scans, are often limited in size due to the difficulty in acquiring and labelling medical images. This limitation can lead to overfitting in CNN models. MRI scans can vary due to different scanning protocols, patient positioning, and anatomical differences. Data augmentation helps CNNs learn to handle this variability effectively. By expanding the variety of the training dataset, enhancing generalization, and decreasing overfitting, data augmentation is a potent method to improve CNN performance on MRI data. By carefully applying spatial, intensity-based, and advanced augmentation methods, models can achieve higher accuracy and robustness, making them more effective for clinical diagnosis and research in the context of AD and other neurological disorders.</p><p>By combining preprocessing with feature extraction, <xref ref-type="bibr" rid="ref_3">Amoroso et al. (2018)</xref> used DL models for AD classification using MRI data. Preprocessing, feature extraction, and the use of DL architectures were all integrated into a thorough procedure. Memory loss, cognitive decline, and behavioural abnormalities are symptoms of AD, a degenerative neurological condition. For the condition to be managed, an early and precise diagnosis is essential. A non-invasive imaging method that produces fine-grained structural images of the brain is MRI. These images are valuable for detecting brain atrophy and other abnormalities associated with AD. The main challenges include the complexity of the brain's anatomy, variability in MRI scans, and the need for robust preprocessing and feature extraction methods to maximize the effectiveness of DL models. By addressing the challenges of small sample sizes, high dimensionality, and the need for interpretability, these models have the ability to significantly improve early diagnosis and treatment of AD, ultimately impacting patient outcomes and advancing research in the field of neuroimaging.</p><p>In order to improve the diagnosis accuracy, <xref ref-type="bibr" rid="ref_14">Khvostikov et al. (2018)</xref> suggested to utilize DL-based multi-modal fusion, which combines MRI and positron emission tomography (PET) data for AD detection by integrating complementary information from both imaging modalities. MRI and PET provide different types of information—structural and functional. Combining them can lead to a more comprehensive understanding of the disease. By fusing data from both modalities, the model can detect subtle changes in the brain that might not be evident from a single modality, leading to improved diagnostic accuracy, particularly in early stages like mild cognitive impairment (MCI). A hybrid approach that involves both feature- and decision-level fusion was implemented. Features from MRI and PET were first fused, and the combined feature set was then employed for training a DL model, whose output was further refined by an ensemble of decision-level fusion methods. The future goal is to use graph neural networks (GNNs) to model the relationships between different brain regions as captured by MRI and PET, providing a more holistic view of how AD affects brain connectivity. With the ability to improve clinical practice and patient outcomes, it makes use of the advantages of both imaging modalities and DL to offer a greater understanding of the disease.</p><p>Using structural MRI data, <xref ref-type="bibr" rid="ref_11">Jain et al. (2019)</xref> created a CNN-based DL framework for the early detection of AD. In order to detect small neurological changes linked to the early stages of AD, this method makes use of CNNs' capacity to automatically extract pertinent features from high-dimensional image data.</p><p>Early detection of AD is critical as it enables interventions that can slow disease progression, improve quality of life, and provide time for planning and treatment. Structural magnetic resonance imaging (sMRI) is one of the most informative imaging modalities for capturing anatomical changes in the brain, such as hippocampal atrophy, which are early indicators of AD. The CNN-based framework can be integrated into clinical workflows, providing radiologists and neurologists with an advanced tool for the early detection of AD, potentially leading to earlier interventions. It achieves a high sensitivity with improved accuracy.</p>
    </sec>
    <sec sec-type="">
      <title>3. Methodology</title>
      <p>Image classification, a core mechanism in computer vision (CV), has been extensively researched and applied across various domains. Classifier, image feature extraction and selection, and image preprocessing are usually its three main parts (<xref ref-type="bibr" rid="ref_18">Lu &amp;amp; Weng, 2007</xref>). A collection of 10,432 JPEG images of patients with four categories (mildly demented, moderately demented, non-demented, and very mildly demented) was used to implement DL features for the classification of AD. In order to speed up the execution of the DL algorithms, the model was created using the Python programming language and the Keras and TensorFlow libraries. The system was backed by a graphics processing unit (GPU) with NVIDIA.</p><p><xref ref-type="fig" rid="fig_3">Figure 3</xref>, <xref ref-type="fig" rid="fig_4">Figure 4</xref>, <xref ref-type="fig" rid="fig_5">Figure 5</xref>, and <xref ref-type="fig" rid="fig_6">Figure 6</xref> illustrate the four classes of the 10,432 images in the dataset, which was made available by Kaggle, for testing purposes.</p>
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>Class 0 (mildly demented) of AD</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_VKfsYMPR-SFzpZoB.jpeg"/>
        </fig>
      
      
        <fig id="fig_4">
          <label>Figure 4</label>
          <caption>
            <title>Class 1 (moderately demented) of AD</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_-TURoF5QlxRJ6vzE.jpeg"/>
        </fig>
      
      
        <fig id="fig_5">
          <label>Figure 5</label>
          <caption>
            <title>Class 2 (non-demented) of AD</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_mpSH-swA1vxIz3ea.jpeg"/>
        </fig>
      
      
        <fig id="fig_6">
          <label>Figure 6</label>
          <caption>
            <title>Class 3 (very mildly demented) of AD</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_M2FTwHc9atPP98iS.jpeg"/>
        </fig>
      
      
        <sec>
          
            <title>3.1. Preprocessing</title>
          
          <p>Getting higher-quality images is the goal of image preprocessing. In this case, the input (IQ) image quality was improved by the application of contrast stretching. In order to preserve the final image's shape without causing damage, the contrast stretching technique was employed in this work. The contrast stretching (<xref ref-type="bibr" rid="ref_29">Widodo et al., 2016</xref>) approach is a subset of the point processing method, which implies that it depends only on the intensity of a single pixel and not on the pixels surrounding it. The image's grey level range is presumed to be between 0 and 255 in this case. The transformation proceeds in a straight line with no alteration to the grey level image generated if the grey level values are <inline-formula>
  <mml:math id="mu18pl8qho">
    <mml:msub>
      <mml:mi>a</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>a</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="m7kdn1n4fz">
    <mml:msub>
      <mml:mi>b</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>b</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula> (<xref ref-type="bibr" rid="ref_21">Radha &amp;amp; Tech, 2012</xref>). However, the method generates a value if the grey level value is considered to be <inline-formula>
  <mml:math id="mifom03l82">
    <mml:msub>
      <mml:mi>a</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>a</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:mo>&amp;lt;</mml:mo>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mnwkaqghce">
    <mml:msub>
      <mml:mi>b</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>b</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:mo>&amp;lt;</mml:mo>
  </mml:math>
</inline-formula>. Three functions to calculate contrast stretching are described in Eqs. (1)-(3). The contrast stretching function is given in <xref ref-type="fig" rid="fig_7">Figure 7</xref>.</p>
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>Contrast stretching function</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_aVjdMW5rFONcbsBN.png"/>
            </fig>
          
          <p>For $0 \leq f_i(x, y) &lt; a_1$, then:</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="mbn0dz2244">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>f</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>0</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>f</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mfrac>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mi>b</mml:mi>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mn>1</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mi>a</mml:mi>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mn>1</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mrow>
                        </mml:mfrac>
                      </mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mi>x</mml:mi>
                      <mml:mi>y</mml:mi>
                      <mml:mi>x</mml:mi>
                      <mml:mi>y</mml:mi>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>For <inline-formula>
  <mml:math id="mmajpl3fya">
    <mml:msub>
      <mml:mi>a</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>f</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>a</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:mo>≤</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>, then:</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="mbctfpanuh">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>f</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>0</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>b</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mo>−</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mrow>
                          <mml:msub>
                            <mml:mrow>
                              <mml:mi>f</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>i</mml:mi>
                            </mml:mrow>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:msub>
                            <mml:mrow>
                              <mml:mi>a</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mn>1</mml:mn>
                            </mml:mrow>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mi>x</mml:mi>
                        <mml:mi>y</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mfrac>
                          <mml:mrow>
                            <mml:mrow>
                              <mml:msub>
                                <mml:mrow>
                                  <mml:mi>b</mml:mi>
                                </mml:mrow>
                                <mml:mrow>
                                  <mml:mn>2</mml:mn>
                                </mml:mrow>
                              </mml:msub>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:msub>
                                <mml:mrow>
                                  <mml:mi>b</mml:mi>
                                </mml:mrow>
                                <mml:mrow>
                                  <mml:mn>1</mml:mn>
                                </mml:mrow>
                              </mml:msub>
                            </mml:mrow>
                            <mml:mo>−</mml:mo>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mrow>
                              <mml:msub>
                                <mml:mrow>
                                  <mml:mi>a</mml:mi>
                                </mml:mrow>
                                <mml:mrow>
                                  <mml:mn>2</mml:mn>
                                </mml:mrow>
                              </mml:msub>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:msub>
                                <mml:mrow>
                                  <mml:mi>a</mml:mi>
                                </mml:mrow>
                                <mml:mrow>
                                  <mml:mn>1</mml:mn>
                                </mml:mrow>
                              </mml:msub>
                            </mml:mrow>
                            <mml:mo>−</mml:mo>
                          </mml:mrow>
                        </mml:mfrac>
                      </mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mi>x</mml:mi>
                      <mml:mi>y</mml:mi>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>For <inline-formula>
  <mml:math id="ml493co0p8">
    <mml:msub>
      <mml:mi>a</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>f</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:mo>≤</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mn>255</mml:mn>
  </mml:math>
</inline-formula>, then:</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="m9jh6dh65n">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>f</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>0</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>b</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>2</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mo>−</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mrow>
                          <mml:msub>
                            <mml:mrow>
                              <mml:mi>f</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>i</mml:mi>
                            </mml:mrow>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:msub>
                            <mml:mrow>
                              <mml:mi>a</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mn>2</mml:mn>
                            </mml:mrow>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mi>x</mml:mi>
                        <mml:mi>y</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mfrac>
                          <mml:mrow>
                            <mml:mn>255</mml:mn>
                            <mml:mo>−</mml:mo>
                            <mml:mrow>
                              <mml:msub>
                                <mml:mrow>
                                  <mml:mi>b</mml:mi>
                                </mml:mrow>
                                <mml:mrow>
                                  <mml:mn>2</mml:mn>
                                </mml:mrow>
                              </mml:msub>
                            </mml:mrow>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>255</mml:mn>
                            <mml:mo>−</mml:mo>
                            <mml:mrow>
                              <mml:msub>
                                <mml:mrow>
                                  <mml:mi>a</mml:mi>
                                </mml:mrow>
                                <mml:mrow>
                                  <mml:mn>2</mml:mn>
                                </mml:mrow>
                              </mml:msub>
                            </mml:mrow>
                          </mml:mrow>
                        </mml:mfrac>
                      </mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mi>x</mml:mi>
                      <mml:mi>y</mml:mi>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>The preprocessing MATLAB screens are shown in <xref ref-type="fig" rid="fig_8">Figure 8</xref>.</p>
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>Preprocessing MATLAB screen window</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_OYJ-X1WJuj-_5t7V.jpeg"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Feature extraction</title>
          
          <p>The retrieved data was normalized to zero mean and unit variance using a conventional scalar function following the preprocessing step. Normalization was conducted in order to eliminate the irregularities in the data that hamper analysis. Eq. (4) provides the normalized matrix of elements <inline-formula>
  <mml:math id="m3i4ibk59z">
    <mml:mi>x</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula>.</p>
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="mptc1f3jcl">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>X</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>n</mml:mi>
                            <mml:mi>o</mml:mi>
                            <mml:mi>r</mml:mi>
                            <mml:mi>m</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mi>j</mml:mi>
                          <mml:mi>m</mml:mi>
                          <mml:mi>e</mml:mi>
                          <mml:mi>a</mml:mi>
                          <mml:mi>n</mml:mi>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mrow>
                            <mml:mo>(</mml:mo>
                            <mml:mo>)</mml:mo>
                            <mml:mrow>
                              <mml:msub>
                                <mml:mrow>
                                  <mml:mi>X</mml:mi>
                                </mml:mrow>
                                <mml:mrow>
                                  <mml:mi>j</mml:mi>
                                </mml:mrow>
                              </mml:msub>
                            </mml:mrow>
                          </mml:mrow>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>s</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>d</mml:mi>
                          <mml:mrow>
                            <mml:mo>(</mml:mo>
                            <mml:mo>)</mml:mo>
                            <mml:mrow>
                              <mml:msub>
                                <mml:mrow>
                                  <mml:mi>x</mml:mi>
                                </mml:mrow>
                                <mml:mrow>
                                  <mml:mi>j</mml:mi>
                                </mml:mrow>
                              </mml:msub>
                            </mml:mrow>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. Feature selection</title>
          
          <p>The feature selection approach reduces the dimension size and selects the best features by combining the PCA and RFE methods.</p><p>A lot of features in the analysis process often lead to dimension issues being highlighted. The effective way to deal with this problem is through PCA. In basic terms, PCA translates the information in $d<inline-formula>
  <mml:math id="mz63ira6bq">
    <mml:mo>−</mml:mo>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
  </mml:math>
</inline-formula>k<inline-formula>
  <mml:math id="m6p1ktf69e">
    <mml:mo>−</mml:mo>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>k &lt; d<inline-formula>
  <mml:math id="mufgwou81v">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>P</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mn>2024</mml:mn>
  </mml:math>
</inline-formula>k$. With the exception of the variance that is already taken into consideration in all of its subsequent components, each PC is addressed towards the maximum variance. Compared to the adhering components, this initial component covers the greater variances. Eq. (5) can be used to determine PCs:</p>
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="mygr78zxxf">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mi>P</mml:mi>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>C</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>a</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>X</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>a</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>2</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>X</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mn>2</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>a</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>d</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>X</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>d</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo>=</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>⋯</mml:mo>
                      <mml:mo>+</mml:mo>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>where,</p><p><inline-formula>
  <mml:math id="m0fhvacyyz">
    <mml:mi>P</mml:mi>
    <mml:msub>
      <mml:mi>C</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> denotes the <inline-formula>
  <mml:math id="myn8zjgerg">
    <mml:msup>
      <mml:mi>i</mml:mi>
      <mml:mrow>
        <mml:mtext>th </mml:mtext>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> PC; <inline-formula>
  <mml:math id="mponu2328p">
    <mml:msub>
      <mml:mi>X</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> represents the original feature $j<inline-formula>
  <mml:math id="mz6bxdmfdt">
    <mml:mo>;</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>a_i<inline-formula>
  <mml:math id="msh8iisj7d">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula>X_i$.</p><p>The most used technique for feature selection is PCA. In this case, it only reduces the dimensionality of the features. However, the model solely operates on the basis of selected features when a feature selection technique is used, and no changes are made (<xref ref-type="bibr" rid="ref_2">Aker, 2022</xref>). PCA is used to initially minimize a feature's dimension, and the RFE feature selection method is used to choose the most significant features.</p><p>An example of a feature selection algorithm wrapper is RFE (<xref ref-type="bibr" rid="ref_6">Chen, 2003</xref>). This approach builds models and chooses the best ones based on performance metrics, by using a variety of subsets of input features. The combined PCA and RFE model is shown in <xref ref-type="fig" rid="fig_9">Figure 9</xref>.</p>
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>
                <title>Combined model of PCA and RFE</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_jqfv93WsoTtC9cdK.png"/>
            </fig>
          
          <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Algorithm 1: Process of RFE</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Step 1: All predictors are used to optimize the model on the training set.</p><p style="text-align: justify">Step 2: Model performance computation.</p><p style="text-align: justify">Step 3: Calculate the importance-based ranking of variables for each subset size.</p><p style="text-align: justify">Step 4: Preserve the most crucial variables.</p><p style="text-align: justify">Step 5: Train the model on the training set using predictors.</p><p style="text-align: justify">Step 6: Determine the model's performance.</p><p style="text-align: justify">Step 7: The computation of the performance profile is complete.</p><p style="text-align: justify">Step 8: Choose an appropriate amount of predictors.</p><p style="text-align: justify">Step 9: Make use of the model that best fits the situation.</p></td></tr></tbody></table>
        </sec>
      
      
        <sec>
          
            <title>3.4. Classification method</title>
          
          <p>When both the cortical and subcortical regions' sMRI features were retrieved, the normalization and feature selection stages were applied to the composite features from both regions (<xref ref-type="bibr" rid="ref_8">Gupta et al., 2019</xref>). It can be utilized to distinguish AD from other groups, once the features have been chosen. This method is demonstrated in <xref ref-type="fig" rid="fig_10">Figure 10</xref>. CNN is suggested to complete the classification task.</p>
          
            <fig id="fig_10">
              <label>Figure 10</label>
              <caption>
                <title>Block diagram of image classification on AD</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_Dmf3ghzUmm-L545n.png"/>
            </fig>
          
          
            <sec>
              
                <title>3.4.1 Cnn</title>
              
              <p>In mathematics, convolution is a crucial analytical procedure. This mathematical operator takes two functions, $f<inline-formula>
  <mml:math id="m9kt7if7tg">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>g<inline-formula>
  <mml:math id="msao29t8kb">
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>W</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>f<inline-formula>
  <mml:math id="mgngm1ns3r">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>g$ are reversed or translated, these functions show the area of overlap between them. Eq. (6) often defines its calculation.</p>
              
                <disp-formula>
                  <label>(6)</label>
                  <mml:math id="m6jjs81jqe">
                    <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                      <mml:mtr>
                        <mml:mtd>
                          <mml:mi>z</mml:mi>
                          <mml:mi>f</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>g</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>τ</mml:mi>
                          <mml:mi>g</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>τ</mml:mi>
                          <mml:mrow>
                            <mml:msup>
                              <mml:mrow>
                                <mml:mo>(</mml:mo>
                                <mml:mo>)</mml:mo>
                                <mml:mi>t</mml:mi>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mi>d</mml:mi>
                                <mml:mi>e</mml:mi>
                                <mml:mi>f</mml:mi>
                              </mml:mrow>
                            </mml:msup>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>f</mml:mi>
                          </mml:mrow>
                          <mml:mo>=</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>∗</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>=</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:munderover>
                            <mml:mo>∑</mml:mo>
                            <mml:mrow>
                              <mml:mi>τ</mml:mi>
                              <mml:mi>∞</mml:mi>
                              <mml:mo>=</mml:mo>
                              <mml:mo>−</mml:mo>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mo>+</mml:mo>
                              <mml:mi>∞</mml:mi>
                            </mml:mrow>
                          </mml:munderover>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:math>
                </disp-formula>
              
              <p>Eq. (7) contains its integral form as follows:</p>
              
                <disp-formula>
                  <label>(7)</label>
                  <mml:math id="mnwxfmtaxg">
                    <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                      <mml:mtr>
                        <mml:mtd>
                          <mml:mi>z</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>f</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>g</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>τ</mml:mi>
                          <mml:mi>g</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>τ</mml:mi>
                          <mml:mi>d</mml:mi>
                          <mml:mi>τ</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>τ</mml:mi>
                          <mml:mi>g</mml:mi>
                          <mml:mi>τ</mml:mi>
                          <mml:mi>d</mml:mi>
                          <mml:mi>τ</mml:mi>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>=</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>∗</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>=</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>=</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:msubsup>
                            <mml:mo>∫</mml:mo>
                            <mml:mrow>
                              <mml:mo>−</mml:mo>
                              <mml:mi>∞</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mo>+</mml:mo>
                              <mml:mi>∞</mml:mi>
                            </mml:mrow>
                          </mml:msubsup>
                          <mml:msubsup>
                            <mml:mo>∫</mml:mo>
                            <mml:mrow>
                              <mml:mo>−</mml:mo>
                              <mml:mi>∞</mml:mi>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mo>+</mml:mo>
                              <mml:mi>∞</mml:mi>
                            </mml:mrow>
                          </mml:msubsup>
                          <mml:mrow>
                            <mml:mi>f</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>f</mml:mi>
                          </mml:mrow>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:math>
                </disp-formula>
              
              <p>A digital image, represented by the symbol <inline-formula>
  <mml:math id="m4n0x9vye2">
    <mml:mi>f</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula>, can be considered as a discrete function of a 2D space in image processing. Eq. (8) can be used to represent the output image <inline-formula>
  <mml:math id="mpsms21w05">
    <mml:mi>z</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula>, assuming that a 2D convolution function <inline-formula>
  <mml:math id="m6qvkd3rpe">
    <mml:mi>g</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula>.</p>
              
                <disp-formula>
                  <label>(8)</label>
                  <mml:math id="m02jsn9a7w">
                    <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                      <mml:mtr>
                        <mml:mtd>
                          <mml:mi>z</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mi>f</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mi>g</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>=</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>∗</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:math>
                </disp-formula>
              
              <p>Thus, following the feature extraction and selection preprocessing, the convolution operation can be utilized to extract the image features. Similar to this, in DL applications, the input is a high-dimensional array of 3 × image width × image length when it is a color image with RGB channels that is made up of individual pixels. The learning algorithm defines the kernel as the accounting (<xref ref-type="bibr" rid="ref_13">Jogin et al., 2018</xref>). The kernel is referred to as the "convolution kernel" in CNN. Another high-dimensional array is the computational parameter. Eq. (9) can therefore be used to represent the analogous convolution operation when 2D images are input.</p>
              
                <disp-formula>
                  <label>(9)</label>
                  <mml:math id="mn93dwu2eb">
                    <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                      <mml:mtr>
                        <mml:mtd>
                          <mml:mi>z</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mi>f</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mi>g</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>h</mml:mi>
                          <mml:mi>g</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mi>h</mml:mi>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>=</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>∗</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>=</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:munder>
                            <mml:mo>∑</mml:mo>
                            <mml:mrow>
                              <mml:mi>t</mml:mi>
                            </mml:mrow>
                          </mml:munder>
                          <mml:mrow>
                            <mml:munder>
                              <mml:mo>∑</mml:mo>
                              <mml:mrow>
                                <mml:mi>h</mml:mi>
                              </mml:mrow>
                            </mml:munder>
                            <mml:mrow>
                              <mml:mi>f</mml:mi>
                            </mml:mrow>
                          </mml:mrow>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:math>
                </disp-formula>
              
              <p>Eq. (10) contains the integral form as follows:</p>
              
                <disp-formula>
                  <label>(10)</label>
                  <mml:math id="my5htoyta8">
                    <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                      <mml:mtr>
                        <mml:mtd>
                          <mml:mi>z</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mi>f</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mi>g</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>h</mml:mi>
                          <mml:mi>g</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mi>h</mml:mi>
                          <mml:mi>d</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>d</mml:mi>
                          <mml:mi>h</mml:mi>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>=</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>∗</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>=</mml:mo>
                          <mml:mo>∬</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mrow>
                            <mml:mi>f</mml:mi>
                          </mml:mrow>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:math>
                </disp-formula>
              
              <p>Given a convolution kernel of size <inline-formula>
  <mml:math id="mk0jx0u8lc">
    <mml:mi>m</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula>, Eq. (11) looks like as follows:</p>
              
                <disp-formula>
                  <label>(11)</label>
                  <mml:math id="mxx23rlg6a">
                    <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                      <mml:mtr>
                        <mml:mtd>
                          <mml:mi>z</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mi>f</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mi>g</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>h</mml:mi>
                          <mml:mi>g</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mi>h</mml:mi>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>=</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>∗</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>=</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:munderover>
                            <mml:mo>∑</mml:mo>
                            <mml:mrow>
                              <mml:mi>t</mml:mi>
                              <mml:mo>=</mml:mo>
                              <mml:mn>0</mml:mn>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mi>t</mml:mi>
                              <mml:mi>m</mml:mi>
                              <mml:mo>=</mml:mo>
                            </mml:mrow>
                          </mml:munderover>
                          <mml:mrow>
                            <mml:munderover>
                              <mml:mo>∑</mml:mo>
                              <mml:mrow>
                                <mml:mi>h</mml:mi>
                                <mml:mo>=</mml:mo>
                                <mml:mn>0</mml:mn>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mi>h</mml:mi>
                                <mml:mi>n</mml:mi>
                                <mml:mo>=</mml:mo>
                              </mml:mrow>
                            </mml:munderover>
                            <mml:mrow>
                              <mml:mi>f</mml:mi>
                            </mml:mrow>
                          </mml:mrow>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:math>
                </disp-formula>
              
              <p>In order to indicate the size of the convolution kernels $m<inline-formula>
  <mml:math id="mhj3ywxk5v">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>n<inline-formula>
  <mml:math id="m43lnv7apf">
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>f<inline-formula>
  <mml:math id="m713s17hna">
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>G<inline-formula>
  <mml:math id="mxh963uy9h">
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mi>A</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>n \times n<inline-formula>
  <mml:math id="m0ys3eqbkf">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>M \times M<inline-formula>
  <mml:math id="m2vkq73eyl">
    <mml:mo>.</mml:mo>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>n \times n<inline-formula>
  <mml:math id="m6cx0v4lql">
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>n \times n<inline-formula>
  <mml:math id="mo28b3wlcc">
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>n \times n$ size is multiplied by the convolution kernel during computation (<xref ref-type="bibr" rid="ref_27">Vasudevan et al., 2017</xref>).</p>
            </sec>
          
          
            <sec>
              
                <title>3.4.2 Image classification based on cnn</title>
              
              <p>The CNN-based image classification model is shown below:</p><p>a) Input: The training set consists of a set of $N<inline-formula>
  <mml:math id="mglnt7fc88">
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>K$ classification tags.</p><p>b) Learning: This step involves learning the precise features of each class using the training set. This stage is commonly referred to as learning a model or a training classifier.</p><p>c) Evaluation: When comparing the classifier's predicted labels with the image's actual feature vectors, it is evident that the classifier's predicted labels are consistent with the image's true classification feature vectors, which is a good thing, and the more such cases, the better. The classifier is used to evaluate the classifiers' quality and predict the classification feature vector of images that hasn't been examined. <xref ref-type="fig" rid="fig_11">Figure 11</xref> shows the classification output.</p>
              
                <fig id="fig_11">
                  <label>Figure 11</label>
                  <caption>
                    <title>Classification outcomes</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_O4HK3VYDEMRJ_K9W.jpeg"/>
                </fig>
              
              <p>A classifier must be used to classify the feature vector after it has been extracted from the image, which can then be described as a fixed-length vector. From input to output, a typical CNN consists of the following layers: input layer, convolutional layer, activation layer, pool layer, fully connected layer, and final output layer (<xref ref-type="bibr" rid="ref_30">Yamashita et al., 2018</xref>). The continuous convolution-pool structure decodes, deduces, converges, and maps the feature signals of the original data to the hidden layer feature space while the CNN layer creates the relationships between various computational neural nodes and transfers input information layer by layer (<xref ref-type="bibr" rid="ref_22">Shocher et al., 2020</xref>). The extracted feature is then used by the fully connected layer to classify and output.</p><p>Therefore, AD is diagnosed by applying CNN and the images are classified based on the proposed model by performing techniques for the image classification. This can result in the high accuracy of images without changing the shape of the image and improved sensitivity for positive findings for patients with the disease.</p>
            </sec>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Results and discussion</title>
      <p>The four classes of AD, i.e., mildly demented, moderately demented, non-demented, and very mildly demented, were identified using the model. The anomalies were removed in feature extraction. It is helpful for clinicians to make faster and easier diagnoses of the disease. It resulted in improved accuracy and sensitivity. The accuracy is based on the training and validation. Training accounts for 70% of the data evaluation, while validation accounts for 30%. The results include 97% validating accuracy, 0.0832 validating loss, 0.0012 training loss, and 100% training accuracy.</p><p>The evaluation metrics are as follows:</p><p>Accuracy is a metric for evaluating classification models that measures how well a model performs across all classes by calculating the ratio of correct predictions to total predictions. Accuracy is represented in Eq. (12).</p>
      
        <disp-formula>
          <label>(12)</label>
          <mml:math id="myi2mszdqa">
            <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
              <mml:mtr>
                <mml:mtd>
                  <mml:mi>A</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:mi>u</mml:mi>
                  <mml:mi>r</mml:mi>
                  <mml:mi>a</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:mi>y</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mi>T</mml:mi>
                      <mml:mi>P</mml:mi>
                      <mml:mi>T</mml:mi>
                      <mml:mi>N</mml:mi>
                      <mml:mo>+</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>T</mml:mi>
                      <mml:mi>P</mml:mi>
                      <mml:mi>T</mml:mi>
                      <mml:mi>N</mml:mi>
                      <mml:mi>F</mml:mi>
                      <mml:mi>P</mml:mi>
                      <mml:mi>F</mml:mi>
                      <mml:mi>N</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>+</mml:mo>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
      
      <p>The ability of a diagnostic test to accurately determine whether a person has a disease or not is known as sensitivity. It is a test's capacity to detect positive results in individuals who have a condition. Eq. (13) represents sensitivity.</p>
      
        <disp-formula>
          <label>(13)</label>
          <mml:math id="m2yp1oe5i8">
            <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
              <mml:mtr>
                <mml:mtd>
                  <mml:mi>S</mml:mi>
                  <mml:mi>e</mml:mi>
                  <mml:mi>n</mml:mi>
                  <mml:mi>s</mml:mi>
                  <mml:mi>i</mml:mi>
                  <mml:mi>t</mml:mi>
                  <mml:mi>i</mml:mi>
                  <mml:mi>v</mml:mi>
                  <mml:mi>i</mml:mi>
                  <mml:mi>t</mml:mi>
                  <mml:mi>y</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mi>T</mml:mi>
                      <mml:mi>P</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>T</mml:mi>
                      <mml:mi>P</mml:mi>
                      <mml:mi>F</mml:mi>
                      <mml:mi>N</mml:mi>
                      <mml:mo>+</mml:mo>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
      
      <p><xref ref-type="fig" rid="fig_12">Figure 12</xref> shows the training accuracy, which compares the accuracy of the other classification system (OCS) algorithm and the CNN algorithm based on image classification. The training accuracy of CNN is 100%. This training accuracy is reached in 19 epochs for the training dataset.</p>
      
        <fig id="fig_12">
          <label>Figure 12</label>
          <caption>
            <title>Training accuracy results</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_DmBvDpy_RKLEokd0.png"/>
        </fig>
      
      <p><xref ref-type="fig" rid="fig_13">Figure 13</xref> shows the validation accuracy, which compares the accuracy of the OCS algorithm and the CNN algorithm based on image classification. The validation accuracy of the CNN is 97%. This validation accuracy is reached in 19 epochs for the training dataset.</p>
      
        <fig id="fig_13">
          <label>Figure 13</label>
          <caption>
            <title>Validation accuracy results</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_MfAwYOhDDg-69CGL.png"/>
        </fig>
      
      <p>Based on image classification, <xref ref-type="fig" rid="fig_14">Figure 14</xref> compares the training loss of the CNN and OCS algorithms. By evaluating the model's inaccuracy on the training set, training loss is a metric that assesses how well a model fits training data. CNN has a training loss of 0.0012, which is lower than that of the OCS algorithm.</p>
      
        <fig id="fig_14">
          <label>Figure 14</label>
          <caption>
            <title>Training loss results</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_h5g_OH2kWZ3_6oEw.png"/>
        </fig>
      
      <p>Based on image classification, <xref ref-type="fig" rid="fig_15">Figure 15</xref> shows the validation loss that contrasts the loss of the CNN and OCS algorithms. A DL model's performance on the validation set is evaluated using a metric called validation loss. CNN has a validation loss of 0.0832, which is lower than that of the OCS method.</p>
      
        <fig id="fig_15">
          <label>Figure 15</label>
          <caption>
            <title>Validation loss results</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_sdhDYZYrKQvr3joo.png"/>
        </fig>
      
      <p><xref ref-type="fig" rid="fig_16">Figure 16</xref> compares the performance of two methods, OCS and CNN, in terms of sensitivity across different numbers of iterations. The results show that the CNN method consistently achieves higher sensitivity than OCS, with a faster and smoother growth trend. In contrast, OCS shows a slower increase with noticeable fluctuations. Ultimately, CNN nearly reaches 100% sensitivity, while OCS remains significantly lower. These findings indicate the superior accuracy and efficiency of CNN compared to OCS in this experiment.</p>
      
        <fig id="fig_16">
          <label>Figure 16</label>
          <caption>
            <title>Sensitivity results</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_eA73Rw_Wh0ZwQ74E.png"/>
        </fig>
      
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>The image classification for AD by CNN based on DL was proposed in this study. This methodology includes preprocessing, feature extraction, feature selection and classification. In the preprocessing technique, the contrast stretching was used to make the shape of the image unchanged as the original image. After preprocessing, feature extraction was used to remove the anomalies in the data. The integration of PCA and RFE was employed to reduce the dimension of the size of the image and select the best features. Finally, CNN was implemented to diagnose AD early through image classification. This classification improved the accuracy of the dataset and it also resulted in high sensitivity. The accuracy was based on the training and validation. The data evaluation consists of training (70%) and validation (30%). Additionally, the validation loss and training loss were attained. The expected increase was attained for both training and validation accuracy. The resulting accuracy helps in image classification without changing the original dataset in the diagnosis of AD. The sensitivity of image classification predicts how well the patient is affected by the disease.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>33</volume>
          <page-range>10415-10428</page-range>
          <issue>16</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>AbdulAzeem</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Bahgat</surname>
              <given-names>W. M.</given-names>
            </name>
            <name>
              <surname>Badawy</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00521-021-05799-w.</pub-id>
          <article-title>A CNN based framework for classification of Alzheimer’s disease</article-title>
          <source>Neural Comput. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>1001-1008</page-range>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Aker</surname>
              <given-names>Y</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.36362/gumus.1078348.</pub-id>
          <article-title>Comparison of PCA and RFE-RF algorithm in bankruptcy prediction</article-title>
          <source>Gümüşhane Üniv. Sos. Bil. Derg.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>302</volume>
          <page-range>3-9</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Amoroso</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Diacono</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Fanizzi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>La Rocca</surname>
              <given-names>M. et al.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.jneumeth.2017.12.011.</pub-id>
          <article-title>Deep learning reveals Alzheimer's disease onset in MCI subjects: Results from an international challenge</article-title>
          <source>J. Neurosci. Methods</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>101645</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Basaia</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Agosta</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Wagner</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Canu</surname>
              <given-names>E. et al.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.nicl.2018.101645.</pub-id>
          <article-title>Automated classification of Alzheimer's disease and mild cognitive impairment using a single MRI and deep neural networks</article-title>
          <source>NeuroImage Clin.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>74</volume>
          <page-range>1249-1266</page-range>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ben Ahmed</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Benois-Pineau</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Allard</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Ben Amar</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Catheline</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-014-2123-y.</pub-id>
          <article-title>Classification of Alzheimer’s disease subjects from MRI using hippocampal visual features</article-title>
          <source>Multimed. Tools Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>504-505</page-range>
          <year>2003</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>X. W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CSB.2003.1227389.</pub-id>
          <article-title>Gene selection for cancer classification using bootstrapped genetic algorithms and support vector machines</article-title>
          <source>, https://doi.org/10.1109/CSB.2003.1227389.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>1844</volume>
          <page-range>012020</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Fu’adah</surname>
              <given-names>Y. N.</given-names>
            </name>
            <name>
              <surname>Wijayanto</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Pratiwi</surname>
              <given-names>N. K. C.</given-names>
            </name>
            <name>
              <surname>Taliningsih</surname>
              <given-names>F. F.</given-names>
            </name>
            <name>
              <surname>Rizal</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Pramudito</surname>
              <given-names>M. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1088/1742-6596/1844/1/012020.</pub-id>
          <article-title>Automated classification of Alzheimer’s disease based on MRI image processing using convolutional neural network (CNN) with AlexNet architecture</article-title>
          <source>J. Phys.: Conf. Ser.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>e0222446</page-range>
          <issue>10</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gupta</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>K. H.</given-names>
            </name>
            <name>
              <surname>Choi</surname>
              <given-names>K. Y.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>J. J. et al.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0222446.</pub-id>
          <article-title>Early diagnosis of Alzheimer’s disease using combined features from voxel-based morphometry and cortical, subcortical, and hippocampus regions of MRI T1 brain images</article-title>
          <source>PLoS One</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>1711-1727</page-range>
          <issue>5</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Helaly</surname>
              <given-names>H. A.</given-names>
            </name>
            <name>
              <surname>Badawy</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Haikal</surname>
              <given-names>A. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s12559-021-09946-2.</pub-id>
          <article-title>Deep learning approach for early detection of Alzheimer’s disease</article-title>
          <source>Cogn. Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>2</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Islam</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s40708-018-0080-3.</pub-id>
          <article-title>Brain MRI analysis for Alzheimer’s disease diagnosis using an ensemble system of deep convolutional neural networks</article-title>
          <source>Brain Inform.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>57</volume>
          <page-range>147-159</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jain</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Jain</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Aggarwal</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Hemanth</surname>
              <given-names>D. J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.cogsys.2018.12.015.</pub-id>
          <article-title>Convolutional neural network based Alzheimer’s disease classification from magnetic resonance brain images</article-title>
          <source>Cogn. Syst. Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>319</page-range>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jiang</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Ke</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/brainsci12030319.</pub-id>
          <article-title>Image classification of Alzheimer’s disease based on external-attention mechanism and fully convolutional network</article-title>
          <source>Brain Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>2319-2323</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jogin</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Madhulika</surname>
              <given-names>M. S.</given-names>
            </name>
            <name>
              <surname>Divya</surname>
              <given-names>G. D.</given-names>
            </name>
            <name>
              <surname>Meghana</surname>
              <given-names>R. K.</given-names>
            </name>
            <name>
              <surname>Apoorva</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/RTEICT42901.2018.9012507.</pub-id>
          <article-title>Feature extraction using convolution neural networks (CNN) and deep learning</article-title>
          <source>, https://doi.org/10.1109/RTEICT42901.2018.9012507.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Khvostikov</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Aderghal</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Krylov</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Catheline</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Benois-Pineau</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1809.03972.</pub-id>
          <article-title>3D inception-based CNN with sMRI and MD-DTI data fusion for Alzheimer's disease diagnostics</article-title>
          <source>arXiv preprint arXiv:1809.03972</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>75</volume>
          <page-range>103565</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kong</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Yi</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.bspc.2022.103565.</pub-id>
          <article-title>Multi-modal data Alzheimer’s disease detection based on 3D convolution</article-title>
          <source>Biomed. Signal Process. Control.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>323</volume>
          <page-range>108-118</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Alzheimer's Disease Neuroimaging Initiative.</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.05.006.</pub-id>
          <article-title>A hybrid Convolutional and Recurrent Neural Network for Hippocampus Analysis in Alzheimer's Disease</article-title>
          <source>J. Neurosci. Methods</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>720226</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Logan</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Williams</surname>
              <given-names>B. G.</given-names>
            </name>
            <name>
              <surname>Ferreira da Silva</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Indani</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Schcolnicov</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Ganguly</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Miller</surname>
              <given-names>S. J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fnagi.2021.720226.</pub-id>
          <article-title>Deep convolutional neural networks with ensemble learning and generative adversarial networks for Alzheimer’s disease image data classification</article-title>
          <source>Front. Aging Neurosci.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>28</volume>
          <page-range>823-870</page-range>
          <issue>5</issue>
          <year>2007</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lu</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Weng</surname>
              <given-names>Q.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/01431160600746456.</pub-id>
          <article-title>A survey of image classification methods and techniques for improving classification performance</article-title>
          <source>Int. J. Remote Sens.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>26</volume>
          <page-range>1650025</page-range>
          <issue>07</issue>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ortiz</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Munilla</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Górriz</surname>
              <given-names>J. M.</given-names>
            </name>
            <name>
              <surname>Ramírez</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1142/S0129065716500258.</pub-id>
          <article-title>Ensembles of deep learning architectures for the early diagnosis of the Alzheimer’s disease</article-title>
          <source>Int. J. Neural Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>419</volume>
          <page-range>126-135</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Poloni</surname>
              <given-names>K. M.</given-names>
            </name>
            <name>
              <surname>de Oliveira</surname>
              <given-names>I. A. D.</given-names>
            </name>
            <name>
              <surname>Tam</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Ferrari</surname>
              <given-names>R. J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neucom.2020.07.102.</pub-id>
          <article-title>Brain MR image classification for Alzheimer’s disease diagnosis using structural hippocampal asymmetrical attributes from directional 3-D log-Gabor filter responses</article-title>
          <source>Neurocomputing</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>1-8</page-range>
          <issue>6</issue>
          <year>2012</year>
          <person-group person-group-type="author">
            <name>
              <surname>Radha</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Tech</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Comparison of contrast stretching methods of image enhancement techniques for acute leukemia images</article-title>
          <source>Int. J. Eng. Res. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shocher</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Feinstein</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Haim</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Irani</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2006.11120.</pub-id>
          <article-title>From discrete to continuous convolution layers</article-title>
          <source>arXiv preprint arXiv:2006.11120</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>529-537</page-range>
          <year>2022</year>
          <publisher-name>Singapore: Springer.</publisher-name>
          <person-group person-group-type="author">
            <name>
              <surname>Singh</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kharkar</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Priyanka</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Parvartikar</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-981-16-7952-0_50.</pub-id>
          <article-title>Alzheimer’s disease detection using deep learning-CNN</article-title>
          <source>, https://doi.org/10.1007/978-981-16-7952-0_50.</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>689-698</page-range>
          <issue>7</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>So</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Madusanka</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Choi</surname>
              <given-names>H. K.</given-names>
            </name>
            <name>
              <surname>Choi</surname>
              <given-names>B. K.</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>H. G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.2174/1573405615666190404163233.</pub-id>
          <article-title>Deep learning for Alzheimer’s disease classification using texture features.</article-title>
          <source>Curr. Med. Imaging Rev.</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>732-735</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tariq</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Shah</surname>
              <given-names>S. K.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/BIBM47256.2019.8983071.</pub-id>
          <article-title>Lung disease classification using deep convolutional neural network</article-title>
          <source>, https://doi.org/10.1109/BIBM47256.2019.8983071.</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>170</volume>
          <page-range>578-595</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tonin</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Tao</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Patrinos</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Suykens</surname>
              <given-names>J. A. K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neunet.2023.11.045.</pub-id>
          <article-title>Deep Kernel Principal Component Analysis for multi-level feature learning</article-title>
          <source>Neural Netw.</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>19-24</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Vasudevan</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Anderson</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Gregg</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ASAP.2017.7995254.</pub-id>
          <article-title>Parallel multi channel convolution using general matrix multiplication</article-title>
          <source>, https://doi.org/10.1109/ASAP.2017.7995254.</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <volume>63</volume>
          <page-range>101694</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wen</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Thibeau-Sutre</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Diaz-Melo</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Samper-González</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Routier</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Bottani</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Dormont</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Durrleman</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Burgos</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Colliot</surname>
              <given-names>O.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.media.2020.101694.</pub-id>
          <article-title>Convolutional neural networks for classification of Alzheimer's disease: Overview and reproducible evaluation</article-title>
          <source>Med. Image Anal.</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <volume>105</volume>
          <page-range>012002</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Widodo</surname>
              <given-names>H. B.</given-names>
            </name>
            <name>
              <surname>Soelaiman</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ramadhani</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Supriyanti</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1088/1757-899X/105/1/012002.</pub-id>
          <article-title>Calculating contrast stretching variables in order to improve dental radiology image quality</article-title>
          <source>IOP Conf. Ser.: Mater. Sci. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>611-629</page-range>
          <issue>4</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yamashita</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Nishio</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Do</surname>
              <given-names>R. K. G.</given-names>
            </name>
            <name>
              <surname>Togashi</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s13244-018-0639-9.</pub-id>
          <article-title>Convolutional neural networks: An overview and application in radiology</article-title>
          <source>Insights Imaging</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>