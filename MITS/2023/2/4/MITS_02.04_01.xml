<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">MITS</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Mechatronics and Intelligent Transportation Systems</journal-title>
        <abbrev-journal-title abbrev-type="issn">Mechatron. Intell Transp. Syst.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">MITS</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-0218</issn>
      <issn publication-format="print">2958-020X</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-WHAGVUCt-pYjboTK1NSZJ7XrWJVAzUkc</article-id>
      <article-id pub-id-type="doi">10.56578/mits020401</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Advanced Vehicle Detection and License Plate Recognition via the Kanade-Lucas-Tomasi Technique</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-7064-5852</contrib-id>
          <name>
            <surname>Nyati</surname>
            <given-names>Egina</given-names>
          </name>
          <email>nyatie@staff.msu.ac.zw</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2543-0854</contrib-id>
          <name>
            <surname>Mahlalela</surname>
            <given-names>John Sabelo</given-names>
          </name>
          <email>jmahlalela@uniswa.sz</email>
        </contrib>
        <aff id="aff_1">Fuels and Energy Department, Midlands State University, ZW170407 Gweru, Zimbabwe</aff>
        <aff id="aff_2">Electrical and Electronic Engineering Department, University of Eswatini, M200 Kwaluseni, Eswatini</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>12</day>
        <month>11</month>
        <year>2023</year>
      </pub-date>
      <volume>2</volume>
      <issue>4</issue>
      <fpage>191</fpage>
      <lpage>200</lpage>
      <page-range>191-200</page-range>
      <history>
        <date date-type="received">
          <day>17</day>
          <month>09</month>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <day>01</day>
          <month>11</month>
          <year>2023</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2023 by the author(s)</copyright-statement>
        <copyright-year>2023</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>The optimization of traffic flow, enhancement of safety measures, and minimization of emissions in intelligent transportation systems (ITS) pivotally depend on the Vehicle License Plate Recognition (VLPR) technology. Challenges predominantly arise in the precise localization and accurate identification of license plates, which are critical for the applicability of VLPR across various domains, including law enforcement, traffic management, and both governmental and private sectors. Utilization in electronic toll collection, personal security, visitor management, and smart parking systems is commercially significant. In this investigation, a novel methodology grounded in the Kanade-Lucas-Tomasi (KLT) algorithm is introduced, targeting the localization, segmentation, and recognition of characters within license plates. Implementation was conducted utilizing MATLAB software, with grayscale images derived from both still cameras and video footage serving as the input. An extensive evaluation of the results revealed an accuracy of 99.267%, a precision of 100%, a recall of 99.267%, and an F-Score of 99.632%, thereby surpassing the performance of existing methodologies. The contribution of this research is significant in addressing critical challenges inherent in VLPR systems and achieving an enhanced performance standard.</p></abstract>
      <kwd-group>
        <kwd>Intelligent transportation system (ITS)</kwd>
        <kwd>MATLAB</kwd>
        <kwd>Vehicle detection</kwd>
        <kwd>Kanade-Lucas-Tomasi (KLT) algorithm</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="2"/>
        <fig-count count="9"/>
        <table-count count="2"/>
        <ref-count count="24"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>The transformative impact of technological advancements on numerous facets of daily life is undeniable, and the realm of transportation management stands as a prime example [<xref ref-type="bibr" rid="ref_1">1</xref>]. Within the ambit of ITS, the development of VLPR systems has been identified as crucial for achieving optimal traffic management and bolstering security measures. These systems are adept at capturing and deciphering vehicle license plate numbers, thereby offering a broad spectrum of applications encompassing access control in parking lots, crime prevention, and traffic analysis. The origins of VLPR can be traced back to the 1970s, marking the commencement of efforts to automate the reading of license plates [<xref ref-type="bibr" rid="ref_2">2</xref>]. Yet, it was the advent of digital cameras and image processing techniques that propelled the widespread adoption of VLPR systems. Early iterations of these systems predominantly employed rule-based approaches, utilizing manually crafted features to discern license plate characters [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>]. These initial systems, however, demonstrated limitations in adapting to variances in lighting conditions, image quality, and license plate formats. A paradigm shift towards machine learning-based approaches for VLPR has been observed in recent years [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>]. Such algorithms, capable of learning from data, exhibit enhanced robustness to fluctuations in image quality and conditions. VLPR systems, it is acknowledged, occupy a central role across diverse sectors, finding application in scenarios ranging from toll collection and traffic enforcement to border control and vehicle tracking [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>]. The present research is committed to contributing to the evolution of VLPR technology, with the objective of forging a system characterized by accuracy, robustness, and efficiency [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>]. Emphasis will be placed on image enhancement, license plate detection, character segmentation, character recognition, and the optimization of the overall system.</p><p>This paper's structure is as follows: Section 2 provides a comprehensive review of the VLPR literature. In Section 3, the research methodology designed to address existing challenges in VLPR and improve its performance is outlined, with innovative techniques integrated to enhance accuracy, robustness, and efficiency. Empirical findings are presented in Section 4, where performance metrics, comparative studies, and practical insights are explored. The paper concludes in Section 5, emphasizing the contributions of this work to the advancement of VLPR systems and the assurance of safer and more efficient transportation networks.</p>
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p>The review encompasses an exploration of various methodologies pertinent to object tracking, with a particular emphasis on applications within traffic monitoring and surveillance contexts. Techniques such as video analytics, vehicle detection, and motion tracking are scrutinized. The evaluation encompasses a range of algorithms, including ne-Class Support Vector Machine (OC-SVM) and Convolutional Neural Network (CNN)-based approaches, with a focus on augmenting accuracy and mitigating false alarm occurrences. The overarching aim is articulated as the enhancement of object tracking efficacy in real-time scenarios, inclusive of challenging environmental conditions. In the work of Velazquez-Pupo et al. [<xref ref-type="bibr" rid="ref_12">12</xref>], a stationary camera is utilized in a video analytics context, serving multifarious functions such as vehicle detection, occlusion handling, vehicle counting, tracking, and classification. Within this context, the application of OC-SVM with an RBF Kernel is highlighted, having demonstrated superior performance, particularly in the classification of midsize vehicles, yielding a F-measure of 98.190% and 99.051% respectively. It is underscored that SVM is acknowledged as the optimal classifier in this scenario. Furthermore, the research conducted by Qu et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] is brought into focus, advocating for the implementation of an accurate moving vehicle detector. This encompasses the incorporation of techniques such as candidate target recognition, CNN-based vehicle screening, and the application of motion sensors with image normalization for real-time scenarios, aiming for a high detection rate. Empirical studies employing diverse datasets underscore the effectiveness of moving vehicle detection, achieving up to 90% detection performance for automobiles, while maintaining an average false alarm rate below 10%.</p><p>In the work presented by Sarcevic and Pletl [<xref ref-type="bibr" rid="ref_14">14</xref>], a novel technique has been introduced for the filtration of false alarms. Regulations have been constructed separately, based on various data types derived from the signals, serving as the foundation for the filtration process. The parameters exerting the most significant influence were subjected to independent examination across each data type. Subsequently, these parameters were amalgamated into sophisticated algorithms to yield more precise outcomes. Optimization of the model parameters was achieved through the application of evolutionary algorithms. Results garnered from this approach indicate that 97% of false detections could be successfully eliminated, with a negligible loss of 0.3% in accurate detection systems, when rules are meticulously crafted. It was observed that even the application of a singular parameter could facilitate this process.</p><p>In a separate study conducted by Guo et al. [<xref ref-type="bibr" rid="ref_15">15</xref>], an augmented Single Shot MultiBox Detector (SSD) method has been proposed, aimed at addressing the shortcomings associated with low accuracy and missing detections in existing SSD methodologies for object tracking. The backbone of the proposed SSD network is ResNet50, selected for its capability to extract intricate details pertaining to vehicle features. The Feature Fusion Model, designed to enhance the accuracy of small target vehicle recognition, amalgamates positional data from shallow features with semantic information from feature representation. The incorporation of a Squeeze-and-Excitation (SE) block within the feature extraction layer further augments the model’s performance, enabling more comprehensive feature extraction and a reevaluation of the channel's significance. Experimental findings attest to the efficacy of the modified approach, as evidenced by an average accuracy of 83.09% on a dataset comprising home-built vehicles, surpassing the accuracy of the preceding algorithm by 3.23%. The work of Ma et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] introduces the Partial Anchors based Detection Network (PADeN), advocating for the identification and subsequent removal of incomplete anchors on vehicles to expedite the object detection process significantly. Contextual information is utilized within PADeN to discern and discard unnecessary anchors, enhancing the efficiency of object detection in images. The integration of the centerness mask branch into the network is highlighted as a pivotal enhancement to PADeN’s performance. Results from this study indicate a Mean Average Precision (mAP) of 76.9%, positioning PADeN as a superior method in comparison to previous object tracking methodologies.</p><p>In another study, Barnouti et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] propose the utilization of the KLT tracker in conjunction with the Two-Dimensional Principal Component Analysis (2DPCA) tracker for the purpose of monitoring and recognizing facial features within video sequences. The initial phase employs the Viola-Jones face identification technique for face detection in images or video sequences, followed by the application of the KLT method for face tracking. The KLT tracker maintains a long-term tracking capability of facial objects across successive frames, ensuring continuity even in instances of facial appearance and disappearance. The 2DPCA feature extraction method is utilized for noise reduction and enhancement of face recognition through a distance classifier. The proposed methodology undergoes validation using the Face94 database and webcam images. Experimental results confirm the efficacy of the Viola-Jones method in frontal face detection, the proficiency of the KLT system in face tracking across diverse webcam-shot videos, and the successful face recognition capabilities of 2DPCA in both the Face94 dataset and computer webcam video series.</p><p>In the work of Yue [<xref ref-type="bibr" rid="ref_18">18</xref>], a recursive tracking system oriented towards Augmented Reality (AR) for human motion tracking is introduced. This system leverages the positional relationship between consecutive frames, employing the KLT approach in tandem with Oriented Rapid and Rotated Brief (ORB) feature descriptors. The KLT tracking technique is applied to track the ORB feature descriptor, matching the first frame image and the reference image, while concurrently tracking the feature descriptor from the preceding frame in the current frame. This approach effectively mitigates the phenomenon of virtual object jitter. Comparative analysis reveals that the recursive tracking method surpasses the detection tracking strategy in terms of both speed and accuracy. Nevertheless, the study acknowledges the existence of challenges, particularly the inability to develop a feature tracking technique with enhanced accuracy and extended tracking longevity to diminish or mitigate the effects of cumulative error.</p><p>In the work conducted by Ramakrishnan et al. [<xref ref-type="bibr" rid="ref_19">19</xref>], an investigation into the optimization of the window size in the KLT tracking algorithm was presented, emphasizing the necessity of adapting the window size to mitigate the impact of distortions surrounding each feature point. The researchers introduced an adaptive window size technique, employing the iterations of the KLT algorithm as a metric to assess the quality of the tracks and consequently determine the optimal window sizes. Experimental results from well-established tracking datasets indicated that this adaptive approach exhibits enhanced robustness in comparison to the conventional fixed-window KLT, and offers a comparable level of robustness to the affine KLT, all the while achieving an average runtime speedup of seven-fold.</p><p>The system “Traffic Sensor” was introduced by Fernández et al. [<xref ref-type="bibr" rid="ref_20">20</xref>], employing deep learning techniques for the automatic detection and classification of vehicles on highways, utilizing a stationary, calibrated camera. The models were trained on a novel traffic image dataset, inclusive of images captured under sub-optimal lighting and weather conditions, as well as low-resolution images. The system is comprised of two principal modules: the first responsible for vehicle detection and classification, and the second for vehicle tracking. Extensive evaluation and comparison of various neural models were conducted for the first module, culminating in the selection of a network based on YOLOv3 or YOLOv4, trained on the new traffic dataset. The second module integrates a straightforward spatial association technique with the more intricate KLT tracker for the tracking of moving vehicles. Validation of the system was undertaken through numerous tests on challenging traffic videos, demonstrating the system's capability to effectively and real-time detect, track, and classify vehicles on highways.</p><p>Yin et al. [<xref ref-type="bibr" rid="ref_21">21</xref>] detailed the development of an optical flow target tracking system based on the KLT algorithm, implemented on the OpenCV platform and evaluated in the context of a water pipeline intelligent inspection competition. The technique leverages the optical flow method, aiming to achieve high detection certainty and rapid operational speed for frame differentiation, with a particular focus on underwater target detection and localization. The system ensures the stable control error of an underwater vehicle's motion through the application of incremental Proportional-Integral-Derivative (PID) control.</p><p>Several limitations have been identified in the prevailing state-of-the-art algorithm employed for the tasks of vehicle detection and tracking. These constraints are primarily attributed to the algorithm’s inherent complexity, its operation within a confined frequency range for feature extraction, and its reliance on the minimum enclosing rectangle (MER) as a mechanism for object detection. The adoption of a SVM for the task of classification, particularly in scenarios characterized by high traffic volumes and noisy datasets, has been observed to yield suboptimal performance [<xref ref-type="bibr" rid="ref_22">22</xref>], [<xref ref-type="bibr" rid="ref_23">23</xref>]. The algorithm’s effectiveness is further compromised by its dependence on a fixed threshold value, a factor that serves to impede its adaptive capabilities.</p><p>The proposed approach distinguishes itself through several innovative facets:</p><p>•The employment of Haar features in conjunction with the KLT algorithm is central to the development of a vehicle detection and tracking algorithm, which is anticipated to demonstrate both computational efficiency and robustness.</p><p>•A deep learning model is integrated with the aim of enhancing the accuracy of the proposed algorithm, particularly when applied to extensive datasets and those characterized by the presence of noise.</p><p>•The introduction of a novel thresholding technique is proposed, with the objective of rendering the algorithm less susceptible to variations in threshold values.</p><p>In alignment with these innovative aspects, the study sets forth several key research objectives:</p><p>•A methodological framework is to be established for the processing and analytical examination of real-time video data, with a particular focus on the accurate localization of vehicle license plates. This effort is expected to significantly contribute to the location and retrieval of lost automobiles.</p><p>•The capabilities of Haar features and the KLT algorithm are to be harnessed for the detection and tracking of vehicles within video streams.</p><p>•A rigorous comparative analysis is planned, wherein the proposed methodology will be evaluated against existing approaches. This evaluation will utilize a comprehensive set of performance metrics, including but not limited to accuracy, efficiency, recall, and precision, ensuring a thorough assessment of the methodologies in question.</p>
    </sec>
    <sec sec-type="">
      <title>3. Methodology</title>
      <p>To fulfill the established research objectives, a novel integration of Haar features and the KLT algorithm is introduced, enhancing the vehicle detection and tracking process. Haar features are utilized for their capacity to robustly identify distinctive object characteristics, while the KLT algorithm is employed to facilitate object tracking across frames within real-time video streams [<xref ref-type="bibr" rid="ref_15">15</xref>], [<xref ref-type="bibr" rid="ref_24">24</xref>]. The objective of employing these techniques is to augment both accuracy and efficiency, thereby mitigating the limitations identified in the extant algorithm.</p>
      
        <sec>
          
            <title>3.1. Proposed methods</title>
          
          <p>In the framework of the study, the following methodologies are employed:</p><p>Haar Features</p><p>Characterized by their simplistic rectangular shape, Haar features serve to represent the edges and corners of objects within images. Their computational efficiency in extraction, coupled with their demonstrated efficacy in diverse image processing tasks, including object detection and tracking, renders them a valuable tool in this context.</p><p>KLT Algorithm</p><p>The KLT algorithm, grounded in optical flow principles, is employed for feature tracking within video sequences. It operates on the premise that the brightness of a pixel remains invariant over time, thus facilitating the tracking of feature movement.</p><p>Deep Learning</p><p>Artificial neural networks form the basis of deep learning, a subset of machine learning. This technique has shown substantial effectiveness across a myriad of image processing tasks, including those pertinent to object detection and tracking.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Workflow</title>
          
          <p>The methodology encompasses several pivotal steps, delineated in <xref ref-type="fig" rid="fig_1">Figure 1</xref>:</p><p>Step 1: Real-Time Video Data Processing: Vehicle license plates are located and extracted through the processing of input video data in real time. </p><p>Step 2: Haar Feature-Based Vehicle Detection: Vehicles within video frames are detected with precision, utilizing Haar features. </p><p>Step 3: KLT-Based Vehicle Tracking: Subsequent to detection, vehicles are continuously monitored across consecutive frames through the application of the KLT algorithm. </p><p>Step 4: Performance Evaluation: The efficacy of the proposed methodology is rigorously assessed through comparative analysis with pre-existing methods. Accuracy, efficiency, recall, and precision are employed as the key performance indicators in this evaluation.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>Workflow of the proposed method</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_cZHIhGgD2YACWG_N.png"/>
            </fig>
          
          <p>The incorporation of Haar features and the KLT algorithm is substantiated by their proven efficacy in this domain. Haar features excel in robustly discerning vehicles within video frames, whereas the KLT algorithm guarantees seamless tracking of the vehicles once identified, across various frames. The integration of an adaptive thresholding technique further refines the system’s adaptability and overall performance, promising enhanced accuracy, particularly in scenarios characterized by high traffic volumes and prevalent noise.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Results and discussion</title>
      <p>In the current urban milieu, the escalating vehicular population necessitates advanced VLPR systems, as instances of vehicle theft, traffic violations, and unauthorized access to restricted areas are witnessing a surge. This manuscript introduces a novel methodology leveraging the KLT) algorithm, meticulously designed for the localization, segmentation, and recognition of characters on license plates. The method is delineated across key phases: detection of the number plate, segmentation of characters, and subsequent character recognition. A thorough comparative analysis, juxtaposed with extant methodologies, is presented herein.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Plate detection of car</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_NzUakhVQERWcI6St.png"/>
        </fig>
      
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>License plate detection (KLT+RCNN)</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_2IxgyJytVTmXBHG4.png"/>
        </fig>
      
      <p> <xref ref-type="fig" rid="fig_2">Figure 2</xref> shows a car with a plate. The plate is rectangular and white. It has black lettering. The car is parked in front of a garage. Whereas <xref ref-type="fig" rid="fig_3">Figure 3</xref> shows the detection of the car plate using KLT and R-CNN. KLT is a feature tracking algorithm that tracks the movement of the plate over time. R-CNN is an object detection algorithm that detects the location of the plate in the image. The efficacy of the proposed approach is rigorously evaluated through a plethora of performance metrics, including but not limited to precision, accuracy, recall, and the F1-Score. These metrics collectively afford a holistic evaluation of the model’s performance capabilities. Furthermore, quantitative insights pertaining to processing time, speed, and computational complexity are elucidated, providing a comprehensive overview of the system's operational efficiency.</p>
      
        <sec>
          
            <title>4.1. Precision</title>
          
          <p>Precision, a prevalent metric in assessing the efficacy of text classification and information retrieval systems, quantifies the proportion of retrieved items that are pertinent to the user's query. </p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="mvx755rbw6">
                <mml:mtext> Precision </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mtext> The number of accurately predicted positive instances </mml:mtext>
                  <mml:mtext> The total count of positive instances in the dataset </mml:mtext>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Comparison of precision</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_hSe1hFY8BnTNEdyV.png"/>
            </fig>
          
          <p> <xref ref-type="fig" rid="fig_4">Figure 4</xref> shows a bar chart comparing the precision of existing and proposed license plate detection methods. The KLT+R-CNN method has the highest precision of the two methods as seen from the figure. This is because KLT+R-CNN use a combination of feature tracking and object detection to identify license plates. Feature tracking helps to identify the plate's location in the image, while object detection helps to confirm whether the object identified is actually a license plate.</p>
        </sec>
      
      
        <sec>
          
            <title>4.2. Accuracy</title>
          
          <p>Accuracy stands as the predominant metric for gauging the performance of a classification model, being derived from the ratio of correctly classified instances to the aggregate sum of instances. On the other hand, the error rate offers an alternative measure of classification efficacy, computed as the quotient of incorrectly classified instances and the total number of instances classified correctly. <xref ref-type="fig" rid="fig_5">Figure 5</xref> compares the accuracy of different plate detection methods. The blue bar represents the average accuracy of the proposed algorithm, whereas the orange bar represents the average accuracy of the existing CNN model. </p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="mxqd0sksjo">
                <mml:mtext> Accuracy </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mtext> The count of positive instancescorrectly predicted </mml:mtext>
                  <mml:mtext> The total count of positive predictions made </mml:mtext>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Comparison of accuracy</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_m99yfue8fRV6iBYI.png"/>
            </fig>
          
          <p>The performance of the proposed method is presented in <xref ref-type="table" rid="table_1">Table 1</xref>, which includes accuracy scores for both our method and existing algorithms. The accuracy is a measure of how much detection are correct. A higher accuracy score indicates that the algorithm is more likely to correctly identify license plates. As shown in the table, the proposed algorithm has the highest accuracy amongst all the other algorithms. This means that the proposed algorithm is more likely to correctly identify license plates than the other algorithms. </p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Accuracy performance of proposed method versus existing algorithms</title>
              </caption>
              <table><tr><th >Algorithm</th><th >Accuracy</th></tr><tr><td >ZF</td><td >0.94</td></tr><tr><td >VGG16</td><td >0.97</td></tr><tr><td >VGG-CNN-M-1024</td><td >0.96</td></tr><tr><td >ResNet101</td><td >0.94</td></tr><tr><td >ResNet50</td><td >0.97</td></tr><tr><td >OKM-CNN</td><td >0.98</td></tr><tr><td >Proposed</td><td >0.99267</td></tr></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>4.3. Recall</title>
          
          <p>It is deemed suitable when the primary objective lies in the minimization of false negatives. On certain occasions, the emphasis is placed on obtaining precise predictions for the positive class. <xref ref-type="fig" rid="fig_6">Figure 6</xref> compares the recall of different plate detection methods. As shown in the figure, the proposed algorithm has a recall of 90%, which is significantly higher than the recall of the existing algorithm, which is 80%. This suggests that the proposed algorithm is more likely to correctly identify plates in an image.</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="m8su563jlp">
                <mml:mtext> Recall </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mtext> No. of correctly predcted positive instances </mml:mtext>
                  <mml:mtext> The total count of positive instances in the dataset </mml:mtext>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Comparison of recall</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_Z_FHx1LnY9jrd7SK.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.4. F1-score</title>
          
          <p>The F-score, alternatively referred to as the F1-score, constitutes a critical metric employed in assessing a model’s performance in dataset analysis. It predominantly finds its application in the scrutiny of binary classification systems, responsible for assigning instances to either ‘positive’ or ‘negative’ classes. Fundamentally, the F-score functions as a balanced mechanism to amalgamate both the precision and recall of the model, being rigorously delineated as the harmonic mean of the model’s respective precision and recall metrics.</p>
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="mjof7e3x1a">
                <mml:mtext> F1-Score </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mo>∗</mml:mo>
                <mml:mn>2</mml:mn>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mtext> Pecision </mml:mtext>
                    <mml:mtext> Recall </mml:mtext>
                    <mml:mo>∗</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mtext> Pecision </mml:mtext>
                    <mml:mtext> Recall </mml:mtext>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>Comparison of F-score</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_Jff928ZOPTcd7ZT8.png"/>
            </fig>
          
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>Performance analysis of proposed work</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_rKebcF-mPnStI2yE.png"/>
            </fig>
          
          <p> <xref ref-type="fig" rid="fig_7">Figure 7</xref> shows the comparison of the model F-score across different feature sets. In this figure, the x-axis represents the different feature sets that were used to train the model, and the y-axis represents the F-score of the model. The blue line shows the F-score of the model that was trained on the full set of features, and the other lines show the F-score of the model that was trained on subsets of the features. As it can be seen, the F-score increases as the number of features increases. However, the increase in F-score starts to level off after a certain number of features. This suggests that there is a point at which adding more features does not significantly improve the performance of the model. <xref ref-type="fig" rid="fig_8">Figure 8</xref> provides a visual representation of the performance of the proposed method compared to the existing work. The figure shows that the proposed method consistently outperforms the existing work across all four parameters.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Parameters performance of proposed method versus existing algorithms</title>
              </caption>
              <table><tr><th >Parameters</th><th >Existing Work</th><th >Proposed Work</th></tr><tr><td >Accuracy</td><td >74.572</td><td >99.267</td></tr><tr><td >Recall</td><td >74.572</td><td >99.267</td></tr><tr><td >Precision</td><td >100</td><td >100</td></tr><tr><td >F-Score</td><td >85.434</td><td >99.632</td></tr></table>
            </table-wrap>
          
          <p><xref ref-type="table" rid="table_2">Table 2</xref> compares the performance of the proposed method to existing algorithms in terms of four key parameters: accuracy, recall, precision, and F1-score. In case of accuracy, the proposed method has an accuracy of 99.267%, while the existing work has an accuracy of 74.572%. This means that the proposed method is much better at correctly classifying instances than the existing work.</p><p>In case of recall, the proposed method has a recall of 99.267%, while the existing work has a recall of 74.572%. This means that the proposed method is also better at correctly classifying positive instances than the existing work. For precision, the proposed method has a precision of 100%, while the existing work has a precision of 100%. This means that the proposed method is very good at avoiding false positives, while the existing work has a similar performance.</p><p>F1-score is the harmonic mean of precision and recall. It is a more balanced measure of overall performance than either precision or recall alone. In this case, the proposed method has an F1-score of 99.632%, which is significantly higher than the F1-score of 85.434% for the existing work. This means that the proposed method is better at both correctly classifying positive instances and avoiding false positives.</p><p>Overall, the proposed method is significantly more accurate, has a higher recall, and has a higher F1-score. These results suggest that the proposed method is a more effective approach to the task of classification.</p>
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>
                <title>Snapshot of coding with extract LBP features</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_oMeKnzIxDXuesjTi.png"/>
            </fig>
          
          <p><xref ref-type="fig" rid="fig_9">Figure 9</xref> presents the performance metrics of the proposed KLT-based approach, revealing an impressive accuracy of 99.267%, a precision of 100%, a recall of 99.267%, and an F-Score of 99.632%. These results markedly outperform those achieved by existing techniques. The KLT method has demonstrated its robustness and precision in character localization and segmentation, culminating in enhanced accuracy for character recognition.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>The burgeoning population density worldwide necessitates efficacious methodologies for vehicle detection, crucial for traffic management optimization. In this study, a robust VLPR system has been elucidated, utilizing Raspberry Pi for video processing. This system adeptly identifies and extracts numerical information from vehicle license plates through a meticulously designed suite of methodologies and algorithms.</p><p>The feasibility of the VLPR system for practical implementation in traffic control and management has been established, with promising implications for enhancing law enforcement, traffic surveillance, and security measures. The KLT-based method proposed herein has demonstrated its capability to develop a vehicle detection and tracking algorithm that is not only computationally efficient but also robust and precise, addressing the limitations inherent in existing methodologies.</p><p>In conclusion, the current research has validated the effectiveness of the VLPR system, laying a solid groundwork for its practical and impactful applications. As continual refinements and expansions are made to this work, there is a potential for technology to play a pivotal role in surmounting contemporary challenges, further harnessing its capacity to contribute to societal advancement.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>38</volume>
          <page-range>13497-13504</page-range>
          <issue>11</issue>
          <year>2011</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sedighi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Vafadust</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2011.02.030</pub-id>
          <article-title>A new and robust method for character segmentation and recognition in license plate images</article-title>
          <source>Expert Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>39</volume>
          <page-range>163-172</page-range>
          <issue>2</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Türkyılmaz</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Kaçan</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.4218/etrij.17.0115.0766</pub-id>
          <article-title>License plate recognition system using artificial neural networks</article-title>
          <source>ETRI J.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>535-544</page-range>
          <issue>8</issue>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Asif</surname>
              <given-names>M. R.</given-names>
            </name>
            <name>
              <surname>Chun</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Hussain</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Fareed</surname>
              <given-names>M. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1049/iet-its.2016.0008</pub-id>
          <article-title>Multiple licence plate detection for Chinese vehicles in dense traffic scenarios</article-title>
          <source>IET Intell. Transp. Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <page-range>4805-4808</page-range>
          <issue>6</issue>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jethwa</surname>
              <given-names>A. H.</given-names>
            </name>
            <name>
              <surname>Sheffer</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Thompson</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Vehicle tracking system using GPS and GSM modem-a review</article-title>
          <source>Int. J. Recent Sci. Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Ambardekar</surname>
              <given-names>A. A.</given-names>
            </name>
          </person-group>
          <source>Efficient Vehicle Tracking and Classification for an Automated Traffic Surveillance System</source>
          <publisher-name>R. University of Nevada</publisher-name>
          <year>2017</year>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>51</volume>
          <page-range>485-497</page-range>
          <issue>3</issue>
          <year>2012</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mazinan</surname>
              <given-names>A. H.</given-names>
            </name>
            <name>
              <surname>Amir-Latifi</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.isatra.2012.02.002</pub-id>
          <article-title>Applying mean shift, motion information and Kalman filtering approaches to object tracking</article-title>
          <source>ISA Trans.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>963-970</page-range>
          <issue>9</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Adinarayana</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Anil</surname>
              <given-names>N. C.</given-names>
            </name>
          </person-group>
          <article-title>The study exploration towards side friction influences by traffic performance measures on roads</article-title>
          <source>Int. J. Sci. Eng. Adv.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>322-327</page-range>
          <issue>5</issue>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bodkhe</surname>
              <given-names>A. P.</given-names>
            </name>
            <name>
              <surname>S. A. Nirmal</surname>
              <given-names/>
            </name>
            <name>
              <surname>Thakre</surname>
              <given-names>S. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.15680/ijircce.2015.0301015</pub-id>
          <article-title>A literature review on different models for human and vehicle tracking</article-title>
          <source>Int. J. Innov. Res. Comput. Commun. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>1-11</page-range>
          <year>2011</year>
          <person-group person-group-type="author">
            <name>
              <surname>Nayyar</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Integrated security solution for moving object tracking system</article-title>
          <source>Int. J. Eng. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>4</volume>
          <page-range>1852-1855</page-range>
          <issue>3</issue>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sankpal</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kadam</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>Land vehicle tracking system</article-title>
          <source>Int. J. Sci. Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>4</volume>
          <page-range>1-7</page-range>
          <issue>3</issue>
          <year>2013</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chaple</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Paygude</surname>
              <given-names>S. S.</given-names>
            </name>
          </person-group>
          <article-title>Vehicle detection and tracking from video frame sequence</article-title>
          <source>Int. J. Sci. Eng. Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>18</volume>
          <page-range>374</page-range>
          <issue>2</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Velazquez-Pupo</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Sierra-Romero</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Torres-Roman</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Shkvarko</surname>
              <given-names>Y.V.</given-names>
            </name>
            <name>
              <surname>Santiago-Paz</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Gomez-Gutierrez</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Robles-Valdez</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>F. Hermosillo-Reynoso</surname>
              <given-names/>
            </name>
            <name>
              <surname>Romero-Delgado</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s18020374</pub-id>
          <article-title>Vehicle detection with occlusion handling, tracking, and OC-SVM classification: A high performance vision-based system</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <page-range>225-229</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Qu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICCAR.2016.7486730</pub-id>
          <article-title>Moving vehicle detection with convolutional networks in UAV videos</article-title>
          <source>in 2nd Int. Conf. Control, Automat. Robot. (ICCAR), Hong Kong, China</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <page-range>277-282</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sarcevic</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Pletl</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/SISY.2018.8524716</pub-id>
          <article-title>False detection filtering method for magnetic sensor-based vehicle detection systems</article-title>
          <source>in 16th IEEE Int. Symp. Intell. Syst. Informatics (SISY), Subotica, Serbia</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <page-range>466-468</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Guo</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Qin</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICCCBDA51879.2021.9442550</pub-id>
          <article-title>Target detection of forward vehicle based on improved SSD</article-title>
          <source>in 6th IEEE Int. Conf. Cloud Comput. Big Data Anal. (ICCCBDA), Chengdu, China</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <page-range>288-291</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ma</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/IGARSS39084.2020.9323956</pub-id>
          <article-title>Vehicle detection with partial anchors in remote sensing images</article-title>
          <source>in 2020 IEEE Int. Geosci. Remote Sens. Symp. (IGARSS), Waikoloa, HI, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <page-range>24-29</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Barnouti</surname>
              <given-names>N. H.</given-names>
            </name>
            <name>
              <surname>Al-Mayyahi</surname>
              <given-names>M. H. N.</given-names>
            </name>
            <name>
              <surname>Al-Dabbagh</surname>
              <given-names>S. S. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICOASE.2018.8548818</pub-id>
          <article-title>Real-time face tracking and recognition system using Kanade-Lucas-Tomasi and two-dimensional principal component analysis</article-title>
          <source>in 2018 Int. Conf. Adv. Sci. Eng. (ICOASE), Duhok, Iraq</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>18</volume>
          <page-range>357-368</page-range>
          <issue>2</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yue</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11554-020-01030-6</pub-id>
          <article-title>Human motion tracking and positioning for augmented reality</article-title>
          <source>J. Real-Time Image Process</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <page-range>355-367</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ramakrishnan</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Srikanthan</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Lam</surname>
              <given-names>S. K.</given-names>
            </name>
            <name>
              <surname>Tulsulkar</surname>
              <given-names>G. R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-319-29451-3_29</pub-id>
          <article-title>Adaptive window strategy for high-speed and robust KLT feature tracker</article-title>
          <source>in Image and Video Technology: 7th Pacific-Rim Symp., PSIVT 2015, Auckland, New Zealand, Auckland, New Zealand</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>2021</volume>
          <page-range>4632353</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Fernandez</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>J. M. Canas</surname>
              <given-names/>
            </name>
            <name>
              <surname>V. Fernandez</surname>
              <given-names/>
            </name>
            <name>
              <surname>Paniego</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2021/4632353</pub-id>
          <article-title>Robust real-time traffic surveillance with deep learning</article-title>
          <source>Comput. Intell. Neurosci.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <page-range>25-34</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yin</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Qin</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Bi</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Fan</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-981-19-8915-5_3</pub-id>
          <article-title>Underwater target tracking algorithm based on optical flow</article-title>
          <source>in China Intelligent Networked Things Conference, Urumqi, China</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <page-range>6-11</page-range>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mahmood</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Khattak</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>S. U.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>L. T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/FIT.2015.13</pub-id>
          <article-title>Automatic vehicle detection and driver identification framework for secure vehicle parking</article-title>
          <source>in 2015 13th International Conference on Frontiers of Information Technology (FIT), Islamabad, Pakistan</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <page-range>224-227</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Seenouvong</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Watchareeruetai</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Nuthong</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Khongsomboon</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Ohnishi</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/KST.2016.7440510</pub-id>
          <article-title>A computer vision based vehicle detection and counting system</article-title>
          <source>in 2016 8th International Conference on Knowledge and Smart Technology (KST), Chiang Mai, Thailand</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <page-range>1-5</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kalyan</surname>
              <given-names>S. S.</given-names>
            </name>
            <name>
              <surname>Pratyusha</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Nishitha</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Ramesh</surname>
              <given-names>T. K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/INOCON50539.2020.9298188</pub-id>
          <article-title>Vehicle detection using image processing</article-title>
          <source>in 2020 IEEE International Conference for Innovation in Technology (INOCON), Bangluru, India</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>