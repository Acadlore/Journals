<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IJKIS</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>International Journal of Knowledge and Innovation Studies</journal-title>
        <abbrev-journal-title abbrev-type="issn">Int J. Knowl. Innov Stud.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IJKIS</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">3005-6101</issn>
      <issn publication-format="print">3005-6098</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-CnPvqwZstltxcDotThB97M_gnCE-7Zpa</article-id>
      <article-id pub-id-type="doi">10.56578/ijkis030202</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Selective Image Segmentation Through Fuzzy Einstein–Dombi Operators and Level Set Energy Minimization</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0006-4939-9130</contrib-id>
          <name>
            <surname>Ahmad</surname>
            <given-names>Uzair</given-names>
          </name>
          <email>uzairahmad@awkum.edu.pk</email>
        </contrib>
        <aff id="aff_1">Department of Mathematics, Abdul Wali Khan University, 23200 Mardan, Pakistan</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>29</day>
        <month>05</month>
        <year>2025</year>
      </pub-date>
      <volume>3</volume>
      <issue>2</issue>
      <fpage>74</fpage>
      <lpage>88</lpage>
      <page-range>74-88</page-range>
      <history>
        <date date-type="received">
          <day>31</day>
          <month>03</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>24</day>
          <month>05</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Accurate selective image segmentation continues to pose substantial challenges, particularly under conditions of noise interference, intensity inhomogeneity, and irregular object boundaries. To address these complexities, a novel framework is introduced that integrates fuzzy Einstein–Dombi (ED) operators with level set energy minimization, guided by marker-based initialization. The proposed approach departs from traditional intensity-driven models by jointly incorporating intensity, texture, and gradient-based features, thereby facilitating improved boundary delineation and enhanced regional homogeneity. A spatially adaptive regularization term has been embedded within the level set formulation to reinforce contour stability and robustness in the presence of artefacts and signal degradation. The fuzzy ED operators enable nuanced fusion of multiple features through non-linear aggregation, yielding a more expressive and resilient energy functional. In contrast to conventional segmentation schemes, the developed method achieves superior convergence and delineation accuracy, particularly within complex grayscale and noisy medical image datasets. Experimental validation has been conducted across a range of imaging conditions, with performance quantitatively assessed using established metrics, including segmentation accuracy (0.95), intersection over union (IoU: 0.89), and Dice similarity coefficient (DSC: 0.94). These results demonstrate statistically significant improvements over comparative models. Additionally, qualitative evaluations reveal enhanced contour fidelity and resistance to local intensity fluctuations. The methodological simplicity and computational efficiency of the framework render it highly suitable for real-time applications in medical imaging diagnostics, object detection, and related image analysis tasks. By offering a robust, interpretable, and generalizable solution, this work establishes a new reference point for selective image segmentation under non-ideal conditions, and paves the way for further exploration of fuzzy operator integration within variational segmentation paradigms.</p></abstract>
      <kwd-group>
        <kwd>Image processing</kwd>
        <kwd>Selective segmentation</kwd>
        <kwd>Fuzzy set theory</kwd>
        <kwd>Einstein–Dombi (ED) operators</kwd>
        <kwd>Level set evolution</kwd>
        <kwd>Medical image analysis</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="1"/>
        <fig-count count="4"/>
        <table-count count="2"/>
        <ref-count count="21"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Image segmentation is a fundamental task in computer vision, playing a pivotal role in applications such as object detection, medical imaging, autonomous driving, and content-based image analysis [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>]. The process involves partitioning an image into meaningful and coherent regions based on attributes like color, intensity, texture, and shape. This step is crucial for isolating relevant features or objects, thereby enabling more advanced image analysis. However, achieving accurate and reliable segmentation in real-world scenarios is challenging due to factors such as complex backgrounds, varying lighting conditions, overlapping objects, and noise [<xref ref-type="bibr" rid="ref_4">4</xref>]. In critical domains like medical imaging, these challenges directly impact clinical outcomes. For example, noise and intensity inhomogeneity can obscure tumor boundaries in MRI scans or CT images, leading to inaccurate diagnoses and treatment planning. Similarly, in remote sensing, intensity variations can result in misclassification of land cover, affecting environmental monitoring and urban planning.</p><p>Conventional segmentation techniques, including thresholding, edge detection, and clustering, rely on deterministic criteria, which often make them susceptible to noise and ineffective in handling smooth intensity transitions [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>]. For instance, edge detection methods excel at identifying regions with high gradients but perform poorly in noisy environments or when edges are blurred. Clustering-based approaches, such as k-means, divide an image into k clusters by minimizing intra-cluster variance. However, they are prone to local minima and are highly sensitive to initialization. While these methods are simple, they struggle to address the complexities of real-world images, where boundaries between regions are often gradual rather than sharp. In medical scenarios, this limitation may result in under-segmentation of organs or over-segmentation of healthy tissues, both of which compromise the reliability of automated diagnostic tools.</p><p>To overcome these limitations, more sophisticated techniques have been developed. Region-growing algorithms [<xref ref-type="bibr" rid="ref_4">4</xref>], [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>] and graph-based methods [<xref ref-type="bibr" rid="ref_9">9</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>] incorporate spatial information to improve segmentation. Region-growing approaches start from seed points and expand regions based on homogeneity criteria, but their performance heavily depends on accurate seed placement and can be adversely affected by noise. Graph-based techniques, such as graph cuts, partition images by minimizing a global energy function. Although effective, these methods often require extensive parameter tuning and significant computational resources, which can hinder their practical use. Such computational demands may not be suitable for time-sensitive medical applications, such as intraoperative image analysis, where fast and precise segmentation is required.</p><p>The rise of machine learning has introduced supervised and unsupervised learning techniques for image segmentation. Convolutional neural networks (CNNs) have achieved state-of-the-art results in semantic segmentation by learning hierarchical features from data [<xref ref-type="bibr" rid="ref_11">11</xref>], [<xref ref-type="bibr" rid="ref_12">12</xref>]. Architectures like fully convolutional networks (FCNs) and U-Net have shown remarkable success in fields such as medical imaging and autonomous driving. However, these methods depend heavily on large annotated datasets and substantial computational power, which may not always be available [<xref ref-type="bibr" rid="ref_13">13</xref>]. Additionally, the lack of interpretability in deep learning models poses a challenge in critical applications like medical imaging, where understanding the decision-making process is essential [<xref ref-type="bibr" rid="ref_14">14</xref>]. This lack of transparency can be problematic for radiologists and clinicians, who need to validate and trust the automated segmentation results before making medical decisions.</p><p>In response to these challenges, fuzzy logic-based segmentation has emerged as a promising approach for handling uncertainty and imprecision in image data [<xref ref-type="bibr" rid="ref_15">15</xref>], [<xref ref-type="bibr" rid="ref_16">16</xref>]. Fuzzy logic provides a mathematical framework to model gradual transitions between regions in an image. Unlike traditional methods that use crisp thresholds, fuzzy segmentation assigns a degree of membership to each pixel, indicating its likelihood of belonging to a specific region. For example, the fuzzy c-means (FCM) algorithm clusters pixels based on intensity while considering spatial coherence [<xref ref-type="bibr" rid="ref_17">17</xref>]. However, FCM is sensitive to initialization and noise, prompting the development of robust variants like spatially constrained FCM.</p><p>Membership functions are central to fuzzy logic and play a key role in defining the relationship between pixel features and their corresponding regions. For instance, Gaussian membership functions model the degree of belongingness of a pixel <inline-formula>
  <mml:math id="mlfvgnyhxb">
    <mml:mi>p</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> based on its intensity <inline-formula>
  <mml:math id="m2fdlcnb4c">
    <mml:mi>I</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula>:</p>
      
        <disp-formula>
          <label>(1)</label>
          <mml:math id="ma7t7e3a9h">
            <mml:mi>μ</mml:mi>
            <mml:mi>I</mml:mi>
            <mml:mi>x</mml:mi>
            <mml:mi>y</mml:mi>
            <mml:mi>exp</mml:mi>
            <mml:mo>(</mml:mo>
            <mml:mo>(</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:mo>⁡</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mrow>
              <mml:mo>(</mml:mo>
              <mml:mo>−</mml:mo>
              <mml:mo>)</mml:mo>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mo>−</mml:mo>
                  <mml:mi>I</mml:mi>
                  <mml:mi>x</mml:mi>
                  <mml:mi>y</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:msup>
                    <mml:mo>)</mml:mo>
                    <mml:mn>2</mml:mn>
                  </mml:msup>
                </mml:mrow>
                <mml:mrow>
                  <mml:mn>2</mml:mn>
                  <mml:msup>
                    <mml:mi>σ</mml:mi>
                    <mml:mn>2</mml:mn>
                  </mml:msup>
                </mml:mrow>
              </mml:mfrac>
            </mml:mrow>
          </mml:math>
        </disp-formula>
      
      <p>where \(c\) represents the center of the intensity range, and \(\sigma\) controls the spread [<xref ref-type="bibr" rid="ref_18">18</xref>]. By adjusting these parameters, fuzzy membership functions can adapt to diverse image characteristics, making them well-suited for complex segmentation tasks. This adaptability is particularly valuable in medical images where soft tissues exhibit gradual intensity transitions that are difficult to segment with crisp threshold-based methods.</p><p>Recent advancements in fuzzy logic-based segmentation include hybrid models that combine fuzzy logic with other computational techniques. For example, integrating fuzzy logic with genetic algorithms and swarm intelligence has been explored to optimize membership functions and segmentation parameters. Additionally, fuzzy-rule-based systems have been developed to incorporate domain knowledge into the segmentation process, improving both accuracy and interpretability [<xref ref-type="bibr" rid="ref_19">19</xref>], [<xref ref-type="bibr" rid="ref_20">20</xref>]. Such hybrid approaches have shown promise in segmenting organs, lesions, and other anatomical structures in MRI and CT scans, where both accuracy and explainability are crucial.</p><p>In medical imaging, fuzzy logic has been successfully applied to segment structures like brain tissues and lesions, where intensity variations are subtle and boundaries are unclear. For instance, fuzzy region-growing techniques have been used to identify tumors in MRI images by leveraging both intensity and texture features [<xref ref-type="bibr" rid="ref_17">17</xref>]. In remote sensing, fuzzy logic-based methods have been employed to classify land cover types, addressing challenges posed by spectral similarities between classes. These real-world successes demonstrate the robustness of fuzzy approaches in scenarios where uncertainty and noise dominate, further motivating the development of enhanced fuzzy-based segmentation models.</p><p>Among the various segmentation tasks, selective segmentation focuses on extracting specific objects or regions while ignoring irrelevant background information. This task is particularly challenging in complex images where the target object may have weak boundaries, overlap with other objects, or be embedded in noisy backgrounds. Traditional segmentation methods, such as thresholding, edge detection, and region-growing, often struggle with these challenges. For example, thresholding methods may fail when the intensity distribution of the target object overlaps with the background, while edge detection methods may produce fragmented boundaries in noisy images. More advanced techniques, such as active contours and level set methods, have shown promise but still face limitations in handling heterogeneous regions and weak boundaries.</p><p>To address these challenges, we propose a novel image selective segmentation model that integrates ED operators, marker points, and level set methods. The proposed model leverages the flexibility of ED operators to combine multiple image features (e.g., intensity, texture, and gradient) into a unified representation, enabling accurate segmentation of complex regions. Marker points provide prior knowledge about the location of the target object, guiding the segmentation process and reducing the influence of background noise. Level set methods, on the other hand, allow the contour to adapt to complex shapes and topological changes, ensuring robust boundary detection.</p><p>The proposed model overcomes several limitations of existing segmentation methods. First, the use of ED operators provides a flexible framework for combining multiple features, addressing the challenge of heterogeneous regions. Unlike traditional methods that rely on a single feature (e.g., intensity or gradient), the proposed model integrates intensity, texture, and gradient information, leading to better region homogeneity and more accurate segmentation. Second, the incorporation of marker points ensures robustness to noise and weak boundaries. By providing prior knowledge about the target object, marker points guide the contour evolution process, reducing the risk of mis-segmentation due to background clutter or weak edges. Finally, the level set framework allows the contour to handle complex shapes and topological changes, overcoming the limitations of rigid models that assume simple geometries.</p><p>The proposed model integrates three fundamental components, each playing a crucial role in enhancing the accuracy and robustness of the segmentation process. These components include ED Operators, Marker Points, and Level Set Methods, which collectively contribute to an improved segmentation framework by effectively handling intensity variations, noise, and complex object boundaries.</p><p>• ED Operators</p><p>– These are fuzzy logic-based aggregation functions that combine multiple image features such as intensity, texture, and gradient in a nonlinear and flexible manner.</p><p>– They provide an effective mechanism for handling uncertainty and imprecision in image data.</p><p>– Einstein Product and Sum: Used to fuse multiple features while preserving essential details and suppressing noise.</p><p>– Dombi Operator: Controls the trade-off between different features by adjusting parameters, enabling adaptive fusion based on local image characteristics.</p><p>– Enhancement of Region Homogeneity: Ensures that similar regions are grouped effectively while maintaining clear object boundaries, leading to better region separation and contrast enhancement.</p><p>• Marker Points</p><p>– These serve as guiding cues for the segmentation process and can be manually selected or automatically detected.</p><p>– Providing Prior Knowledge: Offers initial information about the target object’s location, reducing ambiguity and improving boundary delineation.</p><p>– Facilitating Convergence: Helps in faster convergence of the segmentation process, reducing computational complexity.</p><p>• Level Set Method</p><p>– A geometric framework that evolves contours to capture object boundaries accurately.</p><p>– Implicit Representation: The contour is represented as a zero level of a higher dimensional function, allowing for smooth and topologically adaptable boundary evolution.</p><p>– Robustness to Noise and Occlusions: Can handle partial occlusions, intensity inhomogeneities, and complex shapes without requiring explicit contour initialization.</p><p>– Energy-Based Evolution: Driven by an energy functional that incorporates edge-based, region-based, and prior shape constraints, allowing for fine detail capture while maintaining global consistency.</p><p>In summary, the proposed model offers a robust and flexible solution for image selective segmentation, addressing the limitations of existing methods in handling complex images with heterogeneous regions, weak boundaries, and noise. By combining the strengths of ED operators, marker points, and level set methods, the proposed model achieves superior segmentation accuracy and robustness, making it suitable for a wide range of applications in computer vision and image analysis.</p>
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p><span style="color: rgb(0, 0, 0); font-family: Times New Roman, sans-serif">Mohamed et al. [<xref ref-type="bibr" rid="ref_21">21</xref>] proposed the Total Variation Selective Segmentation (TVSS)-based Active Contour Model (TV-SSM), which integrates a Total Variation (TV) regularizer, a distance function, and local image fitting energy to enhance segmentation performance, particularly for medical images with inhomogeneous intensity. Their approach effectively addresses the limitations of traditional Active Contour Models (ACMs) by ensuring better edge preservation and reduced sensitivity to noise through the incorporation of the TV regularizer. The distance function aids in refining the segmentation boundary by adapting to object contours, while the local image fitting energy improves the model’s adaptability to varying intensity levels within medical images. The energy functional for the TVSS model is defined as:</p>
      
        <disp-formula>
          <label>(2)</label>
          <mml:math id="mqkzx8zrjy">
            <mml:mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
              <mml:mtr>
                <mml:mtd>
                  <mml:mi>E</mml:mi>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:munder>
                      <mml:mo>min</mml:mo>
                      <mml:mi>ϕ</mml:mi>
                    </mml:munder>
                  </mml:mrow>
                </mml:mtd>
                <mml:mtd>
                  <mml:mi/>
                  <mml:mi>ν</mml:mi>
                  <mml:mi>d</mml:mi>
                  <mml:mi>x</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:msub>
                    <mml:mo>∫</mml:mo>
                    <mml:mrow>
                      <mml:mi>Ω</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>δ</mml:mi>
                    <mml:mi>ϕ</mml:mi>
                    <mml:mi>∇</mml:mi>
                    <mml:mi>ϕ</mml:mi>
                    <mml:mrow>
                      <mml:mo>|</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>|</mml:mo>
                    </mml:mrow>
                    <mml:mfrac>
                      <mml:mn>1</mml:mn>
                      <mml:mn>2</mml:mn>
                    </mml:mfrac>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>−</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>I</mml:mi>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>+</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:msub>
                            <mml:mi>n</mml:mi>
                            <mml:mn>1</mml:mn>
                          </mml:msub>
                          <mml:msub>
                            <mml:mi>n</mml:mi>
                            <mml:mn>2</mml:mn>
                          </mml:msub>
                          <mml:mi>H</mml:mi>
                          <mml:mi>ϕ</mml:mi>
                          <mml:mi>H</mml:mi>
                          <mml:mi>ϕ</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:mrow>
                      <mml:mn>2</mml:mn>
                    </mml:msup>
                  </mml:mrow>
                </mml:mtd>
              </mml:mtr>
              <mml:mtr>
                <mml:mtd/>
                <mml:mtd>
                  <mml:mi/>
                  <mml:mi>θ</mml:mi>
                  <mml:mi>H</mml:mi>
                  <mml:mi>ϕ</mml:mi>
                  <mml:mi>d</mml:mi>
                  <mml:mi>x</mml:mi>
                  <mml:mi>d</mml:mi>
                  <mml:mi>y</mml:mi>
                  <mml:mo>+</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mo>∫</mml:mo>
                    <mml:mrow>
                      <mml:mi>Ω</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>P</mml:mi>
                    <mml:mrow>
                      <mml:mi>∂</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
            <mml:mo>,</mml:mo>
          </mml:math>
        </disp-formula>
      
      <p>where, \( n_1(x,y) \) and \( n_2(x,y) \) are formulated as:</p>
      
        <disp-formula>
          <label>(3)</label>
          <mml:math id="m4y7jntlfq">
            <mml:msub>
              <mml:mi>n</mml:mi>
              <mml:mn>1</mml:mn>
            </mml:msub>
            <mml:msub>
              <mml:mi>k</mml:mi>
              <mml:mi>σ</mml:mi>
            </mml:msub>
            <mml:mo>(</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:mo>∗</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mi>x</mml:mi>
            <mml:mi>y</mml:mi>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>H</mml:mi>
                <mml:mi>ϕ</mml:mi>
                <mml:mi>I</mml:mi>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
              </mml:mrow>
              <mml:mrow>
                <mml:msub>
                  <mml:mi>k</mml:mi>
                  <mml:mi>σ</mml:mi>
                </mml:msub>
                <mml:mo>∗</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mi>H</mml:mi>
                <mml:mi>ϕ</mml:mi>
              </mml:mrow>
            </mml:mfrac>
          </mml:math>
        </disp-formula>
      
      
        <disp-formula>
          <label>(4)</label>
          <mml:math id="m1wk5qbyge">
            <mml:msub>
              <mml:mi>n</mml:mi>
              <mml:mn>2</mml:mn>
            </mml:msub>
            <mml:msub>
              <mml:mi>k</mml:mi>
              <mml:mi>σ</mml:mi>
            </mml:msub>
            <mml:mo>(</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:mo>∗</mml:mo>
            <mml:mo>.</mml:mo>
            <mml:mi>x</mml:mi>
            <mml:mi>y</mml:mi>
            <mml:mfrac>
              <mml:mrow>
                <mml:mo>(</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mn>1</mml:mn>
                <mml:mi>H</mml:mi>
                <mml:mi>ϕ</mml:mi>
                <mml:mi>I</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:msub>
                  <mml:mi>k</mml:mi>
                  <mml:mi>σ</mml:mi>
                </mml:msub>
                <mml:mo>∗</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mn>1</mml:mn>
                <mml:mi>H</mml:mi>
                <mml:mi>ϕ</mml:mi>
              </mml:mrow>
            </mml:mfrac>
          </mml:math>
        </disp-formula>
      
      <p>Here, \( k_{\sigma} \) represents a Gaussian kernel with a standard deviation \( \sigma \). The parameter \( \theta \) serves to regulate the contour evolution, preventing it from deviating significantly from the targeted object. In general, a lower value of \( \theta \) is preferred when the target object exhibits clear contrast with the background, allowing better segmentation. On the other hand, the total variation (TV) term, represented by the first integral, plays a crucial role in smoothing the segmentation boundary. The regularization strength is controlled by \( \nu \), which can be set higher for images containing significant noise to ensure a stable contour evolution.</p><p>However, this method has certain limitations, including the staircasing effect introduced by the TV term in smoother regions, which may affect the segmentation of fine structures. Additionally, the model requires parameter tuning, which can be challenging for different types of medical images. While TV-SSM enhances segmentation accuracy for noisy medical data, its performance deteriorates in complex selective segmentation scenarios where weak boundaries or overlapping objects exist. This highlights the need for more adaptive models that can combine local feature information with global shape priors.</p><p>Ibrar et al. [<xref ref-type="bibr" rid="ref_4">4</xref>] introduced a local statistical features selective segmentation model (LSFM) that enhances object detection by integrating local statistical features with edge-based constraints. The model's energy functional is formulated as:</p>
      
        <disp-formula>
          <label>(5)</label>
          <mml:math id="m4kzdkjz77">
            <mml:mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
              <mml:mtr>
                <mml:mtd>
                  <mml:mi>F</mml:mi>
                  <mml:mi>ψ</mml:mi>
                  <mml:mi>x</mml:mi>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>)</mml:mo>
                </mml:mtd>
                <mml:mtd>
                  <mml:mi/>
                  <mml:mi>T</mml:mi>
                  <mml:mi>x</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mo>+</mml:mo>
                  <mml:msub>
                    <mml:mo>∫</mml:mo>
                    <mml:mrow>
                      <mml:mi>Ω</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mo>[</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>]</mml:mo>
                      <mml:msub>
                        <mml:mo>∫</mml:mo>
                        <mml:mrow>
                          <mml:mi>Ω</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>R</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:msub>
                      <mml:mi>H</mml:mi>
                      <mml:mi>ψ</mml:mi>
                      <mml:mi>x</mml:mi>
                      <mml:mi>d</mml:mi>
                      <mml:mi>x</mml:mi>
                    </mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:msup>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mo>[</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>]</mml:mo>
                      <mml:msub>
                        <mml:mo>∫</mml:mo>
                        <mml:mrow>
                          <mml:mi>Ω</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>R</mml:mi>
                        <mml:mn>2</mml:mn>
                      </mml:msub>
                      <mml:mn>1</mml:mn>
                      <mml:mi>H</mml:mi>
                      <mml:mi>ψ</mml:mi>
                      <mml:mi>x</mml:mi>
                      <mml:mi>d</mml:mi>
                      <mml:mi>x</mml:mi>
                    </mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:msup>
                </mml:mtd>
              </mml:mtr>
              <mml:mtr>
                <mml:mtd/>
                <mml:mtd>
                  <mml:mi/>
                  <mml:mi>λ</mml:mi>
                  <mml:mi>H</mml:mi>
                  <mml:mi>ψ</mml:mi>
                  <mml:mi>x</mml:mi>
                  <mml:mi>d</mml:mi>
                  <mml:mi>x</mml:mi>
                  <mml:mi>λ</mml:mi>
                  <mml:mi>d</mml:mi>
                  <mml:mi>x</mml:mi>
                  <mml:mo>+</mml:mo>
                  <mml:mo>⋅</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mo>+</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>−</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mo>∫</mml:mo>
                    <mml:mrow>
                      <mml:mi>Ω</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mo>∫</mml:mo>
                    <mml:mrow>
                      <mml:mi>Ω</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mrow>
                    <mml:mo>[</mml:mo>
                    <mml:mo>]</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mi>log</mml:mi>
                        <mml:mi>u</mml:mi>
                        <mml:mi>y</mml:mi>
                        <mml:mo>⁡</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>u</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>[</mml:mo>
                    <mml:mo>]</mml:mo>
                    <mml:mfrac>
                      <mml:msup>
                        <mml:mrow>
                          <mml:mo>|</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>|</mml:mo>
                          <mml:mi>u</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:msub>
                            <mml:mi>I</mml:mi>
                            <mml:mrow>
                              <mml:mi>c</mml:mi>
                              <mml:mn>2</mml:mn>
                            </mml:mrow>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:msup>
                      <mml:mrow>
                        <mml:mi>d</mml:mi>
                        <mml:mi>y</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:mo>(</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>)</mml:mo>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:mrow>
                  <mml:mn>1</mml:mn>
                </mml:mtd>
              </mml:mtr>
              <mml:mtr>
                <mml:mtd/>
                <mml:mtd>
                  <mml:mi/>
                  <mml:mi>H</mml:mi>
                  <mml:mi>ψ</mml:mi>
                  <mml:mi>x</mml:mi>
                  <mml:mi>d</mml:mi>
                  <mml:mi>x</mml:mi>
                  <mml:mo>+</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>−</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mo>∫</mml:mo>
                    <mml:mrow>
                      <mml:mi>Ω</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mfrac>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mo>|</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mo>−</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mo>|</mml:mo>
                        <mml:mi>u</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:msub>
                          <mml:mi>I</mml:mi>
                          <mml:mrow>
                            <mml:mi>c</mml:mi>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mn>2</mml:mn>
                    </mml:msup>
                    <mml:mrow>
                      <mml:mi>d</mml:mi>
                      <mml:mi>y</mml:mi>
                      <mml:mi>x</mml:mi>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                    </mml:mrow>
                  </mml:mfrac>
                  <mml:mn>1</mml:mn>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
            <mml:mo>.</mml:mo>
          </mml:math>
        </disp-formula>
      
      <p>Where, the regularization parameter is set to \( \mu = 0.1 \), ensuring a smooth level-set function while allowing flexibility in boundary adaptation. The contour evolution strength is controlled by \( v = 1.5 \), making segmentation more aggressive in capturing object boundaries. The weighting factor \( \lambda = 0.8 \) determines the balance between edge-based and region-based energy terms, where a higher value prioritizes statistical region constraints over edge information. Edge sensitivity is fine-tuned using \( \nu = 0.2 \), which adjusts the impact of edge detection on segmentation. The regional area constraints are defined as \( R_1 = 0.6 \) and \( R_2 = 0.4 \), ensuring proper differentiation between the target region and background.</p><p>The model demonstrates high accuracy in segmenting objects, particularly in noisy and intensity-inhomogeneous environments. Its combination of edge detection and statistical region constraints improves boundary localization and robustness. Additionally, it effectively balances global and local image properties, ensuring precise segmentation outcomes.</p><p>However, the approach has notable limitations. The computational complexity remains high due to iterative optimization, making real-time applications challenging. Parameter selection plays a critical role in performance, requiring careful tuning for different image datasets. Moreover, the reliance on manually placed markers can limit automation, reducing its scalability for large-scale segmentation tasks. Furthermore, while LSFM performs well on selective segmentation tasks, its dependency on precise marker placement and sensitivity to parameter tuning limits its applicability in complex medical images where intensity variations and weak edges prevail.</p><p>Although existing methods such as clustering, graph-based models, and machine learning approaches have shown improvements in general segmentation, they often fail to handle selective segmentation in complex environments effectively. For instance, clustering-based approaches struggle when target and background intensities overlap, while deep learning methods require large annotated datasets and lack interpretability for medical diagnostics. These gaps highlight the necessity for a more robust and interpretable model capable of handling noise, intensity inhomogeneity, and weak boundaries simultaneously.</p><p>The primary research objectives of this study are as follows:</p><p>• To develop a selective segmentation model that effectively handles intensity inhomogeneity, noise, and complex object boundaries.</p><p>• To integrate ED operators for adaptive feature fusion, improving region homogeneity and boundary preservation.</p><p>• To incorporate marker points and a level set framework to guide contour evolution, reducing mis-segmentation in complex images.</p><p>• To validate the proposed approach against state-of-the-art models using both qualitative and quantitative metrics.</p>
    </sec>
    <sec sec-type="">
      <title>3. Mathematical framework of the proposed model</title>
      <p>The proposed model is formulated as an energy minimization problem, where the goal is to evolve a contour (represented by a level set function) to accurately segment the target object. The energy functional incorporates region-based and edge-based terms, regularized using ED operators and marker points. The proposed model consists of the following steps.</p>
      
        <sec>
          
            <title>3.1. Preprocessing</title>
          
          <p>In the preprocessing stage, the input to the model is an image \( I: \Omega \rightarrow \mathbb{R} \), where \( \Omega \subset \mathbb{R}^2 \) represents the image domain. To reduce noise and enhance the quality of the image, a Gaussian smoothing filter is applied. This smoothing operation is defined as:</p>
          <p>where, \( G_\sigma \) is a Gaussian kernel with a standard deviation \( \sigma \). The Gaussian kernel effectively blurs the image while preserving important edges, which is crucial for accurate segmentation.</p><p>In this work, the set of marker points \( \mathcal{M} = \{m_1, m_2, \dots, m_N\} \) is selected manually, where each \( m_i \) represents a point located within the target object. Manual selection allows domain experts (e.g., radiologists for medical images) to provide precise guidance for segmentation. Although manual selection may reduce scalability, it ensures high accuracy and reproducibility for challenging cases with weak or overlapping boundaries. These marker points serve as prior knowledge about the location of the object and are used to initialize the level set function, ensuring that the segmentation process starts close to the target region. The marker points play a critical role in guiding the segmentation, particularly in complex images where the target object may have weak boundaries or overlap with other regions. By combining the smoothed image and the marker points, the preprocessing stage sets a robust foundation for the subsequent steps in the segmentation pipeline.</p>
        </sec>
      
      
        <disp-formula>
          <label>(6)</label>
          <mml:math id="muyjicgghf">
            <mml:msub>
              <mml:mi>I</mml:mi>
              <mml:mrow>
                <mml:mtext>smooth</mml:mtext>
              </mml:mrow>
            </mml:msub>
            <mml:msub>
              <mml:mi>G</mml:mi>
              <mml:mi>σ</mml:mi>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mo>∗</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mi>I</mml:mi>
          </mml:math>
        </disp-formula>
      
      
        <sec>
          
            <title>3.2. Feature extraction</title>
          
          <p>In the feature extraction stage, relevant features are extracted from the image to guide the segmentation process. The first feature is the intensity feature, which is simply the smoothed image \( I_{\text{smooth}} \) obtained during preprocessing. This feature captures the overall brightness and intensity distribution of the image. The second feature is the texture feature, which is computed using local texture descriptors such as Gabor filters or local binary patterns. These descriptors capture the spatial variation of pixel intensities, providing information about the texture of the target object and its surroundings. The third feature is the gradient feature, which is computed as the image gradient \( \nabla I_{\text{smooth}} \). This feature highlights edges and boundaries in the image, making it easier to distinguish between different regions. To combine these features into a unified representation, ED operators are used. These fuzzy logic operators provide a flexible and smooth way to integrate intensity, texture, and gradient information, resulting in a unified feature map \( F \). This feature map serves as the basis for the subsequent steps in the segmentation process, ensuring that the model can accurately identify and segment the target object.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Ed operators</title>
          
          <p>ED operators are a class of fuzzy logic operators that provide a flexible and smooth framework for combining multiple image features, making them particularly well-suited for image selective segmentation. These operators are defined for two fuzzy sets \( A \) and \( B \) as follows: the Einstein Product is given by:</p>
          
            <disp-formula>
              <label>(7)</label>
              <mml:math id="mk5ep8nt7q">
                <mml:mi>A</mml:mi>
                <mml:mi>B</mml:mi>
                <mml:mo>⊗</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>A</mml:mi>
                    <mml:mi>B</mml:mi>
                    <mml:mo>⋅</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mn>1</mml:mn>
                    <mml:mn>1</mml:mn>
                    <mml:mn>1</mml:mn>
                    <mml:mo>+</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>A</mml:mi>
                    <mml:mi>B</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>and the Einstein Sum is defined as:</p>
          
            <disp-formula>
              <label>(8)</label>
              <mml:math id="mfowkl247m">
                <mml:mi>A</mml:mi>
                <mml:mi>B</mml:mi>
                <mml:mo>⊕</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>.</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>A</mml:mi>
                    <mml:mi>B</mml:mi>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mn>1</mml:mn>
                    <mml:mo>+</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:mi>A</mml:mi>
                    <mml:mi>B</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>In the context of image selective segmentation, ED operators are used to combine intensity, texture, and gradient features into a unified feature map \( F \). Specifically, the intensity feature is derived from the smoothed image \( I_{\text{smooth}} \), the texture feature is computed using local texture descriptors (e.g., Gabor filters or local binary patterns), and the gradient feature is obtained from the image gradient \( \nabla I_{\text{smooth}} \). The combined feature map \( F \) is computed as:</p>
          
            <disp-formula>
              <label>(9)</label>
              <mml:math id="mvk0dc7fkm">
                <mml:mi>F</mml:mi>
                <mml:mi>∇</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>⊗</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>⊕</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:msub>
                  <mml:mi>I</mml:mi>
                  <mml:mrow>
                    <mml:mtext>smooth</mml:mtext>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>I</mml:mi>
                  <mml:mrow>
                    <mml:mtext>smooth</mml:mtext>
                  </mml:mrow>
                </mml:msub>
                <mml:mtext>Texture</mml:mtext>
              </mml:math>
            </disp-formula>
          
          <p>where, \( \otimes \) and \( \oplus \) are applied pixel-wise. This combination ensures that the feature map captures the most relevant information from the image, enabling the segmentation model to distinguish between the target object and the background effectively. The smooth and flexible nature of ED operators allows the model to handle complex images with heterogeneous regions, weak boundaries, and overlapping objects, making them a powerful tool for accurate and robust image selective segmentation.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Level set initialization</title>
          
          <p>The segmentation boundary is represented as the zero level set of a higher-dimensional function \( \phi: \Omega \rightarrow \mathbb{R} \), where \( \Gamma = \{x \in \Omega \mid \phi(x) = 0\} \) defines the contour. The level set function \( \phi \) is initialized using the marker points \( \mathcal{M} \). Specifically, a signed distance function (SDF) is used to define \( \phi \) such that:</p>
          
            <disp-formula>
              <label>(10)</label>
              <mml:math id="mzvz4t0lah">
                <mml:mi>ϕ</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mrow>
                  <mml:mo>{</mml:mo>
                  <mml:mo fence="true"/>
                  <mml:mtable columnalign="left left" columnspacing="1em" rowspacing=".2em">
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mo>−</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>d</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:mrow>
                          <mml:mi data-mjx-variant="-tex-calligraphic">M</mml:mi>
                        </mml:mrow>
                      </mml:mtd>
                      <mml:mtd>
                        <mml:mtext>if </mml:mtext>
                        <mml:mtext> is inside region</mml:mtext>
                        <mml:mi>x</mml:mi>
                        <mml:mo>,</mml:mo>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mi>d</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:mo>(</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mrow>
                          <mml:mi data-mjx-variant="-tex-calligraphic">M</mml:mi>
                        </mml:mrow>
                      </mml:mtd>
                      <mml:mtd>
                        <mml:mtext>if </mml:mtext>
                        <mml:mtext> is outside region</mml:mtext>
                        <mml:mi>x</mml:mi>
                        <mml:mo>,</mml:mo>
                      </mml:mtd>
                    </mml:mtr>
                  </mml:mtable>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where \( d(x, \mathcal{M}) \) is the distance from \( x \) to the nearest marker point.</p><p>The choice of SDF for initialization is motivated by its robustness and stability compared to other initialization techniques, such as random contours or simple binary masks. SDF ensures that the level set function has smooth and well-defined signed distances, which reduces numerical instabilities during evolution and accelerates convergence. Furthermore, SDF-based initialization minimizes the need for reinitialization steps, thereby improving computational efficiency. In contrast, arbitrary initial contours often require additional regularization to maintain a proper level set structure, which increases both complexity and computation time. This initialization ensures that the level set function starts close to the target object, providing a robust starting point for the contour evolution process.</p>
        </sec>
      
      
        <sec>
          
            <title>3.5. Energy functional</title>
          
          <p>The energy functional \( E(\phi) \) is designed to guide the evolution of the level set function \( \phi \) and consists of three key terms: the region-based term, the edge-based term, and the regularization term. Each term is specifically designed to address a particular challenge in selective segmentation. The region-based term improves homogeneity in regions affected by noise or intensity variations, the edge-based term ensures accurate alignment with object boundaries, and the regularization term prevents contour irregularities, making the method robust against artifacts and uneven shapes.The energy functional is defined as:</p>
          
            <disp-formula>
              <label>(11)</label>
              <mml:math id="mnm4gsm37l">
                <mml:mi>E</mml:mi>
                <mml:mi>ϕ</mml:mi>
                <mml:mi>ϕ</mml:mi>
                <mml:mi>ϕ</mml:mi>
                <mml:mi>ϕ</mml:mi>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>.</mml:mo>
                <mml:msub>
                  <mml:mi>E</mml:mi>
                  <mml:mrow>
                    <mml:mtext>region</mml:mtext>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>E</mml:mi>
                  <mml:mrow>
                    <mml:mtext>edge</mml:mtext>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>E</mml:mi>
                  <mml:mrow>
                    <mml:mtext>reg</mml:mtext>
                  </mml:mrow>
                </mml:msub>
              </mml:math>
            </disp-formula>
          
          
            <sec>
              
                <title>3.5.1 Region-based term</title>
              
              <p>The region-based term ensures that the contour separates regions with distinct feature properties. It uses ED operators to combine intensity, texture, and gradient features into a unified feature map. This term plays a crucial role in tackling intensity inhomogeneity by grouping pixels based on feature similarity rather than relying on a single intensity value, thereby improving segmentation accuracy in medical and noisy images. The term is formulated as:</p>
              
                <disp-formula>
                  <label>(12)</label>
                  <mml:math id="mfkkg8o1re">
                    <mml:mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
                      <mml:mtr>
                        <mml:mtd>
                          <mml:msub>
                            <mml:mi>E</mml:mi>
                            <mml:mrow>
                              <mml:mtext>region </mml:mtext>
                            </mml:mrow>
                          </mml:msub>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mi>ϕ</mml:mi>
                        </mml:mtd>
                        <mml:mtd>
                          <mml:mi/>
                          <mml:mi>H</mml:mi>
                          <mml:mi>ϕ</mml:mi>
                          <mml:mi>d</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:msub>
                            <mml:mi>λ</mml:mi>
                            <mml:mn>1</mml:mn>
                          </mml:msub>
                          <mml:msub>
                            <mml:mo>∫</mml:mo>
                            <mml:mrow>
                              <mml:mi>Ω</mml:mi>
                            </mml:mrow>
                          </mml:msub>
                          <mml:msup>
                            <mml:mrow>
                              <mml:mo>(</mml:mo>
                              <mml:mo>⊗</mml:mo>
                              <mml:mo>)</mml:mo>
                              <mml:mi>F</mml:mi>
                              <mml:msub>
                                <mml:mi>c</mml:mi>
                                <mml:mn>1</mml:mn>
                              </mml:msub>
                            </mml:mrow>
                            <mml:mn>2</mml:mn>
                          </mml:msup>
                        </mml:mtd>
                      </mml:mtr>
                      <mml:mtr>
                        <mml:mtd/>
                        <mml:mtd>
                          <mml:mi/>
                          <mml:mi>H</mml:mi>
                          <mml:mi>ϕ</mml:mi>
                          <mml:mi>d</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mo>+</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:msub>
                            <mml:mi>λ</mml:mi>
                            <mml:mn>2</mml:mn>
                          </mml:msub>
                          <mml:msub>
                            <mml:mo>∫</mml:mo>
                            <mml:mrow>
                              <mml:mi>Ω</mml:mi>
                            </mml:mrow>
                          </mml:msub>
                          <mml:msup>
                            <mml:mrow>
                              <mml:mo>(</mml:mo>
                              <mml:mo>⊕</mml:mo>
                              <mml:mo>)</mml:mo>
                              <mml:mi>F</mml:mi>
                              <mml:msub>
                                <mml:mi>c</mml:mi>
                                <mml:mn>2</mml:mn>
                              </mml:msub>
                            </mml:mrow>
                            <mml:mn>2</mml:mn>
                          </mml:msup>
                          <mml:mn>1</mml:mn>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:math>
                </disp-formula>
              
              <p>where, \( \lambda_1 \) and \( \lambda_2 \) are weighting parameters that control the influence of the region-based term. \( c_1 \) and \( c_2 \) represent the average feature values inside and outside the contour, respectively. The term \( (F \otimes c_1)^2 \) measures the difference between the feature map \( F \) and the average feature value \( c_1 \) inside the contour. This term ensures that the contour evolves to align with regions where the feature map closely matches the average feature value inside the target object. Similarly, \( (F \oplus c_2)^2 \) measures the difference between \( F \) and the average feature value \( c_2 \) outside the contour, encouraging the contour to separate regions with distinct feature properties. The Heaviside function \( H(\phi) \) ensures that the energy term is active only in the relevant regions (inside or outside the contour). Specifically, \( H(\phi) \) is defined as:</p>
              
                <disp-formula>
                  <label>(13)</label>
                  <mml:math id="mdn7uyj8dr">
                    <mml:mi>H</mml:mi>
                    <mml:mi>ϕ</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo>{</mml:mo>
                      <mml:mo fence="true"/>
                      <mml:mtable columnalign="left left" columnspacing="1em" rowspacing=".2em">
                        <mml:mtr>
                          <mml:mtd>
                            <mml:mn>1</mml:mn>
                          </mml:mtd>
                          <mml:mtd>
                            <mml:mtext>if </mml:mtext>
                            <mml:mi>ϕ</mml:mi>
                            <mml:mo>≥</mml:mo>
                            <mml:mo>,</mml:mo>
                            <mml:mn>0</mml:mn>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd>
                            <mml:mn>0</mml:mn>
                          </mml:mtd>
                          <mml:mtd>
                            <mml:mtext>if </mml:mtext>
                            <mml:mi>ϕ</mml:mi>
                            <mml:mo>&lt;</mml:mo>
                            <mml:mo>,</mml:mo>
                            <mml:mn>0</mml:mn>
                          </mml:mtd>
                        </mml:mtr>
                      </mml:mtable>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where, \( \phi \geq 0 \) corresponds to the inside of the contour and \( \phi &lt; 0 \) corresponds to the outside. By incorporating the Heaviside function, the region-based energy term effectively distinguishes between the target object and the background, ensuring accurate segmentation.</p>
            </sec>
          
          
            <sec>
              
                <title>3.5.2 Edge-based term</title>
              
              <p>The edge-based term plays a crucial role in attracting the contour to object boundaries by leveraging the image gradient. This term directly addresses the challenge of weak or blurred edges, ensuring that the evolving contour locks onto the most prominent gradient changes. It is mathematically expressed as:</p>
              
                <disp-formula>
                  <label>(14)</label>
                  <mml:math id="mkpn48yn0w">
                    <mml:msub>
                      <mml:mi>E</mml:mi>
                      <mml:mrow>
                        <mml:mtext>edge</mml:mtext>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mo>∫</mml:mo>
                      <mml:mrow>
                        <mml:mi>Ω</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mi>ϕ</mml:mi>
                    <mml:mi>μ</mml:mi>
                    <mml:mi>g</mml:mi>
                    <mml:mi>∇</mml:mi>
                    <mml:mi>I</mml:mi>
                    <mml:mi>δ</mml:mi>
                    <mml:mi>ϕ</mml:mi>
                    <mml:mi>∇</mml:mi>
                    <mml:mi>ϕ</mml:mi>
                    <mml:mi>d</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mrow>
                      <mml:mo>|</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>|</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>|</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>|</mml:mo>
                    </mml:mrow>
                    <mml:mstyle scriptlevel="0">
                      <mml:mspace width="0.167em"/>
                    </mml:mstyle>
                  </mml:math>
                </disp-formula>
              
              <p>where, \( g(|\nabla I|) = \frac{1}{1 + |\nabla I|^2} \) is an edge indicator function. This function takes small values in regions with strong gradients (edges) and large values in homogeneous regions, effectively emphasizing object boundaries. The Dirac delta function \( \delta(\phi) \), defined as \( \delta(\phi) = \frac{dH(\phi)}{d\phi} \), ensures that the energy term is active only near the zero level set (the contour). This restriction prevents unnecessary computations in regions far from the contour. The parameter \( \mu \) controls the weight of the edge-based term, balancing its influence relative to the other terms in the energy functional.</p>
            </sec>
          
          
            <sec>
              
                <title>3.5.3 Regularization term</title>
              
              <p>The regularization term ensures the smoothness of the contour and prevents irregularities such as sharp corners or jagged edges. This is particularly important in medical imaging where noisy regions or artifacts can cause the contour to become irregular. The regularization term enforces a smooth boundary, ensuring clinical interpretability of the segmented results.It is given by:</p>
              
                <disp-formula>
                  <label>(15)</label>
                  <mml:math id="mh38b0c7b0">
                    <mml:msub>
                      <mml:mi>E</mml:mi>
                      <mml:mrow>
                        <mml:mtext>reg</mml:mtext>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mo>∫</mml:mo>
                      <mml:mrow>
                        <mml:mi>Ω</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mi>ϕ</mml:mi>
                    <mml:mi>ν</mml:mi>
                    <mml:mi>∇</mml:mi>
                    <mml:mi>H</mml:mi>
                    <mml:mi>ϕ</mml:mi>
                    <mml:mi>d</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mrow>
                      <mml:mo>|</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>|</mml:mo>
                    </mml:mrow>
                    <mml:mstyle scriptlevel="0">
                      <mml:mspace width="0.167em"/>
                    </mml:mstyle>
                  </mml:math>
                </disp-formula>
              
              <p>where, \( \nu \) is a weighting parameter that balances the trade-off between contour smoothness and adherence to image features. The term \( |\nabla H(\phi)| \) penalizes abrupt changes in the contour, ensuring that it evolves smoothly and maintains a regular shape. This term is particularly important in noisy images, where the contour might otherwise become fragmented or irregular.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.6. Energy minimization</title>
          
          <p>The energy functional \( E(\phi) \), which combines the region-based, edge-based, and regularization terms, is minimized using gradient descent. The evolution of the level set function \( \phi \) is governed by the partial differential equation:</p>
          
            <disp-formula>
              <label>(16)</label>
              <mml:math id="mqkxqg82ww">
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>∂</mml:mi>
                    <mml:mi>ϕ</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>∂</mml:mi>
                    <mml:mi>t</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>∂</mml:mi>
                    <mml:mi>E</mml:mi>
                    <mml:mi>ϕ</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>∂</mml:mi>
                    <mml:mi>ϕ</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mo>=</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>.</mml:mo>
              </mml:math>
            </disp-formula>
          
          <p>This equation describes how the level set function changes over time to minimize the energy functional. The gradient descent update rule is:</p>
          
            <disp-formula>
              <label>(17)</label>
              <mml:math id="mrfgstaxci">
                <mml:msup>
                  <mml:mi>ϕ</mml:mi>
                  <mml:mrow>
                    <mml:mi>k</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:msup>
                  <mml:mi>ϕ</mml:mi>
                  <mml:mi>k</mml:mi>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mi>Δ</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>∂</mml:mi>
                    <mml:mi>E</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msup>
                      <mml:mi>ϕ</mml:mi>
                      <mml:mi>k</mml:mi>
                    </mml:msup>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>∂</mml:mi>
                    <mml:mi>ϕ</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>where, \( \Delta t \) is the time step controlling the rate of evolution. A smaller \( \Delta t \) ensures stability but may require more iterations, while a larger \( \Delta t \) speeds up convergence but risks instability. The term \( \frac{\partial E(\phi)}{\partial \phi} \) represents the derivative of the energy functional with respect to \( \phi \), guiding the contour toward the optimal segmentation.</p><p>Computational Complexity: The proposed approach involves iterative updates of the level set function, where each iteration requires evaluating feature maps and gradient terms. The overall complexity is approximately \( O(n \cdot m) \), where \( n \) is the number of iterations and \( m \) is the number of pixels. Although ED-operator-based feature fusion introduces additional computations, it significantly reduces the number of iterations needed for convergence compared to conventional region-based models, thus providing a practical trade-off between accuracy and computational cost. On a standard CPU implementation, convergence is typically achieved within 1–2 seconds for \( 256 \times 256 \) images.</p>
        </sec>
      
      
        <sec>
          
            <title>3.7. Contour evolution</title>
          
          <p>The level set function \( \phi \) is evolved iteratively to refine the segmentation boundary. During each iteration, the contour is updated based on the gradient descent rule, moving closer to the target object's boundaries. The evolution process continues until the change in \( \phi \) between consecutive iterations falls below a predefined threshold \( \epsilon \), indicating convergence. This stopping criterion is expressed as:</p>
          
            <disp-formula>
              <label>(18)</label>
              <mml:math id="msmlz0iixs">
                <mml:mo fence="false">‖</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo fence="false">‖</mml:mo>
                <mml:mo>&lt;</mml:mo>
                <mml:mo>.</mml:mo>
                <mml:msup>
                  <mml:mi>ϕ</mml:mi>
                  <mml:mrow>
                    <mml:mi>k</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:msup>
                  <mml:mi>ϕ</mml:mi>
                  <mml:mi>k</mml:mi>
                </mml:msup>
                <mml:mi>ϵ</mml:mi>
              </mml:math>
            </disp-formula>
          
          <p>Once the evolution process terminates, the final contour represents the segmentation boundary of the target object. The combination of the three energy terms ensures that the final boundary is both smooth and accurately aligned with edges, while remaining robust against noise and intensity inhomogeneity.</p><p>After the contour evolution process terminates, the final segmentation result may still contain minor imperfections, such as irregularities in the boundary or small artifacts within the segmented region. To address these issues, postprocessing techniques are applied to refine the segmentation result. First, morphological operations, such as dilation and erosion, are used to smooth the contour and eliminate small irregularities. Dilation expands the boundary of the segmented region, filling in small gaps, while erosion shrinks the boundary, removing small protrusions. These operations are often applied sequentially (e.g., opening or closing) to achieve a balance between smoothing and preserving the overall shape of the target object. Additionally, small artifacts or holes within the segmented region are removed using connected component analysis. This involves identifying and filtering out regions that are too small to be part of the target object, ensuring that the final segmentation is clean and accurate. By applying these postprocessing steps, the segmentation result is further refined, resulting in a smooth and well-defined boundary that accurately represents the target object.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Experimental validation</title>
      <p>The experimental validation of the proposed segmentation model was conducted using a structured approach to ensure robustness and reproducibility. The model integrates three key components: ED Operators, Marker Points, and Level Set Methods—to enhance segmentation accuracy by effectively managing intensity variations, noise, and complex object boundaries. The experiments were carried out on a diverse dataset comprising grayscale medical images (e.g., MRI brain scans and CT slices) and synthetic images with varying noise levels and intensity inhomogeneity. The dataset included 150 images, with approximately 60% medical images and 40% synthetic test cases, providing a balanced evaluation of both real-world and controlled scenarios. Gaussian noise with variances ranging from 0.01 to 0.05 was added to certain synthetic images to evaluate noise robustness, while intensity inhomogeneity was simulated using bias field distortions.</p><p>All images were resized to a resolution of $255 \times 255$ pixels to ensure a standardized evaluation across different test cases. MATLAB R2015a was used as the primary software environment for implementing and testing the proposed model, with custom scripts designed to handle image preprocessing, feature extraction, and segmentation. Given the computational constraints of MATLAB R2015a, special considerations were made to optimize performance while ensuring the accuracy of the results.</p><p>The evaluation framework included both qualitative assessments—such as visual inspection of segmented contours—and quantitative metrics, including accuracy, IoU, and DSC. For a fair comparison, all competing methods were tested under identical conditions with the same dataset and noise configurations.</p><p>The parameters for optimal segmentation performance are set empirically. Gaussian smoothing uses a standard deviation of \( \sigma = 1.5 \) to reduce noise while preserving edges. The region-based term weights are \( \lambda_1 = 1.2 \) and \( \lambda_2 = 1.0 \), while the edge-based term is controlled by \( \mu = 0.8 \). The regularization term is \( \nu = 0.5 \), and the gradient descent time step is \( \Delta t = 0.1 \), ensuring stable convergence. The process halts when \( \epsilon = 10^{-3} \) is met, stabilizing the segmentation boundary. The ED operators integrate multiple image features, with Einstein Product and Sum applied pixel-wise using parameters \( \alpha = 1.2 \) and \( \beta = 1.0 \). Feature integration weights are set as \( w_1 = 0.5 \) for intensity, \( w_2 = 0.3 \) for texture, and \( w_3 = 0.2 \) for gradient, ensuring robust segmentation across diverse image conditions.</p><p>The proposed model demonstrates an effective segmentation performance, as shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>. The model utilizes a feature map-based approach to enhance boundary detection, improving segmentation accuracy compared to traditional methods. The first column presents the original images, while the second column provides ground truth segmentation with purple contours for reference. The third column illustrates the intermediate segmentation results obtained using the fuzzy feature map-based method, highlighting significant structural details. Finally, the fourth column presents the final results of the segmentation produced by the proposed model, where the blue contours accurately delineate the boundaries of the objects. The consistency and precision of these results indicate the robustness of the proposed method in handling diverse image structures and intensity variations.</p><p><xref ref-type="fig" rid="fig_2">Figure 2</xref> presents segmentation results on real noisy medical images, comparing the performance of the TV-SSM [<xref ref-type="bibr" rid="ref_21">21</xref>], LSFM [<xref ref-type="bibr" rid="ref_4">4</xref>], and Khan et al. [<xref ref-type="bibr" rid="ref_8">8</xref>] modelled with the proposed model. The first column displays the original images, followed by segmentation outputs from the competing models. The primary challenge in these images is the presence of noise, weak boundaries, and intensity variations, which significantly affect the accuracy of traditional segmentation techniques. Competing models TV-SSM, LSFM, and Khan et al. struggle to maintain clear boundary separation due to their reliance on conventional edge-based or region-based energy minimization. However, the proposed model integrates ED operators, and fuzzy energy functional to achieve superior segmentation performance. The Einstein Product and Sum effectively fuse intensity, texture, and gradient features while preserving crucial boundary details and reducing noise interference. Additionally, the Dombi operator adapts to local image characteristics, enhancing region homogeneity while ensuring smooth boundary evolution. The use of marker points as guiding cues significantly improves segmentation precision by offering prior knowledge about object locations, thereby reducing ambiguity and improving convergence speed. The last column of <xref ref-type="fig" rid="fig_2">Figure 2</xref> clearly illustrates that the proposed model, highlighted with blue contours, provides more accurate segmentation by effectively distinguishing objects from noisy backgrounds and preserving fine anatomical structures.</p><p><xref ref-type="fig" rid="fig_3">Figure 3</xref> extends the segmentation analysis to X-ray images, emphasizing the importance of ED operators, and level set evolution in handling complex anatomical structures. The first column presents the original images, followed by segmentation results from TV-SSM , LSFM, and Khan et al.'s model. While these models provide reasonable approximations of object boundaries, they exhibit sensitivity to intensity inhomogeneities and weak edges, leading to segmentation errors such as boundary leakage and over-segmentation. The proposed model, shown in the last column, integrates entropy-based marker point selection, which guides the level set initialization process and improves boundary detection. The ED operators play a crucial role in adaptive feature fusion, enabling a more robust segmentation process by maintaining a balance between texture, gradient, and intensity variations. This ensures better region separation and contrast enhancement, reducing false region detection observed in competing models. The level set method, with its implicit representation and energy-based evolution, further enhances segmentation robustness by ensuring smooth boundary progression even in the presence of occlusions or intensity variations. As seen in the results, the proposed model exhibits strong boundary adherence, enhanced anatomical detail preservation, and superior segmentation accuracy compared to traditional methods.</p><p>To evaluate the performance of selective segmentation models, we employ a suite of well-established metrics that quantify both the accuracy and efficiency of the segmentation process. These include Accuracy, Precision, Recall, F1 Score, IoU, DSC. Additionally, CIs are computed to assess the statistical reliability of these metrics, ensuring a robust performance evaluation. Below, we present the mathematical formulations for each metric.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Segmentation results on synthetic images</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_czd18jopZvY6O4zf.png"/>
        </fig>
      
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Segmentation results on real noisy medical images</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_tcP57YlTRt06NNFc.png"/>
        </fig>
      
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>Segmentation results on X-ray images</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_W7qvZkoXneOd6w5a.png"/>
        </fig>
      
      <p>Accuracy (\(Acc\))</p><p>Accuracy is defined as the proportion of correctly classified pixels (both true positives and true negatives) relative to the total number of pixels in the image. It is mathematically expressed as:</p>
      
        <disp-formula>
          <label>(19)</label>
          <mml:math id="mrqd1htpct">
            <mml:mi>A</mml:mi>
            <mml:mi>c</mml:mi>
            <mml:mi>c</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>T</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mi>T</mml:mi>
                <mml:mi>N</mml:mi>
                <mml:mo>+</mml:mo>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>T</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mi>T</mml:mi>
                <mml:mi>N</mml:mi>
                <mml:mi>F</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mi>F</mml:mi>
                <mml:mi>N</mml:mi>
                <mml:mo>+</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>+</mml:mo>
              </mml:mrow>
            </mml:mfrac>
          </mml:math>
        </disp-formula>
      
      <p>where:</p><p>•\(TP\) = True Positives (correctly predicted foreground pixels),</p><p>•\(TN\) = True Negatives (correctly predicted background pixels),</p><p>•\(FP\) = False Positives (incorrectly predicted foreground pixels),</p><p>•\(FN\) = False Negatives (incorrectly predicted background pixels).</p>
      <p>Precision (\(P\)) and Recall (\(R\))</p><p>Precision quantifies the proportion of predicted positive pixels that are actually correct. Recall, also referred to as sensitivity, measures the proportion of actual positive pixels correctly identified by the model. These are computed as:</p>
      
        <disp-formula>
          <label>(20)</label>
          <mml:math id="mwpr3j8g9b">
            <mml:mi>P</mml:mi>
            <mml:mi>R</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:mo>.</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>T</mml:mi>
                <mml:mi>P</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>T</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mi>F</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mo>+</mml:mo>
              </mml:mrow>
            </mml:mfrac>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>T</mml:mi>
                <mml:mi>P</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>T</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mi>F</mml:mi>
                <mml:mi>N</mml:mi>
                <mml:mo>+</mml:mo>
              </mml:mrow>
            </mml:mfrac>
            <mml:mstyle scriptlevel="0">
              <mml:mspace width="1em"/>
            </mml:mstyle>
          </mml:math>
        </disp-formula>
      
      <p>These metrics emphasize the accuracy of positive predictions.</p>
      <p>F1 Score (\(F_1\))</p><p>The F1 Score is the harmonic mean of Precision and Recall, providing a balanced measure between the two. It is especially useful when there is an imbalance between foreground and background pixels. The F1 Score is computed as:</p>
      
        <disp-formula>
          <label>(21)</label>
          <mml:math id="m2yut1dk3t">
            <mml:msub>
              <mml:mi>F</mml:mi>
              <mml:mn>1</mml:mn>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mo>⋅</mml:mo>
            <mml:mo>.</mml:mo>
            <mml:mn>2</mml:mn>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>P</mml:mi>
                <mml:mi>R</mml:mi>
                <mml:mo>⋅</mml:mo>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>P</mml:mi>
                <mml:mi>R</mml:mi>
                <mml:mo>+</mml:mo>
              </mml:mrow>
            </mml:mfrac>
          </mml:math>
        </disp-formula>
      
      <p>This metric combines both the sensitivity and precision into a single value.</p>
      <p>IoU and DSC</p><p>The IoU, also known as the Jaccard Index, measures the overlap between the predicted segmentation and the ground truth. The DSC also quantifies the overlap between the predicted and ground truth segmentation regions. These are expressed as:</p>
      
        <disp-formula>
          <label>(22)</label>
          <mml:math id="mfx3ktpt30">
            <mml:mi>I</mml:mi>
            <mml:mi>o</mml:mi>
            <mml:mi>U</mml:mi>
            <mml:mi>D</mml:mi>
            <mml:mi>S</mml:mi>
            <mml:mi>C</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mo>|</mml:mo>
                <mml:mo>∩</mml:mo>
                <mml:mo>|</mml:mo>
                <mml:mi>P</mml:mi>
                <mml:mi>G</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:mo>|</mml:mo>
                <mml:mo>∪</mml:mo>
                <mml:mo>|</mml:mo>
                <mml:mi>P</mml:mi>
                <mml:mi>G</mml:mi>
              </mml:mrow>
            </mml:mfrac>
            <mml:mfrac>
              <mml:mrow>
                <mml:mn>2</mml:mn>
                <mml:mo>⋅</mml:mo>
                <mml:mo>∩</mml:mo>
                <mml:mo>|</mml:mo>
                <mml:mrow>
                  <mml:mo>|</mml:mo>
                </mml:mrow>
                <mml:mi>P</mml:mi>
                <mml:mi>G</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:mo>|</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>|</mml:mo>
                <mml:mi>P</mml:mi>
                <mml:mi>G</mml:mi>
                <mml:mrow>
                  <mml:mo>|</mml:mo>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>|</mml:mo>
                </mml:mrow>
              </mml:mrow>
            </mml:mfrac>
            <mml:mstyle scriptlevel="0">
              <mml:mspace width="1em"/>
            </mml:mstyle>
          </mml:math>
        </disp-formula>
      
      <p>where, \(P\) = Predicted segmentation region, and \(G\) = Ground truth region.</p><p>A higher IoU indicates better segmentation performance, particularly in delineating object boundaries. The DSC is widely used in applications such as medical imaging and natural image segmentation, where precise boundary detection is critical.</p>
      <p>CIs</p><p>CIs are used to quantify the uncertainty of the performance metrics. They provide a statistical range within which the true value of a metric is likely to fall with a specified confidence level (usually 95%). The CI for a given metric \(\mu\) is calculated as:</p>
      
        <disp-formula>
          <label>(23)</label>
          <mml:math id="m1jgaxyzrl">
            <mml:mi>C</mml:mi>
            <mml:mi>I</mml:mi>
            <mml:mi>μ</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mo>±</mml:mo>
            <mml:mo>⋅</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:msub>
              <mml:mi>Z</mml:mi>
              <mml:mrow>
                <mml:mi>α</mml:mi>
                <mml:mrow>
                  <mml:mo>/</mml:mo>
                </mml:mrow>
                <mml:mn>2</mml:mn>
              </mml:mrow>
            </mml:msub>
            <mml:mfrac>
              <mml:mi>σ</mml:mi>
              <mml:msqrt>
                <mml:mi>n</mml:mi>
              </mml:msqrt>
            </mml:mfrac>
          </mml:math>
        </disp-formula>
      
      <p>where, \(\mu\) is the mean value of the metric, \(Z_{\alpha/2}\) refers to the Z-score corresponding to the desired confidence level, \(\sigma\) is the standard deviation of the metric, and \(n\) is the number of samples.</p><p>By incorporating these performance metrics and CIs, we ensure a comprehensive and statistically sound evaluation of the selective segmentation model. This approach not only provides a quantitative assessment of accuracy but also highlights the computational efficiency and the reliability of the results.</p><p><xref ref-type="table" rid="table_1">Table 1</xref> presents the performance evaluation of the proposed model in comparison with three other models (TV-SSM, LSFM and Khan et al.) using six key metrics: Accuracy, Precision, Recall, F1 Score, IoU, and DSC. The proposed model outperforms all competing models across all metrics, achieving the highest Accuracy (0.95), Precision (0.93), Recall (0.90), F1 Score (0.91), IoU (0.89), and DSC (0.94). TV-SSM shows competitive performance with an Accuracy of 0.90 and a DSC of 0.88, followed by LSFM, which achieves an Accuracy of 0.88 and a DSC of 0.86. Khan et al. demonstrates the lowest performance, with an Accuracy of 0.86 and an IoU of 0.75. The higher IoU and DSC values of the proposed model indicate better segmentation quality and overlap with ground truth data. These results highlight the effectiveness of the proposed model in achieving superior segmentation performance compared to the existing approaches (see <xref ref-type="fig" rid="fig_4">Figure 4</xref>).</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Performance metrics for proposed and competing models</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Metric</p></td><td colspan="1" rowspan="1"><p>Proposed Model</p></td><td colspan="1" rowspan="1"><p>TV-SSM</p></td><td colspan="1" rowspan="1"><p>LSFM</p></td><td colspan="1" rowspan="1"><p>Khan et al. Model</p></td></tr><tr><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>0.95</p></td><td colspan="1" rowspan="1"><p>0.90</p></td><td colspan="1" rowspan="1"><p>0.88</p></td><td colspan="1" rowspan="1"><p>0.86</p></td></tr><tr><td colspan="1" rowspan="1"><p>Precision</p></td><td colspan="1" rowspan="1"><p>0.93</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>0.86</p></td></tr><tr><td colspan="1" rowspan="1"><p>Recall</p></td><td colspan="1" rowspan="1"><p>0.90</p></td><td colspan="1" rowspan="1"><p>0.87</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>0.83</p></td></tr><tr><td colspan="1" rowspan="1"><p>F1 Score</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>0.87</p></td><td colspan="1" rowspan="1"><p>0.86</p></td></tr><tr><td colspan="1" rowspan="1"><p>IoU</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>0.79</p></td><td colspan="1" rowspan="1"><p>0.77</p></td><td colspan="1" rowspan="1"><p>0.75</p></td></tr><tr><td colspan="1" rowspan="1"><p>DSC</p></td><td colspan="1" rowspan="1"><p>0.94</p></td><td colspan="1" rowspan="1"><p>0.88</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>0.84</p></td></tr></tbody></table>
        </table-wrap>
      
      
        <fig id="fig_4">
          <label>Figure 4</label>
          <caption>
            <title>Quantitative performance comparison of the proposed segmentation model with competing models</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_q66ik588XgPP302M.png"/>
        </fig>
      
      <p><xref ref-type="table" rid="table_2">Table 2</xref> shows that the proposed selective segmentation model achieves the highest performance across all metrics, with consistently narrower 95% confidence intervals compared to the competing models (TV-SSM, LSFM, and Shahkar et al.). For instance, its Accuracy of 0.95 is accompanied by a tight CI of [0.931, 0.969], indicating high precision and reliability in the estimates. Similarly, Precision ([0.910, 0.950]), Recall ([0.880, 0.920]), F1 Score ([0.891, 0.929]), and IoU ([0.870, 0.910]) all display superior central values and smaller intervals, reflecting reduced variability. In contrast, the competing models not only have lower central metric values but also exhibit wider intervals, suggesting less consistent performance. These results confirm that the proposed model offers both statistically higher accuracy and greater stability.</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>Confidence intervals (95% CIs) for performance metrics of the proposed selective segmentation model and competing models (TV-SSM, LSFM and Khan et al. model)</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Metric</p></td><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="1" rowspan="1"><p>Mean</p></td><td colspan="1" rowspan="1"><p>95% CIs</p></td></tr><tr><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>Proposed Model</p></td><td colspan="1" rowspan="1"><p>0.95</p></td><td colspan="1" rowspan="1"><p>[0.931, 0.969]</p></td></tr><tr><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>TV-SSM</p></td><td colspan="1" rowspan="1"><p>0.90</p></td><td colspan="1" rowspan="1"><p>[0.880, 0.920]</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>LSFM</p></td><td colspan="1" rowspan="1"><p>0.88</p></td><td colspan="1" rowspan="1"><p>[0.860, 0.900]</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>Khan et al. Model</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>[0.840, 0.880]</p></td></tr><tr><td colspan="1" rowspan="1"><p>Precision</p></td><td colspan="1" rowspan="1"><p>Proposed Model</p></td><td colspan="1" rowspan="1"><p>0.93</p></td><td colspan="1" rowspan="1"><p>[0.910, 0.950]</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>TV-SSM</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>[0.870, 0.910]</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>LSFM</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>[0.840, 0.880]</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>Khan et al. Model</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>[0.840, 0.880]</p></td></tr><tr><td colspan="1" rowspan="1"><p>Recall</p></td><td colspan="1" rowspan="1"><p>Proposed Model</p></td><td colspan="1" rowspan="1"><p>0.90</p></td><td colspan="1" rowspan="1"><p>[0.880, 0.920]</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>TV-SSM</p></td><td colspan="1" rowspan="1"><p>0.87</p></td><td colspan="1" rowspan="1"><p>[0.850, 0.890]</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>LSFM</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>[0.830, 0.870]</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>Khan et al. Model</p></td><td colspan="1" rowspan="1"><p>0.83</p></td><td colspan="1" rowspan="1"><p>[0.810, 0.850]</p></td></tr><tr><td colspan="1" rowspan="1"><p>F1 Score</p></td><td colspan="1" rowspan="1"><p>Proposed Model</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>[0.891, 0.929]</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>TV-SSM</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>[0.840, 0.880]</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>LSFM</p></td><td colspan="1" rowspan="1"><p>0.87</p></td><td colspan="1" rowspan="1"><p>[0.850, 0.890]</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>Khan et al. Model</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>[0.840, 0.880]</p></td></tr><tr><td colspan="1" rowspan="1"><p>IoU</p></td><td colspan="1" rowspan="1"><p>Proposed Model</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>[0.870, 0.910]</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>TV-SSM</p></td><td colspan="1" rowspan="1"><p>0.79</p></td><td colspan="1" rowspan="1"><p>[0.760, 0.820]</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>LSFM</p></td><td colspan="1" rowspan="1"><p>0.77</p></td><td colspan="1" rowspan="1"><p>[0.750, 0.790]</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>Khan et al. Model</p></td><td colspan="1" rowspan="1"><p>0.75</p></td><td colspan="1" rowspan="1"><p>[0.730, 0.770]</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>The proposed model holds significant potential for real-world applications, particularly in medical imaging. By providing accurate and robust segmentation of anatomical structures even in the presence of noise and intensity inhomogeneity, the model can assist clinicians in tasks such as tumor boundary delineation, organ volume estimation, and pre-operative planning. Its ability to reduce manual intervention through marker-based guidance can streamline clinical workflows, minimize inter-observer variability, and support more reliable decision-making in diagnostic and therapeutic procedures.</p>
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>This paper introduced a novel selective segmentation model that integrated region- and edge-based energy terms with ED operators to achieve robust and accurate image segmentation. The proposed approach effectively combined intensity, texture, and gradient information through weighted feature integration, enhancing segmentation precision, particularly in challenging conditions such as noise, blur, and intensity inhomogeneity. Experimental validation was conducted using a dataset comprising blurred and noisy images, where the model was compared with existing state-of-the-art techniques. The results demonstrate superior performance in terms of accuracy, precision, recall, F1 score, and IoU, confirming the effectiveness of the proposed method. Statistical significance tests further validate that the improvements are not random but rather a result of the novel fusion strategy.</p><p>The broader implications of the findings are noteworthy. The ability of the model to handle noisy and intensity-inhomogeneous images highlights its potential for real-world applications, especially in fields like medical imaging, where accurate segmentation of degraded or low-quality scans is critical for diagnosis and treatment planning. Such robustness can reduce manual corrections, thereby improve workflow efficiency and reduce diagnostic errors.</p><p>Despite its advantages, the proposed model has certain limitations. The computational complexity remains relatively high due to the involvement of multiple feature fusion mechanisms and iterative optimizations. Moreover, the model exhibits sensitivity to parameter settings (e.g., weighting parameters and time-step size), which can affect segmentation performance if not carefully tuned. In addition, while the model performs well under moderate noise and blur, potential failure cases may arise in scenarios with extreme occlusions, very low contrast, or highly irregular textures where feature fusion alone may be insufficient. To address these challenges, future work will focus on optimizing the algorithm’s computational efficiency, exploring deep learning-based feature extraction for improved robustness, and extending the framework to multi-modal image segmentation. We also plan to investigate automated parameter selection strategies and adaptive feature weighting to reduce sensitivity and improve generalization across diverse image conditions. Further improvements in real-time processing capabilities will also be explored to make the model suitable for time-sensitive applications such as medical imaging and autonomous navigation.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>The author solely conducted the conceptualization, methodology, data analysis, and writing of this manuscript.</p>
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The author declares no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hussain</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Muhammad</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3934/ipi.2022074</pub-id>
          <article-title>Efficient convex region-based segmentation for noising and inhomogeneous patterns</article-title>
          <source>Inverse Probl. Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>30</volume>
          <page-range>240-257</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Suneetha</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Reddy</surname>
              <given-names>E. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1515/jisys-2019-0211</pub-id>
          <article-title>Robust gaussian noise detection and removal in color images using modified fuzzy set filter</article-title>
          <source>J. Intell. Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <issue>7</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Abdelazeem</surname>
              <given-names>R. M.</given-names>
            </name>
            <name>
              <surname>Youssef</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>El-Azab</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hassab-Elnaby</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Agour</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0236835</pub-id>
          <article-title>Three-dimensional visualization of brain tumor progression based on accurate segmentation via comparative holographic projection</article-title>
          <source>PLOS ONE</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <page-range>1113-1136</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ibrar</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>M. S.</given-names>
            </name>
            <name>
              <surname>Niu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Rada</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3934/ipi.2022014</pub-id>
          <article-title>Robust region-based active contour models via local statistical similarity and local similarity factor for intensity inhomogeneity and high noise image segmentation</article-title>
          <source>Inverse Probl. Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>116-126</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hussain</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Muhammad</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/ijkis010204</pub-id>
          <article-title>Enhanced global image segmentation: Addressing pixel inhomogeneity and noise with average convolution and entropy-based local factor</article-title>
          <source>Int. J. Knowl. Innov. Stud.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>1–15</page-range>
          <issue>1</issue>
          <year>2009</year>
          <person-group person-group-type="author">
            <name>
              <surname>Maini</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Aggarwal</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Study and comparison of various image edge detection techniques</article-title>
          <source>Int. J. Image Process.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>119-135</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Deng</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Galetto</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Al-Nasrawi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Waheed</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/OJSP.2021.3063076</pub-id>
          <article-title>A guided edge-aware smoothing-sharpening filter based on patch interpolation model and generalized gamma distribution</article-title>
          <source>IEEE Open J. Signal Process.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Khan</surname>
              <given-names>M. S.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zakarya</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Tirunagari</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>A. A.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>A.</surname>
              <given-names>Ahmed</given-names>
            </name>
            <name>
              <surname>L.</surname>
              <given-names>Rada</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00500-023-08173-1</pub-id>
          <article-title>A convex selective segmentation model based on a piece-wise constant metric-guided edge detector function</article-title>
          <source>Soft Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>1199</page-range>
          <issue>5</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Kou</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/electronics12051199</pub-id>
          <article-title>Techniques and challenges of image segmentation: A review</article-title>
          <source>Electronics</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhou</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Xia</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Chang</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Konukoglu</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Cremers</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2408.12957</pub-id>
          <article-title>Image segmentation in foundation model era: A survey</article-title>
          <source>arXiv</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>pp. 186–198</page-range>
          <issue>4</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jereni</surname>
              <given-names>B. H. N.</given-names>
            </name>
            <name>
              <surname>Sundire</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/ida020403</pub-id>
          <article-title>Enhanced detection of COVID-19 in Chest X-ray images: A comparative analysis of CNNs and the DL+ ensemble technique</article-title>
          <source>Inf. Dyn. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Long</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Shelhamer</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Darrell</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Fully convolutional networks for semantic segmentation</article-title>
          <source>2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
          <publisher-name>IEEE Computer Society</publisher-name>
          <year>2015</year>
          <page-range>3431–3440</page-range>
          <pub-id pub-id-type="doi">10.1109/CVPR.2015.7298965</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Ronneberger</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Fischer</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Brox</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>U-net: Convolutional networks for biomedical image segmentation</article-title>
          <source>Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015</source>
          <publisher-name>Cham: Springer International Publishing</publisher-name>
          <year>2015</year>
          <page-range>234–241</page-range>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>70-83</page-range>
          <issue>2</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Abo-El-Rejal</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ayman</surname>
              <given-names>S. E.</given-names>
            </name>
            <name>
              <surname>Aymen</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/ataiml030201</pub-id>
          <article-title>Advances in breast cancer segmentation: A comprehensive review</article-title>
          <source>Acadlore Trans. Mach. Learn.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <page-range>45-57</page-range>
          <issue>2</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Balovsyak</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Derevyanchuk</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Kovalchuk</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Kravchenko</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Ushenko</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.5815/ijmecs.2024.02.04</pub-id>
          <article-title>STEM project for vehicle image segmentation using fuzzy logic</article-title>
          <source>Int. J. Mod. Educ. Comput. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>183–192</page-range>
          <issue>3</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hussain</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/ataiml030305</pub-id>
          <article-title>Robust leaf disease detection using complex fuzzy sets and hsv-based color segmentation techniques</article-title>
          <source>Acadlore Trans. Mach. Learn.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Jobin Christ</surname>
              <given-names>M. C.</given-names>
            </name>
            <name>
              <surname>Parvathi</surname>
              <given-names>R. M. S.</given-names>
            </name>
          </person-group>
          <article-title>Fuzzy c-means algorithm for medical image segmentation</article-title>
          <source>2011 3rd International Conference on Electronics Computer Technology</source>
          <publisher-name>2011</publisher-name>
          <page-range>33–36</page-range>
          <pub-id pub-id-type="doi">10.1109/ICECTECH.2011.5941851</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Sojodishĳani</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Rostami</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Ramli</surname>
              <given-names>A. R.</given-names>
            </name>
          </person-group>
          <article-title>Real-time colour image segmentation with non-symmetric gaussian membership functions</article-title>
          <source>2008 Fifth International Conference on Computer Graphics, Imaging and Visualisation</source>
          <year>2008</year>
          <page-range>165–170</page-range>
          <pub-id pub-id-type="doi">10.1109/CGIV.2008.68</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>40</volume>
          <page-range>200-210</page-range>
          <issue>1</issue>
          <year>2012</year>
          <person-group person-group-type="author">
            <name>
              <surname>Celebi</surname>
              <given-names>M. E.</given-names>
            </name>
            <name>
              <surname>Kingravi</surname>
              <given-names>H. A.</given-names>
            </name>
            <name>
              <surname>Vela</surname>
              <given-names>P. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2012.07.021</pub-id>
          <article-title>A comparative study of efficient initialization methods for the k-means clustering algorithm</article-title>
          <source>Expert Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>4–10</page-range>
          <issue>1</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ganesh</surname>
              <given-names>E.N.</given-names>
            </name>
          </person-group>
          <article-title>Image segmentation using contemporary fuzzy logic</article-title>
          <source>Int. J. Comput. Sci. Eng. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>57-69</page-range>
          <issue>2</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mohamed</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Jumaat</surname>
              <given-names>A. K.</given-names>
            </name>
            <name>
              <surname>Mahmud</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.24191/mij.v5i2.926</pub-id>
          <article-title>Total variation selective segmentation-based active contour model with distance function and local image fitting energy for medical images</article-title>
          <source>Math. Sci. Inform. J.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>