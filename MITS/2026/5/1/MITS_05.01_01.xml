<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">MITS</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Mechatronics and Intelligent Transportation Systems</journal-title>
        <abbrev-journal-title abbrev-type="issn">Mechatron. Intell Transp. Syst.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">MITS</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-0218</issn>
      <issn publication-format="print">2958-020X</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-r2zD2haTlGTthf5mgBtnlQIAMejNbIhF</article-id>
      <article-id pub-id-type="doi">10.56578/mits050101</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>A Lightweight Real-Time Vision Framework for Road Infrastructure Monitoring in Intelligent Transportation Systems</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0006-0337-8705</contrib-id>
          <name>
            <surname>Chen</surname>
            <given-names>Quanliang</given-names>
          </name>
          <email>212309611232@hyit.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-0488-4011</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Lin</given-names>
          </name>
          <email>zlmjl@hyit.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3612-5051</contrib-id>
          <name>
            <surname>Ma</surname>
            <given-names>Jialin</given-names>
          </name>
          <email>zlmjl@hyit.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7715-3070</contrib-id>
          <name>
            <surname>Khadka</surname>
            <given-names>Ashim</given-names>
          </name>
          <email>ashim.khadka@ncit.edu.np</email>
        </contrib>
        <aff id="aff_1">Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, 223003 Huaian, China</aff>
        <aff id="aff_2">Faculty of Management Engineering, Huaiyin Institute of Technology, 223003 Huaian, China</aff>
        <aff id="aff_3">Nepal College of Information Technology, Pokhara University, 44700 Lalitpur, Nepal</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>29</day>
        <month>01</month>
        <year>2026</year>
      </pub-date>
      <volume>5</volume>
      <issue>1</issue>
      <fpage>1</fpage>
      <lpage>10</lpage>
      <page-range>1-10</page-range>
      <history>
        <date date-type="received">
          <day>14</day>
          <month>11</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>21</day>
          <month>01</month>
          <year>2026</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2026 by the author(s)</copyright-statement>
        <copyright-year>2026</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Reliable and timely perception of road surface conditions is a fundamental requirement in intelligent transportation systems (ITS), as it directly affects traffic safety, infrastructure maintenance, and the operation of connected and autonomous vehicles. Vision-based pothole detection has emerged as a practical solution due to its low sensing cost and deployment flexibility; however, existing deep learning approaches often struggle to achieve a satisfactory balance between detection accuracy, robustness to scale variations, and real-time performance on resource-constrained platforms. This study presents Partial Group-You Only Look Once (PG-YOLO), a lightweight real-time vision framework designed for road infrastructure monitoring in ITS. Built upon a compact one-stage detector, the proposed framework introduces a Partial Multi-Scale Feature Aggregation (PMFA) module to enhance the representation of small and irregular potholes under complex road conditions, as well as a Grouped Semantic Enhancement Attention (GSEA) module to improve high-level semantic discrimination with limited computational overhead. The framework is specifically designed to meet the low-latency and low-complexity requirements of vehicle-mounted and roadside sensing devices. Experimental evaluations conducted on a mixed road damage dataset demonstrate that the proposed approach achieves consistent improvements in detection accuracy while reducing model parameters and maintaining real-time inference speed. Compared with the baseline model, PG-YOLO improves precision, recall, and detection stability under challenging illumination and scale variations, while remaining suitable for edge deployment. These results indicate that the proposed framework can serve as an effective perception component for ITS, supporting continuous road condition awareness and data-driven maintenance and safety management.</p></abstract>
      <kwd-group>
        <kwd>Intelligent transportation systems</kwd>
        <kwd>Road infrastructure monitoring</kwd>
        <kwd>Pothole detection</kwd>
        <kwd>Real-time vision perception</kwd>
        <kwd>Lightweight deep learning</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="4"/>
        <fig-count count="5"/>
        <table-count count="4"/>
        <ref-count count="24"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Road potholes are a pervasive form of pavement distress that degrades ride comfort, accelerates vehicle wear, and may trigger loss-of-control events, particularly for two-wheelers and at high speed. For highway agencies and city operators, rapid pothole discovery is also a prerequisite for preventive maintenance, cost-effective rehabilitation, and data-driven prioritization of repair actions [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>]. In intelligent transportation systems (ITS), pothole perception is therefore not merely an inspection task; it is an enabling capability for safety warning, infrastructure health monitoring, and reliable autonomous driving.</p><p>A vision-based pothole detector can be naturally integrated into several ITS and mechatronic mobility architectures. First, it can run on vehicle-mounted cameras with an on-board computer to provide real-time driver warnings or to support motion planning modules in autonomous vehicles. Second, it can be deployed on roadside monitoring units or smart city sensing nodes to continuously assess pavement condition on key road segments. Third, it can be integrated into infrastructure inspection platforms, such as maintenance vehicles or inspection robots, to automate routine road surface surveys.</p><p>With the rapid advancement of deep learning, convolutional neural network (CNN)-based detectors have become a mainstream solution for road damage perception. Representative frameworks include one-stage models (e.g., the You Only Look Once (YOLO) family [<xref ref-type="bibr" rid="ref_3">3</xref>]) and two-stage models (e.g., Faster Regions with Convolutional Neural Networks (Faster R-CNN) [<xref ref-type="bibr" rid="ref_4">4</xref>]), as well as Single Shot MultiBox Detector (SSD) [<xref ref-type="bibr" rid="ref_5">5</xref>] and transformer-based detectors such as DEtection TRansformer (DETR) variants [<xref ref-type="bibr" rid="ref_6">6</xref>]. Early work mainly used CNNs for classification or coarse recognition. For example, Mittal et al. demonstrated the effectiveness of Visual Geometry Group 16 (VGG16) for pothole recognition [<xref ref-type="bibr" rid="ref_7">7</xref>], and Ye et al [<xref ref-type="bibr" rid="ref_8">8</xref>]. improved robustness to noise and scale variations by introducing a pre-pooling strategy. Subsequent studies investigated two-stage detection pipelines, for example, Yebes et al [<xref ref-type="bibr" rid="ref_9">9</xref>]. introduced a large-scale pothole dataset and performed comparisons using Faster R-CNN, while Lan et al.  [<xref ref-type="bibr" rid="ref_10">10</xref>] combined detection with pixel-level statistics to quantify pothole areas. Although accurate, two-stage methods often incur high computational overhead, which makes them less suitable for real-time ITS deployments.</p><p>With the development of one-stage detection algorithms, research attention has gradually shifted toward lightweight models that balance detection accuracy and inference speed. Tithi et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] implemented pothole detection and vehicle speed control based on SSD and MobileNet architectures. Ahmed [<xref ref-type="bibr" rid="ref_12">12</xref>] proposed VGG16 with dilated convolutions as Faster R-CNN backbone, reducing computation and improving pothole detection accuracy and speed. Building on this progress, Maeda et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] released a public dataset covering multiple categories of road surface distress, which has facilitated standardized evaluation in related research. Some studies [<xref ref-type="bibr" rid="ref_14">14</xref>], [<xref ref-type="bibr" rid="ref_15">15</xref>] further improved the detection performance of small-scale potholes under complex illumination and occlusion conditions by incorporating multi-scale feature fusion and attention mechanisms. In recent years, models such as YOLOv4, YOLOv7, and YOLOv8 [<xref ref-type="bibr" rid="ref_16">16</xref>], [<xref ref-type="bibr" rid="ref_17">17</xref>], [<xref ref-type="bibr" rid="ref_18">18</xref>] have been successively introduced into pothole detection tasks, achieving notable improvements in both real-time performance and detection accuracy. Improving Detection Stability of Small-Scale Potholes in Complex Backgrounds through Multi-Scale Feature Modeling and Contextual Information Enhancement Strategies [<xref ref-type="bibr" rid="ref_19">19</xref>]. In addition, certain works enhance structural modeling of road defects using depth or geometric cues, whereas others improve deployment efficiency on embedded and edge devices through efficient feature representation and network architecture optimization [<xref ref-type="bibr" rid="ref_20">20</xref>], [<xref ref-type="bibr" rid="ref_21">21</xref>].</p><p>Despite these successes, some challenges remain unresolved. Existing methods struggle to effectively detect potholes with highly variable morphologies and scale differences, and detection models are often computationally expensive, limiting their practical applicability. Building upon previous research, this study proposes a novel pothole detection model, Partial Group-YOLO (PG-YOLO), by integrating the Partial Multi-Scale Feature Aggregation (PMFA) and Grouped Semantic Enhancement Attention (GSEA) modules. The contributions of this study are summarized as follows:</p><p>(1) A PMFA module is designed to enhance the extraction of fine-grained details and small-object features through multi-scale convolutions and partial channel fusion, thereby effectively improving small-object detection performance in complex environments.</p><p>(2) A GSEA module is proposed, which employs parallel modeling of grouped convolutions and global attention to enhance high-level semantic representation and salient-region awareness, thereby further improving detection robustness under complex environments and interference conditions.</p>
    </sec>
    <sec sec-type="">
      <title>2. Proposed method</title>
      <p>The development of the PG-YOLO model is aimed at addressing two key challenges in road pothole detection:</p><p>(a) Road potholes exhibit arbitrary shapes and complex geometric structures, making them considerably more difficult to detect than objects with relatively fixed shapes, such as vehicles, pedestrians, and cyclists. Moreover, potholes generally occupy a small proportion of image pixels, resulting in limited extractable appearance information and a high risk of feature loss during forward propagation in deep networks, which in turn constrains detection accuracy.</p><p>(b) ITS deployments typically require low latency and low power consumption on embedded or edge hardware, which constrains model size and computational complexity.</p><p>To address these challenges, we enhance the YOLOv11n model and propose the PG-YOLO framework. By replacing the C3k2 modules in the YOLOv11n backbone with the PMFA module, the model captures multi-scale receptive fields from local to global regions, improving edge detail representation and detection accuracy. Additionally, the GSEA module is incorporated to disperse fused high-level semantic features across different detection layers, shortening the feature propagation path, reducing information loss, and increasing detection speed.</p><p>The architecture of PG-YOLO is illustrated in <xref ref-type="fig" rid="fig_1">Figure 1</xref>. </p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Partial Group-You Only Look Once (PG-YOLO) network architecture</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_0K-VICf3frVsvYPI.png"/>
        </fig>
      
      
        <sec>
          
            <title>2.1. Partial multi-scale feature aggregation module</title>
          
          <p>Road potholes typically present large scale variations, irregular shapes, and intricate edge details, which can lead existing detection networks to lose fine-grained features or fail to adequately represent small and irregular targets. To enhance the network’s perception and representation of such targets, this study proposes a PMFA module. </p><p>The PMFA incorporates residual connections, as introduced in ResNet [<xref ref-type="bibr" rid="ref_22">22</xref>], effectively mitigating the vanishing gradient problem. It also adopts the 1 <inline-formula>
  <mml:math id="m2hgurzh22">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 1 convolution strategy from FasterNet [<xref ref-type="bibr" rid="ref_23">23</xref>] to reduce channel dimensionality, thereby decreasing model parameters and computational cost. Consequently, the module integrates multi-scale feature extraction, edge information enhancement, and convolutional feature fusion to improve the model’s representational capacity.</p><p>In the PMFA module, the input feature map is processed through stacked multi-scale convolutions (3 <inline-formula>
  <mml:math id="msd8500e1q">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 3, 5 <inline-formula>
  <mml:math id="mbxp8m6v5v">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 5, 7 <inline-formula>
  <mml:math id="mvs1ze85sa">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 7), enabling progressive extraction and fusion of features across different receptive fields. This captures both local and global information, allowing comprehensive modeling from fine edge details to larger contextual regions. Potholes often vary in size and shape, and the PMFA module’s multi-scale pathways enable the network to capture spatial features across scales, improving small-object detection. Moreover, its partial channel convolution reduces redundant computation, preserves feature density, and enhances both efficiency and feature attention.</p><p>The PMFA module also employs 1 <inline-formula>
  <mml:math id="mhd26moplu">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 1 convolution and residual connections to fuse multi-scale features while retaining the original input, strengthening feature correlations and preserving maximal information. This feature reconstruction mechanism enables the model to maintain global structural consistency while more effectively extracting key pothole edge information. As a result, the backbone network can fully capture features of targets at different scales, further enhancing the overall feature extraction capability. The structure is shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>, and the implementation process of the PMFA module is as follows:</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="ma2fiy5o4t">
                <mml:mi>Y</mml:mi>
                <mml:mi>X</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:msub>
                  <mml:mi>Conv</mml:mi>
                  <mml:mrow>
                    <mml:mn>1</mml:mn>
                    <mml:mn>1</mml:mn>
                    <mml:mo>×</mml:mo>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mi>Concat</mml:mi>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mrow>
                        <mml:mn>3</mml:mn>
                        <mml:mn>3</mml:mn>
                        <mml:mo>×</mml:mo>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mrow>
                        <mml:mn>5</mml:mn>
                        <mml:mn>5</mml:mn>
                        <mml:mo>×</mml:mo>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mrow>
                        <mml:mn>7</mml:mn>
                        <mml:mn>7</mml:mn>
                        <mml:mo>×</mml:mo>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mkyg5bxs9v">
    <mml:msub>
      <mml:mi>F</mml:mi>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>×</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>k</mml:mi>
        <mml:mi>k</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> represents the intermediate feature maps produced by <inline-formula>
  <mml:math id="mrsv9m6d8m">
    <mml:mi>k</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> convolutions on partial channels, $X<inline-formula>
  <mml:math id="mf28v29zoj">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>Y<inline-formula>
  <mml:math id="mr13zicy5b">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>Concat$ represents channel-wise concatenation.</p><p>In summary, PMFA expands the effective receptive field across multiple scales while preserving fine edge details through partial computation and residual fusion, thereby improving small and irregular pothole detection with limited overhead.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Partial Multi-Scale Feature Aggregation (PMFA) module</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_CUR_zfNwAf0o6IMY.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>2.2. Grouped semantic enhancement attention module</title>
          
          <p>To further enhance semantic representation and salient-region awareness, we propose the GSEA module, which is designed to be computationally efficient for edge deployment. As shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>, GSEA adopts a dual-branch architecture that combines local grouped interactions and global channel attention, and fuses them via residual learning.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Grouped Semantic Enhancement Attention (GSEA) module</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_lpoMN1KPC_AwnX05.png"/>
            </fig>
          
          <p>In the local branch, the input feature map is first processed by a 1 <inline-formula>
  <mml:math id="mutyiy2nb4">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 1 convolution to adjust channel dimensions and reduce redundancy, enhancing the efficiency of subsequent operations. It then passes through two stages of 3 <inline-formula>
  <mml:math id="mzs8t1jcs3">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 3 grouped convolutions, where each group models local patterns, retaining fine details while lowering computational overhead.</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="mrsbq6heos">
                <mml:msub>
                  <mml:mi>F</mml:mi>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mi>o</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mi>v</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mtext> Concat </mml:mtext>
                <mml:mrow>
                  <mml:mo>[</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>]</mml:mo>
                  <mml:msubsup>
                    <mml:mi>W</mml:mi>
                    <mml:mrow>
                      <mml:mn>3</mml:mn>
                      <mml:mn>3</mml:mn>
                      <mml:mo>×</mml:mo>
                    </mml:mrow>
                    <mml:mn>1</mml:mn>
                  </mml:msubsup>
                  <mml:msubsup>
                    <mml:mi>W</mml:mi>
                    <mml:mrow>
                      <mml:mn>3</mml:mn>
                      <mml:mn>3</mml:mn>
                      <mml:mo>×</mml:mo>
                    </mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:msubsup>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msubsup>
                      <mml:mi>Y</mml:mi>
                      <mml:mn>1</mml:mn>
                      <mml:mrow>
                        <mml:mtext>Group </mml:mtext>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msubsup>
                      <mml:mi>Y</mml:mi>
                      <mml:mn>2</mml:mn>
                      <mml:mrow>
                        <mml:mtext>Group </mml:mtext>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, Group 1 has $1-\hat{C} / 2<inline-formula>
  <mml:math id="misrhlcqyg">
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mn>2</mml:mn>
  </mml:math>
</inline-formula>\hat{C} / 2+1-\hat{C}<inline-formula>
  <mml:math id="mp2kiaubrk">
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>Y<inline-formula>
  <mml:math id="ma2zqhftoc">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>1 \times 1<inline-formula>
  <mml:math id="m2hmssq54w">
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>3 \times 3<inline-formula>
  <mml:math id="mdummat0n5">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>(Q)<inline-formula>
  <mml:math id="mpku8km9a4">
    <mml:mo>,</mml:mo>
    <mml:mi>k</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>(K)<inline-formula>
  <mml:math id="mlz86thb3f">
    <mml:mo>,</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>(V)<inline-formula>
  <mml:math id="mf9xahszt9">
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>F</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>(Q)<inline-formula>
  <mml:math id="mxhh5i4d2e">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
  </mml:math>
</inline-formula>\left(\hat{Q} \in \mathbb{R}^{H W \times \hat{C}}\right)<inline-formula>
  <mml:math id="m8clkxu9ks">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>(K)<inline-formula>
  <mml:math id="mbhqron2lo">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mo>(</mml:mo>
  </mml:math>
</inline-formula>\widehat{K} \in \mathbb{R}^{H W \times \hat{C}}$), followed by feature weighting to produce the output.</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="mxrr885qyg">
                <mml:mi>Q</mml:mi>
                <mml:mi>K</mml:mi>
                <mml:mi>V</mml:mi>
                <mml:mo>,</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo fence="true"/>
                  <mml:mi>Split</mml:mi>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msubsup>
                      <mml:mi>W</mml:mi>
                      <mml:mrow>
                        <mml:mn>3</mml:mn>
                        <mml:mn>3</mml:mn>
                        <mml:mo>×</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>d</mml:mi>
                        <mml:mi>w</mml:mi>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>W</mml:mi>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                          <mml:mn>1</mml:mn>
                          <mml:mo>×</mml:mo>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mi>Y</mml:mi>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="mm8miqw0i2">
                <mml:mi>A</mml:mi>
                <mml:mi>Softmax</mml:mi>
                <mml:mi>α</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>∈</mml:mo>
                <mml:mrow>
                  <mml:mover>
                    <mml:mi>K</mml:mi>
                    <mml:mo>^</mml:mo>
                  </mml:mover>
                </mml:mrow>
                <mml:mrow>
                  <mml:mover>
                    <mml:mi>Q</mml:mi>
                    <mml:mo>^</mml:mo>
                  </mml:mover>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>/</mml:mo>
                </mml:mrow>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>R</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mover>
                        <mml:mi>C</mml:mi>
                        <mml:mo>^</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mover>
                        <mml:mi>C</mml:mi>
                        <mml:mo>^</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                    <mml:mo>×</mml:mo>
                  </mml:mrow>
                </mml:msup>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="mrj3n2144x">
                <mml:msub>
                  <mml:mi>F</mml:mi>
                  <mml:mrow>
                    <mml:mi>a</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mi>t</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>W</mml:mi>
                  <mml:mrow>
                    <mml:mn>1</mml:mn>
                    <mml:mn>1</mml:mn>
                    <mml:mo>×</mml:mo>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>×</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mi>V</mml:mi>
                <mml:mi>A</mml:mi>
                <mml:mi>Y</mml:mi>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mluqoygvd3">
    <mml:mi>S</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
  </mml:math>
</inline-formula> denotes the channel-wise split operation; $A<inline-formula>
  <mml:math id="mc299rz1z8">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>\alpha<inline-formula>
  <mml:math id="mox7x11gfb">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>;</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>Y$ represents the module output.</p><p>Finally, the local features from the local branch are fused with the attention features from the global branch via a residual connection. In summary, GSEA enhances discriminative semantics by jointly modeling efficient local interactions and global channel dependencies, improving robustness under complex backgrounds with modest computational overhead.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Experimental design and result analysis</title>
      
        <sec>
          
            <title>3.1. Dataset</title>
          
          <p>In this study, model training is conducted on MyDataset, which is constructed by combining the open-source Road Damage Dataset 2022 (RDD2022) dataset [<xref ref-type="bibr" rid="ref_24">24</xref>] with a self-collected dataset. RDD2022 contains 47,420 images captured using smartphones and high-resolution cameras across six countries—Japan, India, the Czech Republic, Norway, the United States, and China—and includes over 55,000 annotated instances of four types of road damage: longitudinal cracks, transverse cracks, alligator cracks, and potholes. After data filtering, 3,195 valid images were retained. To further enhance the model’s generalization ability in complex environments, additional images collected from online sources and field acquisition were incorporated, resulting in a final dataset of 4,920 images.</p><p>All images were randomly shuffled and annotated using the LabelImg tool in the PASCAL Visual Object Classes (VOC) format, with object locations and class labels stored in XML files. The dataset was then split into training, validation, and test sets with a ratio of 7:1:2.</p><p>The multi-country and multi-device nature of MyDataset introduces diversity in pavement texture, lane markings, camera viewpoints, and ambient illumination, which helps approximate real transportation environments. Nevertheless, some conditions remain under-represented (e.g., severe nighttime scenes, heavy rain, extreme motion blur, and rare pavement materials). Accordingly, the reported results should be interpreted as performance on the available visual conditions, and domain adaptation or additional data collection may be required for deployment in unseen environments.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Experimental environment</title>
          
          <p>The experimental environment configuration is shown in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Experimental environment</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Environment Settings</p></th><th colspan="1" rowspan="1"><p>Configuration</p></th></tr><tr><td colspan="1" rowspan="1"><p>System environment</p></td><td colspan="1" rowspan="1"><p>Windows 11</p></td></tr><tr><td colspan="1" rowspan="1"><p>Programming language version</p></td><td colspan="1" rowspan="1"><p>Python 3.10</p></td></tr><tr><td colspan="1" rowspan="1"><p>Deep learning framework</p></td><td colspan="1" rowspan="1"><p>PyTorch 2.2.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>GPU</p></td><td colspan="1" rowspan="1"><p>RTX 4070 Ti Super (16 GB)</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>All models were trained with identical hyperparameter settings, using Stochastic Gradient Descent (SGD) optimization and a cosine annealing learning rate strategy. The specific training hyperparameters are summarized in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Training hyperparameters</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Parameter</p></th><th colspan="1" rowspan="1"><p>Value</p></th></tr><tr><td colspan="1" rowspan="1"><p>Learning rate</p></td><td colspan="1" rowspan="1"><p>0.01</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image size</p></td><td colspan="1" rowspan="1"><p>640 <mml:math id="m3dsezyv2p">
  <mml:mo>×</mml:mo>
</mml:math> 640</p></td></tr><tr><td colspan="1" rowspan="1"><p>Optimizer</p></td><td colspan="1" rowspan="1"><p>Stochastic Gradient Descent (SGD)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Batch size</p></td><td colspan="1" rowspan="1"><p>32</p></td></tr><tr><td colspan="1" rowspan="1"><p>Epochs</p></td><td colspan="1" rowspan="1"><p>200</p></td></tr><tr><td colspan="1" rowspan="1"><p>Weight decay</p></td><td colspan="1" rowspan="1"><p>0.0005</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. Model evaluation indicators</title>
          
          <p>In this study, the evaluation metrics include:</p><p>Giga Floating-Point Operations (GFLOPs): the number of floating-point operations required by the model, used to measure its computational complexity and estimate execution cost. Params (Parameters): the total number of model parameters, which reflects the model size and structural complexity. FPS (Frames Per Second): the number of images processed per second, serving as a key indicator of the model’s real-time inference capability.</p><p>Precision (P): The proportion of true positive samples among all samples predicted as positive by the model.</p>
          
            <disp-formula>
              <label>(6)</label>
              <mml:math id="mfnjdp0po6">
                <mml:mi>P</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>Recall (R): The proportion of true positive samples correctly detected by the model among all actual positive samples.</p>
          
            <disp-formula>
              <label>(7)</label>
              <mml:math id="mj4qb4xdt2">
                <mml:mi>R</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>Average Precision (AP): The average precision value for a single category.</p>
          
            <disp-formula>
              <label>(8)</label>
              <mml:math id="mv15q12ob2">
                <mml:mi>A</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mi>R</mml:mi>
                <mml:mi>d</mml:mi>
                <mml:mi>R</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:msubsup>
                  <mml:mo>∫</mml:mo>
                  <mml:mn>0</mml:mn>
                  <mml:mn>1</mml:mn>
                </mml:msubsup>
              </mml:math>
            </disp-formula>
          
          <p>Mean Average Precision (mAP): The mean of Average Precision values over all object categories.</p><p>where, $N<inline-formula>
  <mml:math id="my0dj8frjl">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>A P_i<inline-formula>
  <mml:math id="mug9a0u44q">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>AP<inline-formula>
  <mml:math id="mi38xa3b22">
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>i$-th class.</p>
          
            <disp-formula>
              <label>(9)</label>
              <mml:math id="mn833btbsj">
                <mml:mi>m</mml:mi>
                <mml:mi>A</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mi>A</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mn>1</mml:mn>
                  <mml:mi>N</mml:mi>
                </mml:mfrac>
                <mml:munderover>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>N</mml:mi>
                </mml:munderover>
                <mml:msub>
                  <mml:mi>P</mml:mi>
                  <mml:mi>i</mml:mi>
                </mml:msub>
              </mml:math>
            </disp-formula>
          
        </sec>
      
      
        <sec>
          
            <title>3.4. Ablation experiment</title>
          
          <p>To evaluate the effectiveness of the proposed modules, YOLOv11n is used as the baseline for ablation experiments on the test set. Each module is incrementally removed to quantify its contribution to detection performance. The results are reported in <xref ref-type="table" rid="table_3">Table 3</xref>, where bold values indicate the best results and "<inline-formula>
  <mml:math id="mkqhbhu7i9">
    <mml:mi>✓</mml:mi>
  </mml:math>
</inline-formula>" denotes the presence of a module.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Ablation study evaluating the impact of model components</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p>PMFA</p></th><th colspan="1" rowspan="1"><p>GSEA</p></th><th colspan="1" rowspan="1"><p>mAP50</p></th><th colspan="1" rowspan="1"><p>P (%)</p></th><th colspan="1" rowspan="1"><p>R (%)</p></th><th colspan="1" rowspan="1"><p>Param (M)</p></th><th colspan="1" rowspan="1"><p>GFLOPs</p></th><th colspan="1" rowspan="1"><p>FPS (f/s)</p></th></tr><tr><td colspan="1" rowspan="1"><p>Yolov11n</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>67.5</p></td><td colspan="1" rowspan="1"><p>75.4</p></td><td colspan="1" rowspan="1"><p>58.5</p></td><td colspan="1" rowspan="1"><p>2.58</p></td><td colspan="1" rowspan="1"><p>6.5</p></td><td colspan="1" rowspan="1"><p>143</p></td></tr><tr><td colspan="1" rowspan="1"><p>Model_1</p></td><td colspan="1" rowspan="1"><p><mml:math id="mah6e88j0f">
  <mml:mi>✓</mml:mi>
</mml:math></p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>68.3</p></td><td colspan="1" rowspan="1"><p>76.1</p></td><td colspan="1" rowspan="1"><p>61.5</p></td><td colspan="1" rowspan="1"><p>2.13</p></td><td colspan="1" rowspan="1"><p>5.9</p></td><td colspan="1" rowspan="1"><p>172</p></td></tr><tr><td colspan="1" rowspan="1"><p>Model_2</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p><mml:math id="mz9sz4xydi">
  <mml:mi>✓</mml:mi>
</mml:math></p></td><td colspan="1" rowspan="1"><p>69.4</p></td><td colspan="1" rowspan="1"><p>77.6</p></td><td colspan="1" rowspan="1"><p>62.1</p></td><td colspan="1" rowspan="1"><p>2.37</p></td><td colspan="1" rowspan="1"><p>7.1</p></td><td colspan="1" rowspan="1"><p>122</p></td></tr><tr><td colspan="1" rowspan="1"><p>Ours</p></td><td colspan="1" rowspan="1"><p><mml:math id="m5d7bbta26">
  <mml:mi>✓</mml:mi>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="mu0hxeh3nt">
  <mml:mi>✓</mml:mi>
</mml:math></p></td><td colspan="1" rowspan="1"><p>70.7</p></td><td colspan="1" rowspan="1"><p>78.8</p></td><td colspan="1" rowspan="1"><p>64.4</p></td><td colspan="1" rowspan="1"><p>2.31</p></td><td colspan="1" rowspan="1"><p>6.3</p></td><td colspan="1" rowspan="1"><p>158</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>As shown in <xref ref-type="table" rid="table_3">Table 3</xref>, adding PMFA alone improves mAP@0.5 from 67.5% to 68.3%, precision from 75.4% to 76.1%, and recall from 58.5% to 61.5%, while reducing parameters from 2.58 M to 2.13 M . This indicates that multi-scale aggregation and partial computation help preserve small-pothole cues without increasing complexity.</p><p>Adding GSEA alone improves mAP@0.5 to 69.4% and increases precision and recall to 77.6% and 62.1%, respectively, demonstrating that efficient local-global attention strengthens semantic discrimination. When both modules are enabled, PG-YOLO achieves the best overall accuracy (70.7% mAP@0.5) with 2.31 M parameters and 6.3 GFLOPs , and it runs at 158 FPS . These results suggest that PMFA and GSEA are complementary and jointly improve detection robustness under complex roadscene conditions.</p><p>The proposed model is lightweight (2.31 M parameters and 6.3 GFLOPs at 640 <inline-formula>
  <mml:math id="mign97wg2c">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 1640 input), which makes it suitable for resource-constrained edge deployment. On a desktop GPU (RTX 4070 Ti Super), PG-YOLO achieves 158 FPS, corresponding to a per-frame latency of about 6.3 ms . Although embedded platforms typically provide lower peak throughput, the low parameter count and modest compute budget indicate that real-time operation at common camera rates (e.g., 30 FPS) is feasible with hardware acceleration. Practical deployment can further benefit from engineering optimizations such as TensorRT compilation, FP16 quantization, and efficient batching or pipelining, without changing the model structure. Therefore, PG-YOLO is designed as a perception module that can be embedded into ITS pipelines. In vehicle-mounted settings, the model can process camera streams in real time to support driver warnings, ADAS functions, or autonomous driving planning.</p><p>Overall, the ablation results in <xref ref-type="table" rid="table_3">Table 3</xref> demonstrate that both proposed modules are effective and complementary, with no performance conflict when combined, and confirm the effectiveness of the proposed method for multi-scale pothole detection.</p><p>As shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>, the precision, recall, and mAP (mAP@50 and mAP@50–95) curves of PG-YOLO (Ours) gradually converge after approximately 150 training epochs. Compared with other models, PG-YOLO exhibits significantly smaller fluctuations, indicating a more stable training process, more balanced parameter optimization, and superior convergence and reliability.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Comparison of different evaluation metrics</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_5s9JFk0wX-IJhf8f.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.5. Comparative experiment</title>
          
          <p>To further validate the effectiveness and generalization capability of the proposed PG-YOLO model, comparative experiments were conducted against several state-of-the-art object detection models under identical dataset settings. The experimental results are summarized in <xref ref-type="table" rid="table_4">Table 4</xref>, where bold values indicate the best performance.</p>
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Comparison results of different models on the MyDataset dataset</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Method</p></th><th colspan="1" rowspan="1"><p>P (%)</p></th><th colspan="1" rowspan="1"><p>R (%)</p></th><th colspan="1" rowspan="1"><p>mAP50 (%)</p></th><th colspan="1" rowspan="1"><p>Param (M)</p></th><th colspan="1" rowspan="1"><p>GFLOPs</p></th><th colspan="1" rowspan="1"><p>FPS (f/s)</p></th></tr><tr><td colspan="1" rowspan="1"><p>Faster R-CNN</p></td><td colspan="1" rowspan="1"><p>60.2</p></td><td colspan="1" rowspan="1"><p>42.6</p></td><td colspan="1" rowspan="1"><p>48.3</p></td><td colspan="1" rowspan="1"><p>36.3</p></td><td colspan="1" rowspan="1"><p>190.4</p></td><td colspan="1" rowspan="1"><p>35</p></td></tr><tr><td colspan="1" rowspan="1"><p>SSD</p></td><td colspan="1" rowspan="1"><p>53.2</p></td><td colspan="1" rowspan="1"><p>37.1</p></td><td colspan="1" rowspan="1"><p>41.1</p></td><td colspan="1" rowspan="1"><p>39.5</p></td><td colspan="1" rowspan="1"><p>160.8</p></td><td colspan="1" rowspan="1"><p>43</p></td></tr><tr><td colspan="1" rowspan="1"><p>RT-DETR-r18</p></td><td colspan="1" rowspan="1"><p>54.9</p></td><td colspan="1" rowspan="1"><p>44.4</p></td><td colspan="1" rowspan="1"><p>45.3</p></td><td colspan="1" rowspan="1"><p>19.8</p></td><td colspan="1" rowspan="1"><p>56.9</p></td><td colspan="1" rowspan="1"><p>58</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv5n</p></td><td colspan="1" rowspan="1"><p>75.4</p></td><td colspan="1" rowspan="1"><p>58.6</p></td><td colspan="1" rowspan="1"><p>66.2</p></td><td colspan="1" rowspan="1"><p>2.50</p></td><td colspan="1" rowspan="1"><p>7.1</p></td><td colspan="1" rowspan="1"><p>173</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv7tiny</p></td><td colspan="1" rowspan="1"><p>76.0</p></td><td colspan="1" rowspan="1"><p>60.5</p></td><td colspan="1" rowspan="1"><p>65.9</p></td><td colspan="1" rowspan="1"><p>8.32</p></td><td colspan="1" rowspan="1"><p>21.8</p></td><td colspan="1" rowspan="1"><p>108</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv8n</p></td><td colspan="1" rowspan="1"><p>76.5</p></td><td colspan="1" rowspan="1"><p>59.5</p></td><td colspan="1" rowspan="1"><p>67.3</p></td><td colspan="1" rowspan="1"><p>3.01</p></td><td colspan="1" rowspan="1"><p>8.1</p></td><td colspan="1" rowspan="1"><p>151</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv10n</p></td><td colspan="1" rowspan="1"><p>73.4</p></td><td colspan="1" rowspan="1"><p>60.1</p></td><td colspan="1" rowspan="1"><p>67.6</p></td><td colspan="1" rowspan="1"><p>2.56</p></td><td colspan="1" rowspan="1"><p>6.7</p></td><td colspan="1" rowspan="1"><p>136</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv11n</p></td><td colspan="1" rowspan="1"><p>75.4</p></td><td colspan="1" rowspan="1"><p>58.5</p></td><td colspan="1" rowspan="1"><p>67.5</p></td><td colspan="1" rowspan="1"><p>2.58</p></td><td colspan="1" rowspan="1"><p>6.5</p></td><td colspan="1" rowspan="1"><p>143</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv11s</p></td><td colspan="1" rowspan="1"><p>76.0</p></td><td colspan="1" rowspan="1"><p>64.2</p></td><td colspan="1" rowspan="1"><p>70.5</p></td><td colspan="1" rowspan="1"><p>9.41</p></td><td colspan="1" rowspan="1"><p>21.3</p></td><td colspan="1" rowspan="1"><p>117</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv12n</p></td><td colspan="1" rowspan="1"><p>75.1</p></td><td colspan="1" rowspan="1"><p>58.3</p></td><td colspan="1" rowspan="1"><p>66.1</p></td><td colspan="1" rowspan="1"><p>2.51</p></td><td colspan="1" rowspan="1"><p>5.8</p></td><td colspan="1" rowspan="1"><p>103</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv13n</p></td><td colspan="1" rowspan="1"><p>73.0</p></td><td colspan="1" rowspan="1"><p>59.4</p></td><td colspan="1" rowspan="1"><p>65.6</p></td><td colspan="1" rowspan="1"><p>2.45</p></td><td colspan="1" rowspan="1"><p>6.1</p></td><td colspan="1" rowspan="1"><p>91</p></td></tr><tr><td colspan="1" rowspan="1"><p>Ours</p></td><td colspan="1" rowspan="1"><p>78.8</p></td><td colspan="1" rowspan="1"><p>64.4</p></td><td colspan="1" rowspan="1"><p>70.7</p></td><td colspan="1" rowspan="1"><p>2.31</p></td><td colspan="1" rowspan="1"><p>6.3</p></td><td colspan="1" rowspan="1"><p>158</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>As shown in <xref ref-type="table" rid="table_4">Table 4</xref>, PG-YOLO achieves the highest precision (78.8%) and recall (64.4%) among the compared lightweight models, with 70.7% mAP@0.5. Compared with YOLOv11s, PG-YOLO reduces parameters by 7.10M and computation by 15.0 GFLOPs, while slightly improving precision, recall, and mAP@0.5, and increasing inference speed by 41 FPS. These results demonstrate that the proposed design improves detection accuracy without sacrificing deployment efficiency.</p><p>Overall, PG-YOLO shows strong adaptability to multi-scale pothole detection in complex road scenes, enabling more effective feature extraction when visual cues are weak and thus improving detection robustness.</p>
        </sec>
      
      
        <sec>
          
            <title>3.6. Comparison of detection results</title>
          
          <p>To further evaluate the effectiveness of the improved model, PG-YOLO and the original YOLOv11 were tested, and the results are illustrated in <xref ref-type="fig" rid="fig_5">Figure 5</xref>. In <xref ref-type="fig" rid="fig_5">Figure 5</xref>, subfigures (1) and (3) present the detection results and corresponding heatmaps generated by YOLOv11, while subfigures (2) and (4) show the detection results and heatmaps produced by PG-YOLO.</p><p>As can be observed from <xref ref-type="fig" rid="fig_5">Figure 5</xref>, the original YOLOv11 model is prone to false positives and missed detections when dealing with small-sized potholes and low-light conditions. In contrast, PG-YOLO demonstrates noticeable improvements under these challenging scenarios. These results indicate that PG-YOLO consistently outperforms the original YOLOv11 in detection performance.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Comparison of different metrics</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_Q9wFC0vio5CisTZ7.png"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>4. Conclusions</title>
      <p>This paper presented PG-YOLO, a real-time and lightweight pothole detector tailored for intelligent transportation and mechatronic mobility applications. Building on YOLOv11n, we introduced the PMFA module to improve multi-scale feature representation and preserve fine edge cues via partial-channel aggregation, and the GSEA module to strengthen salient-region semantics through efficient local–global attention.</p><p>Experiments on a mixed pothole dataset (4,920 images) showed that PG-YOLO improves mAP@0.5, precision, and recall by 3.2, 3.4, and 5.9 percentage points over YOLOv11n while reducing parameters by 10%. PG-YOLO reaches 158 FPS on an RTX 4070 Ti Super, suggesting strong potential for real-time operation on vehicle-mounted cameras, roadside monitoring units, and inspection robots.</p><p>Future work will focus on expanding the dataset to better cover extreme transportation conditions, improving robustness via context-aware modeling and background suppression, and validating end-to-end deployment in real ITS pipelines with localization and maintenance decision modules.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>Conceptualization, Q.L.C. and L.Z.; methodology, Q.L.C.; investigation, Q.L.C.; data curation, Q.L.C. and A.K.; writing—original draft preparation, Q.L.C.; writing—review and editing, L.Z., J.L.M., and A.K.; visualization, Q.L.C.; supervision, L.Z. All authors have read and agreed to the published version of the manuscript.</p>
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="conf-paper">
          <page-range>157-162</page-range>
          <publisher-place>Karaganda, Kazakhstan</publisher-place>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bakirbayeva</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Rakhimov</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Aubakirova</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Aldungarova</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Menendez</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Mukhambetkaliev</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/LINDI63813.2024.10820422</pub-id>
          <article-title>Impact of pavement quality on improving logistics schemes in the transportation industry</article-title>
          <source>2024 IEEE 6th International Symposium on Logistics and Industrial Informatics</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>4581</page-range>
          <issue>12</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Braunfelds</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Senkans</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Skels</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Janeliukstis</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Porins</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Spolitis</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Bobrovs</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s22124581</pub-id>
          <article-title>Road pavement structural health monitoring by embedded fiber-bragg-grating-based optical sensors</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="conf-paper">
          <page-range>779-788</page-range>
          <publisher-place>Las Vegas, NV, USA</publisher-place>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Redmon</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Divvala</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Girshick</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Farhadi</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CVPR.2016.91</pub-id>
          <article-title>You Only Look Once: Unified, real-time object detection</article-title>
          <source>2016 IEEE Conference on Computer Vision and Pattern Recognition</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>39</volume>
          <page-range>1137-1149</page-range>
          <issue>6</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ren</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Girshick</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TPAMI.2016.2577031</pub-id>
          <article-title>Faster R-CNN: Towards real-time object detection with region proposal networks</article-title>
          <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="conf-paper">
          <page-range>21-37</page-range>
          <publisher-place>Cham, Switzerland</publisher-place>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Anguelov</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Erhan</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Szegedy</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Reed</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>C. Y.</given-names>
            </name>
            <name>
              <surname>Berg</surname>
              <given-names>A. C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-319-46448-0_2</pub-id>
          <article-title>SSD: Single shot multibox detector</article-title>
          <source>European Conference on Computer Vision 2016</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="conf-paper">
          <page-range>213-229</page-range>
          <publisher-place>Cham, Switzerland</publisher-place>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Carion</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Massa</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Synnaeve</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Usunier</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Kirillov</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Zagoruyko</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-030-58452-8_13</pub-id>
          <article-title>End-to-end object detection with transformers</article-title>
          <source>European Conference on Computer Vision 2020</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="conf-paper">
          <page-range>94-97</page-range>
          <publisher-place>Waknaghat, Solan, India</publisher-place>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mittal</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Kumar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Negi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Rathi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Nautiyal</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Mittal</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/PDGC64653.2024.10984411</pub-id>
          <article-title>Potholes detection system utilizing visual geometry group-16 (VGG16)</article-title>
          <source>2024 8th International Conference on Parallel, Distributed and Grid Computing</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>42-58</page-range>
          <issue>1</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ye</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Tong</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/14680629.2019.1615533</pub-id>
          <article-title>Convolutional neural network for pothole detection in asphalt pavement</article-title>
          <source>Road Mater. Pavement Des.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>192-205</page-range>
          <issue>3</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yebes</surname>
              <given-names>J. J.</given-names>
            </name>
            <name>
              <surname>Montero</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Arriola</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/MITS.2019.2926370</pub-id>
          <article-title>Learning to automatically catch potholes in worldwide road scene images</article-title>
          <source>IEEE Intell. Transp. Syst. Mag.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="conf-paper">
          <page-range>505-509</page-range>
          <publisher-place>Zhuhai, China</publisher-place>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lan</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Q.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CVIDL62147.2024.10603934</pub-id>
          <article-title>Computer vision based pothole road detection and recognition</article-title>
          <source>2024 5th International Conference on Computer Vision, Image and Deep Learning</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-5</page-range>
          <publisher-place>Rajshahi, Bangladesh</publisher-place>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tithi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Azrof</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACMI53878.2021.9528185</pub-id>
          <article-title>Speed bump &amp; pothole detection with single shot multibox detector algorithm &amp; speed control for autonomous vehicle</article-title>
          <source>2021 International Conference on Automation, Control and Mechatronics for Industry 4.0</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>8406</page-range>
          <issue>24</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ahmed</surname>
              <given-names>K. R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s21248406</pub-id>
          <article-title>Smart pothole detection using deep learning based on dilated convolution</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>33</volume>
          <page-range>1127-1141</page-range>
          <issue>12</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Maeda</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Sekimoto</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Seto</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Kashiyama</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Omata</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1111/mice.12387</pub-id>
          <article-title>Road damage detection and classification using deep neural networks with smartphone images</article-title>
          <source>Comput. Aided. Civ. Infrastruct. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>11229</page-range>
          <issue>23</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Park</surname>
              <given-names>S. S.</given-names>
            </name>
            <name>
              <surname>Tran</surname>
              <given-names>V. T.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>D. E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app112311229</pub-id>
          <article-title>Application of various YOLO models for computer vision-based real-time pothole detection</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>917</page-range>
          <issue>3</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ko</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Jang</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Choi</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Lim</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>D. U.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s21030917</pub-id>
          <article-title>Stochastic decision fusion of convolutional neural networks for tomato ripeness detection in agricultural sorting systems</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-6</page-range>
          <publisher-place>Belagavi, Karnataka, India</publisher-place>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sai</surname>
              <given-names>K. K.</given-names>
            </name>
            <name>
              <surname>Kumar</surname>
              <given-names>D. D. V.</given-names>
            </name>
            <name>
              <surname>Sahrudhay</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Dharavath</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/INCOFT60753.2023.10425461</pub-id>
          <article-title>Pothole detection using deep learning</article-title>
          <source>2023 2nd International Conference on Futuristic Technologies</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1207-1213</page-range>
          <publisher-place>Coimbatore, India</publisher-place>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Rao</surname>
              <given-names>M. V.</given-names>
            </name>
            <name>
              <surname>Dubey</surname>
              <given-names>S. K.</given-names>
            </name>
            <name>
              <surname>Badholia</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICIRCA54612.2022.9985669</pub-id>
          <article-title>Pothole identification and dimension approximation with YOLO Darknet CNN</article-title>
          <source>2022 4th International Conference on Inventive Research in Computing Applications</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1991-1997</page-range>
          <publisher-place>Coimbatore, India</publisher-place>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Saranya</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Nivetha</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Abirami</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Mohaideen Arsath</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Dharaneesh</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICACCS60874.2024.10716937</pub-id>
          <article-title>Revolutionizing road maintenance: YOLO based pothole detection system</article-title>
          <source>2024 10th International Conference on Advanced Computing and Communication Systems</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>137</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Abdelwahed</surname>
              <given-names>S. H.</given-names>
            </name>
            <name>
              <surname>Sharobim</surname>
              <given-names>B. K.</given-names>
            </name>
            <name>
              <surname>Wasfey</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Said</surname>
              <given-names>L. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11554-025-01683-1</pub-id>
          <article-title>Advancements in real-time road damage detection: A comprehensive survey of methodologies and datasets</article-title>
          <source>J. Real-Time Image Process.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>4687-4697</page-range>
          <issue>4</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Feng</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TIV.2024.3376534</pub-id>
          <article-title>Segmentation of road negative obstacles based on dual semantic-feature complementary fusion for autonomous driving</article-title>
          <source>IEEE Trans. Intell. Vehicles</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lin</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Pan</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.21203/rs.3.rs-7221917/v1</pub-id>
          <article-title>YOLO-ROC: A high-precision and ultra-lightweight model for real-time road damage detection</article-title>
          <source>Research Square</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="conf-paper">
          <page-range>770-778</page-range>
          <publisher-place>Las Vegas, NV, USA</publisher-place>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>He</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Ren</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id>
          <article-title>Deep residual learning for image recognition</article-title>
          <source>2016 IEEE Conference on Computer Vision and Pattern Recognition</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Kao</surname>
              <given-names>S. H.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhuo</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Wen</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>C. H.</given-names>
            </name>
            <name>
              <surname>Gary Chan</surname>
              <given-names>S. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2303.03667</pub-id>
          <article-title>Run, don’t walk: Chasing higher FLOPS for faster neural networks</article-title>
          <source>arXiv:2303.03667</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>846-862</page-range>
          <issue>4</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Arya</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Maeda</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Ghosh</surname>
              <given-names>S. K.</given-names>
            </name>
            <name>
              <surname>Toshniwal</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Sekimoto</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1002/gdj3.260</pub-id>
          <article-title>RDD2022: A multi-national image dataset for automatic road damage detection</article-title>
          <source>Geoscience Data J.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>