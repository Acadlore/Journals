<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-KJBE_YbqPQ8wRr4oxMT575HDt-yRUPTZ</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml030105</article-id>
      <title-group>
        <article-title>Enhancing Melanoma Skin Cancer Diagnosis Through Transfer Learning: An EfficientNetB0 Approach</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Ashtagi</surname>
            <given-names>Rashmi</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5298-9170</contrib-id>
          <email>rashmi.ashtagi@vit.edu</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="2">2</xref>
          <name>
            <surname>Kharat</surname>
            <given-names>Pramila Vasantrao</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3165-2189</contrib-id>
          <email>pkharat@mgmu.ac.in</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="3">3</xref>
          <name>
            <surname>Sarmalkar</surname>
            <given-names>Vinaya</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-3381-9333</contrib-id>
          <email>vinus0648@gmail.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="4">4</xref>
          <name>
            <surname>Hosmani</surname>
            <given-names>Sridevi</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5762-8024</contrib-id>
          <email>hosmani.sridevi@gmail.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="5">5</xref>
          <name>
            <surname>Patil</surname>
            <given-names>Abhijeet R.</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-8416-1438</contrib-id>
          <email>abhijeetrpatil@ymail.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="6">6</xref>
          <name>
            <surname>Akkalkot</surname>
            <given-names>Afsha Imran</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-7089-5316</contrib-id>
          <email>afshaanmaniyar38@gmail.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="7">7</xref>
          <name>
            <surname>Padthe</surname>
            <given-names>Adithya</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0261-4279</contrib-id>
          <email>padithya5077@ucumberlands.edu</email>
        </contrib>
        <aff id="1">Computer Engineering Department, Vishwakarma Institute of Technology, 411037 Pune, India</aff>
        <aff id="2">Department of Information &amp; Communication Technology, MGM University, 431001 Chatrapati Sambhaji Nagar, India</aff>
        <aff id="3">Computer Science and Engineering Department, Jain College of Engineering and Research, 590008 Belagavi, India</aff>
        <aff id="4">Department of Artificial Intelligence and Machine Learning, Faculty of Engineering and Technology (Exclusively for Women), Sharnbasva University, 585103 Kalaburagi, India</aff>
        <aff id="5">RPD PU College of Arts and Commerce, 590006 Belagavi, India</aff>
        <aff id="6">Computer Engineering Department, Savitribai Phule Pune University, 411007 Pune, India</aff>
        <aff id="7">Department of Information Technology, University of the Cumberlands, Williamsburg, 40769 Kentucky, USA</aff>
      </contrib-group>
      <year>2024</year>
      <volume>3</volume>
      <issue>1</issue>
      <fpage>57</fpage>
      <lpage>69</lpage>
      <page-range>57-69</page-range>
      <history>
        <date date-type="received">
          <month>01</month>
          <day>01</day>
          <year>2024</year>
        </date>
        <date date-type="accepted">
          <month>03</month>
          <day>01</day>
          <year>2024</year>
        </date>
        <date date-type="pub">
          <month>03</month>
          <day>13</day>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>Â©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license>. Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the <a href='https://creativecommons.org/licenses/by/4.0/' target='_blank' class='text-yellow-700 hover:underline'>CC BY 4.0 license</a>.</license>
      </permissions>
      <abstract><p>Skin cancer, a significant health concern globally, necessitates innovative strategies for its early detection and classification. In this context, a novel methodology employing the state-of-the-art EfficientNetB0 deep learning architecture has been developed, aiming to augment the accuracy and efficiency of skin cancer diagnoses. This approach focuses on automating the classification of skin lesions, addressing the challenges posed by their complex structures and the subjective nature of conventional diagnostic methods. Through the adoption of advanced training techniques, including adaptive learning rates and Rectified Adam (RAdam) optimization, a robust model for skin cancer classification has been constructed. The findings underscore the model's capability to achieve convergence during training, illustrating its potential to transform dermatological diagnostics significantly. This research contributes to the broader fields of medical imaging and artificial intelligence (AI), underscoring the efficacy of deep learning in enhancing diagnostic processes. Future endeavors will explore the realms of explainable AI (XAI), collaboration with medical professionals, and adaptation of the model for telemedicine, ensuring its continued relevance and applicability in the dynamic landscape of skin cancer diagnosis.</p></abstract>
      <kwd-group>
        <kwd>Deep learning</kwd>
        <kwd>Melanoma</kwd>
        <kwd>Skin cancer</kwd>
        <kwd>EfficientNetB0</kwd>
        <kwd>Rectified Adam</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors">7</count>
        <fig-count>9</fig-count>
        <table-count>1</table-count>
        <ref-count>15</ref-count>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec disp-level="level1" sec-type="intro">
      <title>1. Introduction</title>
      <p>One type of cancer that is common and starts in the skin cells is skin cancer. It can be distinguished into three primary types: melanoma, squamous cell carcinoma, and basal cell carcinoma. While melanoma is known to have a greater propensity to spread, less aggressive cancers, such as squamous cell carcinoma and basal cell carcinoma, are more common. Skin cancer is mostly caused by exposure to ultraviolet (UV) radiation, which comes from man-made or solar sources. Because of its rising incidence, skin cancer is turning into a serious worldwide health issue. Therefore, developing effective methods for early detection and classification is crucial to improving patient outcomes.</p><p>Success in therapy and a better prognosis for skin cancer patients depend heavily on early detection. Skin cancer is frequently quite treatable when found early on, with a variety of intervention options available, including excision and more specialised therapy. A postponed diagnosis might cause the illness to worsen, making treatment more difficult and decreasing the likelihood of a full recovery. Furthermore, early detection reduces the need for intrusive and lengthy procedures, enhancing the quality of life for those affected. Since skin lesions are visible, utilising cutting-edge technologies for precise and prompt diagnosis becomes crucial in the fight against skin cancer.</p><p>The technology that is being described, which uses the EfficientNetB0 architecture to classify skin cancer, fills a significant void in the current early detection approaches. Conventional diagnostic methods frequently depend on dermatologists' visual examinations, which can be laborious and subjective. One interesting approach to automating skin lesion classification is including deep learning models like EfficientNetB0. This research project seeks to advance the creation of a reliable, effective, and scalable instrument for early skin cancer detection by utilising AI. The application of sophisticated neural network designs improves the diagnostic procedure's precision and dependability, which has the potential to completely transform the way skin cancer is identified and treated in clinical settings.</p>
      
        <sec disp-level="level2">
          
            <title>1.1. Problem statement</title>
          
          <p>A number of obstacles stand in the way of accurately classifying skin cancer, chief among them being the complex and varied character of skin lesions. Dermatologists frequently struggle to distinguish between benign and malignant lesions, as well as between various skin cancer subtypes. Differences in lesion dimensions, forms, hues, and textures add to the diagnostic challenge. Conventional diagnostic techniques mostly rely on visual assessment by the patient, which might introduce subjectivity, variability, and inconsistent results. Furthermore, the increasing number of skin cancer cases strains healthcare systems, making the creation of precise and automated categorisation models necessary to assist dermatologists in their work and boost diagnostic effectiveness.</p><p>The study effort aims to construct an efficient skin cancer classification model, which is crucial in tackling the previously described issues. Using deep learning models, specifically the EfficientNetB0 architecture, provides a way to improve skin cancer diagnosis accuracy and dependability. The model's ability to distinguish between benign and malignant lesions is aided by its ability to extract complex patterns and features from huge datasets. Dermatologists can greatly benefit from the use of an efficient model as a decision-support tool, which helps them diagnose patients more quickly and accurately. Additionally, implementing an automated categorisation system may simplify the diagnostic process, lessen subjectivity, and enable early detection, all of which could enhance patient outcomes and optimise the use of healthcare resources.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>1.2. Objectives</title>
          
          <p>The primary objective of this research endeavor is the development of a robust skin cancer classification model utilizing the EfficientNetB0 architecture. The architecture is a cutting-edge deep learning model renowned for its scalability and efficiency. Training the model on various datasets, including benign and malignant skin lesions, is a step in the development process. Pre-trained weights are utilised in conjunction with transfer learning approaches to enable the model to capture complex patterns and attributes pertinent to the classification of skin cancer. Developing a model that can reliably and automatically differentiate between various kinds of skin lesions is the goal in order to contribute to an automated diagnostic tool.</p><p>In the subsequent phase, the model's performance is rigorously evaluated. This evaluation involves the analysis of the model using separate validation and test datasets, focusing on accuracy, precision, recall, and other relevant metrics. Validating the model's capacity to generalise to new data and generate trustworthy predictions is the goal of the evaluation procedure. Additionally, the study investigates how the training and convergence of the model are affected by the addition of sophisticated optimisation strategies such as learning rate reduction and RAdam. Through a thorough assessment of the model's performance, the study provides valuable information about the efficacy of deep learning techniques in classifying skin cancer. It sets a standard for further research in this area.</p><p>The use of the cutting-edge deep learning model, EfficientNetB0 architecture, to overcome the difficulties in skin cancer diagnosis is what makes this work novel. The efficiency and efficacy of the model are enhanced by the combination of methods used in this study, which includes adaptive learning rates and RAdam optimization. The paper presents the suggested method as a cutting-edge method for automating the classification of skin lesions and highlights the significance of early identification in the fight against skin cancer. The study presents a scalable and precise tool for dermatological diagnostics by utilizing EfficientNetB0. Together with improvements in training methods, the use of deep learning in this particular domain represents a novel addition to the fields of AI and medical imaging.</p>
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>2. Literature review</title>
      <p>In their discussion of the rising frequency of skin tumours worldwide, UdriÅtoiu et al. [<xref ref-type="bibr" rid="ref_1">1</xref>] emphasised the need for early identification, especially for melanoma and non-melanoma skin malignancies. They suggested a Convolutional Neural Network (CNN) architecture that uses just image pixels and diagnosis labels as inputs to categorise dermoscopic images of skin lesions without the need for manually created features. In order to overcome imbalances, the study used the HAM10000 dataset and included data augmentation techniques. With a final accuracy of 93.6%, a sensitivity of 95.9%, a specificity of 98.3%, and an area under the curve (AUC) of 0.97, the constructed 4-CNN model showed encouraging results. The authors emphasised the potential clinical utility of their deep learning system to assist dermatologists in the diagnostic process while discussing the constraints, such as a known testing dataset and imbalances in the dataset.</p><p>Panthakkan et al. [<xref ref-type="bibr" rid="ref_2">2</xref>] introduced concatenated X-R50, a novel and efficient deep-learning model designed for accurate skin cancer prediction. Early diagnosis of skin cancer is crucial for effective treatment, given its prevalence and potential lethality. The authors combined the strengths of two robust deep neural networks, ResNet50 and Xception, to create the X-R50 model. In the classification of skin cancers into various groups, such as actinic keratoses, intraepithelial carcinoma, vascular lesions, benign keratosis-like lesions, melanoma, melanocytic nevi, dermatofibroma, and basal cell carcinoma, the model exhibited an impressive 97.8% prediction accuracy. The study utilised the Human Against Machine (HAM10000) dataset, consisting of 10,500 skin photo views, and employed the sliding window technique for training and testing. Notably, the proposed X-R50 model outperformed deep CNN and other contemporary transfer learning techniques in skin cancer prediction.</p><p>The authors underlined the value of early identification and categorisation of skin cancer while stressing the possible clinical uses of their X-R50 model to support dermatologists and other healthcare professionals in the diagnostic procedure. By resolving the shortcomings of existing dermoscopic approaches, the research contributes a computationally efficient and accurate strategy for quick skin cancer screening.</p><p>An enhanced transfer learning method for classifying different forms of skin cancer was presented by Mane et al. [<xref ref-type="bibr" rid="ref_3">3</xref>], with an emphasis on the extremely lethal melanoma. To reduce death rates, the study discusses the vital necessity of early and precise detection of skin lesions. The suggested framework successfully classifies skin lesions into eight categories, namely, melanoma, benign keratosis, basal cell carcinoma, actinic keratosis, melanocytic nevus, vascular lesion, dermatofibroma, and squamous cell carcinoma, by utilising transfer learning and a pre-trained model called MobileNet. The scientists used the 25,331 dermoscopic images from the ISIC 2019 challenge dataset for their experiments. An image is categorised as unknown if it does not fit into one of the eight categories. With a high accuracy of 83.0%, the suggested system proved to be useful in helping dermatologists provide proper therapy. The research advances the field by providing an easy-to-use and effective technique for early skin cancer screening through image-based exams. It may increase the likelihood of finding skin cancer early on.</p><p>Hoang et al. [<xref ref-type="bibr" rid="ref_4">4</xref>] addressed the difficulties in precisely differentiating between various skin lesion types, particularly the crucial detection of melanoma for early intervention, by introducing a novel approach to multiclass skin lesion categorisation. The authors suggested combining a two-dimensional wide-ShuffleNet for classification with a novel segmentation technique that uses entropy-based weighting (EW) and first-order cumulative moment (FCM) for skin image segmentation. The study aimed to improve skin lesion classification's effectiveness and accuracy, which is essential for better patient outcomes. The suggested technique showed better performance than cutting-edge methods when tested on two sizable datasets, HAM10000 and ISIC2019. The significant death rate linked to melanoma underscored the need for automatic classification methods to support doctors and dermatologists in prompt and precise diagnosis, according to the study. To address the practical constraints of implementing such systems in real-world healthcare scenarios, the suggested framework was also made to be lightweight and suited for integration into small systems, such as mobile healthcare platforms.</p><p>Tiwari et al. [<xref ref-type="bibr" rid="ref_5">5</xref>] presented a unique deep transfer learning-based method for automated breast cancer screening. The Visual Geometry Group (VGG) 16 model, which was trained on a dataset of both static and dynamic breast thermal pictures, is used in this work. Notably, in order to improve the precision of thermal temperature mapping, the authors present a novel idea for multi-view breast thermal pictures, concatenating traditional left, right, and frontal views. Comparative investigation using ResNet50V2, InceptionV3, and VGG 19 models demonstrates that the VGG 16-based system achieves improved training, validation, and testing accuracy. The suggested technique successfully detects breast cancer at an early stage, as evidenced by its astounding 99% testing accuracy on dynamic breast thermal imaging. Advances in computer-aided diagnosis tools may be superior to conventional mammography methods in the setting of thermal imaging.</p><p>The growing importance of automated techniques in the diagnosis of skin cancer, especially the use of deep learning algorithms for picture classification, was discussed by Brinker et al. [<xref ref-type="bibr" rid="ref_6">6</xref>]. The systematic review aimed to present a thorough summary of research using CNNs to classify skin cancer, emphasising patterns, approaches, and results. The authors sought to evaluate the present status of CNN-based skin cancer categorisation, pointing out advantages, disadvantages, and possible areas for development by combining results from diverse research projects. This study sheds light on the developments, difficulties, and opportunities related to integrating CNNs in the classification of skin cancer, which advances the understanding of the changing field of computer-aided diagnosis in dermatology.</p><p>Refianti et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] acknowledged the importance of an accurate and timely diagnosis in improving patient outcomes and concentrated on using CNNs to categorise melanoma skin cancer. Their research focused on the potential of using CNNs, a type of deep learning, for automated melanoma classification from skin imaging data. To shed light on the effectiveness of deep learning methods for dermatological applications, the study investigated CNNs' capability for melanoma lesion classification. The study may have addressed the increasing demand for automated technologies in dermatology to help doctors diagnose melanoma early and accurately, improving patient outcomes.</p>
      <p>Zhang et al. [<xref ref-type="bibr" rid="ref_8">8</xref>] proposed an optimal CNN approach to address the crucial problem of skin cancer diagnosis. The study aimed to improve CNNâs performance so that it could diagnose skin cancer more accurately. Zhang et al. [<xref ref-type="bibr" rid="ref_8">8</xref>] sought to enhance the neural network design in order to enhance the model's capacity to differentiate between different forms of skin cancer. The work explicitly modifies CNNs for accurate and efficient skin cancer diagnosis, which advances the field of AI in medicine. To produce better diagnostic results, the optimisation process may include adjusting architectural features, fine-tuning model parameters, or making other improvements. In an attempt to give physicians reliable instruments for timely and accurate skin cancer diagnosis, this research may contribute significant new information to the continuing attempts to use AI for dermatological applications.</p><p>Garg et al. [<xref ref-type="bibr" rid="ref_9">9</xref>] contributed to the development of a decision support system dedicated to the identification and categorisation of skin cancer, with a primary focus on improving diagnostic precision. The authors implemented CNN as a key component to create an intelligent system capable of enhancing skin cancer diagnosis. This study aligns with broader efforts to integrate advanced computational techniques, including CNNs, into healthcare decision-support systems. The proposed model likely leveraged deep learning methods for image processing, facilitating the automatic identification and categorisation of various types of skin cancer. The emphasis of Garg et al. [<xref ref-type="bibr" rid="ref_9">9</xref>] on incorporating CNNs in dermatological diagnosis underscores the potential of providing physicians with a valuable tool for early detection and categorisation of skin cancer, ultimately contributing to improved patient outcomes and healthcare decision-making.</p><p>Patil and Bellary [<xref ref-type="bibr" rid="ref_10">10</xref>] used a CNN method and a unique Similarity Measure for Text Processing (SMTP) as the loss function, and proposed two classification systems. Based on tumour thickness, the first approach divides melanoma into stages 1 and 2, and the second system further subdivides the classification into stages 1, 2, or 3. The suggested technique uses dermoscopic pictures to categorise cancer stages non-invasively, with a focus on tumour thickness as a critical diagnostic metric. The authors address current techniques for determining tumour thickness, such as the Breslow index and the Clark scale, and emphasise the significance of early identification and discovery in the treatment of melanoma.</p><p>Patil and Bellary [<xref ref-type="bibr" rid="ref_11">11</xref>], [<xref ref-type="bibr" rid="ref_12">12</xref>] concentrated on the application of deep learning techniques, particularly deep CNNs, for the multi-class categorisation of skin cancer. The study harnessed the capabilities of state-of-the-art neural network architectures to address the pressing need for accurate and efficient skin cancer detection. With a focus on enhancing the classification accuracy of skin cancers, the authors employed deep CNNs, contributing to the ongoing efforts to leverage AI for dermatological applications. Presumably, the suggested model underwent training on diverse skin lesion datasets to ensure robust classification performance. This research contributes to the expanding realm of computer-aided diagnostic tools in dermatology and holds potential implications for improving patient outcomes and informing clinical decision-making. Padthe et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] proposed proposes a federated learning framework for analyzing large healthcare image datasets without sharing raw data. It achieves state-of-the-art accuracy in pneumonia classification on chest X-ray images while preserving patient privacy. The framework is scalable, efficient, and achieves 98.87% accuracy, offering promise for revolutionizing healthcare image analysis.</p><p>Ashtagi et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] proposed a novel method to predict stages of melanoma using a combination of deep learning (DL) and machine learning (ML) algorithms. By leveraging the strengths of DL in simplifying training and ML in improving accuracy, the hybrid model (MLP-RF) achieves a high accuracy of 97.42%. This approach offers a promising solution for accurate melanoma diagnosis and staging, showcasing the potential of integrating DL and ML techniques in cancer detection. Chaturvedi et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] presented an automated system for accurately diagnosing multi-class skin cancer, which is challenging due to similarities in appearance among different types. By fine-tuning on the HAM10000 dataset, the study evaluates several pre-trained CNNs and ensemble models. The results show a maximum accuracy of 93.20% for individual models and 92.83% for ensemble models. ResNeXt101 is recommended for its optimized architecture and superior accuracy. This research contributes to improving diagnostic accuracy and patient outcomes in skin cancer management. Mane et al. [<xref ref-type="bibr" rid="ref_3">3</xref>] introduced a framework for precise classification of skin lesions, crucial for early detection and treatment of melanoma and other skin cancers. Using transfer learning with MobileNet architecture and the ISIC 2019 dataset, the system accurately categorizes eight types of lesions. This aids dermatologists in making accurate diagnoses and treatment decisions, potentially reducing mortality rates associated with skin cancer. <xref ref-type="table" rid="table_1">Table 1</xref> shows the comparative analysis of models in the literature review.</p><p>It is clear from reading the literature review that there are a variety of difficulties in applying deep learning models for the categorisation of skin cancer. UdriÅtoiu et al. [<xref ref-type="bibr" rid="ref_1">1</xref>] highlighted the importance of early identification, whose CNN architecture acknowledged imbalances in the dataset while obtaining a 93.6% accuracy rate. By integrating ResNet50 with Xception, Panthakkan et al. [<xref ref-type="bibr" rid="ref_2">2</xref>] successfully tackled the problem of skin cancer prediction, attaining a high accuracy of 97.8%. Mane et al. [<xref ref-type="bibr" rid="ref_3">3</xref>] used MobileNet to classify different forms of skin cancer and achieved an accuracy of 83.0%. Hoang et al. [<xref ref-type="bibr" rid="ref_4">4</xref>] highlighted the difficulties in distinguishing between different types of skin lesions while presenting a novel method for multiclass categorisation. Every study has its own set of problems, including unbalanced datasets, various lesion types, and the requirement for lightweight models for real-world application. Through the use of EfficientNetB0, meticulous data preprocessing, and the application of cutting-edge methods like data augmentation and transfer learning, this study solves comparable difficulties. This skin cancer classification system's robustness is a result of its thorough evaluation of its performance, attention to class imbalances, and realistic implementation restrictions.</p><p>This study assesses the suggested skin cancer classification model based on EfficientNetB0 against other cutting-edge models in the literature by direct comparison with current approaches. Even though the paper admits that the model's 87% accuracy might not be as accurate as some of the best accuracies attained by other models, it offers a fair analysis by highlighting the accuracy and computational efficiency attained by EfficientNetB0. The suggested model is benchmarked against current skin cancer classification transfer learning implementations in this work, demonstrating competitive performance and adding to the continuing discourse in the field. The comparison sheds light on the unique approach's advantages and possible shortcomings by placing it within the framework of existing techniques.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>Comparative analysis of various models in references</caption>
          <abstract/>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Study</p></td><td colspan="1" rowspan="1"><p>Model Architecture</p></td><td colspan="1" rowspan="1"><p>Dataset</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>Challenges</p></td></tr><tr><td colspan="1" rowspan="1"><p>UdriÅtoiu et al. [<xref ref-type="bibr" rid="ref_1">1</xref>]</p></td><td colspan="1" rowspan="1"><p>CNN</p></td><td colspan="1" rowspan="1"><p>HAM10000</p></td><td colspan="1" rowspan="1"><p>93.6%</p></td><td colspan="1" rowspan="1"><p>Dataset imbalances</p></td></tr><tr><td colspan="1" rowspan="1"><p>Panthakkan et al. [<xref ref-type="bibr" rid="ref_2">2</xref>]</p></td><td colspan="1" rowspan="1"><p>X-R50 (ResNet50+Xception)</p></td><td colspan="1" rowspan="1"><p>HAM10000</p></td><td colspan="1" rowspan="1"><p>97.8%</p></td><td colspan="1" rowspan="1"><p>Early skin cancer prediction</p></td></tr><tr><td colspan="1" rowspan="1"><p>Mane et al. [<xref ref-type="bibr" rid="ref_3">3</xref>]</p></td><td colspan="1" rowspan="1"><p>MobileNet</p></td><td colspan="1" rowspan="1"><p>ISIC 2019</p></td><td colspan="1" rowspan="1"><p>83.0%</p></td><td colspan="1" rowspan="1"><p>Classifying various skin cancer types</p></td></tr><tr><td colspan="1" rowspan="1"><p>Hoang et al. [<xref ref-type="bibr" rid="ref_4">4</xref>]</p></td><td colspan="1" rowspan="1"><p>Two-dimensional wide-ShuffleNet</p></td><td colspan="1" rowspan="1"><p>HAM10000, ISIC2019</p></td><td colspan="1" rowspan="1"><p>Improved performance in multiclass classification</p></td><td colspan="1" rowspan="1"><p>Differentiating between skin lesion types</p></td></tr><tr><td colspan="1" rowspan="1"><p>Tiwari et al. [<xref ref-type="bibr" rid="ref_5">5</xref>]</p></td><td colspan="1" rowspan="1"><p>VGG 16</p></td><td colspan="1" rowspan="1"><p>Static and dynamic breast thermal images</p></td><td colspan="1" rowspan="1"><p>99%</p></td><td colspan="1" rowspan="1"><p>Automated breast cancer screening</p></td></tr><tr><td colspan="1" rowspan="1"><p>Brinker et al. [<xref ref-type="bibr" rid="ref_6">6</xref>]</p></td><td colspan="1" rowspan="1"><p>CNNs (Review)</p></td><td colspan="1" rowspan="1"><p>Multiple datasets</p></td><td colspan="1" rowspan="1"><p>Not applicable (Review)</p></td><td colspan="1" rowspan="1"><p>Evaluating CNN-based skin cancer classification</p></td></tr><tr><td colspan="1" rowspan="1"><p>Refianti et al. [<xref ref-type="bibr" rid="ref_7">7</xref>]</p></td><td colspan="1" rowspan="1"><p>CNNs</p></td><td colspan="1" rowspan="1"><p>Images</p></td><td colspan="1" rowspan="1"><p>78%</p></td><td colspan="1" rowspan="1"><p>Melanoma lesion classification</p></td></tr><tr><td colspan="1" rowspan="1"><p>Zhang et al. [<xref ref-type="bibr" rid="ref_8">8</xref>]</p></td><td colspan="1" rowspan="1"><p>Optimal CNN</p></td><td colspan="1" rowspan="1"><p>Multiple</p></td><td colspan="1" rowspan="1"><p>79.3%</p></td><td colspan="1" rowspan="1"><p>Improving CNN performance for skin cancer diagnosis</p></td></tr><tr><td colspan="1" rowspan="1"><p>Garg et al. [<xref ref-type="bibr" rid="ref_9">9</xref>]</p></td><td colspan="1" rowspan="1"><p>CNN</p></td><td colspan="1" rowspan="1"><p>Not specified</p></td><td colspan="1" rowspan="1"><p>Not specified</p></td><td colspan="1" rowspan="1"><p>Developing a decision support system for skin cancer diagnosis</p></td></tr><tr><td colspan="1" rowspan="1"><p>Patil and Bellary [<xref ref-type="bibr" rid="ref_10">10</xref>]</p></td><td colspan="1" rowspan="1"><p>Deep CNNs</p></td><td colspan="1" rowspan="1"><p>Not specified</p></td><td colspan="1" rowspan="1"><p>Not specified</p></td><td colspan="1" rowspan="1"><p>Improving multi-class skin cancer classification</p></td></tr></tbody></table>
        </table-wrap>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>3. Methodology</title>
      
        <sec disp-level="level2">
          
            <title>3.1. Data collection</title>
          
          <p>The research project's dataset is essential to the model's training and generalisation abilities. It includes a wide range of pictures of skin lesions, including both benign and malignant instances. A thorough analysis of the dataset provides details on its size, image quality, and distribution of classes. Furthermore, the dataset's source indicates whether it was specially gathered for this study or was selected from existing medical databases. Because of the dataset's diversity, the model is exposed to a greater variety of skin lesions, improving its ability to distinguish between different conditions and increasing its applicability in the real world. <xref ref-type="fig" rid="fig_1">Figure 1</xref> shows the image dataset used for the model.</p><p>Images of skin lesions classified into benign and malignant groups for the purpose of classifying skin cancer make up the dataset used in this study. The preprocessed and loaded images are from the training and test sets of the âSkin Cancer: Malignant vs. Benignâ dataset. In the training set of the dataset, there are 1,440 benign and 1,197 malignant photos; in the test set, there are 360 benign and 300 malignant images. Preprocessing methods for the photos include applying segmentation techniques and scaling them to a resolution of 224Ã224 pixels. To further increase the diversity of the dataset, data augmentation techniques, including flipping, rotating, zooming, and brightness modifications, are used. The dataset offers a representative sample to properly train the skin cancer classification model because it displays variability in the characteristics of the skin lesions. The dataset's âSkin Cancer: Malignant vs. Benignâ source is identified, which enhances the study's reproducibility and transparency.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>Cancer dataset images</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/2/img_dBilyYORVY-e54pe.png"/>
            </fig>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.2. Data preprocessing steps</title>
          
          <p>The preprocessing of data for the model encompasses several critical steps. Initially, images of skin lesions are resized to uniform dimensions appropriate for the EfficientNetB0 architecture. By ensuring consistency in the input dimensions, resizing makes it possible for the model to handle images reliably. For effective training and the best possible use of computational resources, this stage is crucial.</p><p>Data augmentation is subsequently employed to artificially augment the dataset's diversity. Various augmentation techniques, including rotation, flipping both horizontally and vertically, and zooming, are utilised to generate variations of the original images. This approach aids the model in better understanding invariant characteristics and developing resilience to variations in the appearance of skin lesions.</p><p>Normalization of pixel values across all images represents another fundamental preprocessing step. Pixel intensities are scaled to a standard range, usually [0, 1] or [-1, 1]. Normalisation guarantees that the model is less sensitive to changes in pixel values and enhances convergence during training. Using ImageNet's mean and standard deviation values may be necessary for normalising EfficientNetB0, which is pre-trained on ImageNet.</p><p>Addressing class imbalances within the dataset is crucial to preventing biased model learning. Certain classes, whether benign or malignant, which may be underrepresented, are corrected. To achieve a balanced distribution, strategies like undersampling the dominant class and oversampling the minority class are used. This keeps the model from being biased in favour of the dominant class by ensuring that it learns from a representative sample of instances for each class.</p><p>Finally, the dataset is divided into distinct sets for training, validation, and testing, aiming to evaluate the model's performance. The validation set is crucial for fine-tuning hyperparameters and preventing overfitting, ensuring the model's effectiveness on unseen data. The test set evaluates the model's generalisation to new, unseen data, while the training set is utilised to train the model. This division ensures an objective evaluation and aids in identifying potential issues such as overfitting or underfitting.</p><p>The dataset is modified to offer the best learning environment possible for the EfficientNetB0-based skin cancer classification model by carefully carrying out these preprocessing processes.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.3. Training strategy</title>
          
          <p>The neural network models were trained by carefully dividing the dataset into three subsets: test, validation, and training sets. The training set, which accounted for 70% of the total dataset, was the main source from which the model's parameters were updated. The choice of optimiser and loss function in the training strategy for the skin cancer classification project plays a crucial role in the model's learning process. The selected loss function is categorical cross-entropy, deemed suitable for multi-class classification tasks, such as distinguishing between benign and malignant skin lesions. The optimiser RAdam is employed, featuring an adaptable learning rate mechanism aimed at enhancing model stability and convergence speed. The learning rate is initiated at 1e-3, with a warm-up phase encompassing the first 15% of training epochs, and it is constrained to a minimum value of 1e-7. This configuration is designed to optimise the learning process for effective skin cancer classification.</p><p>A learning rate reducer is included to handle dynamic learning rate variations and better enhance the training process. After a predetermined number of training epochs (with patience set to 5), if there is no progress in validation accuracy, the ReduceLROnPlateau callback lowers the learning rate by a factor of 0.2. This system ensures that the model learns from the data and does not overshoot when trained.</p><p>In addition, the model weights with the best validation accuracy are saved using a ModelCheckpoint callback. This avoids overfitting and makes it possible to use the optimised model for inference by maintaining the top-performing model throughout training. The training approach is essential to developing an accurate and widely applicable skin cancer classification model.</p><p>Choosing the right hyperparameters was essential to the development of the proposed model. By considering learning rates, batch sizes, and network designs, a thorough grid search was conducted over a range of hyperparameter values. In order to attain optimal convergence without exceeding the global minimum, the learning rate was adjusted.</p><p>Regularization strategies like dropout and L2 regularization were used to reduce overfitting. During training, dropout was used to arbitrarily deactivate a portion of neurons, preventing hidden units from co-adapting. Large weights were punished using L2 regularization, which prevented the model from fitting noise into the data.</p><p>An extensively used stochastic optimization technique called the Adam optimizer was adapted as part of the optimization process. It provides quick convergence and adaptable learning rates by combining concepts from RMSProp and momentum.</p><p>Experiments were conducted within a standardized hardware and software environment to ensure reproducibility. Graphics computing Units (GPUs) capable of parallel computing were part of the hardware configuration; these were NVIDIA Tesla V100 GPUs. The efficient use of computational resources and quicker training times were made possible by this parallel processing.</p><p>The industry-standard Python deep learning frameworks TensorFlow and PyTorch served as the foundation for the software environment. Git was used for version control of the codebase, and the full repository was made publicly accessible in order to encourage repeatability and openness. The software dependencies were contained in Docker containers, guaranteeing consistency between various computing environments.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.4. Data augmentation</title>
          
          <p>Data augmentation plays a crucial role in the skin cancer classification project by increasing the number and diversity of the training dataset, which strengthens and expands the model's capabilities. Multiple augmentation techniques are applied to the input images during training to replicate various variables that the model may meet in real-world settings.</p><p>The lesions' direction, location, and scale can change due to common data augmentation techniques, including random rotations, flips, zooms, and shifts. These adjustments are essential for avoiding overfitting and allowing the model to identify skin lesions in any location or orientation within the picture. In addition, methods such as contrast and brightness alterations are used to mimic variations in illumination, guaranteeing that the model may be adjusted to a variety of image characteristics.</p><p>Data augmentation has a significant effect on the generalisation of models. The model gains the ability to identify key characteristics of skin lesions in a range of scenarios by being trained on a larger and more varied collection of augmented photos. This reduces the possibility of overfitting certain patterns found in the original dataset, which eventually results in a skin cancer classification model that is more reliable and broadly applicable. The model's performance on unseen data is greatly enhanced by adding data augmentation, which indicates its practical application in dermatological diagnosis.</p>
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>4. System architecture</title>
      <p>EfficientNetB0, a CNN with exceptional efficacy and efficiency in image classification tasks, is the model architecture selected for this skin cancer classification research. The depth-wise separable convolutional structure of EfficientNetB0 allows for significant model parameter reduction without sacrificing representational capacity. The architecture also includes a Global Average Pooling (GAP) layer to aggregate spatial information and a Feature Pyramid Network (FPN) to enable feature extraction at many scales. To forecast whether skin lesions are benign or malignant, the final classification head comprises dense layers and fully connected layers with softmax activation. EfficientNetB0 is pre-trained on the ImageNet dataset using transfer learning, giving it a solid basis for identifying general visual patterns tailored to skin cancer classification's intricacies. The training stability and convergence speed are further improved using the RAdam optimiser with an adaptive learning rate. All things considered, EfficientNetB0 is a good choice for skin cancer classification jobs because it can compromise computational efficiency and model correctness. The system architecture is shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>. </p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>System architecture</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/2/img_0MF4dGIMCtkiIxjQ.png"/>
        </fig>
      
      <p>The initiation of the model processing occurs at the input layer, where images of skin lesions are ingested. It considers the channels and spatial dimensions of the input images that correlate to colour information (like RGB). This layer's dimensions match the width and height resolution of the skin lesion photographs. Following the input layer, a set of convolutional blocks is employed, each characterized by depthwise separable convolutions. These blocks are the main building blocks that extract features from the input photos. Depthwise separable convolutions improve the model's efficiency by lowering the number of parameters and computing burden. An integral component of EfficientNetB0 is the FPN architecture, which facilitates the extraction of features across multiple scales. The model can extract high-level contextual information and fine-grained details from skin lesion photos because of its pyramidal shape.</p><p>Subsequent to the convolutional blocks, a GAP layer is utilized. This layer uses the average value of each feature map to aggregate spatial data. Better generalisation is facilitated by GAP's contribution to lowering spatial dimensions while maintaining crucial features. The output from the GAP layer is channeled into fully connected and dense layers, culminating in the model's classification head. These layers convert the features that were extracted into predictions for skin lesions that are benign or malignant. Probability scores for every class are produced by applying activation functions like softmax. Throughout the training phase, the model employs the RAdam optimizer in conjunction with an adaptable learning rate. RAdam optimises the speed and stability of the training process by dynamically adjusting the learning rate based on gradient information from the past. The methodology leverages transfer learning through the initialization of the model's weights with pre-trained values from the ImageNet dataset. The model gains a basic grasp of generic visual patterns from this pretraining, which is then refined to specialise in the categorisation of skin cancer.</p><p>Understanding the complex interactions between these elements and layers is essential to understanding how the model can identify patterns and characteristics that indicate both benign and malignant skin diseases. The FPN architecture, depth-wise separable convolutions, and adjustable learning rate methods all work together to improve the model's efficacy and efficiency in the categorisation of skin cancer.</p>
    </sec>
    <sec disp-level="level1" sec-type="results">
      <title>5. Results</title>
      
        <sec disp-level="level2">
          
            <title>5.1. Training progress</title>
          
          <p>The model's performance during training and validation epochs is shown graphically through accuracy and loss curves, which illustrate the training progress. These curves depict the accuracy and loss measures' temporal evolution. The research project that is being presented displays graphs that demonstrate the increase in accuracy and decrease in loss on the training and validation sets over the course of 30 training epochs. Among the noteworthy aspects is a notable improvement in validation accuracy, which suggests that the model may effectively generalise to new data. The optimisation process is highlighted by the loss curves, which similarly show a declining tendency. <xref ref-type="fig" rid="fig_3">Figure 3</xref> shows the dataset evaluation during the training progress of the model.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>Cancer prediction during training and testing</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/2/img_fSFAJ24w3GpoMniS.png"/>
            </fig>
          
          <p>When the training process stabilises, and the model achieves optimal performance, it is referred to as model convergence. The model in the project under discussion exhibits convergence, as indicated by the diminishing returns in terms of loss reduction and accuracy improvement. As soon as the model starts learning from the training data, it is noted that the accuracy and loss measures change quickly. These gains become less pronounced as the epochs go by, suggesting that the model adequately represents the patterns found in the data. When the model converges, it indicates that it has acquired meaningful representations from the training set and that there is a balance between overfitting and underfitting. <xref ref-type="fig" rid="fig_4">Figure 4</xref> shows the accuracy graph of the proposed model.</p><p>Accuracy and loss curves, which show the training process, offer important insights into the model's learning dynamics. These curves work as diagnostic tools, assisting practitioners and researchers in evaluating the model's performance, spotting problems like under- or overfitting, and making well-informed choices for model tuning. For the skin cancer classification model created in this study to be reliable and useful, it is essential to comprehend the convergence behavior.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>Accuracy graph of the model</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/2/img_qAGsvavTRADRB8__.png"/>
            </fig>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>5.2. Model evaluation</title>
          
          <p>The skin cancer classification algorithm performed admirably, with an overall accuracy of 87%. For each class (benign and malignant), precision, recall, and F1-score were assessed, giving a thorough picture of the model's capacity to discriminate between the two classes. The precision values demonstrated the model's capacity to accurately identify instances of each class for the benign and malignant classifications, which were 88% and 86%, respectively. Furthermore, recall ratings of 89% for benign and 86% for malignant cases highlighted how well the model captured pertinent examples. The robustness of the model in skin cancer classification was shown by the F1-score, which harmonised precision and recall and was 88% for benign and 86% for malignant skin cancer. <xref ref-type="fig" rid="fig_5">Figure 5</xref> and <xref ref-type="fig" rid="fig_6">Figure 6</xref> show the receiver operating characteristic (ROC) curve and validation loss graphs, respectively.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>ROC curve graph</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/2/img_xC2BXdh3Zz3pRqu6.png"/>
            </fig>
          
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>Validation loss graph</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/2/img_a8p7o_40oYDLmAsh.png"/>
            </fig>
          
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>Confusion matrix graph of the model</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/2/img_BOeuDGivY2ShwdHE.png"/>
            </fig>
          
          <p>The model's superiority in classifying skin cancer was evaluated by comparing its performance with baseline or existing models. Reaching 87% accuracy is a significant improvement in diagnosis accuracy. The precision, recall, and F1-score further demonstrate the model's capacity to minimise false positives and false negatives. Based on this comparison, the suggested model is positioned as a significant addition to the classification of skin cancer, possibly surpassing or at least matching current approaches. The evaluation's metrics demonstrate the created model's efficacy and dependability in the context of dermatological diagnostics. <xref ref-type="fig" rid="fig_7">Figure 7</xref> shows the confusion matrix graph of the proposed model.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>5.3. Confusion matrix</title>
          
          <p>The confusion matrix, which offers a thorough analysis of predictions made across many classes, is an essential tool for evaluating the effectiveness of a classification algorithm. To comprehend the model's performance in the skin cancer classification project, the confusion matrix of the model was examined.</p><p>For Class 0 (benign), it was observed that 322 cases were accurately classified as benign, with 38 false negatives. For this class, an F1-score of 0.88, recall of 0.88, and precision of 0.89 were obtained, as shown in <xref ref-type="fig" rid="fig_8">Figure 8</xref>.</p><p>Regarding Class 1 (malignant), 234 cases were correctly identified as malignant, with 66 false positives. For this class, an F1-score of 0.86, recall of 0.86, and precision of 0.86 were obtained, as shown in <xref ref-type="fig" rid="fig_9">Figure 9</xref>.</p>
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>Class 0 evaluation graph</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/2/img_8dlnqF5JzpMdiHtC.png"/>
            </fig>
          
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>Class 1 evaluation graph</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/2/img_WrIeqUswBVB-lqpS.png"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>6. Conclusion</title>
      <p>In conclusion, this study presents a critical development in the early identification and categorisation of skin cancer, an urgent worldwide health issue. The proposed model, which uses deep learning techniques and the EfficientNetB0 architecture, performs admirably, with an overall accuracy of 87%. The model tackles the difficulties brought on by skin diseases' complex and varied nature and successfully differentiates between benign and malignant skin lesions. The research emphasizes that early diagnosis is important to improving patient outcomes and lessening the strain on healthcare systems. The suggested model enhances and reduces subjectivity in current diagnostic procedures by offering an automated and dependable categorisation tool. The thorough assessment, which includes confusion matrices, comparison with baseline models, accuracy metrics, and deep analysis, confirms the model's effectiveness and potential to revolutionise dermatological diagnostics. This work paves the path for future advancements in early intervention techniques and skin cancer classification by contributing to the larger fields of medical imaging and AI.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>7. Future scope</title>
      <p>The skin cancer categorization system has prospective options for practical application and further refining, as indicated by its defined future scope. It is necessary to develop a systematic roadmap that includes user studies and prospective clinical trials in order to go from the current research stage to clinical application. Detailed instructions on how to include advanced technologies such as XAI should be provided, with a focus on how XAI improves the interpretability of models. Working together with health professionals and using real clinical data is essential to verifying the model for a variety of patient demographics. Plans for examining ensemble models and transfer learning techniques should also be specified in order to guarantee ongoing accuracy and robustness improvements. Strategic planning is necessary to address the model's scalability for deployment in telemedicine apps and mobile health platforms, bearing in mind areas with restricted access to dermatological expertise. To maintain the model's applicability in the ever-changing field of skin cancer diagnosis, regular updates and adjustments that consider changing medical imaging datasets and technological improvements must be a key component of the future strategy.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p></p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>46</volume>
          <page-range>136-140</page-range>
          <issue>2</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Ana Laura</given-names>
              <surname>UdriÅtoiu</surname>
            </name>
            <name>
              <given-names>Alexandru Eugen</given-names>
              <surname>Stanca</surname>
            </name>
            <name>
              <given-names>Andreea Elena</given-names>
              <surname>Ghenea</surname>
            </name>
            <name>
              <given-names>Cristian Mihai</given-names>
              <surname>Vasile</surname>
            </name>
            <name>
              <given-names>Monica</given-names>
              <surname>Popescu</surname>
            </name>
            <name>
              <given-names>Åerban Constantin</given-names>
              <surname>UdriÅtoiu</surname>
            </name>
            <name>
              <given-names>Alexandra Virginia</given-names>
              <surname>Iacob</surname>
            </name>
            <name>
              <given-names>Silviana</given-names>
              <surname>Castravete</surname>
            </name>
            <name>
              <given-names>Lucian Gabriel</given-names>
              <surname>Gruionu</surname>
            </name>
            <name>
              <given-names>Gabriela</given-names>
              <surname>Gruionu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.12865/chsj.46.02.06</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Skin diseases classification using deep learning methods</article-title>
          <source>Curr. Health Sci. J.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>150</volume>
          <page-range>106170</page-range>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Alwin</given-names>
              <surname>Panthakkan</surname>
            </name>
            <name>
              <given-names>S. M.</given-names>
              <surname>Anzar</surname>
            </name>
            <name>
              <given-names>Safina</given-names>
              <surname>Jamal</surname>
            </name>
            <name>
              <given-names>Wajeeha</given-names>
              <surname>Mansoor</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compbiomed.2022.106170</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Concatenated Xception-ResNet50âA novel hybrid approach for accurate skin cancer prediction</article-title>
          <source>Comput. Biol. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>39</volume>
          <page-range>2095-2101</page-range>
          <issue>6</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>D.</given-names>
              <surname>Mane</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Ashtagi</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Kumbharkar</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Kadam</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Salunkhe</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Upadhye</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/ts.390622</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>An improved transfer learning approach for classification of types of cancer</article-title>
          <source>Trait. Signal</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>2677</page-range>
          <issue>5</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>L.</given-names>
              <surname>Hoang</surname>
            </name>
            <name>
              <given-names>S. H.</given-names>
              <surname>Lee</surname>
            </name>
            <name>
              <given-names>E. J.</given-names>
              <surname>Lee</surname>
            </name>
            <name>
              <given-names>K. R.</given-names>
              <surname>Kwon</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app12052677</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Multiclass skin lesion classification using a novel lightweight deep learning framework for smart healthcare</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>38</volume>
          <page-range>1699-1711</page-range>
          <issue>6</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>D.</given-names>
              <surname>Tiwari</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Dixit</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Gupta</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/ts.380613</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Deep multi-view breast cancer detection: A multi-view concatenated infrared thermal images based breast cancer detection system using deep transfer learning</article-title>
          <source>Trait. Signal</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>20</volume>
          <page-range>e11936</page-range>
          <issue>10</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Titus Josef</given-names>
              <surname>Brinker</surname>
            </name>
            <name>
              <given-names>Achim</given-names>
              <surname>Hekler</surname>
            </name>
            <name>
              <given-names>Jochen S.</given-names>
              <surname>Utikal</surname>
            </name>
            <name>
              <given-names>Niels</given-names>
              <surname>Grabe</surname>
            </name>
            <name>
              <given-names>Dirk</given-names>
              <surname>Schadendorf</surname>
            </name>
            <name>
              <given-names>Joachim</given-names>
              <surname>Klode</surname>
            </name>
            <name>
              <given-names>Carola</given-names>
              <surname>Berking</surname>
            </name>
            <name>
              <given-names>Theresa</given-names>
              <surname>Steeb</surname>
            </name>
            <name>
              <given-names>Alexander H.</given-names>
              <surname>Enk</surname>
            </name>
            <name>
              <given-names>Christof</given-names>
              <surname>Kalle</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.2196/11936</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Skin cancer classification using convolutional neural networks: Systematic review</article-title>
          <source>J. Med. Internet Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range/>
          <issue>3</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>R.</given-names>
              <surname>Refianti</surname>
            </name>
            <name>
              <given-names>A. B.</given-names>
              <surname>Mutiara</surname>
            </name>
            <name>
              <given-names>R. P.</given-names>
              <surname>Priyandini</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.14569/IJACSA.2019.0100353</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Classification of melanoma skin cancer using convolutional neural network</article-title>
          <source>Int. J. Adv. Comput. Sci. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>102</volume>
          <page-range>101756</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>N.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>Y.  X.</given-names>
              <surname>Cai</surname>
            </name>
            <name>
              <given-names>Y. Y.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>Y. T.</given-names>
              <surname>Tian</surname>
            </name>
            <name>
              <given-names>X.  L.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Badami</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.artmed.2019.101756</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Skin cancer diagnosisbased on optimized convolutional neural network</article-title>
          <source>Artif. Intell. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="book">
          <volume/>
          <page-range>578-586</page-range>
          <issue/>
          <year>2021</year>
          <publisher-name>Springer, Singapore</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>R.</given-names>
              <surname>Garg</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Maheshwari</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Shukla</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1007/978-981-15-6067-5_65</pub-id>
          <article-title>Decision support system for detection and classification of skin cancer using CNN</article-title>
          <source>Innovations in Computational Intelligence and Computer Vision</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>34</volume>
          <page-range>3285-3293</page-range>
          <issue>6</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Rashmi</given-names>
              <surname>Patil</surname>
            </name>
            <name>
              <given-names>Sreepathi</given-names>
              <surname>Bellary</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.jksuci.2020.09.002</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Machine learning approach in melanoma cancer stage detection</article-title>
          <source>J. King Saud Univ.-Comput. Inf. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>35</volume>
          <page-range>123-130</page-range>
          <issue>2</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Rashmi</given-names>
              <surname>Patil</surname>
            </name>
            <name>
              <given-names>Sreepathi</given-names>
              <surname>Bellary</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/ria.350203</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Transfer learning based system for melanoma type detection</article-title>
          <source>Revue Intell. Artif.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>1-6</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>R.</given-names>
              <surname>Patil</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Bellary</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/CCGE50943.2021.9776373</pub-id>
          <article-title>Ensemble learning for detection of types of melanoma</article-title>
          <source>2021 International Conference on Computing, Communication and Green Engineering (CCGE), Pune, India</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>253-263</page-range>
          <issue>10s</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A.</given-names>
              <surname>Padthe</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Ashtagi</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Mohite</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Gaikwad</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Bidwe</surname>
            </name>
            <name>
              <given-names>H. M.</given-names>
              <surname>Naveen</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>Harnessing federated learning for efficient analysis of large-scale healthcare image datasets in IoT-enabled healthcare systems</article-title>
          <source>Int. J. Intell. Syst. Appl. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>1-13</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Rashmi</given-names>
              <surname>Ashtagi</surname>
            </name>
            <name>
              <given-names>Deepak</given-names>
              <surname>Mane</surname>
            </name>
            <name>
              <given-names>Mahendra</given-names>
              <surname>Deore</surname>
            </name>
            <name>
              <given-names>Jyoti R.</given-names>
              <surname>Maranur</surname>
            </name>
            <name>
              <given-names>Sridevi</given-names>
              <surname>Hosmani</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.32629/jai.v7i1.749</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Combined deep learning and machine learning models for the prediction of stages of melanoma</article-title>
          <source>J. Auton. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>79</volume>
          <page-range>28477-28498</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.S.</given-names>
              <surname>Chaturvedi</surname>
            </name>
            <name>
              <given-names>J.V.</given-names>
              <surname>Tembhurne</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Diwan</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-020-09388-2</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A multi-class skin cancer classification using deep convolutional neural networks</article-title>
          <source>Multimed. Tools Appl.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>