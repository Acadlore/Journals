<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IJTDI</journal-id>
      <journal-id journal-id-type="doi">10.18280</journal-id>
      <journal-title-group>
        <journal-title>International Journal of Transport Development and Integration</journal-title>
        <abbrev-journal-title abbrev-type="issn">Int. J. Transp. Dev. Integr.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IJTDI</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2058-8313</issn>
      <issn publication-format="print">2058-8305</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-PXhHB8RCRtH4wIz180PO4OEaFf4NTSRm</article-id>
      <article-id pub-id-type="doi">10.18280/ijtdi.090401</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Real-Time Driver Drowsiness Detection Using ViViT for In-Vehicle Monitoring Systems Under Diverse Driving Conditions</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1,2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4476-6529</contrib-id>
          <name>
            <surname>Nurnaningsih</surname>
            <given-names>Desi</given-names>
          </name>
          <email>desinurnaningsih@telkomuniversity.ac.id</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7702-6554</contrib-id>
          <name>
            <surname>Adi</surname>
            <given-names>Kusworo</given-names>
          </name>
          <email>kusworoadi@lecturer.undip.ac.id</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_4">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1528-841X</contrib-id>
          <name>
            <surname>Surarso</surname>
            <given-names>Bayu</given-names>
          </name>
          <email>bayus@lecturer.undip.ac.id</email>
        </contrib>
        <aff id="aff_1">Doctoral Program in Information Systems, School of Postgraduate Studies, Diponegoro University, 50275 Semarang, Indonesia</aff>
        <aff id="aff_2">School of Computing the University Center of Exellence Artificial Intelligence for Learning &amp; Optimization (AILO), Telkom University, 12980 Jakarta, Indonesia</aff>
        <aff id="aff_3">Department of Physics, Faculty of Science and Mathematics, Diponegoro University, 50275 Semarang, Indonesia</aff>
        <aff id="aff_4">Department of Mathematics, Faculty of Science and Mathematics, Diponegoro University, 50275 Semarang, Indonesia</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>09</day>
        <month>11</month>
        <year>2025</year>
      </pub-date>
      <volume>9</volume>
      <issue>4</issue>
      <fpage>688</fpage>
      <lpage>699</lpage>
      <page-range>688-699</page-range>
      <history>
        <date date-type="received">
          <day>04</day>
          <month>06</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>03</day>
          <month>08</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>This study proposes a novel approach to driver drowsiness detection using the Video Vision Transformer (ViViT) model, which captures both spatial and temporal dynamics simultaneously to analyze eye conditions and head movements. The National Tsing Hua University Driver Drowsiness Detection (NTHU-DDD) dataset, which consists of 36,000 annotated video clips, was utilized for both training and evaluation. The ViViT model is compared to traditional Convolutional Neural Network (CNN) and Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) models, demonstrating superior performance with 96.2% accuracy and 95.9% F1-Score, while maintaining a 28.9 ms/frame inference time suitable for real-time deployment. The ablation study indicates that integrating spatial and temporal attention yields a notable improvement in model accuracy. Furthermore, positional encoding proves essential in preserving spatial coherence within video-based inputs. The model’s resilience was tested across a range of challenging conditions including low-light settings, partial occlusions, and drastic head movements and it consistently maintained reliable performance. With a compact footprint of just 89 MB, the ViViT model has been fine-tuned for deployment on embedded platforms such as the Jetson Nano, making it well-suited for edge AI applications. These findings highlight ViViT’s promise as a practical and high-performing solution for real-time driver drowsiness detection in real-world scenarios.</p></abstract>
      <kwd-group>
        <kwd>Driver drowsiness</kwd>
        <kwd>Video Vision Transformer</kwd>
        <kwd>Head movement analysis</kwd>
        <kwd>Eye condition</kwd>
        <kwd>Transportation safety</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="3"/>
        <fig-count count="7"/>
        <table-count count="5"/>
        <ref-count count="36"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Drowsiness during driving is a quiet yet deadly contributor to road accidents, often creeping in unnoticed and steadily impairing the driver’s alertness. The World Health Organization reports that over 1.3 million people die each year in traffic-related incidents [<xref ref-type="bibr" rid="ref_1">1</xref>]. With driver fatigue cited as a key cause, particularly on long or late-night journeys [<xref ref-type="bibr" rid="ref_2">2</xref>]. As this issue continues to claim lives globally, there is a pressing need for intelligent systems that can recognize the early onset of fatigue before it escalates into a life-threatening event.</p><p>According to the National Highway Traffic Safety Administration (NHTSA), driver fatigue contributes to nearly 20% of all fatal road accidents, underscoring the critical need for reliable monitoring systems that can recognize drowsiness before it leads to disaster [<xref ref-type="bibr" rid="ref_3">3</xref>]. Over the past decade, various detection systems have been developed to address this issue, employing different strategies that range from visual analysis of facial cues, physiological signal monitoring, to vehicle behavior tracking. These methods are often categorized into four main groups: image-based techniques, biometric signal interpretation, driving pattern analysis, and integrated hybrid models. Each approach has been evaluated based on its classification performance and practicality in real-world driving environments, particularly in detecting subtle behavioral indicators like eye closure, facial muscle changes, steering irregularities, and lane deviation [<xref ref-type="bibr" rid="ref_4">4</xref>]. Have been identified as more reliable early signs of fatigue than merely analyzing facial expressions or ocular conditions [<xref ref-type="bibr" rid="ref_5">5</xref>]. The study analyzed various sensors and machine learning algorithms employed in this domain, discussing their respective advantages and limitations. Physiological systems, such as Electroencephalography (EEG) and Electrocardiography (ECG) sensors, offer high accuracy, particularly in clinical and lab settings [<xref ref-type="bibr" rid="ref_6">6</xref>].</p><p>Nevertheless, their intrusive nature, high cost, and deployment complexity severely limit real-world use, especially in personal or commercial vehicles [<xref ref-type="bibr" rid="ref_7">7</xref>]. In contrast, camera-based methods that rely on facial features are non-invasive and cost-effective, making them attractive for embedded automotive systems. Several recent methods integrate You Only Look Once Version 3-Long Short Term Memory (YOLOv3-LSTM) or Convolutional Neural Network-Long Short Term Memory (CNN-LSTM) pipelines to capture both spatial and temporal cuest [<xref ref-type="bibr" rid="ref_8">8</xref>]. However, these hybrid models inherit structural inefficiencies: CNNs are effective in extracting spatial features (e.g., closed eyes), but inherently struggle to track the time-evolving patterns of drowsiness. Meanwhile, LSTM networks, though capable of processing sequences, are computationally heavy, slow to train, and sensitive to data noise, making real-time implementation problematic [<xref ref-type="bibr" rid="ref_9">9</xref>].</p><p>Many studies on driver drowsiness detection face significant gaps, particularly with traditional CNN-based methods that struggle to capture temporal dynamics like eye blinks or head nods, focusing on spatial features but failing to track changes over time. LSTMs handle sequential data better but are computationally heavy, making them inefficient for real-time applications on devices with limited resources. Hybrid models, combining CNNs and LSTMs or transformers, often suffer from redundancy and increased computational costs, resulting in slower inference times, which makes them less suitable for fast, real-time detection. Unlike conventional models, Video Vision Transformer (ViViT) adopts a pure transformer architecture that naturally accommodates both spatial and temporal patterns within video frames. In this study, we investigate how ViViT can effectively recognize subtle driver behaviors such as slight head tilts or shifts in gaze direction that often signal the early onset of drowsiness. By doing so, we aim to offer a more responsive and precise detection system that outperforms existing approaches in real-time scenarios.</p><p>This research sets out to bridge the current gaps in drowsiness detection by leveraging the capabilities of a pure ViViT framework. The model places emphasis on analyzing eye states and head posture to identify signs of fatigue. Our main goal is to develop and implement the ViViT architecture, while thoroughly evaluating its performance in terms of accuracy, stability, and computational efficiency, especially in comparison with traditional CNN-based and hybrid solutions. In addition, the study delves into how well the model copes with real-world complexities such as poor lighting, varied head positions, and the presence of accessories like eyeglasses or face masks. A final and critical aspect of this work is to determine whether ViViT can be practically deployed in real time on lightweight, embedded systems for in-vehicle driver monitoring.</p><p>The research questions driving this study are centered on ViViT’s ability to outperform existing models: Can ViViT achieve better accuracy than CNN and CNN-LSTM models in diverse scenarios? Does it show superior resilience in challenging conditions such as occlusions or lighting variations? And finally, is ViViT computationally feasible for real-time systems on embedded devices?</p><p>This study makes meaningful contributions to the field of intelligent transportation systems. One of its key innovations lies in the development of a stand-alone ViViT-based model for drowsiness detection, eliminating the need for traditional CNN or LSTM components. By relying solely on the transformer architecture, the approach offers a fresh perspective on how spatio-temporal features can be effectively harnessed to monitor driver alertness. The study also presents a thorough performance evaluation using the NTHU-DDD dataset, encompassing 36,000 video samples under varied conditions. With 98% classification accuracy, the model outperforms multiple baseline models and shows strong potential for practical application. Additionally, the research provides valuable insights into the model’s resilience against environmental challenges, laying the groundwork for real-world deployment of advanced driver monitoring technologies.</p>
    </sec>
    <sec sec-type="">
      <title>2. Related work</title>
      <p>In Section 2, each research on driver drowsiness significantly contributes to improving both external and internal road safety to enhance safety and reduce or prevent all risks associated with driving while drowsy. Several vision-based studies have demonstrated respectable performance under controlled conditions. For instance, a Face Mesh model trained on the Learning Without Forgetting (LWF) dataset enhanced recognition accuracy up to 94.23%, even under varied lighting and backgrounds [<xref ref-type="bibr" rid="ref_10">10</xref>]. Similarly, an Support Vector Machine (SVM)-based approach for eye closure and yawning detection achieved 98% and 92.5% accuracy, respectively. However, these models were tested on limited and manually labeled datasets, reducing their generalizability to real-world traffic scenarios [<xref ref-type="bibr" rid="ref_11">11</xref>]. On the other hand, vision-based methods like CNN are easier to implement because they only require a camera, but they are less effective in capturing the temporal relationships of facial expression changes and head movements [<xref ref-type="bibr" rid="ref_12">12</xref>]. Several handcrafted-feature-based approaches, such as Histogram of Oriented Gradients (HOG) + Naïve Bayes, have also been proposed. Although simple, they lack the expressive power and flexibility of deep models, and often perform poorly in varying head orientations or lighting conditions [<xref ref-type="bibr" rid="ref_13">13</xref>]. Some studies addressed this limitation by integrating LSTM modules into CNN pipelines, enabling the model to capture sequential patterns [<xref ref-type="bibr" rid="ref_14">14</xref>]. However, this hybrid architecture increases computational load and latency, posing challenges for real-time processing on embedded systems [<xref ref-type="bibr" rid="ref_15">15</xref>]. Moreover, LSTMs are prone to overfitting, especially when trained on small or noisy datasets, and tend to struggle in generalizing across diverse driving behaviors.</p><p>Deep feature extractors like FaceNet have been combined with K-Nearest Neighbor (KNN) classifiers to improve classification in datasets like University of Texas at Arlington Real-Life Drowsiness Dataset (UTA-RLDD). While achieving 94.68% accuracy [<xref ref-type="bibr" rid="ref_16">16</xref>], these methods still depend heavily on static features and lack sensitivity to temporal transitions, which are crucial for detecting early signs of fatigue. To bridge this gap, recent works have begun incorporating transformer-based architectures. These models excel in learning long-range dependencies in both space and time, offering a powerful alternative to LSTM. For instance, CNN-transformer hybrids have been applied to detect obstructive sleep apnea (OSA) via ECG data, yielding high diagnostic accuracy [<xref ref-type="bibr" rid="ref_17">17</xref>]. However, such models still inherit CNN's dependence on spatial priors and suffer from computational overhead.</p><p>TFormer, a time-frequency transformer for EEG-based fatigue detection, achieved superior generalization across subjects [<xref ref-type="bibr" rid="ref_18">18</xref>]. Yet, its lack of performance benchmarking against GNNs or larger-scale temporal architectures highlights a gap in evaluating generalizability and scalability. Other models, like Gabor Phase Biometric-Convolutional Neural Network (GP-BCCN) with Gabor filters, enhanced feature security but struggled with occlusion and rapid head motion, limiting their real-world utility.</p><p>CaTNet, a lightweight CNN-transformer hybrid, demonstrated impressive accuracy (up to 99.91%) in distraction detection [<xref ref-type="bibr" rid="ref_19">19</xref>]. However, its evaluation did not include dynamic, uncontrolled environments such as night driving or low-resolution footage, making its robustness unclear. Some researchers introduced DA-CapsNet with adversarial domain adaptation to handle inter-subject EEG variation [<xref ref-type="bibr" rid="ref_20">20</xref>]. while others explored Fusion ViViT, combining RGB and NIR signals for remote heart rate estimation [<xref ref-type="bibr" rid="ref_21">21</xref>]. These works reflect an increasing shift toward multimodal data and transformer integration, but often demand high-quality inputs, limiting field deployment.</p><p>In terms of efficiency, the Spatial-Temporal Token Selection (STTS) method reduced transformer computation by 33% [<xref ref-type="bibr" rid="ref_22">22</xref>]. Nonetheless, these studies focus on general video classification or object tracking, not explicitly on fatigue-related facial or posture cues.</p><p>In the context of explainable AI, Spatio-Temporal Attention Attribution (STAA) offers real-time attribution visualization within transformer models [<xref ref-type="bibr" rid="ref_23">23</xref>], [<xref ref-type="bibr" rid="ref_24">24</xref>], but has yet to be applied to in-vehicle drowsiness detection. Meanwhile, DA-ViViT shows promising results by fusing facial and body posture analysis [<xref ref-type="bibr" rid="ref_25">25</xref>], yet remains reliant on pose estimation techniques prone to occlusion errors in practical settings.</p><p>ViViT introduces a pure transformer-based approach that processes video data as spatio-temporal token sequences through self-attention mechanisms [<xref ref-type="bibr" rid="ref_26">26</xref>]. Compared to 3D CNNs or hybrid networks, ViViT offers a unified framework that learns temporal dependencies without additional LSTM/CNN components, making it ideal for fatigue detection that evolves over time, as shown in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Key parameters of our model</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Dataset</p></td><td colspan="1" rowspan="1"><p>Features</p></td><td colspan="1" rowspan="1"><p>Model Used</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td></tr><tr><td colspan="1" rowspan="1"><p>NTHU-DDD [<xref ref-type="bibr" rid="ref_12">12</xref>]</p></td><td colspan="1" rowspan="1"><p>Eye, Mouth</p></td><td colspan="1" rowspan="1"><p>3D CNN</p></td><td colspan="1" rowspan="1"><p>94.7%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Driver Drowsiness Detection (DDD) dataset [<xref ref-type="bibr" rid="ref_27">27</xref>]</p></td><td colspan="1" rowspan="1"><p>Eye, Mouth</p></td><td colspan="1" rowspan="1"><p>3D NN</p></td><td colspan="1" rowspan="1"><p>73.9%</p></td></tr><tr><td colspan="1" rowspan="1"><p>NTHU-DDD [<xref ref-type="bibr" rid="ref_28">28</xref>]</p></td><td colspan="1" rowspan="1"><p>Eye, Mouth</p></td><td colspan="1" rowspan="1"><p>3D CNN</p></td><td colspan="1" rowspan="1"><p>88.6%</p></td></tr><tr><td colspan="1" rowspan="1"><p>YawDDD [<xref ref-type="bibr" rid="ref_9">9</xref>]</p></td><td colspan="1" rowspan="1"><p>Head, Emotion</p></td><td colspan="1" rowspan="1"><p>Support Vector Machine (SVM)</p></td><td colspan="1" rowspan="1"><p>78%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Ours NTHU-DDD</p></td><td colspan="1" rowspan="1"><p>Head Movement, Eye</p></td><td colspan="1" rowspan="1"><p>Video Vision Transformer (ViViT)</p></td><td colspan="1" rowspan="1"><p>96%</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>Most reviewed methods either capture spatial features without temporal continuity (CNNs) or attempt to learn sequences through heavy and redundant architectures (CNN-LSTM, CNN-Transformer). Even recent ViViT-related studies adopt multimodal or hybrid forms, yet none have fully explored a pure ViViT-based approach focused solely on head movement and eye dynamics for real-time drowsiness detection. This study aims to fill this void.</p>
    </sec>
    <sec sec-type="">
      <title>3. Methodology</title>
      
        <sec>
          
            <title>3.1. Dataset description and preprocessing</title>
          
          <p>This study makes use of the NTHU Driver Drowsiness Detection (NTHU-DDD) dataset [<xref ref-type="bibr" rid="ref_29">29</xref>], known for its rich variety of scenarios and realistic depiction of real-world driving conditions. Its comprehensive coverage of different environments makes it a reliable benchmark for evaluating drowsiness detection systems.</p><p>To support this study, we utilized the NTHU-DDD dataset, which comprises video recordings of 36 individuals balanced by gender (18 men and 18 women), ranging in age from 18 to 40 years. The participants represent a diverse range of ethnic backgrounds, including 32.5% Black/Brown, 32.5% White, and 35% Yellow/Asian. Recordings were conducted under five distinct scenarios: normal daytime driving, nighttime conditions, while wearing sunglasses, with regular glasses, and during various head movements, specifically yaw (side-to-side turns), pitch (up-and-down nods), and tilt (side inclination).</p><p>Each video was meticulously labeled by experts who evaluated drowsiness levels based on observable signs such as prolonged eye closure and frequency of head nodding. The participants were then classified into three categories: alert, slightly drowsy, and severely drowsy, as shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>. To maintain consistency and ensure the integrity of the labeling, the annotation process involved cross-validation by multiple reviewers, thereby strengthening the dataset’s reliability for supervised learning applications.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>The dataset collection includes participants with a diverse range of skin tones, genders, and ethnic backgrounds</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_4UupQY21jRv9TweO.png"/>
            </fig>
          
          <p>To prepare the input data for the model, each original video was segmented into shorter clips lasting between 15 and 30 seconds. These segments were then broken down into individual frames at a rate of 30 frames per second (FPS), allowing the model to capture subtle temporal patterns over time. Frame extraction was carried out using OpenCV, and each image was resized to 224 × 224 pixels with the help of the Torchvision library, ensuring compatibility with the ViViT model’s input specifications. As a final preprocessing step, pixel values were scaled to a [0, 1] range to support smoother learning and improve overall training stability.</p><p>To strengthen the model’s ability to generalize across varying conditions, several data augmentation strategies were employed. These included random horizontal flips, slight rotations of up to <inline-formula>
  <mml:math id="mp6wdxpwtm">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula> 15 degrees, and modifications to brightness and contrast mimicking the fluctuations in real-world lighting environments. Since ViViT operates not on full images but on smaller patches, each frame was segmented into 16 × 16 pixel grids, resulting in 196 distinct tokens per frame. This patch-based representation allows the model to focus on localized features while preserving the broader spatial structure.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Vivit architecture overview</title>
          
          <p>In this section, we briefly recall the early aspects related to vision transformers [<xref ref-type="bibr" rid="ref_30">30</xref>], [<xref ref-type="bibr" rid="ref_31">31</xref>], and subsequently discuss position encoding and resolution.</p><p>ViViT represents an advanced pure transformer-based model designed specifically for video classification tasks. Unlike traditional methods such as CNNs or hybrid CNN-LSTMs, ViViT is capable of simultaneously processing both spatial (within-frame) and temporal (across-frame) information using its innovative self-attention mechanism. This dual focus enables the model to capture and understand dynamic changes in video sequences, such as subtle shifts in gaze or head movement over time, which are critical for tasks like drowsiness detection.</p><p>In its operation, ViViT processes video data as a series of frames, where each frame is divided into 16 × 16 patches. These patches are then flattened and mapped to a 768-dimensional embedding to facilitate learning. To ensure that the model understands the spatial arrangement of these patches, learnable class tokens and positional embeddings are added. The Factorized Encoder architecture is employed to separately handle spatial and temporal attention. The spatial attention focuses on analyzing appearance-based features such as closed eyes or gaze shifts, while the temporal attention captures how these features evolve across frames, like blinking frequency or head nodding.</p><p>The ViViT model consists of 12 transformer blocks, each with 8 attention heads. The model’s hidden size is set to 768, with an Multilayer Perceptron (MLP) dimension of 3072 and a dropout rate of 0.1 to prevent overfitting. No CNN or LSTM components are used, which allows for greater scalability and parallel processing, making ViViT an efficient solution for tasks that require handling large video data while maintaining high accuracy.</p><p>This unique architecture sets ViViT apart from conventional approaches, as it not only captures both spatial and temporal dependencies but also does so in a way that is computationally efficient and well-suited for real-time applications. The Multi-Head Self-Attention (MSA) mechanism processes input by calculating attention weights from key-value pairs, allowing the model to focus on different parts of the data through multiple attention heads. These outputs are then combined, passed through a linear transformation, and used to produce a final representation that captures both spatial and temporal dependencies in the data. This produces an output matrix [<xref ref-type="bibr" rid="ref_31">31</xref>]:</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="mk8onv0kjb">
                <mml:mtext> Attention </mml:mtext>
                <mml:mo>(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mi>Q</mml:mi>
                <mml:mi>K</mml:mi>
                <mml:mi>V</mml:mi>
                <mml:mi>Softmax</mml:mi>
                <mml:mi>V</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mi>Q</mml:mi>
                      <mml:msup>
                        <mml:mi>K</mml:mi>
                        <mml:mi>T</mml:mi>
                      </mml:msup>
                    </mml:mrow>
                    <mml:msqrt>
                      <mml:mi>d</mml:mi>
                    </mml:msqrt>
                  </mml:mfrac>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>Transformer block for images. The transformer block for images combines MSA with a Feed-Forward Network (FFN), where the FFN consists of two linear transformations with a GeLU activation to introduce non-linearity, operating within a residual framework. Input images are divided into 16 × 16 patches, each processed independently, with positional embeddings added to maintain spatial context, enabling the model to capture both spatial and temporal dependencies across the image [<xref ref-type="bibr" rid="ref_32">32</xref>], [<xref ref-type="bibr" rid="ref_33">33</xref>]. The class token, attached to the patch tokens before the first layer, is processed through the transformer layers and used to predict the output, differentiating it from traditional pooling methods in computer vision. During training, the model relies on self-attention to exchange information between the class token and patch tokens, with supervision provided only through the class embedding [<xref ref-type="bibr" rid="ref_34">34</xref>]. Maintaining consistent positional encoding across different resolutions enhances training efficiency by allowing the model to be initially trained at a lower resolution and fine-tuned at a higher one, improving both speed and accuracy. When increasing image resolution, the patch size stays the same, but the number of patches changes, requiring interpolation of positional embeddings to accommodate the updated resolution [<xref ref-type="bibr" rid="ref_35">35</xref>]. <xref ref-type="fig" rid="fig_2">Figure 2</xref> shows the two-stream (spatial and temporal) attention pipeline, followed by an MLP for final classification into three classes.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Vision transformer architecture for video data analysis with factorized attention mechanism</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_gnzycfaxOipO_hBc.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. Training configuration</title>
          
          <p>In the section, to ensure optimal performance and reproducibility of the model, a carefully considered training configuration was established. The Adam optimizer was selected due to its well-documented success in deep learning tasks, providing both robustness and stability when working with high-dimensional input data. The learning rate was set to 1e-4 following empirical testing, where higher values led to oscillation in the loss, and lower values resulted in slower convergence. The batch size was chosen as 16, which offered a good balance between GPU memory constraints and maintaining gradient stability throughout the training process.</p><p>In terms of training duration, the model was evaluated over 8 epochs, as accuracy appeared to plateau beyond this point. To optimize the classification performance, Categorical Cross-Entropy was used as the loss function, making it suitable for the multi-class classification task at hand.</p><p>The entire training process was carried out on Google Colab Pro, utilizing the NVIDIA Tesla T4 GPU for accelerated computations. The PyTorch 1.10 framework was leveraged for model implementation, with essential libraries such as OpenCV, Torchvision, and NumPy aiding in image processing and numerical computations. This environment ensured that the training was efficient and aligned with the complexity of the ViViT model.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Evaluation strategy</title>
          
          <p>To ensure a comprehensive and reliable evaluation of the model, a hold-out validation strategy was employed, with the dataset split into 70% training, 15% validation, and 15% testing. This distribution was carefully designed to ensure that each class, Alert, Mild, and Severe, was proportionally represented, maintaining a balanced representation across all levels of drowsiness. To further assess the model’s robustness, a 3-fold cross-validation was performed on the training data. This approach allowed us to validate the model’s performance across multiple data splits, ensuring that the results were consistent and not reliant on any single subset of the data.</p><p>The performance of the model was assessed using a variety of metrics, including Accuracy, Precision, Recall, and F1-Score, all of which were essential for understanding the model’s effectiveness in distinguishing between different levels of drowsiness. Additionally, the Confusion Matrix was visualized to provide a more granular look at the classification results, highlighting areas where the model excelled or struggled.</p><p>For a fair and direct comparison, baseline models based on CNN and CNN-LSTM were reimplemented using the same preprocessing steps and evaluation protocols, ensuring consistency in the benchmarking process. This allowed for a clear comparison of the ViViT model’s performance against existing state-of-the-art techniques in the field.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Training configuration</title>
      
        <sec>
          
            <title>4.1. Experimental setup</title>
          
          <p>The experiments were conducted using Google Colab Pro equipped with an NVIDIA Tesla T4 GPU, ensuring fast training cycles and smooth execution. The system environment included Python 3.8, PyTorch 1.10, and TensorFlow 2.8, alongside OpenCV, NumPy, Matplotlib, and TorchVision for data manipulation and visualization.</p><p>The dataset comprised 36,000 annotated video clips with 640 × 480 resolution at 30 FPS. Each clip was labeled into one of three drowsiness levels: Alert, Mild Drowsiness, and Severe Drowsiness, based on blinking frequency, gaze behavior, and head posture analysis. The dataset included diverse lighting conditions (daylight, nighttime, low light) and occlusion scenarios (e.g., sunglasses, medical masks), designed to simulate real-world challenges.</p>
        </sec>
      
      
        <sec>
          
            <title>4.2. Training and validation metrics</title>
          
          <p>The dataset used in this study was divided into three main portions: 70% for training, 15% for validation, and the remaining 15% for final testing. To ensure the stability of the model’s performance during the training phase, a 3-fold cross-validation technique was applied exclusively to the training set. The training results showed a consistent improvement in accuracy, beginning at around 90% in the first epoch and stabilizing at 98.1% by the eighth epoch (see <xref ref-type="fig" rid="fig_3">Figure 3</xref>). The downward trend in the loss values, as illustrated in the graph, indicates the model’s effectiveness in learning the underlying patterns in the data. This is evidenced by a significant drop in loss from around 0.24 to nearly 0.05, suggesting a well-converging training process and improved generalization capability, as shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Training loss of the ViViT method acquisition</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_LWHJPqvlhL4lxv--.png"/>
            </fig>
          
          <p>The model’s training accuracy over the course of eight training epochs. At the initial stage, the model recorded an accuracy of 90%, indicating that the learning process had just begun and was still in its early, unrefined phase. As the number of epochs increased, accuracy rose sharply, reaching approximately 96% by the third epoch. This was followed by a continued but slower improvement. The training accuracy stabilized within the range of 97–98%, and by the eighth epoch, the model achieved its peak accuracy at 98%. This pattern demonstrates that the model consistently learned from the data, progressively improved its classification ability, and underwent an effective training process without significant overfitting, as shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>.</p><p>Validation accuracy, on the other hand, fluctuated slightly but remained high, peaking at 96% in the final epoch. A minor dip in epoch 6 (to ~94.3%) suggests the model briefly struggled with generalization, but recovered in later epochs, as shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Training accuracy of the ViViT method acquisition</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_hLhUDal9hwWRm4iH.png"/>
            </fig>
          
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Graph of the results of the ViViT method acquisition</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_Yy2XMyXOO6GDBno-.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.3. Evaluation metrics</title>
          
          <p>The model’s performance is evaluated by training the vision transformer model using the dataset. The performance metrics used are Accuracy, Recall, Precision, and F1-Score. These metrics are important for assessing how well the model accurately classifies markers. The equations for these metrics are as follows:</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="mjzih2m5a7">
                <mml:mtext> Accuracy </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mo>×</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">TP</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">TN</mml:mi>
                    </mml:mrow>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">TP</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">TN</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">FP</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">EN</mml:mi>
                    </mml:mrow>
                    <mml:mo>+</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mn>100</mml:mn>
                <mml:mi>%</mml:mi>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="mzlcg6dgq1">
                <mml:mtext> Precision </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mo>×</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi data-mjx-auto-op="false">TP</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">TP</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">FN</mml:mi>
                    </mml:mrow>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mn>100</mml:mn>
                <mml:mi>%</mml:mi>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="mlx15j63lm">
                <mml:mtext> Recall </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mo>×</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi data-mjx-auto-op="false">TP</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">TP</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">FN</mml:mi>
                    </mml:mrow>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mn>100</mml:mn>
                <mml:mi>%</mml:mi>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="mt6r11lr3f">
                <mml:mtext> F1 Score </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mo>×</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mn>2</mml:mn>
                    <mml:mo>∗</mml:mo>
                    <mml:mi>P</mml:mi>
                    <mml:mi>R</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>P</mml:mi>
                    <mml:mi>R</mml:mi>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mn>100</mml:mn>
                <mml:mi>%</mml:mi>
              </mml:math>
            </disp-formula>
          
          <p>To provide a holistic understanding of the model’s effectiveness, we report multiple evaluation metrics on the testing set. These metrics offer deeper insights into how well the proposed ViViT model performs in distinguishing different levels of driver drowsiness. For more details, the complete results can be seen in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Evaluation metrics for the DDD model</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Metric</p></td><td colspan="1" rowspan="1"><p>Value (%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>96.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>Precision</p></td><td colspan="1" rowspan="1"><p>95.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>Recall</p></td><td colspan="1" rowspan="1"><p>96.7</p></td></tr><tr><td colspan="1" rowspan="1"><p>F1-Score</p></td><td colspan="1" rowspan="1"><p>95.9</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>4.4. Comparative performance with baseline models</title>
          
          <p>The original version lacked comparative analysis. We now present a direct performance comparison with baseline models, reimplemented using identical preprocessing and training pipelines for fairness.</p><p>ViViT outperformed all other methods in both accuracy and F1-Score, while maintaining a manageable inference time suitable for real-time deployment, as shown in <xref ref-type="table" rid="table_3">Table 3</xref>. Although 3D CNNs showed strong performance, their inference latency was more than twice that of ViViT, making them less ideal for embedded systems.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Comparison of model performance for DDD</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>F1-Score</p></td><td colspan="1" rowspan="1"><p>Inference Time (ms/frame)</p></td></tr><tr><td colspan="1" rowspan="1"><p>CNN [<xref ref-type="bibr" rid="ref_35">35</xref>]</p></td><td colspan="1" rowspan="1"><p>99.4%</p></td><td colspan="1" rowspan="1"><p>99.3%</p></td><td colspan="1" rowspan="1"><p>71.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>CNN-LSTM [<xref ref-type="bibr" rid="ref_36">36</xref>]</p></td><td colspan="1" rowspan="1"><p>91.5%</p></td><td colspan="1" rowspan="1"><p>90.2%</p></td><td colspan="1" rowspan="1"><p>54.7</p></td></tr><tr><td colspan="1" rowspan="1"><p>3D CNN [<xref ref-type="bibr" rid="ref_12">12</xref>]</p></td><td colspan="1" rowspan="1"><p>94.7%</p></td><td colspan="1" rowspan="1"><p>93.1%</p></td><td colspan="1" rowspan="1"><p>69.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>ViViT (Ours)</p></td><td colspan="1" rowspan="1"><p>96.2%</p></td><td colspan="1" rowspan="1"><p>95.9%</p></td><td colspan="1" rowspan="1"><p>28.9</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>4.5. Ablation study</title>
          
          <p>This study aimed to isolate the impact of individual features, such as spatial attention, temporal attention, and positional encoding, by systematically removing or modifying each component and observing the resulting changes in performance. The results confirmed that combining both spatial and temporal attention significantly improved the model’s accuracy, underscoring the importance of this integrated approach for effective driver drowsiness detection can be seen in <xref ref-type="table" rid="table_4">Table 4</xref>. Additionally, the positional encoding was shown to be crucial for maintaining the spatial context across patches, with its removal leading to a noticeable drop in performance. These findings not only highlight the effectiveness of the full ViViT architecture but also validate its components as essential to its success.</p>
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Ablation study on ViViT architecture</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Configuration</p></td><td colspan="1" rowspan="1"><p>Accuracy (%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>ViViT with spatial attention only</p></td><td colspan="1" rowspan="1"><p>91.6</p></td></tr><tr><td colspan="1" rowspan="1"><p>ViViT with temporal attention only</p></td><td colspan="1" rowspan="1"><p>89.4</p></td></tr><tr><td colspan="1" rowspan="1"><p>ViViT without positional encoding</p></td><td colspan="1" rowspan="1"><p>87.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>Full ViViT (ours)</p></td><td colspan="1" rowspan="1"><p>96.2</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>The ablation study results presented in <xref ref-type="fig" rid="fig_6">Figure 6</xref> illustrate the contribution of each key component in the ViViT architecture to the overall model accuracy. The full ViViT configuration, which integrates both spatial and temporal attention along with positional encoding, achieved the highest accuracy of 96.2%. When relying solely on spatial attention, the performance dropped to 91.6%, while using only temporal attention further reduced the accuracy to 89.4%. The lowest performance was observed when positional encoding was removed, resulting in a significant decrease to 87.1%.</p>
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Graph the ablation study showing the contribution of each major component to the ViViT architecture</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_rEGe5PSuFRlvEl3X.png"/>
            </fig>
          
          <p>The results confirm that combining spatial and temporal attention yields a substantial gain, and that positional encoding plays a vital role in maintaining frame-order awareness.</p>
        </sec>
      
      
        <sec>
          
            <title>4.6. Model resilience under challenging conditions</title>
          
          <p>To assess robustness, we evaluated the model under various simulated conditions. Quantitative results are presented below in <xref ref-type="table" rid="table_5">Table 5</xref>, which presents the performance of the ViViT model under various testing scenarios that reflect real-world driving challenges. Under normal daylight conditions, the model achieved its highest accuracy of 96.5%. The performance remained stable in limited lighting environments, recording 95.2% in low-light settings and 94.8% during nighttime driving. When tested with visual obstructions such as sunglasses and medical masks, the accuracy slightly declined to 93.7% and 94.3%, respectively. The most challenging scenario occurred during extreme head movements, where accuracy dropped to 92.9%. These results indicate that, despite variations in environmental conditions and visual disturbances, ViViT consistently maintained accuracy above 92%, demonstrating its robustness in complex real-world situations.</p>
          
            <table-wrap id="table_5">
              <label>Table 5</label>
              <caption>
                <title>Model resilience under challenging conditions</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Condition</p></td><td colspan="1" rowspan="1"><p>Accuracy (%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Daylight (normal)</p></td><td colspan="1" rowspan="1"><p>96.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>Low light</p></td><td colspan="1" rowspan="1"><p>95.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>Nighttime</p></td><td colspan="1" rowspan="1"><p>94.8</p></td></tr><tr><td colspan="1" rowspan="1"><p>Wearing sunglasses</p></td><td colspan="1" rowspan="1"><p>93.7</p></td></tr><tr><td colspan="1" rowspan="1"><p>Wearing medical mask</p></td><td colspan="1" rowspan="1"><p>94.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>Extreme head movement</p></td><td colspan="1" rowspan="1"><p>92.9</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>Unlike CNN models, which saw accuracy drops up to 30–40% in low light or occlusion, ViViT’s accuracy stayed consistently above 92%, affirming its robustness across environments. These results are illustrated in <xref ref-type="fig" rid="fig_7">Figure 7</xref>, which compares ViViT’s performance with CNN in different scenarios.</p>
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>The comparison graphs of model durability</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_mSWQqOIvXw33pq9O.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.7. Resource requirements and deployment feasibility</title>
          
          <p>From a deployment standpoint, the ViViT model has been streamlined for real-time operation, with a compact size of around 89 MB, making it a strong candidate for implementation on edge AI devices such as the Jetson Nano, which is widely used in embedded system environments. On average, the model processes each frame in approximately 28.9 milliseconds, allowing it to exceed 30 FPS, a threshold necessary for smooth, real-time performance in automotive settings. In terms of hardware demand, ViViT requires roughly 1.2 GB of GPU memory during inference, a footprint that remains within the capacity of many modern embedded platforms. These characteristics demonstrate not only the model’s precision but also its practical efficiency, underscoring its readiness for integration into real-world driver monitoring systems where speed and reliability are non-negotiable.</p>
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>This study demonstrates the effectiveness of the ViViT model for real-time driver drowsiness detection, leveraging its ability to simultaneously capture spatial and temporal information. The model outperforms traditional methods, including CNN and CNN-LSTM, achieving a high accuracy of 96.2% and F1-Score of 95.9%, with an efficient inference time of 28.9 ms per frame. The ablation study highlights the importance of spatial and temporal attention, as well as positional encoding, in improving the model's performance. ViViT’s resilience under diverse conditions, such as varying lighting, head movements, and accessory usage, further proves its robustness. Additionally, the model’s small size (89 MB) and low GPU memory requirement make it well-suited for real-time deployment on embedded systems, such as the Jetson Nano, offering both performance and efficiency. These findings underscore ViViT’s potential as a powerful, scalable solution for driver monitoring systems, with the ability to operate effectively in real-world environments.</p><p>For future research, it is recommended to explore model optimization through knowledge distillation techniques to reduce computational requirements without sacrificing accuracy. Furthermore, a federated learning approach could be considered to enhance privacy and improve training efficiency in real-world scenarios. Further testing on various vehicle types and a broader population of drivers would also help ensure better generalization of the model.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>Conceptualization, methodology, software and original draft preparation are done by D.N.; supervision, review, and formal analysis are done by K.A. and B.S. All authors have read and agreed to the published version of the manuscript.</p>
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="webpage">
          <article-title>Social Determinants of Health</article-title>
          <source>, https://www.who.int/teams/social-determinants-of-health</source>
          <year>2025</year>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jose</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Raimond</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Vincent</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2211.0071</pub-id>
          <article-title>SleepyWheels: An ensemble model for drowsiness detection leading to accident prevention</article-title>
          <source>arXiv preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Singh</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Kanojia</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Bansal</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Bansal</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2303.06310</pub-id>
          <article-title>Driver drowsiness detection system: An approach by machine learning application</article-title>
          <source>arXiv preprint, arXiv:2303.06310</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>2069</page-range>
          <issue>5</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Albadawi</surname>
              <given-names>Yaman</given-names>
            </name>
            <name>
              <surname>Takruri</surname>
              <given-names>Maen</given-names>
            </name>
            <name>
              <surname>Awad</surname>
              <given-names>Mohammed</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s22052069</pub-id>
          <article-title>A review of recent developments in driver drowsiness detection systems</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>34</volume>
          <page-range>13883-13893</page-range>
          <issue>16</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pandey</surname>
              <given-names>Nageshwar Nath</given-names>
            </name>
            <name>
              <surname>Muppalaneni</surname>
              <given-names>Naresh Babu</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00521-022-07209-1</pub-id>
          <article-title>A novel drowsiness detection model using composite features of head, eye, and facial expression</article-title>
          <source>Neural Comput. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>1229-1259</page-range>
          <issue>5</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Saleem</surname>
              <given-names>Adil Ali</given-names>
            </name>
            <name>
              <surname>Siddiqui</surname>
              <given-names>Hafeez Ur Rehman</given-names>
            </name>
            <name>
              <surname>Raza</surname>
              <given-names>Muhammad Amjad</given-names>
            </name>
            <name>
              <surname>Rustam</surname>
              <given-names>Furqan</given-names>
            </name>
            <name>
              <surname>Dudley</surname>
              <given-names>Sandra</given-names>
            </name>
            <name>
              <surname>Ashraf</surname>
              <given-names>Imran</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11571-022-09898-9</pub-id>
          <article-title>A systematic review of physiological signals based driver drowsiness detection systems</article-title>
          <source>Cogn. Neurodyn.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>214</volume>
          <page-range>106535</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gangadharan</surname>
              <given-names>Sreejith</given-names>
            </name>
            <name>
              <surname>Vinod</surname>
              <given-names>A. Prathosh</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.cmpb.2021.106535</pub-id>
          <article-title>Drowsiness detection using portable wireless EEG</article-title>
          <source>Comput. Methods Programs Biomed.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>119</volume>
          <page-range>105759</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pandey</surname>
              <given-names>Nageshwar Nath</given-names>
            </name>
            <name>
              <surname>Muppalaneni</surname>
              <given-names>Naresh Babu</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.engappai.2022.105759</pub-id>
          <article-title>Dumodds: Dual modeling approach for drowsiness detection based on spatial and spatio-temporal features</article-title>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="conf-paper">
          <page-range>207-212</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Joshi</surname>
              <given-names>Aditi</given-names>
            </name>
            <name>
              <surname>Kyal</surname>
              <given-names>Suvasis</given-names>
            </name>
            <name>
              <surname>Banerjee</surname>
              <given-names>Soumya</given-names>
            </name>
            <name>
              <surname>Mishra</surname>
              <given-names>Tanupriya</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/IV47402.2020.9304579</pub-id>
          <article-title>In-the-wild drowsiness detection from facial expressions</article-title>
          <source>2020 IEEE Intelligent Vehicles Symposium (IV), Las Vegas, NV, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>218</volume>
          <page-range>741-749</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hangaragi</surname>
              <given-names>Shreyas</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>Tarun</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.procs.2023.01.054</pub-id>
          <article-title>Face detection and recognition using Face Mesh and deep neural network</article-title>
          <source>Procedia Comput. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>91</page-range>
          <issue>5</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Albadawi</surname>
              <given-names>Yaman</given-names>
            </name>
            <name>
              <surname>AlRedhaei</surname>
              <given-names>Aneesa</given-names>
            </name>
            <name>
              <surname>Takruri</surname>
              <given-names>Maen</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/jimaging9050091</pub-id>
          <article-title>Real-time machine learning-based driver drowsiness detection using visual features</article-title>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <page-range>425-428</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kang</surname>
              <given-names>Nari</given-names>
            </name>
            <name>
              <surname>Han</surname>
              <given-names>Seongmin</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>Sangmin</given-names>
            </name>
            <name>
              <surname>Kwon</surname>
              <given-names>Seungjae</given-names>
            </name>
            <name>
              <surname>Choi</surname>
              <given-names>Yeongwook</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>Yong Taek</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>Seung In</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICTC55196.2022.9952988</pub-id>
          <article-title>Driver drowsiness detection based on 3D convolution neural network with optimized window size</article-title>
          <source>2022 13th International Conference on Information and Communication Technology Convergence (ICTC), Jeju Island, Korea</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>240</page-range>
          <issue>2</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bakheet</surname>
              <given-names>Samy</given-names>
            </name>
            <name>
              <surname>Al-Hamadi</surname>
              <given-names>Ayoub</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/brainsci11020240</pub-id>
          <article-title>A framework for instantaneous driver drowsiness detection based on improved HOG features and naïve Bayesian classification</article-title>
          <source>Brain Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>168</volume>
          <page-range>114334</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Moujahid</surname>
              <given-names>Abdelatif</given-names>
            </name>
            <name>
              <surname>Dornaika</surname>
              <given-names>Fadi</given-names>
            </name>
            <name>
              <surname>Arganda-Carreras</surname>
              <given-names>Ignacio</given-names>
            </name>
            <name>
              <surname>Reta</surname>
              <given-names>Jorge</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2020.114334</pub-id>
          <article-title>Efficient and compact face descriptor for driver drowsiness detection</article-title>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="conf-paper">
          <page-range>237-242</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jabbar</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Shinoy</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kharbeche</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Al-Khalifa</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Krichen</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Barkaoui</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICIoT48696.2020.9089484</pub-id>
          <article-title>Driver drowsiness detection model using convolutional neural networks techniques for Android application</article-title>
          <source>2020 IEEE International Conference on Informatics, IoT, and Enabling Technologies (ICIoT), Doha, Qatar</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>22-30</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Adhinata</surname>
              <given-names>Faisal Dharma</given-names>
            </name>
            <name>
              <surname>Rakhmadani</surname>
              <given-names>Diovianto Putra</given-names>
            </name>
            <name>
              <surname>Wijayanto</surname>
              <given-names>Danur</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.20473/jisebi.7.1.22-30</pub-id>
          <article-title>Fatigue detection on face image using FaceNet algorithm and K-nearest neighbor classifier</article-title>
          <source>J. Inf. Syst. Eng. Bus. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>82</volume>
          <page-range>104581</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Cui</surname>
              <given-names>S. W.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>X. H.</given-names>
            </name>
            <name>
              <surname>Cong</surname>
              <given-names>F. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.bspc.2023.104581</pub-id>
          <article-title>Detection of obstructive sleep apnea from single-channel ECG signals using a CNN-transformer architecture</article-title>
          <source>Biomed. Signal Process. Control</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>62</volume>
          <page-range>102575</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>R. L.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>M. H.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>R. B.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L. P.</given-names>
            </name>
            <name>
              <surname>Suganthan</surname>
              <given-names>P. N.</given-names>
            </name>
            <name>
              <surname>Sourina</surname>
              <given-names>O.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.aei.2024.102575</pub-id>
          <article-title>TFormer: A time-frequency transformer with batch normalization for driver fatigue recognition</article-title>
          <source>Adv. Eng. Inform.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>132</volume>
          <page-range>107910</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tang</surname>
              <given-names>X. X.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>Y. F.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>W. X.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>H. P.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>J. Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.engappai.2024.107910</pub-id>
          <article-title>A lightweight model combining convolutional neural network and transformer for driver distraction recognition</article-title>
          <source>Eng. Appl. Artif. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>283</volume>
          <page-range>111137</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>S. Q.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z. Y.</given-names>
            </name>
            <name>
              <surname>An</surname>
              <given-names>Y. L.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X. R.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y. D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.knosys.2023.111137</pub-id>
          <article-title>DA-CapsNet: A multi-branch capsule network based on adversarial domain adaption for cross-subject EEG emotion recognition</article-title>
          <source>Knowl.-Based Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>71</volume>
          <page-range>1-10</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Park</surname>
              <given-names>Seongjun</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>Byung Kwon</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>Seung Yong</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TIM.2022.3217867</pub-id>
          <article-title>Self-supervised RGB-NIR fusion video vision transformer framework for rPPG estimation</article-title>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="conf-paper">
          <volume>13695</volume>
          <page-range>69-86</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>J. K.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>X. T.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>H. D.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Z. X.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>Y. G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-031-19833-5_5</pub-id>
          <article-title>Efficient video transformers with spatial-temporal token selection</article-title>
          <source>Lecture Notes in Computer Science, Computer Vision—ECCV 2022, Tel Aviv, Israel</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>101647-101661</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Zhen</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Y. F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2025.3575440</pub-id>
          <article-title>STAA: Spatio-temporal attention attribution for real-time interpreting transformer-based AI video models</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Deng</surname>
              <given-names>F. J.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Chao</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>Hui</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y. S.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>L. P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.21203/rs.3.rs-4546491/v1</pub-id>
          <article-title>DA-ViViT: Fatigue detection framework using joint and facial keypoint features with dynamic distributed attention video vision transformer</article-title>
          <source>Research Square, Preprint (version 1)</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="conf-paper">
          <page-range>6816-6826</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Arnab</surname>
              <given-names>Anurag</given-names>
            </name>
            <name>
              <surname>Dehghani</surname>
              <given-names>Mostafa</given-names>
            </name>
            <name>
              <surname>Heigold</surname>
              <given-names>Georg</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>Chen</given-names>
            </name>
            <name>
              <surname>Lučić</surname>
              <given-names>Mario</given-names>
            </name>
            <name>
              <surname>Schmid</surname>
              <given-names>Cordelia</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICCV48922.2021.00676</pub-id>
          <article-title>ViViT: A video vision transformer</article-title>
          <source>2021 IEEE/CVF International Conference on Computer Vision (ICCV), Montreal, QC, Canada</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>32</volume>
          <page-range>9731-9743</page-range>
          <issue>13</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wijnands</surname>
              <given-names>Jasper S.</given-names>
            </name>
            <name>
              <surname>Thompson</surname>
              <given-names>Jason</given-names>
            </name>
            <name>
              <surname>Nice</surname>
              <given-names>Kerry A.</given-names>
            </name>
            <name>
              <surname>Aschwanden</surname>
              <given-names>Gideon D. P. A.</given-names>
            </name>
            <name>
              <surname>Stevenson</surname>
              <given-names>Mark</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00521-019-04506-0</pub-id>
          <article-title>Real-time monitoring of driver drowsiness on mobile platforms using 3D neural networks</article-title>
          <source>Neural Comput. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>79</volume>
          <page-range>26683-26701</page-range>
          <issue>35</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>Lei</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z. C.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>G. X.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>H. B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-020-09259-w</pub-id>
          <article-title>Driver drowsiness recognition via transferred deep 3D convolutional network and state probability vector</article-title>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="conf-paper">
          <page-range>117-133</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Weng</surname>
              <given-names>C. H.</given-names>
            </name>
            <name>
              <surname>Lai</surname>
              <given-names>Y. H.</given-names>
            </name>
            <name>
              <surname>Lai</surname>
              <given-names>S. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-319-54526-4_9</pub-id>
          <article-title>Driver drowsiness detection via a hierarchical temporal deep belief network</article-title>
          <source>Asian Conference on Computer Vision (ACCV), Taipei</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-22</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Dosovitskiy</surname>
              <given-names>Alexey</given-names>
            </name>
            <name>
              <surname>Beyer</surname>
              <given-names>Lucas</given-names>
            </name>
            <name>
              <surname>Kolesnikov</surname>
              <given-names>Alexander</given-names>
            </name>
            <name>
              <surname>Weissenborn</surname>
              <given-names>Dirk</given-names>
            </name>
            <name>
              <surname>Zhai</surname>
              <given-names>X. H.</given-names>
            </name>
            <name>
              <surname>Unterthiner</surname>
              <given-names>Thomas</given-names>
            </name>
            <name>
              <surname>Dehghani</surname>
              <given-names>Mostafa</given-names>
            </name>
            <name>
              <surname>Minderer</surname>
              <given-names>Matthias</given-names>
            </name>
            <name>
              <surname>Heigold</surname>
              <given-names>Georg</given-names>
            </name>
            <name>
              <surname>Gelly</surname>
              <given-names>Sylvain</given-names>
            </name>
            <name>
              <surname>at el.</surname>
            </name>
          </person-group>
          <article-title>An image is worth 16 × 16 words: Transformers for image recognition at scale</article-title>
          <source>International Conference on Learning Representations (ICLR), Vienna, Austria</source>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="conf-paper">
          <volume>30</volume>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ashish Vaswani</surname>
            </name>
            <name>
              <surname>Noam Shazeer</surname>
            </name>
            <name>
              <surname>Niki Parmar</surname>
            </name>
            <name>
              <surname>Jakob Uszkoreit</surname>
            </name>
            <name>
              <surname>Llion Jones</surname>
            </name>
            <name>
              <surname>Aidan  Gomez</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Łukasz Kaiser</surname>
            </name>
            <name>
              <surname>Illia Polosukhin</surname>
            </name>
          </person-group>
          <article-title>Attention is all you need</article-title>
          <source>Advances in Neural Information Processing Systems 30 (NIPS 2017)</source>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="journal">
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hendrycks</surname>
              <given-names>Dan</given-names>
            </name>
            <name>
              <surname>Gimpel</surname>
              <given-names>Kevin</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1606.08415</pub-id>
          <article-title>Gaussian error linear units (GELUs)</article-title>
          <source>arXiv preprint, arXiv:1606.08415</source>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="conf-paper">
          <page-range>289-305</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Heo</surname>
              <given-names>Byeongho</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>Song</given-names>
            </name>
            <name>
              <surname>Han</surname>
              <given-names>Dongyoon</given-names>
            </name>
            <name>
              <surname>Yun</surname>
              <given-names>Sangdoo</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-031-72684-2_17</pub-id>
          <article-title>Rotary position embedding for vision transformer</article-title>
          <source>European Conference on Computer Vision (ECCV), Milan, Italy</source>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="conf-paper">
          <page-range>10347-10357</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Touvron</surname>
              <given-names>Hugo</given-names>
            </name>
            <name>
              <surname>Cord</surname>
              <given-names>Matthieu</given-names>
            </name>
            <name>
              <surname>Douze</surname>
              <given-names>Matthijs</given-names>
            </name>
            <name>
              <surname>Massa</surname>
              <given-names>Francisco</given-names>
            </name>
            <name>
              <surname>Sablayrolles</surname>
              <given-names>Alexandre</given-names>
            </name>
            <name>
              <surname>Jégou</surname>
              <given-names>Hervé</given-names>
            </name>
          </person-group>
          <article-title>Training data-efficient image transformers &amp; distillation through attention</article-title>
          <source>38th International Conference on Machine Learning (ICML)</source>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="conf-paper">
          <volume>32</volume>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Touvron</surname>
              <given-names>Hugo</given-names>
            </name>
            <name>
              <surname>Vedaldi</surname>
              <given-names>Andrea</given-names>
            </name>
            <name>
              <surname>Douze</surname>
              <given-names>Matthijs</given-names>
            </name>
            <name>
              <surname>Jégou</surname>
              <given-names>Hervé</given-names>
            </name>
          </person-group>
          <article-title>Fixing the train-test resolution discrepancy</article-title>
          <source>Advances in Neural Information Processing Systems 32 (NeurIPS 2019)</source>
        </element-citation>
      </ref>
      <ref id="ref_35">
        <label>35.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>7849</page-range>
          <issue>13</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Florez</surname>
              <given-names>Ruben</given-names>
            </name>
            <name>
              <surname>Palomino-Quispe</surname>
              <given-names>Facundo</given-names>
            </name>
            <name>
              <surname>Coaquira-Castillo</surname>
              <given-names>Roger Jesus</given-names>
            </name>
            <name>
              <surname>Herrera-Levano</surname>
              <given-names>Julio Cesar</given-names>
            </name>
            <name>
              <surname>Paixão</surname>
              <given-names>Thuanne</given-names>
            </name>
            <name>
              <surname>Alvarez</surname>
              <given-names>Ana Beatriz</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app13137849</pub-id>
          <article-title>A CNN-based approach for driver drowsiness detection by real-time eye state identification</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_36">
        <label>36.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <page-range>59-70</page-range>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gomaa</surname>
              <given-names>Mohamed</given-names>
            </name>
            <name>
              <surname>Mahmoud</surname>
              <given-names>Rania</given-names>
            </name>
            <name>
              <surname>Sarhan</surname>
              <given-names>Ahmed</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.21608/erjeng.2022.141514.1067</pub-id>
          <article-title>A CNN-LSTM-based deep learning approach for driver drowsiness prediction</article-title>
          <source>J. Eng. Res.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>