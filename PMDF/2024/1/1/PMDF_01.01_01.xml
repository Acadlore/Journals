<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">PMDF</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Precision Mechanics &amp; Digital Fabrication</journal-title>
        <abbrev-journal-title abbrev-type="issn">Precis. Mech. Digit. Fabr.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">PMDF</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">3006-9742</issn>
      <issn publication-format="print">3006-9734</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-G9qkh1-5FqcB_GFPYBz79eNsQV6GRy_e</article-id>
      <article-id pub-id-type="doi">10.56578/pmdf010101</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Automated Alignment of U-Notch in Iron Caps Using Machine Vision: A System for Suspended Insulators</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-5646-3960</contrib-id>
          <name>
            <surname>Kang</surname>
            <given-names>Kui</given-names>
          </name>
          <email>1421330890@qq.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-7661-7453</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Huiyu</given-names>
          </name>
          <email>huaiyuzhang812@foxmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5147-7931</contrib-id>
          <name>
            <surname>Wang</surname>
            <given-names>Yu</given-names>
          </name>
          <email>wangyu@mail.xhu.edu.cn</email>
        </contrib>
        <aff id="aff_1">School of Machine and Engineering, Xihua University, 610039 Chengdu, China</aff>
        <aff id="aff_2">School of Intelligent Manufacturing, Yibin Vocational and Technical College, 644003 Yibin, China</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>30</day>
        <month>03</month>
        <year>2024</year>
      </pub-date>
      <volume>1</volume>
      <issue>1</issue>
      <fpage>1</fpage>
      <lpage>10</lpage>
      <page-range>1-10</page-range>
      <history>
        <date date-type="received">
          <day>28</day>
          <month>11</month>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <day>04</day>
          <month>03</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>In the automated production line for suspended insulators, precise alignment of the U-shaped notch in iron caps is crucial for effective gluing. This study introduces a system based on machine vision that automates the alignment process. The system initially preprocesses the images of iron caps to segment the U-shaped contour. It utilizes the method of quadratic maximum contour connectivity domain to accurately identify the target U-shaped region. The alignment process involves calculating the coordinates of the largest external rectangle's longest edge and the external circle's center point. These coordinates are instrumental in determining the necessary rotation angle for proper notch alignment. The fixture then adjusts the iron cap based on this calculated angle, ensuring precise alignment. Experimental validations of this system have demonstrated a notch alignment error within 0.5 degrees with 96.51% accuracy and an error within 1 degree with 100% accuracy. The algorithm's execution time is a swift 0.034 seconds. Both the error margins and operational speed satisfy the stringent requirements of the automatic production line.</p></abstract>
      <kwd-group>
        <kwd>Machine vision</kwd>
        <kwd>Suspended insulator</kwd>
        <kwd>Iron cap</kwd>
        <kwd>Image processing</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="3"/>
        <fig-count count="10"/>
        <table-count count="1"/>
        <ref-count count="24"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Suspension insulators are an important part of high-voltage transmission lines. It ensures connections and insulation between high-potential conductors and low-potential towers. Suspended insulators usually consist of glass discs, steel hinges, and iron caps. The working scenario and composition of a suspended insulator are shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Working scenario and composition of suspended insulators</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/4/img_m_Wyz7rh04K9oEpt.png"/>
        </fig>
      
      <p>At present, the traditional production of suspended insulators is carried out manually, including manual alignment assembly, gluing, and stringing. Among them, in the stringing process, the steel hinge needs to be connected to the iron notch, but the direction of the notch is not fixed, and the notch needs to be aligned in order to carry out the stringing. To achieve automation in the production of suspended insulators, the alignment of the iron notch must be absent. In traditional automated production measurement, the typical method is to use calipers, goniometers, or angle gauges to measure a certain parameter of the workpiece several times, then the average value [<xref ref-type="bibr" rid="ref_1">1</xref>]. With the rapid development of science and technology and the popularity of machine vision technology, there are more and more algorithms for image detection and recognition, such as the non-contact rotation angle measurement method based on a monocular camera [<xref ref-type="bibr" rid="ref_2">2</xref>], image direction determination based on feature extraction [<xref ref-type="bibr" rid="ref_3">3</xref>], dynamic angle measurement method based on machine vision [<xref ref-type="bibr" rid="ref_4">4</xref>], 3D vertebra rotation angle automatic measurement direction recognition based on deep learning [<xref ref-type="bibr" rid="ref_5">5</xref>]. The traditional machine vision algorithm based on image processing adopted in this paper has the advantages of non-contact [<xref ref-type="bibr" rid="ref_6">6</xref>], high resolution, and robustness to free motion and rotational fluctuations, which makes it easy to get the angle between the U-shaped notch of the iron cold and the actual alignment [<xref ref-type="bibr" rid="ref_7">7</xref>], and then achieve the notch of the iron cold to realize automatic alignment through the rotation of the iron cold driven by the motor.</p>
    </sec>
    <sec sec-type="">
      <title>2. Image pre-processing</title>
      <p>The visual image of the calibrated area is captured by an industrial camera [<xref ref-type="bibr" rid="ref_8">8</xref>], but in the process of industrial image acquisition, due to the field environment, lighting changes, background interference, camera shake, and other factors will affect the image quality. To reduce the interference information in the image and enhance the target feature information, the original image needs to be preprocessed. The purpose of image preprocessing is to eliminate the noise that exists in the acquisition process of the iron cap image and to improve the image characteristics of the target area, thus improving the robustness of the algorithm. In this paper, the flow chart of the iron cap preprocessing is shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Flowchart of image pre-processing</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/4/img_QRx9NYMxHUnzn7rf.jpeg"/>
        </fig>
      
      
        <sec>
          
            <title>2.1. Filter processing</title>
          
          <p>As the filtering process's main goal is to denoise or smooth an image or signal to improve image quality, extract features, reduce noise interference, or provide better input for subsequent image processing algorithms. Images or signals are often subject to various types of noise interference during acquisition or transmission, such as Gaussian noise, pretzel noise, and so on. Filtering processes can reduce the noise level in an image by smoothing or suppressing the noise, improving the quality and clarity of the image, and thus increasing the robustness of the algorithm.</p>
          
            <sec>
              
                <title>2.1.1 Mean filtering</title>
              
              <p>Mean filtering is also known as linear filtering, where the gray value of the central pixel point is equal to the average value of some surrounding critical domain with the formula [<xref ref-type="bibr" rid="ref_9">9</xref>]:</p>
              
                <disp-formula>
                  <label>(1)</label>
                  <mml:math id="m6l9n9fitx">
                    <mml:mi>g</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>y</mml:mi>
                    <mml:mi>f</mml:mi>
                    <mml:mi>m</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mfrac>
                      <mml:mn>1</mml:mn>
                      <mml:mi>N</mml:mi>
                    </mml:mfrac>
                    <mml:munder>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mo>∈</mml:mo>
                        <mml:mi>m</mml:mi>
                        <mml:mi>n</mml:mi>
                        <mml:mi>S</mml:mi>
                      </mml:mrow>
                    </mml:munder>
                  </mml:math>
                </disp-formula>
              
              <p>where, $S<inline-formula>
  <mml:math id="m5ub5gbeuj">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>(</mml:mo>
  </mml:math>
</inline-formula>x<inline-formula>
  <mml:math id="ms6rqa3cp0">
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>y<inline-formula>
  <mml:math id="m8i6qqhhuu">
    <mml:mo>)</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>N<inline-formula>
  <mml:math id="mkvp4uqpam">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>S$. Pixel points on the boundary can be left unfiltered and their pixel values retained.</p><p>Mean value filtering is fast, but it has its own defects: It can't eliminate the noise, and it will cause the blurring of the details of the image at the same time of denoising. But the details of the U-shaped profile of the iron cap and the outer horseshoe profile are especially important, so this system does not choose to use mean value filtering.</p>
            </sec>
          
          
            <sec>
              
                <title>2.1.2 Median filtering</title>
              
              <p>Median filtering is a common image processing technique used to remove noise from an image. It is based on the principle that the neighboring pixels around each pixel are sorted according to the size of the gray value, and then the median value is taken as the new value of the pixel, thus eliminating the effect of noise. Median filtering uses a non-linear method for processing, which ensures that the details of the image are not lost while suppressing the noise, and it is one of the more widely used image processing methods [<xref ref-type="bibr" rid="ref_10">10</xref>]. The number of pixels in the template of median filtering is generally odd, and the target pixel value is equal to the middle value of all pixels in its proximity domain [<xref ref-type="bibr" rid="ref_11">11</xref>] with the formula:</p>
              
                <disp-formula>
                  <label>(2)</label>
                  <mml:math id="m7a14xv5h0">
                    <mml:mi>g</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>y</mml:mi>
                    <mml:mi>f</mml:mi>
                    <mml:mi>m</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mi>m</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mi>S</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo fence="false">{</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>∈</mml:mo>
                    <mml:mo fence="false">}</mml:mo>
                    <mml:mrow>
                      <mml:mi>m</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>d</mml:mi>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>To select the filtering method with the best effect, this paper adds 3% pretzel noise to the image after gray scaling, and then use the three filtering methods mentioned above to process them separately. The filtering is selected on a 5×5 cross-shaped template. The filtering results are shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>, in which picture a is the original image; picture b is the image after adding 3% salt and pepper noise; picture c is the effect of mean filter, which can be seen that the mean filter fails to eliminate the noise but makes the image blurred and even loses the details; picture d is the effect of median filter, which can be seen that the median filter not only completes the noise reduction but also retains the details of the image; Comparing the two different filtering processes, this paper chooses the median filter for filtering.</p>
              
                <fig id="fig_3">
                  <label>Figure 3</label>
                  <caption>
                    <title>Filter processing</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/4/img_Y1dl5ntFI9OmEN0N.png"/>
                </fig>
              
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>2.2. Otus threshold segmentation</title>
          
          <p>After pre-processing, the image quality is improved and the noise is removed. Since this paper only focuses on the topmost U-shaped contour and the contour groove of the iron bubble, the extracted image is binarized to distinguish the pixels on the edges from the other pixels. The principle of binarization is to obtain a threshold value by some algorithm, and then the pixel points within the image are classified into foreground and background categories based on the threshold value.</p><p>When fixed thresholding is used under complex illumination conditions, it fails to extract feature points or extracts only a small number of feature points. In this paper, the Otsu thresholding method is used [<xref ref-type="bibr" rid="ref_12">12</xref>], which is an adaptive state-value segmentation algorithm based on the histogram of an image that is able to classify the image into two categories, one for the target and the other for the background. The Otsu thresholding method is suitable for the fields of binarization and image segmentation, where the image is enhanced by applying Gaussian filtering for noise reduction, truncation of adaptive luminance adjustments, and unsharpening of the masking operation. Then, the enhanced image is segmented into subregions of specified size, and the truncated Otsu method is used to calculate the adaptive threshold for each subregion [<xref ref-type="bibr" rid="ref_13">13</xref>]. It has higher classification accuracy and better adaptability than other read-value methods. Advantages of the Otsu Thresholding Method.</p><p>(1) The Otsu queuing method is an adaptive threshold selection method without human intervention that can automatically adapt to the complexity of the image and the change in grayscale distribution.</p><p>(2) The Otsu queuing method has better segmentation results, higher classification accuracy, and better adaptability than the general gray level-based threshold selection method. The segmentation results of this algorithm are obvious, and the important structures are clear.</p><p>(3) The Otsu queuing method is simple to compute, the algorithm complexity is low, and it can be implemented quickly.</p><p>The threshold segmentation effect is shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Otus threshold segmentation</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/4/img_U11T8P7yO4Rij6sX.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>2.3. Morphological processing</title>
          
          <p>Morphological processing is a commonly used technique in image processing for performing morphological operations on images such as erosion, expansion, open operation, closed operation, etc., which can be used for tasks such as denoising, segmentation, edge detection, etc. of images [<xref ref-type="bibr" rid="ref_14">14</xref>]. In this paper, since the found image is not good enough to distinguish the U-shaped notch completely, erosion operations in morphological processing are used to reduce or eliminate the boundary of the object in the image.</p><p>The principle of the erosion operation is to slide a structural element (also known as a kernel or convolution kernel) in the image, compare the center of the kernel with the position of each pixel in the image, and if all the elements of the kernel match with pixels at the corresponding positions in the image (i.e., they are all foreground pixels), the center pixel is set to be a foreground pixel, otherwise it is set to be a background pixel [<xref ref-type="bibr" rid="ref_15">15</xref>]. The erosion operation is actually achieved by gradually eroding away the foreground pixels in the image, and the size and shape of the structure elements affect the erosion effect. The larger the structuring element is, the more obvious the effect of erosion is, and the object will become smaller; while the smaller the structuring element is, the slighter the effect of erosion is, so it can be used for removing small noise points, separating the connections between objects, etc. In this paper, 5×5 structural elements are used to separate the U-shaped contour from the peripheral contour to get the binarized image that is not connected to the periphery at all, which improves the accuracy of contour finding, and the processing results are shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Morphological processing</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/4/img_KOxbR3ZBSauJcbdJ.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>2.4. Secondary maximum connectivity domain</title>
          
          <p>In image processing, a connected field is the set of pixels in an image that have the same pixel value and are connected to each other. The connected field marking operation for a binary image is to mark the connected field from a dot matrix image consisting of white pixels (usually represented by “1” for binary images and “255” for grayscale images) and black pixels (usually represented by “0” for grayscale images) that are adjacent to each other with pixel values. In a dot-matrix image composed of two pixels (usually represented by “1” for binary images and “255” for grayscale images) and black pixels (usually represented by “0”), a set of pixels adjacent to each other with pixel values of “1” or “255” is extracted, and a numerical marking is filled in for the different connectivity domains in the image in a varying degree, and the number of connectivity domains is counted [<xref ref-type="bibr" rid="ref_16">16</xref>]. The number of connected domains is counted. The image is usually traversed at the pixel level, starting from a single pixel and recursively or iteratively finding all pixels with the same pixel value that are connected to that pixel and grouping them into the same connected domain.</p><p>After morphological operations, the required part of the image has been segmented to retain the target feature area, specifically the U-shaped contour in the center. Through extensive experimentation, it was discovered that the contour area of the target feature is smaller than that of the peripheral contour yet is the largest when excluding the peripheral contour. Leveraging this key insight, this study introduces a method to find the maximal contour connectivity domain for the second time, calculating the area of the connected domain. Initially, the largest peripheral closed contour is identified and used to generate a mask image, which covers the peripheral contours in black, isolating only the target feature area. However, due to the potential presence of incomplete noise filtering, when seeking the maximal closed contour connected domain a second time, a mask is not generated. Instead, only the largest contour image is retained. This approach successfully extracts and segments the U-shaped feature area. The process of U-shaped contour extraction and segmentation is illustrated in <xref ref-type="fig" rid="fig_6">Figure 6</xref>.</p>
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>U-profile</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/4/img_IFubHW3kzEmvsEGx.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>2.5. Canny edge detection</title>
          
          <p>Among many computer vision algorithms, the Canny edge detection algorithm performs significantly better than existing edge detection techniques [<xref ref-type="bibr" rid="ref_17">17</xref>]. When using edge detection for image edge recognition, the choice of edge detection algorithm affects the results in order to obtain a clear image [<xref ref-type="bibr" rid="ref_18">18</xref>]. In order to extract the outer contour edge features of the image, this paper first processed the target image with hole filling, which means that under binarization, there are “white dots” or “black dots” in the image, affecting our calculation of the area inside the contour [<xref ref-type="bibr" rid="ref_19">19</xref>]. In OpenCV, we can use contour filling and the flooding algorithm to achieve hole filling and the flooding algorithm to fill the hole area, so as to achieve the filling of the holes in the image.</p><p>Flood fill is an algorithm for filling connected regions in an image [<xref ref-type="bibr" rid="ref_20">20</xref>]. In OpenCV, the flood fill is implemented using the cv2.floodFill function [<xref ref-type="bibr" rid="ref_21">21</xref>]. The algorithm starts from a specified seed point and continues to expand the fill in all four directions until it encounters a boundary or fails to satisfy the fill conditions, thus filling regions with the same pixel values. The flood algorithm is usually used to fill closed regions, remove noise, select regions, and perform other image processing tasks.</p><p>After filling, the Canny edge detection algorithm is further used to extract the edge features of the image. Canny edge detection is less affected by image noise and the detected edges are more continuous, with clear edge lines and high detection accuracy [<xref ref-type="bibr" rid="ref_19">19</xref>]. And calculate the contour area; the calculated area within a certain range is to meet the desired U-shaped target feature contour and thus improve the accuracy of image segmentation. The effect is shown in <xref ref-type="fig" rid="fig_7">Figure 7</xref>.</p>
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>Canny edge detection</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/4/img_lwNtMZ47A1ehWmDB.png"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Rotation angle measurement</title>
      <p>In industrial environments, workpieces are mostly cluttered, and some contour detection algorithms, which often get the coordinate information of the object, find it difficult to get the accurate angle between the workpiece and the actual alignment. Since the U-shaped contour in <xref ref-type="fig" rid="fig_7">Figure 7</xref> is composed of a rectangle and a semicircle in proportion, using this information, this paper proposes to establish the minimum outer rectangle and outer circle of the target iron counterfeit, and by extracting the specific straight line as well as the coordinate information of the key point, through the measurement based on the machine vision with the specific straight line as the reference target, the angle on the image can be calculated [<xref ref-type="bibr" rid="ref_22">22</xref>], and the angle between the target workpiece and the actual alignment is acquired information.</p>
      
        <sec>
          
            <title>3.1. Minimum outer rectangle</title>
          
          <p>A minimum outer rectangle is a rectangle of minimum size around a given shape and is commonly used to describe and enclose objects or contours in a found image. For polygonal objects, approximating their shape with their outer rectangles is a common method in the fields of GIS and graphics [<xref ref-type="bibr" rid="ref_23">23</xref>]. After obtaining the information of the boundary pixels of each closed image region, it is easy to filter the pixels to get two special points P(x<sub>min</sub>,<sub>min</sub>) and P(x<sub>max</sub>, y<sub>max</sub>), and then the outer rectangle of the diagonal line connecting these two points is the minimum outer rectangle of the image region.</p><p>This article uses the OpenCV library inside the function for cv2.minAreaRect(cnt) [<xref ref-type="bibr" rid="ref_21">21</xref>], cnt is an array or vector of point sets (which holds the coordinates of the points), and there are an indefinite number of elements in this point set. The rectangle returned by the function is usually a rotated rectangle, which can be described by a center point, width, height, and rotation angle, and the coordinates of the four vertices of the rectangle are returned in the form of coordinates. The coordinates of the four vertices returned are at the bottom point for the first point, which is box [0], and clockwise around the second, third, and fourth points. The relationship between the four vertices is shown in <xref ref-type="fig" rid="fig_8">Figure 8</xref>.</p>
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>External rectangle vertex relationship</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/4/img_DA1nbcIq3KWWOqXs.png"/>
            </fig>
          
          <p>As the position of the four vertices is not fixed, combined with the characteristics of the coordinates of the four vertices of the minimum outer rectangle, through the box [ 0 ], box [ 1 ] and box [ 2 ] three points to find the length of the box [ 0 ] and box [ 1 ], box [ 1 ] and box [ 2 ], resulting in the length of the rectangle adjacent to the length of the two sides of the distance 1 and distance 2, through the length of the length of the comparison can be determined the long side and short side, can be fixed to get the long side (AB) coordinates, the formula is:</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="m5scvvgt51">
                <mml:mrow>
                  <mml:mo>{</mml:mo>
                  <mml:mo fence="true"/>
                  <mml:mtable columnalign="left" columnspacing="1em" rowspacing="4pt">
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mrow>
                          <mml:mi>d</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mi>s</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>a</mml:mi>
                          <mml:mi>n</mml:mi>
                          <mml:mi>c</mml:mi>
                          <mml:mi>e</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>d</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mi>s</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>a</mml:mi>
                          <mml:mi>n</mml:mi>
                          <mml:mi>c</mml:mi>
                          <mml:mi>e</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>b</mml:mi>
                          <mml:mi>o</mml:mi>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>b</mml:mi>
                          <mml:mi>o</mml:mi>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mstyle scriptlevel="0">
                          <mml:mspace width="0.167em"/>
                        </mml:mstyle>
                        <mml:mstyle scriptlevel="0">
                          <mml:mspace width="0.167em"/>
                        </mml:mstyle>
                        <mml:mstyle scriptlevel="0">
                          <mml:mspace width="1em"/>
                        </mml:mstyle>
                        <mml:mstyle scriptlevel="0">
                          <mml:mspace width="1em"/>
                        </mml:mstyle>
                        <mml:mn>1</mml:mn>
                        <mml:mn>2</mml:mn>
                        <mml:mn>0</mml:mn>
                        <mml:mn>1</mml:mn>
                        <mml:mn>2</mml:mn>
                        <mml:mn>3</mml:mn>
                        <mml:mo>&gt;</mml:mo>
                        <mml:mo>→</mml:mo>
                        <mml:mo>=</mml:mo>
                        <mml:mo>[</mml:mo>
                        <mml:mo>]</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>=</mml:mo>
                        <mml:mo>[</mml:mo>
                        <mml:mo>]</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>=</mml:mo>
                        <mml:mo>[</mml:mo>
                        <mml:mo>]</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>=</mml:mo>
                        <mml:mo>[</mml:mo>
                        <mml:mo>]</mml:mo>
                        <mml:mi>A</mml:mi>
                        <mml:mi>B</mml:mi>
                        <mml:mi>C</mml:mi>
                        <mml:mi>b</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:mi>D</mml:mi>
                        <mml:mi>b</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>x</mml:mi>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mrow>
                          <mml:mi>d</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mi>s</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>a</mml:mi>
                          <mml:mi>n</mml:mi>
                          <mml:mi>c</mml:mi>
                          <mml:mi>e</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>d</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mi>s</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>a</mml:mi>
                          <mml:mi>n</mml:mi>
                          <mml:mi>c</mml:mi>
                          <mml:mi>e</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>b</mml:mi>
                          <mml:mi>o</mml:mi>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>b</mml:mi>
                          <mml:mi>o</mml:mi>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                        <mml:mstyle scriptlevel="0">
                          <mml:mspace width="0.167em"/>
                        </mml:mstyle>
                        <mml:mstyle scriptlevel="0">
                          <mml:mspace width="0.167em"/>
                        </mml:mstyle>
                        <mml:mstyle scriptlevel="0">
                          <mml:mspace width="1em"/>
                        </mml:mstyle>
                        <mml:mstyle scriptlevel="0">
                          <mml:mspace width="1em"/>
                        </mml:mstyle>
                        <mml:mn>1</mml:mn>
                        <mml:mn>2</mml:mn>
                        <mml:mn>1</mml:mn>
                        <mml:mn>2</mml:mn>
                        <mml:mn>3</mml:mn>
                        <mml:mn>0</mml:mn>
                        <mml:mo>&lt;</mml:mo>
                        <mml:mo>→</mml:mo>
                        <mml:mo>=</mml:mo>
                        <mml:mo>[</mml:mo>
                        <mml:mo>]</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>=</mml:mo>
                        <mml:mo>[</mml:mo>
                        <mml:mo>]</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>=</mml:mo>
                        <mml:mo>[</mml:mo>
                        <mml:mo>]</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>=</mml:mo>
                        <mml:mo>[</mml:mo>
                        <mml:mo>]</mml:mo>
                        <mml:mi>A</mml:mi>
                        <mml:mi>B</mml:mi>
                        <mml:mi>C</mml:mi>
                        <mml:mi>b</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:mi>D</mml:mi>
                        <mml:mi>b</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>x</mml:mi>
                      </mml:mtd>
                    </mml:mtr>
                  </mml:mtable>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p><inline-formula>
  <mml:math id="m6u32jjryu">
    <mml:mi>A</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mfufcag40d">
    <mml:mi>B</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> in the formula are the coordinates of the two points on the long side, and then use the coordinates of the two points to find the slope $k<inline-formula>
  <mml:math id="mdvlnpirwo">
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>\theta<inline-formula>
  <mml:math id="m5pbn67zvj">
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>4</mml:mn>
    <mml:mn>8</mml:mn>
  </mml:math>
</inline-formula>b$ is a negative number.</p>
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="mgd88xwaok">
                <mml:mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd/>
                    <mml:mtd>
                      <mml:mi>k</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:msub>
                            <mml:mi>y</mml:mi>
                            <mml:mn>1</mml:mn>
                          </mml:msub>
                          <mml:msub>
                            <mml:mi>y</mml:mi>
                            <mml:mn>2</mml:mn>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:msub>
                            <mml:mi>x</mml:mi>
                            <mml:mn>1</mml:mn>
                          </mml:msub>
                          <mml:msub>
                            <mml:mi>x</mml:mi>
                            <mml:mn>2</mml:mn>
                          </mml:msub>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd/>
                    <mml:mtd>
                      <mml:mi>θ</mml:mi>
                      <mml:mi>a</mml:mi>
                      <mml:mi>r</mml:mi>
                      <mml:mi>c</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>a</mml:mi>
                      <mml:mi>n</mml:mi>
                      <mml:mi>k</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Minimum external circle</title>
          
          <p>A least outer circle is the smallest circle that can enclose a given shape, described by a center coordinate and a radius. In image processing, the minimum enclosing circle is often used to describe and analyze the shape and position of an object [<xref ref-type="bibr" rid="ref_24">24</xref>]. In OpenCV, the function cv2.minEnclosingCircle() can be used to compute the minimum enclosing circle [<xref ref-type="bibr" rid="ref_21">21</xref>], which is usually returned as a tuple (center, radius), where center is the coordinates of the center of the circle and radius is the radius.</p><p>Since the notch direction of the iron cap has a rotation angle of 360 degrees in the plane, to get the direction of the notch, we also need to judge the external rectangle to determine which side of the notch is in the two short edges. Based on the geometric relationship of the semicircular arc in the U-shaped profile of the iron cap, we can use the OpenCV library cv2.minEnclosingCircle() minimum external circle function to get the center of the circle coordinates ($x<inline-formula>
  <mml:math id="m9i3b2ipkz">
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>y$). Then find the distance from the center of the circle coordinates to the two short sides (AD and BC) with the following formula:</p>
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="mq8y35z9m3">
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mo>|</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>|</mml:mo>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>y</mml:mi>
                        <mml:mn>2</mml:mn>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>y</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>x</mml:mi>
                        <mml:mn>2</mml:mn>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>x</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:msub>
                    </mml:mrow>
                    <mml:msub>
                      <mml:mi>x</mml:mi>
                      <mml:mn>3</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>y</mml:mi>
                      <mml:mn>3</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>y</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>y</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>x</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>y</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msub>
                  </mml:mrow>
                  <mml:msqrt>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>−</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mi>x</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>x</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mn>2</mml:mn>
                    </mml:msup>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>−</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mi>y</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>y</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mn>2</mml:mn>
                    </mml:msup>
                    <mml:mo>+</mml:mo>
                  </mml:msqrt>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>You can find out the distances 3 and 4 from the center point to the two short edges, compare the size of the distance, and the short side of the rectangle with the short distance is the position where the gap is located. When combined with the characteristics of the image, the distance from the center point to the two short edges, and the size of the angle obtained in the formula (4) to judge the size of the angle, the following three situations occur:</p><p>(1) It’s found that when the angle is negative, the two forms of in subgraphs (b) and (d) of <xref ref-type="fig" rid="fig_9">Figure 9</xref> at this time in the distance 3 and distance 4 according to the size of the judgement of the direction of the opening, when distance 3 <inline-formula>
  <mml:math id="mzi4zp1mvs">
    <mml:mo>&amp;gt;</mml:mo>
  </mml:math>
</inline-formula> distance 4 can be derived from the angle of c is: 90+<inline-formula>
  <mml:math id="ma03oz1zor">
    <mml:mi>θ</mml:mi>
  </mml:math>
</inline-formula>=-60.4, when distance 3 <inline-formula>
  <mml:math id="m62l6mo7w7">
    <mml:mo>&amp;lt;</mml:mo>
  </mml:math>
</inline-formula> When distance 3 <inline-formula>
  <mml:math id="mlfq7nbp7c">
    <mml:mo>&amp;gt;</mml:mo>
  </mml:math>
</inline-formula> distance 4 it can be concluded that the angle of e is: 90-<inline-formula>
  <mml:math id="mkjtqg87wb">
    <mml:mi>θ</mml:mi>
  </mml:math>
</inline-formula>=115.0;</p><p>(2) When the angle is positive, it is the two forms of in subgraphs (c) and (f) of <xref ref-type="fig" rid="fig_9">Figure 9</xref>. At this time, in the judgment of the opening direction according to the size of distance 3 and distance 4, when distance 3 <inline-formula>
  <mml:math id="mg67qqgoac">
    <mml:mo>&amp;lt;</mml:mo>
  </mml:math>
</inline-formula> distance 4, it can be concluded that the angle of d is: -90-<inline-formula>
  <mml:math id="moip4xdn7t">
    <mml:mi>θ</mml:mi>
  </mml:math>
</inline-formula>=133.3, when distance 3 <inline-formula>
  <mml:math id="mte7p8bcek">
    <mml:mo>&amp;lt;</mml:mo>
  </mml:math>
</inline-formula> distance 4. When distance 3 <inline-formula>
  <mml:math id="mmyb109uy5">
    <mml:mo>&amp;lt;</mml:mo>
  </mml:math>
</inline-formula> distance 4, the angle of (f) is 90-<inline-formula>
  <mml:math id="mt32u3bpo2">
    <mml:mi>θ</mml:mi>
  </mml:math>
</inline-formula>=52.6;</p><p>(3) As arctan (k) to find the value can only be in (0, 90) of the open area, cannot get 0 and 90 of these two special cases, this time, according to the short side of the gap in the longitudinal coordinates of the gap is equal and the distance 3 and distance 4 size, when distance 3 <inline-formula>
  <mml:math id="mf2smf9wxj">
    <mml:mo>&amp;gt;</mml:mo>
  </mml:math>
</inline-formula> distance 4, the angle is 0, as in subgraph (a) of <xref ref-type="fig" rid="fig_9">Figure 9</xref>; when distance 3 <inline-formula>
  <mml:math id="mmlhtwvq9c">
    <mml:mo>&amp;lt;</mml:mo>
  </mml:math>
</inline-formula> distance 4, the angle is 180, as in subgraph (b) of <xref ref-type="fig" rid="fig_9">Figure 9</xref>.</p><p>The angle sought is the angle of rotation of the iron cap, with positive values being clockwise and negative values being anti-clockwise. The result of the rotation angle is shown in <xref ref-type="fig" rid="fig_9">Figure 9</xref>, with subgraph (a) in <xref ref-type="fig" rid="fig_9">Figure 9</xref> as the counter-positive position, counter-clockwise rotation for negative values (e.g., in subgraphs (c) and (d) of <xref ref-type="fig" rid="fig_9">Figure 9</xref>), and clockwise rotation for positive values (e.g., in subgraphs (e) and (f) of <xref ref-type="fig" rid="fig_9">Figure 9</xref>).</p>
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>
                <title>Rotation angle</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/4/img_FAum4qc_4RvLFdt1.png"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Analysis of experimental data and results</title>
      <p>This paper combines the laboratory's existing equipment, such as the HuaRay Technology A3600MG100 camera and the M2016-12MP-2 lens, and, based on the PyCharm platform, the OpenCV3.10 vision function library, for the development and experimental verification and analysis. The experimental environment is shown in <xref ref-type="fig" rid="fig_10">Figure 10</xref>.</p>
      
        <fig id="fig_10">
          <label>Figure 10</label>
          <caption>
            <title>Experimental environment</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/4/img_DqwTPyQacB-h9_F0.png"/>
        </fig>
      
      <p>A total of 20 iron cap pictures with different angles were collected, totaling 732, manually measured to mark the angle value, marked the picture name as the measured value, and verified the value calculated by the algorithm with the measured value.</p><p>Using the machine vision-based iron cap U-shaped notch alignment angle measurement system algorithm designed in this paper, repeated test experiments were conducted on different iron cap angles, respectively. The algorithm's single running time, the standardized average deviation of the measurement angle, the maximum error, and the detection accuracy in the range of error allowed to test the three indicators are shown in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Results of related experiments</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Time to Run Once</p></td><td colspan="2" rowspan="1"><p>0.034s</p></td></tr><tr><td colspan="1" rowspan="1"><p>Standardized Mean Value</p></td><td colspan="2" rowspan="1"><p>0.3°</p></td></tr><tr><td colspan="1" rowspan="1"><p>Maximum Error Value</p></td><td colspan="2" rowspan="1"><p>0.8°</p></td></tr><tr><td colspan="1" rowspan="1"><p>Tolerance Range</p></td><td colspan="1" rowspan="1"><p>±0.5° range</p></td><td colspan="1" rowspan="1"><p>±1° range</p></td></tr><tr><td colspan="1" rowspan="1"><p>Detection Accuracy</p></td><td colspan="1" rowspan="1"><p>96.51%</p></td><td colspan="1" rowspan="1"><p>100%</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>A total of 732 experiments were carried out. The results of the experiments in the iron cap U-shaped notch angle error 0.5° range of accuracy is 96.51%, error 1° range of accuracy is 100%, the algorithm runs a single time 0.034 seconds, the two error ranges and running speed meet the standards required by the automatic production line, and because the manual measurement of the angle may not be as accurate as the results of the detection, this algorithm is higher, but they all meet the system requirements for the accuracy of the U-shaped angle of the iron cap. And because the manually measured angle may not be as accurate as the detected results, the detection accuracy of this algorithm is higher, but all meet the system requirements for the accuracy of the U-shaped angle of the iron cap. It can be seen that within the error range, the machine vision-based iron cap U-shaped notch angle measurement system proposed in this paper can achieve accurate measurement of the iron cap U-shaped angle within the error tolerance.</p>
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>In this system, first of all, the image is filtered to remove the noise to get a better recognition effect after filtering, and then binarized after corrosion to separate the target area from the interference by finding the second largest connected domain area to get the U-type target area after the processing of the above process, and then through the traditional image of the outline of the smallest external rectangular box, the target area positioning After the above process, the target area is positioned by the minimum external rectangular box of the contour in the traditional image, and the positive and negative values of the size of the angle and the minimum external circle center are compared with the distance of the two short edges to determine the direction of the gap. The rotation angle of the gap is then achieved, and the angle derived from the experiment meets the standard required by the automatic production line. However, there may still be deficiencies in this system, such as the fact that the image acquisition interference is large and the algorithm used cannot be very good to get the angle information. In the future, we can use machine learning or deep learning algorithms to use the manually labeled pictures as a data set, divided into a 70% training set, a 15% validation set, and a 15% test set to input the neural network for training, to get the trained model, and then test to get the accuracy rate and combine the accuracy rate to continuously improve the network model to get a better accuracy rate, thus improving the accuracy of neural network detection.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>J. Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X. P.</given-names>
            </name>
            <name>
              <surname>Tian</surname>
              <given-names>G. Q.</given-names>
            </name>
            <name>
              <surname>Fan</surname>
              <given-names>J. F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/mi13030447</pub-id>
          <article-title>Machine vision-based method for measuring and controlling the angle of conductive slip ring brushes</article-title>
          <source>Micromachines</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="conf-paper">
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gan</surname>
              <given-names>X. C.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>A. B.</given-names>
            </name>
            <name>
              <surname>Ye</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>L. Q.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1117/12.2180752</pub-id>
          <article-title>Non-contact measurement of rotation angle with solo camera</article-title>
          <source>Ninth International Symposium on Precision Engineering Measurement and Instrumentation, Changsha, China</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>40</volume>
          <page-range>376-388</page-range>
          <issue>2</issue>
          <year>2006</year>
          <person-group person-group-type="author">
            <name>
              <surname>Blumenstein</surname>
              <given-names>Michael</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Xin Yu</given-names>
            </name>
            <name>
              <surname>Verma</surname>
              <given-names>Brijesh</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.patcog.2006.05.017</pub-id>
          <article-title>An investigation of the modified direction feature for cursive character recognition</article-title>
          <source>Pattern Recognit.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="conf-paper">
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Han</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1117/12.2540332</pub-id>
          <article-title>Research on dynamic-angle measurement method for machine vision</article-title>
          <source>Proceedings Volume 11341, AOPC 2019: Space Optics, Telescopes, and Instrumentation, Beijing, China</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>26</volume>
          <issue>2</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Huo</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Shao</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/e26020097</pub-id>
          <article-title>Automatic vertebral rotation angle measurement of 3D vertebrae based on an improved transformer network</article-title>
          <source>Entropy</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Malarvel</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Nayak</surname>
              <given-names>S. R.</given-names>
            </name>
            <name>
              <surname>Pattnaik</surname>
              <given-names>P. K.</given-names>
            </name>
            <name>
              <surname>Panda</surname>
              <given-names>S. N.</given-names>
            </name>
          </person-group>
          <article-title>Machine learning-based approaches</article-title>
          <source>Machine Vision Inspection Systems</source>
          <publisher-name>John Wiley Sons, Inc.</publisher-name>
          <year>2021</year>
          <pub-id pub-id-type="doi">10.1002/9781119786122</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>24</volume>
          <page-range>13375-13386</page-range>
          <issue>12</issue>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kim</surname>
              <given-names>Hyuno</given-names>
            </name>
            <name>
              <surname>Yamakawa</surname>
              <given-names>Yuji</given-names>
            </name>
            <name>
              <surname>Senoo</surname>
              <given-names>Taku</given-names>
            </name>
            <name>
              <surname>Ishikawa</surname>
              <given-names>Masatoshi</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1364/OE.24.013375</pub-id>
          <article-title>Visual encoder: Robust and precise measurement method of rotation angle via high-speed RGB vision</article-title>
          <source>Opt. Express</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>49</volume>
          <page-range>1001-1006</page-range>
          <year>2010</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Wei Min</given-names>
            </name>
            <name>
              <surname>Jin</surname>
              <given-names>Jing</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Xiao Feng</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Bin</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1364/AO.49.001001</pub-id>
          <article-title>Method of rotation angle measurement in machine vision based on calibration pattern with spot array</article-title>
          <source>Appl. Opt.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>167</volume>
          <page-range>677-685</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yugander</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Tejaswini</surname>
              <given-names>C. H.</given-names>
            </name>
            <name>
              <surname>Meenakshi</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Samapath Kumar</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Suresh Varma</surname>
              <given-names>B. V. N.</given-names>
            </name>
            <name>
              <surname>Jagannath</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.procs.2020.03.334</pub-id>
          <article-title>MR image enhancement using adaptive weighted mean filtering and homomorphic filtering</article-title>
          <source>Procedia Comput. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>42</volume>
          <page-range>5545-5558</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Draz</surname>
              <given-names>H. H.</given-names>
            </name>
            <name>
              <surname>Elashker</surname>
              <given-names>N. E.</given-names>
            </name>
            <name>
              <surname>Mahmoud</surname>
              <given-names>M. M. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00034-023-02370-x</pub-id>
          <article-title>Optimized algorithms and hardware implementation of median filter for image processing</article-title>
          <source>Circuits Syst. Signal Process.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Kurita</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Maruyama</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>An image filter system based on dynamic partial reconfiguration on FPGA</article-title>
          <source>Advances in Parallel Computing</source>
          <year>2014</year>
          <page-range>540-547</page-range>
          <pub-id pub-id-type="doi">10.3233/978-1-61499-381-0-540</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <issue>2</issue>
          <year>2005</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Xu Ming</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Bin Shi</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>Shi Yun</given-names>
            </name>
          </person-group>
          <article-title>Adaptive median filtering for image processing</article-title>
          <source>J. Comput. Aided Des. Graph.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>2161-2176</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Xiao</surname>
              <given-names>Le Yi</given-names>
            </name>
            <name>
              <surname>Fan</surname>
              <given-names>Chao Dong</given-names>
            </name>
            <name>
              <surname>Ouyang</surname>
              <given-names>Hong Lin</given-names>
            </name>
            <name>
              <surname>Abate</surname>
              <given-names>Andrea F.</given-names>
            </name>
            <name>
              <surname>Wan</surname>
              <given-names>Shaohua</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/S12652-021-02976-6</pub-id>
          <article-title>Adaptive trapezoid region intercept histogram based Otsu method for brain MR image segmentation</article-title>
          <source>J. Ambient Intell. Human. Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>24</volume>
          <page-range>2778</page-range>
          <issue>9</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>J. Q.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>A. J.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>S. S.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>W. K.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>G. P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s24092778</pub-id>
          <article-title>Research on tire surface damage detection method based on image processing</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>e30486</page-range>
          <issue>9</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chiu</surname>
              <given-names>J. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.heliyon.2024.e30486</pub-id>
          <article-title>Automated medication verification system (AMVS): System based on edge detection and CNN classification drug on embedded systems</article-title>
          <source>Heliyon</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>143-144</volume>
          <page-range>227-231</page-range>
          <year>2010</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.4028/www.scientific.net/AMR.143-144.227</pub-id>
          <article-title>Ancient books Chinese characters segmentation based on connected domain and Chinese characters feature</article-title>
          <source>Adv. Mater. Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <page-range>957-970</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sangeetha</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Deepa</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11554-016-0582-2</pub-id>
          <article-title>FPGA implementation of cost-effective robust canny edge detection algorithm</article-title>
          <source>J. Real-Time Image Proc.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>133-144</page-range>
          <issue>2</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sundani</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Widiyanto</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Karyanti</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wardani</surname>
              <given-names>D. T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.5614/itbj.ict.res.appl.2019.13.2.4</pub-id>
          <article-title>Identification of image edge using quantum canny edge detection algorithm</article-title>
          <source>J. ICT Res. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>38</volume>
          <page-range>29-33</page-range>
          <issue>6</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Miao</surname>
              <given-names>H. B.</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>G. P.</given-names>
            </name>
            <name>
              <surname>Su</surname>
              <given-names>H. P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.19557/j.cnki.1001-9944.2023.06.007</pub-id>
          <article-title>Workpiece recognition and positioning algorithm based on machine vision</article-title>
          <source>Autom. Instrum.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>36</volume>
          <page-range>123-128</page-range>
          <issue>8</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Qi</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>Jia Kun</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.13652/j.issn.1003-5788.2020.08.022</pub-id>
          <article-title>Online grading of apples based on machine vision</article-title>
          <source>Food Mach.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>63</volume>
          <page-range>328-335</page-range>
          <issue>4</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sigut</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Castro</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Arnay</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Sigut</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/te.2020.2993013</pub-id>
          <article-title>OpenCV basics: A mobile application to support the teaching of computer vision concepts</article-title>
          <source>IEEE Trans. Educ.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="conf-paper">
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cheng</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Cai</surname>
              <given-names>C. G.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Z. H.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1088/1742-6596/1627/1/012016</pub-id>
          <article-title>A high precision rotating line detection method for the rotation angle measurement based on machine vision</article-title>
          <source>4th International Conference on Computer Graphics and Digital Image Processing (CGDIP 2020), Kunming, China</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>200</volume>
          <page-range>689-693</page-range>
          <year>2012</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>M. X.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Q. X.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>C. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.4028/www.scientific.net/AMM.200.689</pub-id>
          <article-title>Research and achievement on cigarette label printing defect detection algorithm</article-title>
          <source>Appl. Mech. Mater.</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>201</volume>
          <page-range>111754</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Xiu Ming</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Yu Jie</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Run Qi</given-names>
            </name>
            <name>
              <surname>Chi</surname>
              <given-names>Jin Ling</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>Xian Jin</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Hu</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.measurement.2022.111754</pub-id>
          <article-title>Evaluation of the minimum circumscribed circle based on the chord and its two corresponding minimum angles</article-title>
          <source>Meas.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>