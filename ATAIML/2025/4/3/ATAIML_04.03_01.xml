<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-BnfWK6SgWDXehYV0l8f11BaPTnBNzLVS</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml040301</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Development of a Machine Learning-Driven Web Platform for Automated Identification of Rice Insect Pests</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7225-8823</contrib-id>
          <name>
            <surname>John</surname>
            <given-names>Samuel N.</given-names>
          </name>
          <email>samuel.john@nda.edu.ng</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0008-3389-7623</contrib-id>
          <name>
            <surname>Musa</surname>
            <given-names>Nasiru A.</given-names>
          </name>
          <email>nasiruameh.musa2021@nda.edu.ng</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-9996-8424</contrib-id>
          <name>
            <surname>Mommoh</surname>
            <given-names>Joshua S.</given-names>
          </name>
          <email>mommoh.joshua@mudiameuniversity.edu.ng</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4030-2321</contrib-id>
          <name>
            <surname>Noma-Osaghe</surname>
            <given-names>Etinosa</given-names>
          </name>
          <email>etinosa.noma-osaghae@oouagoiwoye.edu.ng</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3029-7349</contrib-id>
          <name>
            <surname>Udioko</surname>
            <given-names>Ukeme I.</given-names>
          </name>
          <email>ukeme.udioko2020@nda.edu.ng</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_4">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-6355-4769</contrib-id>
          <name>
            <surname>Obetta</surname>
            <given-names>James L.</given-names>
          </name>
          <email>james.obetta@afit.edu.ng</email>
        </contrib>
        <aff id="aff_1">Department of Electrical Electronic Engineering, Nigerian Defence Academy, 800281 Kaduna, Nigeria</aff>
        <aff id="aff_2">Department of Software Engineering, Mudiame University Irrua, 310112 Edo, Nigeria</aff>
        <aff id="aff_3">Department of Electrical and Electronics Engineering, Olabisi Onabanjo University, Ogun 120107, Nigeria</aff>
        <aff id="aff_4">Department of Information and Communication Technology, Air Force Institution of Technology, 800282 Kaduna, Nigeria</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>22</day>
        <month>05</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>3</issue>
      <fpage>137</fpage>
      <lpage>156</lpage>
      <page-range>137-156</page-range>
      <history>
        <date date-type="received">
          <day>25</day>
          <month>03</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>17</day>
          <month>05</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>An advanced machine learning (ML)-driven web platform was developed and deployed to automate the identification of rice insect pests, addressing limitations associated with traditional pest detection methods and conventional ML algorithms. Historically, pest identification in rice cultivation has relied on expert evaluation of pest species and their associated crop damage, a process that is labor-intensive, time-consuming, and prone to inaccuracies, particularly in the misclassification of pest species. In this study, a subset of the publicly available IP102 benchmark dataset, consisting of 7,736 images across 12 rice pest categories, was curated for model training and evaluation. Two classification models—a Support Vector Machine (SVM) and a deep Convolutional Neural Network (CNN) based on the Inception_ResNetV2 architecture—were implemented and assessed using standard performance metrics. Experimental results demonstrated that the Inception_ResNetV2 model significantly outperformed SVM, achieving an accuracy of 99.97%, a precision of 99.46%, a recall of 99.81%, and an F1-score of 99.53%. Owing to its superior performance, the Inception_ResNetV2 model was integrated into a web-based application designed for real-time pest identification. The deployed system exhibited an average response time of 5.70 seconds, representing a notable improvement in operational efficiency and usability over previous implementations. The results underscore the potential of artificial intelligence in transforming agricultural practices by enabling accurate, scalable, and timely pest diagnostics, thereby enhancing pest management strategies, mitigating crop losses, and supporting global food security initiatives.</p></abstract>
      <kwd-group>
        <kwd>Automated rice pest identification</kwd>
        <kwd>Machine learning</kwd>
        <kwd>Convolutional neural network</kwd>
        <kwd>Support vector machine</kwd>
        <kwd>IP102 dataset</kwd>
        <kwd>Agriculture</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="6"/>
        <fig-count count="11"/>
        <table-count count="8"/>
        <ref-count count="29"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Agricultural productivity receives direct support from insect pest management. Its impacts can be seen on grain production, development in agriculture, as well as on the economy of a farmer. Detecting insect pests within agricultural settings is a process that is rather complex [<xref ref-type="bibr" rid="ref_1">1</xref>]. Rice is one of the crops that is vital regarding food security globally. Rice crops face high vulnerability to diseases that cause major decreases in both yield and quality. The stability of global food supplies faces significant risk. To maintain rice production at peak efficiency, implementing disease prevention and effective pest control strategies is vital [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>]. Accurate and timely diagnosis of plant diseases is at the core of effective disease management, as it enables immediate application of efficient pesticide control measures.</p><p>Crop insect pests cause an annual loss of 40–50% of yields, posing an immediate threat to food security on a global level [<xref ref-type="bibr" rid="ref_4">4</xref>]. Rice crops are particularly vulnerable to destructive insect pests such as the rice stem borer, brown plant hopper, rice gall midge, and rice water weevil, which can significantly reduce yield if not detected and controlled early. Rice cultivation in Nigeria suffers up to 75% yield reduction because of insect pest damage based on statistics [<xref ref-type="bibr" rid="ref_5">5</xref>]. Severe food shortages, along with increased rice importation and higher market prices, result from these losses, which create economic instability for both farmers and the country. There is a need for a system that can accurately detect and identify insect pests in rice and provide early warning for farmers to take necessary measures to mitigate the damage caused by these pests. Pests and diseases constitute a major problem for agricultural production and a threat to food security.</p><p>Manual diagnosis of rice crop diseases is still the most prevalent method at present, relying primarily on visual observation of disease symptoms. The most prevalent method of diagnosing rice crop diseases today is manual inspection by visually observing disease symptoms [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>]. This method faces multiple difficulties, which include the limited availability of trained agricultural workers capable of providing quick and accurate diagnoses. The current constraints indicate an immediate requirement for an automated and scalable disease diagnosis method that also manages pest infestations more efficiently.</p><p>With the advent of smart agriculture, ML has been integrated in various fields that need to process data holistically. Smart systems have various elements that enable local data fetching, analysis, monitoring, and prediction [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>]. The application of ML in agriculture has greatly assisted in identifying optimal methods of farming and crop management, improving crop quality and total agricultural output. Particularly, extensive farming has immensely gained from the capabilities of ML's strong automation and computation. Some of the major uses of ML in agriculture are forecasting agricultural output, detecting and identifying insect pests, and streamlining pest management practices [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>], [<xref ref-type="bibr" rid="ref_12">12</xref>]. Massive research has proved that systems based on ML can potentially be used to diagnose and solve problems that hamper agricultural efficiency. These advanced technologies support various phases of agricultural production, including inspection, transportation, and storage of farm produce. Based on the insights from ML, agricultural experts can make more efficient and informed decisions regarding pest control and crop management.</p><p>Recent advancements in neural networks have obtained promising results in addressing complex issues in agriculture such as feature extraction, image segmentation, image classification, and other vision tasks. CNNs have particularly excelled at classifying images [<xref ref-type="bibr" rid="ref_13">13</xref>], [<xref ref-type="bibr" rid="ref_14">14</xref>], [<xref ref-type="bibr" rid="ref_15">15</xref>]. Their ability to decipher high amounts of visual data has seen them become one of the most successful means of disease and pest identification in plants. Through CNNs, researchers and agriculturalists can develop automatic disease identification systems that surpass traditional manual methods in both speed and accuracy. This research seeks to explore two ML algorithms, namely SVM and the Inception_ResNetV2 CNN model, for the detection of rice diseases, helping to mitigate losses associated with rice cultivation. Finally, this study concentrates on the web-based implementation of the disease detection model, guaranteeing that farmers and agricultural specialists have real-time access. Besides, the users can also upload photos of rice crops, obtain real-time illness diagnosis, and see prescribed management practices from an online interface. The present study extends scalability, convenience, and impact to rice disease detection by synergistically integrating cloud computing with ML to contribute to sustainable farming practices.</p>
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p>ML and deep learning (DL) techniques have been employed by various researchers to identify various rice insect pest diseases. The techniques, along with evaluation outcomes, are presented in this section.</p><p>Doan [<xref ref-type="bibr" rid="ref_7">7</xref>] introduced a novel approach that combines Power Mean SVM (PmSVM) with EfficientNet fine-tuning to classify insect pest images on a wide scale. EfficientNet models were first adjusted and re-trained using fresh image datasets of insect pests. The ML classifier was then constructed using the features that were recovered from the EfficientNet models. During the classification step of the network, PmSVM, a non-linear classifier, replaced the conventional Softmax function. The suggested method's classification accuracy for IP102 large insect pest datasets is 72.31% after fine-tuning EfficientNets and PmSVM. The model considered was unsuitable to handle large datasets; hence, the accuracy was low.</p><p>Deng et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] built a smartphone application for automatic rice disease diagnosis. The method was built using DL, trained on a large dataset of 33,026 images of six types of rice diseases, which are leaf blast, false smut, neck blast, sheath blight, bacterial stripe disease, and brown spot. The highlight of this method was an ensemble model, which combined multiple submodels. For evaluating its performance, the ensemble model was evaluated using an independent image set. The result indicated that DenseNet-121, SE-ResNet-50, and ResNeSt-50 were the three best-performing submodels based on parameters such as learning rate, precision, recall, and accuracy in recognizing diseases. Consequently, these three submodels were selected and combined to devise the final ensemble model. The integration of these submodels made it possible to minimize confusion among similar kinds of diseases, thereby reducing the possibility of wrong diagnosis. Used to diagnose the six rice diseases, the ensemble model achieved an overall accuracy of 91%. This performance was considered reasonably high, given the visual similarities among certain rice diseases. However, the model could only identify six rice diseases.</p><p>Similarly, Bhartiya et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] proposed a novel approach for rice disease detection using ML techniques. Significant diseases affecting different rice leaves were classified using different machine-learning methods. The rice leaf disease images were taken, from which features of their corresponding diseases were initially extracted. Finally, the features were classified by applying different ML techniques to classify the images as per the methodology explained in the content. It was found that the quadratic SVM classifier gave an accuracy of around 81.8%. Further, the shape features such as area, roundness, and area-to-lesion ratio were used to differentiate various types of rice diseases. Further, the developed model could not actually determine the effectiveness of the model in real time. Furthermore, Naresh Kumar and Sakthivel [<xref ref-type="bibr" rid="ref_18">18</xref>] proposed a new detection method of rice diseases based on the Fusion Vision Boosted Classifier (FVBC) with Visual Geometry Group (VGG)19 for features and LightGBM for classification. A specially prepared dataset of 2,627 leaf images of rice, divided between training, validation, and testing sets, were used to approximate the model performance. The FVBC model achieved accuracies of 97.78% on the training set, 97.5% on the validation set, and 97.6% on the test set, showing that it had a high performance in detecting rice diseases.</p><p>Deng et al. [<xref ref-type="bibr" rid="ref_19">19</xref>] presented a DL-based rice disease and insect pest detection method based on a mobile phone. This paper is a comparative study between two enhanced models in detection of six high-frequency diseases and insect pest detection—the improved You Only Look Once (YOLO)v5 and YOLOv7-tiny. They are both models based on lightweight object detection networks. The former uses the Ghost module for computation reduction while improving the structure of the model, and the latter uses the Convolutional Block Attention Module (CBAM) and SIoU for better learning precision of the model. Initially, the detection accuracy and operational efficiency of the models were evaluated and analyzed. Subsequently, the two proposed methods were deployed on a mobile phone, and an application was designed to further verify their practicality in detecting rice diseases and insect pests. The results indicated that the improved YOLOv5s achieved the highest F1-score of 0.931 and a mean average precision of 0.5.</p><p>Ayan et al. [<xref ref-type="bibr" rid="ref_20">20</xref>] proposed the use of a genetic algorithm-based weighted ensemble of deep CNNs for crop pest classification. Using suitable transfer learning and fine-tuning techniques, seven distinct pre-trained CNN models (VGG-16, VGG-19, ResNet-50, Inception-V3, Xception, MobileNet, and SqueezeNet) were adjusted and re-trained on the 40-class D0 dataset that is made available to the public during the study. Later, to improve classification performance, the top three CNN models—Inception-V3, Xception, and MobileNet—were ensembled using the sum of maximum probabilities technique. This new model was called SMPEnsemble. Following that, weighted voting was used to ensemble these models. The genetic algorithm, known as GAEnsemble, was used to establish the weights. The algorithm considers the predictive stability and success rate of three CNN models. With 98.81% classification accuracy for the D0 dataset, GAEnsemble achieved the top performance. The procedure was repeated using two other datasets—the SMALL dataset with 10 classes and the IP102 dataset with 102 classes—in order to improve the robustness of the ensembled model without affecting the first best-performing CNN models on D0. For the SMALL dataset and IP102, the accuracy numbers for GAEnsemble were 95.15% and 67.13%, respectively. However, the version of the IP102 dataset used is outdated as it does not contain the images of the recent insect pest.</p><p>In similar research, Pattnaik and Parvathy [<xref ref-type="bibr" rid="ref_21">21</xref>] proposed the use of ML-based approaches for tomato pest classification. In the paper, automatic tomato pest identification and categorization utilizing ML-based image processing techniques was reported. The study took into consideration the textural properties of pest photos that were recovered using feature extraction methods such as local binary pattern (LBP), histogram of oriented gradient (HOG), gray level co-occurrence matrix (GLCM), and sped up robust features (SURF). For the classification process, three common classification techniques were applied: decision tree (DT), k-nearest neighbor (KNN), and SVM. To show which classifier produces the best accuracy with which feature, a thorough analysis of the three classifiers was conducted. The experiment's findings demonstrated that the greatest value of 81.02% is achieved by the SVM classifier's precision when employing the feature extracted by the LBP approach. However, no part of the work specified the dataset used to train the algorithms, rendering the presented results questionable.</p><p>In the work of Kasinathan et al. [<xref ref-type="bibr" rid="ref_22">22</xref>], modern ML techniques were proposed for insect classification and detection in field crops. The Wang and Xie dataset's nine and twenty-four insect classes were the subjects of classification experiments utilizing shape features and ML techniques like CNN, ANN, SVM, KNN, and Naive Bayes (NB). Also the research paper presents the insect pest detection algorithm that consists of foreground extraction and contour identification to detect the insects for Wang, Xie, Deng, and IP102 datasets in a highly complex background. Using the CNN model, the insects in the nine and twenty-four classes had the highest classification rates—91.5% and 90%, respectively. The data augmentation technique used increased the computational complexity of the model by increasing the training time.</p><p>Adi et al. [<xref ref-type="bibr" rid="ref_23">23</xref>] adopted the use of CNN for the identification of rice plant disease. The dataset of the study consisted of 1,600 images of rice leaves, divided into 4 classes: 400 images of rice affected by leaf blight disease, 400 images of rice affected by brown spot disease, 400 images of rice affected by leaf smut disease, and 400 images of healthy rice. In each class, the images were further divided into 380 for training and 20 for testing. During the rice image training process using the Python programming language, the accuracy achieved was 83%. The results were then saved as a model file and imported as training data into the program in Android Studio for use in an application. Testing the application with 20 rice leaf images from each class resulted in an actual accuracy of 94%. However, the size of the dataset class used was small, and the model was only capable of detecting a few diseases.</p><p>Singh et al. [<xref ref-type="bibr" rid="ref_24">24</xref>] introduced a custom CNN architecture designed to detect and classify common diseases in rice plants while reducing the network's parameter count. The proposed model was trained on a dataset comprising four common rice plant diseases. Additionally, the research incorporated 1,400 on-field images of healthy rice leaves to facilitate the identification of disease-free plants. Independent experiments were conducted both with and without the inclusion of the healthy leaf dataset. The performance of the proposed model was assessed using stochastic gradient descent with momentum (SGDM) and adaptive moment estimation (Adam) optimization techniques by employing multiple performance metrics. Experimental findings indicated that, for the classification of four rice plant diseases, the model achieved a maximum test accuracy of 99.66% with SGDM and 99.83% with Adam in the 7th epoch. Furthermore, when the healthy leaf dataset was included, the model utilizing the Adam optimizer outperformed the SGDM-based model, achieving maximum test accuracies of 99.66% and 97.61% in the 7th epoch, respectively. The study achieved high accuracy; however, the model was not deployed in real time, which may affect the performance in field conditions, which are essential for practical agricultural applications.</p><p>Nayem et al. [<xref ref-type="bibr" rid="ref_25">25</xref>] used a custom CNN to detect ten different classes of rice pests, including bacterial leaf blight, bacterial leaf streak, bacterial panicle blight, blast, brown spot, dead heart, downy mildew, healthy, hispa, and tungro. The proposed model was trained on a dataset of 10,400 images and implemented with the Keras framework and a TensorFlow backend. The model achieved a validation accuracy of 88.18% using only 0.57 million parameters, demonstrating its effectiveness and efficiency in identifying rice pests.</p><p>From the review conducted, extensive research has been done by quite a number of scholars with respect to the detection and identification of insect pests and has achieved positive results. However, previous researches have been faced with challenges such as the use of small-sized datasets, which affects the model’s performance on unseen examples; the use of outdated datasets that do not contain images of recent insect pests; the use of datasets from unknown sources; models that perform poorly on large datasets due to shallow depth; and the use of data augmentation techniques that increase the training time of the model. In addition, most research was not deployed in real time. As such, this research seeks to adopt and deploy a model that can perform well on a large dataset that contains recent images of insect pests and improved classes of insect pest species to solve the problem of generalization to unseen examples and prevent the use of data augmentation to decrease the training time of the model.</p>
    </sec>
    <sec sec-type="">
      <title>3. Methodology</title>
      <p>The methodology used to develop a ML-driven web platform for automated insect pest identification is explained in detail in this section. The section starts with a conceptual framework, as illustrated in <xref ref-type="fig" rid="fig_1">Figure 1</xref>, which lists the steps used, i.e., data collection from the maize leaf image, image preprocessing, fine-tuning of the model, training and evaluation of the model, and finally, model deployment. </p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Conceptual framework for model development and deployment</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_69KGwswEPNO7fcYc.png"/>
        </fig>
      
      
        <sec>
          
            <title>3.1. Data acquisition</title>
          
          <p>The necessity for efficient rice pest identification has encouraged numerous researchers to create benchmark datasets appropriate for ML models. The IP102 dataset is a set of images specially designed for insect pest classification tasks. It comprises 102 classes of insect pests and 75,000 images that are widely encountered in agricultural environments [<xref ref-type="bibr" rid="ref_26">26</xref>]. Each category is a varied species of insect pest, and the dataset provides a lot of images for a single category in varied poses, angles, and lighting conditions. For this work, considering the species of the insect pest affecting rice, 12 classes of rice pest disease were selected with about 7,736 images, which occupy a space of 371 MB of memory from the IP102 dataset. The IP102 benchmark dataset contains rice images with unique characteristics like uniform backgrounds with relative intensity, complex back-grounds with occluded images, complex backgrounds with multiple leaves in an image, and varying image quality (low quality, high-fidelity, and multiple backgrounds) for the rice pest dataset. This necessitates the need for effective preprocessing. Therefore, the obtained dataset was preprocessed using a uniform image size of 224 by 224 for compatibility, lower memory and resource demands. The IP102 dataset consists of rice pest images categorized into 12 classes, as presented in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Classification of rice pest diseases</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>S/N</p></td><td colspan="1" rowspan="1"><p>Rice Rest</p></td><td colspan="1" rowspan="1"><p>Quantity of Images</p></td></tr><tr><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>Rice leaf roller</p></td><td colspan="1" rowspan="1"><p>605</p></td></tr><tr><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>Rice leaf caterpillar</p></td><td colspan="1" rowspan="1"><p>475</p></td></tr><tr><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p>Paddy stem maggot</p></td><td colspan="1" rowspan="1"><p>325</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>Asiatic rice borer</p></td><td colspan="1" rowspan="1"><p>745</p></td></tr><tr><td colspan="1" rowspan="1"><p>5</p></td><td colspan="1" rowspan="1"><p>Yellow rice borer</p></td><td colspan="1" rowspan="1"><p>455</p></td></tr><tr><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p>Rice gall midge</p></td><td colspan="1" rowspan="1"><p>791</p></td></tr><tr><td colspan="1" rowspan="1"><p>7</p></td><td colspan="1" rowspan="1"><p>Brown planthopper</p></td><td colspan="1" rowspan="1"><p>290</p></td></tr><tr><td colspan="1" rowspan="1"><p>8</p></td><td colspan="1" rowspan="1"><p>Rice stem fly</p></td><td colspan="1" rowspan="1"><p>1,110</p></td></tr><tr><td colspan="1" rowspan="1"><p>9</p></td><td colspan="1" rowspan="1"><p>Rice water weevil</p></td><td colspan="1" rowspan="1"><p>1,194</p></td></tr><tr><td colspan="1" rowspan="1"><p>10</p></td><td colspan="1" rowspan="1"><p>Rice leaf hopper</p></td><td colspan="1" rowspan="1"><p>686</p></td></tr><tr><td colspan="1" rowspan="1"><p>11</p></td><td colspan="1" rowspan="1"><p>Rice shell pest</p></td><td colspan="1" rowspan="1"><p>480</p></td></tr><tr><td colspan="1" rowspan="1"><p>12</p></td><td colspan="1" rowspan="1"><p>Thrips</p></td><td colspan="1" rowspan="1"><p>580</p></td></tr><tr><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>Total</p></td><td colspan="1" rowspan="1"><p>7,736</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>From <xref ref-type="table" rid="table_1">Table 1</xref>, it is evident that the distribution of images across the different rice pest classes is imbalanced. Such data imbalance can negatively affect model performance, especially by causing the model to become biased toward the classes with more samples. To mitigate this issue and ensure equal representation of each class during training, class weights were computed and assigned accordingly. The weights were calculated using Eq. (1).</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="ma0mg0rdtm">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mtext> Class Weight </mml:mtext>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mtext> Total Number of Samples </mml:mtext>
                        <mml:mrow>
                          <mml:mtext> Number of Classes </mml:mtext>
                          <mml:mtext> Number of Samples in Class </mml:mtext>
                          <mml:mo>×</mml:mo>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>This formula ensures that minority classes are assigned higher weights, thereby penalizing their misclassification more heavily during model training. By incorporating these weights into the categorical cross-entropy loss function, the model gives more importance to underrepresented classes, effectively reducing bias and improving classification performance across all categories. Many researchers have attributed the existing model's computationally expensive nature to larger dataset sizes and high-resolution images. The IP02 rice pest dataset images have varying sizes; this shows that the images in the dataset contain high-resolution images, which demand more computational and memory resources if adopted as they are. Therefore, it is important to ensure that the images are resized into uniform sizes to ensure consistency and compatibility across the dataset, preventing errors during model training and inference. Therefore, the images were resized to 224 by 224. Furthermore, to reduce the storage size (50%), improve efficient querying, faster data access (random access and optimized I/O), and improve data loading/saving across multi-platform, the Python programming language was used to convert the rice pest dataset into hierarchical data (.h) format. Thus, the new improved dataset, an HDF5 dataset, has .h as an extension (RicePest.h). <xref ref-type="fig" rid="fig_2">Figure 2</xref> depicts sample images loaded and saved from the IP102 dataset.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Sample images from the IP102 dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_8Elw03fNkBW2Ix69.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Data preprocessing</title>
          
          <p>Data preprocessing was conducted to enhance the quality of the images and the performance of the model. The normalization of the pixel values ranged from 0 to 1, determined by the pixel value divided by 255. This step helped reduce variance at training and allowed faster convergence at optimization. The Gaussian filter was applied to remove image noise without destroying crucial visual characteristics so as not to confuse the model with disease-specific features. Background noise removal algorithms were also employed to crop the images and delineate areas of interest, particularly the rice leaves without eliminating the background noise. This allowed the model to focus on the most important features required for accurate disease classification, improving its predictability. Preprocessing was not just resizing but was a series of techniques to improve dataset quality and readability. Normalization gave a standard pixel value range and also prevented the model from being overwhelmed by larger values during training. This ensured learning stability and feature weighting balance. Preprocessing also addressed any potential data imperfections lastly. Non-functional or anomalous images were detected and removed to ensure the quality of the training dataset. Such quality control was performed manually and using the properties of automated functions, i.e., checksum verification, to detect and remove anomalies from the data. Elimination of irrelevant photos allowed the model to learn from pertinent data only for the classification problem, thus improving overall accuracy and reliability. For this research, <inline-formula>
  <mml:math id="mqtj6uq10e">
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mrow>
        <mml:mtext>orginal</mml:mtext>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> denotes the original image with dimensions <inline-formula>
  <mml:math id="m36o7lul7w">
    <mml:msub>
      <mml:mi>H</mml:mi>
      <mml:mrow>
        <mml:mtext>original</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mi>W</mml:mi>
      <mml:mrow>
        <mml:mtext>original</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula>. After resizing to the target size, <inline-formula>
  <mml:math id="mm170od3aj">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
    <mml:msub>
      <mml:mi>H</mml:mi>
      <mml:mrow>
        <mml:mtext>target</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mi>W</mml:mi>
      <mml:mrow>
        <mml:mtext>target</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula>, the resized image (<inline-formula>
  <mml:math id="m8jdu96y04">
    <mml:mrow>
      <mml:mo fence="true"/>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>I</mml:mi>
        <mml:mrow>
          <mml:mtext>resized</mml:mtext>
        </mml:mrow>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> is denoted by Eq. (2).</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="mw13okpp4w">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>I</mml:mi>
                        <mml:mrow>
                          <mml:mtext>resized</mml:mtext>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mi>resize</mml:mi>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mi>I</mml:mi>
                          <mml:mrow>
                            <mml:mtext>original,</mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>×</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:msub>
                            <mml:mi>H</mml:mi>
                            <mml:mrow>
                              <mml:mtext>target</mml:mtext>
                            </mml:mrow>
                          </mml:msub>
                          <mml:msub>
                            <mml:mi>W</mml:mi>
                            <mml:mrow>
                              <mml:mtext>target</mml:mtext>
                            </mml:mrow>
                          </mml:msub>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>Let <inline-formula>
  <mml:math id="mv74qijxr0">
    <mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mrow>
        <mml:mo>min</mml:mo>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mrow>
        <mml:mo>min</mml:mo>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mrow>
        <mml:mo>max</mml:mo>
      </mml:mrow>
    </mml:msub>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="mvgo4mrrgv">
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mrow>
        <mml:mo>max</mml:mo>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> denote the coordinates of the bounding box defining the region of interest. The cropped image (<inline-formula>
  <mml:math id="m4nm5u3r3v">
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mrow>
        <mml:mtext>cropped</mml:mtext>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula>) can be obtained by extracting the region of interest from the original image, as presented in Eq. (3).</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="mmkbqh2cn5">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>I</mml:mi>
                        <mml:mrow>
                          <mml:mtext>cropped</mml:mtext>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>I</mml:mi>
                        <mml:mrow>
                          <mml:mtext>original</mml:mtext>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mrow>
                        <mml:mo>[</mml:mo>
                        <mml:mo>:</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>:</mml:mo>
                        <mml:mo>]</mml:mo>
                        <mml:msub>
                          <mml:mi>y</mml:mi>
                          <mml:mrow>
                            <mml:mo>min</mml:mo>
                          </mml:mrow>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>y</mml:mi>
                          <mml:mrow>
                            <mml:mo>max</mml:mo>
                          </mml:mrow>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>x</mml:mi>
                          <mml:mrow>
                            <mml:mo>min</mml:mo>
                          </mml:mrow>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>x</mml:mi>
                          <mml:mrow>
                            <mml:mo>max</mml:mo>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>Preprocessing may also entail trimming photos to eliminate extraneous background or unimportant elements. It is noteworthy that by restricting the area of interest to these locations exclusively, this study may reduce the noise level resulting from superfluous photos of other regions and improve the model's performance when comparing the model and the real result for the position of the rice leaves. Normalization is a crucial preprocessing step for data. To reduce the run time of optimization approaches during model training, it is a usual practice to standardize the pixel values at multiples of [0,1] or [-1,1]. This entails normalizing each pixel value by the maximum possible pixel value, which for 8-bit pictures NAIVE is 255, and then putting all of the pixel values inside that range. Naturally, there is also the choice to divide the mean pixel value by the standard deviation, which can yield values that are roughly equal to zero and have a unit standard deviation. In this research, <inline-formula>
  <mml:math id="mmozunfue3">
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mtext>normalized</mml:mtext>
    </mml:msub>
  </mml:math>
</inline-formula> denotes the normalized image. Eq. (4) shows the normalization of the range if it is between 0 and 1.</p>
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="mov2p3flpl">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>I</mml:mi>
                        <mml:mrow>
                          <mml:mtext>normalized</mml:mtext>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:msub>
                          <mml:mi>I</mml:mi>
                          <mml:mrow>
                            <mml:mtext>original</mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mn>255</mml:mn>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>Standardization was conducted by dividing by the standard deviation after subtracting the mean, as presented in Eq. (5).</p>
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="mj7eaydh8m">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>I</mml:mi>
                        <mml:mrow>
                          <mml:mtext>normalized</mml:mtext>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:msub>
                            <mml:mi>I</mml:mi>
                            <mml:mrow>
                              <mml:mtext>original</mml:mtext>
                            </mml:mrow>
                          </mml:msub>
                          <mml:mo>−</mml:mo>
                          <mml:mi>μ</mml:mi>
                        </mml:mrow>
                        <mml:mi>σ</mml:mi>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mh9h5hcfwp">
    <mml:mi>σ</mml:mi>
  </mml:math>
</inline-formula> is the standard deviation of the dataset's pixel values, and <inline-formula>
  <mml:math id="mqucz7sln8">
    <mml:mi>μ</mml:mi>
  </mml:math>
</inline-formula> is the mean pixel value.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Dataset splitting</title>
          
          <p>The dataset needs to be split in order to employ any ML technique: one for testing, one for model validation checks, and one for model training. This is called data partitioning and needs to be completed before applying any ML techniques. The dataset for this study was split into three parts: testing (10%), validation (10%), and training (80%). This split is based on accepted practices and literature. The majority of the dataset was allotted to the training set, which is in charge of discovering the patterns, connections, and characteristics of the dataset.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Development of the inception_resnetv2 and svm model for effective pest classification</title>
          
          <p>For the purpose of enhancing the robustness, accuracy, and computational expense of rice pest identification operations, this research designed an improved Inception_ResNetV2 and SVM model. Inception_ResNetV2 is a variant deep CNN architecture and is highly trending for its factorized convolutions in feature extraction optimization and reduced computational complexity. SVM, however, is a strong machine-learning algorithm that is well known for its better classification performance in high-dimensional feature spaces. Hence, the improved model took the best of both approaches.</p>
          
            <sec>
              
                <title>3.4.1 Inception_resnetv2 cnn</title>
              
              <p>The strength of an Inception ResNet-v2 network is its ability to combine a convolutional and a pooling layer, which eliminates the requirement to choose the appropriate filter size and its dimensions from a wide range of options for a convolutional layer or between a conv and a pooling layer. As a result, they make excellent selections for images of various sizes. In this research, to train the proposed Inception ResNet-v2 model to learn representative features from the insect pest dataset, the standard architecture was adopted. <xref ref-type="fig" rid="fig_3">Figure 3</xref> shows the Inception_ResNetV2 architecture.</p>
              
                <fig id="fig_3">
                  <label>Figure 3</label>
                  <caption>
                    <title>Inception_ResNetV2 architecture</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img__YBBRT5S9CSQ8Ab-.png"/>
                </fig>
              
              <p>In this research, Inception_ResNetV2 extracts deep hierarchical features from the input rice pest images which can be mathematically expressed by Eq. (6).</p>
              
                <disp-formula>
                  <label>(6)</label>
                  <mml:math id="mwiv94a2fq">
                    <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                      <mml:mtr>
                        <mml:mtd>
                          <mml:mi>F</mml:mi>
                          <mml:mi>X</mml:mi>
                          <mml:mi>W</mml:mi>
                          <mml:mi>b</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mtext>Inception_RestNetV2</mml:mtext>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:math>
                </disp-formula>
              
              <p>where, $X<inline-formula>
  <mml:math id="mly2b0hgjb">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>W<inline-formula>
  <mml:math id="micr7fuyhf">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>b<inline-formula>
  <mml:math id="maul801hie">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>F \in R^d<inline-formula>
  <mml:math id="mzsg8sy5yk">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>d$. The Inception_ResNetV2 algorithm is presented as below.</p><table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Algorithm 1 Inception_ResNetV2</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">1: Step 1: Load Dataset</p><p style="text-align: justify">2: The input images of pests from the IP02 dataset were collected.</p><p style="text-align: justify">3: Then the images were preprocessed (resized and normalized).</p><p style="text-align: justify">4: Step 2: Feature Extraction Using Inception ResNetV2</p><p style="text-align: justify">5: The Inception ResNeXt model (trained from scratch) was imported.</p><p style="text-align: justify">6: The last fully-connected (FC) layer was cut off.</p><p style="text-align: justify">7: Input the images<italic> X<sub>i</sub></italic> one-by-one for feature extraction using Eq. (6).</p><p style="text-align: justify">8: Save the feature vectors Fi extracted for all training images.</p><p style="text-align: justify">9: Step 3: Train the Model</p><p style="text-align: justify">10: Train with the extracted features <italic>F<sub>i</sub> </italic>and respective labels <italic>y<sub>i</sub></italic>.</p><p style="text-align: justify">11: Hyper-parameter tuning for cross-validation.</p><p style="text-align: justify">12: Step 4: Model Evaluation</p><p style="text-align: justify">13: Test the hybrid model on the validation/test dataset.</p><p style="text-align: justify">14: Measure performance in terms of accuracy, precision, recall, and F1 score.</p><p style="text-align: justify">15: Step 5: Deployment</p><p style="text-align: justify">16: Deploy a production-level model for real-time pest classification in agricultural environments.</p></td></tr></tbody></table><p>This Inception_ResNetV2 model addressed the challenges of class imbalance, misclassification due to background noise, and variations in pest morphology.</p>
            </sec>
          
          
            <sec>
              
                <title>3.4.2 Svm</title>
              
              <p>SVM is the technique that allows effectively separating various classes of data points with the best hyperplane in an associated high-dimensional feature space. The data were projected into this feature space using kernel functions, where SVM maximized the smallest margin obtained by the closest support vectors to different classes. A regularization parameter (C) was used to create the trade-off such that maximizing margins and keeping classification errors to a minimum on the training data were optimized. The SVM architecture is shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>.</p>
              
                <fig id="fig_4">
                  <label>Figure 4</label>
                  <caption>
                    <title>Architecture of SVM [<xref ref-type="bibr" rid="ref_27">27</xref>]</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_f0l2w3_Dq2T-vIfX.png"/>
                </fig>
              
              <p>Since SVM relies on determining the optimal decision boundary based on the distribution of the input data in the feature space rather than layered structures, it is especially useful for classification tasks with intricate, nonlinear decision borders.</p><p>a) Feature extraction</p><p>In this research, after the dataset of rice insect pest images was retrieved globally and preprocessed by resizing images, it was converted to grayscale or Red, Green, and Blue (RGB) format, and pixel values were normalized. The relevant features were extracted from these preprocessed images. Features, including color histograms, texture, shape, and descriptors, serve as the input to the SVM classifier.</p><p>b) Training the SVM model</p><p>To train the SVM model, the extracted features and their corresponding class labels were used. The SVM algorithm attempted, during training, to identify the hyperplane that maximizes the margin between classes and most correctly separated the feature vectors of the various classes. The data points closest to the decision boundary, or support vectors, have a very important role to play in deciding this hyperplane. The regularization parameter (C) and the type of kernel function affect the performance and the generalization ability of SVM. SVM was used to continuously re-tune the hyperplane to minimize the classification errors on the training set while maintaining a maximum margin.</p><p>c) Classification of test images</p><p>After training, the SVM model was able to classify new images of rice insect pests. This entailed extracting the same feature set in the test images as what had been done to train on them. These features were then passed to the SVM model, which evaluated them for distance calculations from the decision boundary. SVM categorized the test images into one of the pre-defined classes based on the distances. The comparison for the predicted test image class label starts from analyzing the minimum distance from the hyperplane or the highest class confidence score. The insect pest photo classification outcome is simply an outworking of applying this procedure for every test image and using the patterns learned from the training set. The SVM model has a special property that can distinguish well among different species of rice pests. This is given by Eq. (7).</p>
              
                <disp-formula>
                  <label>(7)</label>
                  <mml:math id="mw77xqexaa">
                    <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                      <mml:mtr>
                        <mml:mtd>
                          <mml:mi>f</mml:mi>
                          <mml:mi>X</mml:mi>
                          <mml:mi>sign</mml:mi>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>=</mml:mo>
                          <mml:mrow>
                            <mml:mo>(</mml:mo>
                            <mml:mo>+</mml:mo>
                            <mml:mo>)</mml:mo>
                            <mml:munderover>
                              <mml:mo>∑</mml:mo>
                              <mml:mrow>
                                <mml:mi>i</mml:mi>
                                <mml:mo>=</mml:mo>
                                <mml:mn>1</mml:mn>
                              </mml:mrow>
                              <mml:mi>n</mml:mi>
                            </mml:munderover>
                            <mml:msub>
                              <mml:mi>α</mml:mi>
                              <mml:mi>i</mml:mi>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>y</mml:mi>
                              <mml:mi>i</mml:mi>
                            </mml:msub>
                            <mml:mi>K</mml:mi>
                            <mml:mi>b</mml:mi>
                            <mml:mrow>
                              <mml:mo>(</mml:mo>
                              <mml:mo>,</mml:mo>
                              <mml:mo>)</mml:mo>
                              <mml:msub>
                                <mml:mi>F</mml:mi>
                                <mml:mi>i</mml:mi>
                              </mml:msub>
                              <mml:mi>F</mml:mi>
                            </mml:mrow>
                          </mml:mrow>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="mw4rcv80jg">
    <mml:msub>
      <mml:mi>α</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> are the langrage multipliers, <inline-formula>
  <mml:math id="mv1b0kin4u">
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:mo>∈</mml:mo>
    <mml:mo fence="false">{</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>+</mml:mo>
    <mml:mo fence="false">}</mml:mo>
    <mml:mn>1</mml:mn>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula> are the class labels, <inline-formula>
  <mml:math id="mr0d04eng6">
    <mml:mi>K</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>F</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:msub>
      <mml:mi>F</mml:mi>
    </mml:mrow>
  </mml:math>
</inline-formula> is the kernel function, e.g., Radial Basis Function (RBF) or linear kernel, and $b$ is the bias term. </p><p>SVM optimizes the hinge loss function to maximize the margin between classes, as shown in Eq. (8).</p>
              
                <disp-formula>
                  <label>(8)</label>
                  <mml:math id="mojxqm5hwb">
                    <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                      <mml:mtr>
                        <mml:mtd>
                          <mml:mrow>
                            <mml:mi>L</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mo>(</mml:mo>
                            <mml:mo>,</mml:mo>
                            <mml:mo>−</mml:mo>
                            <mml:mo>)</mml:mo>
                            <mml:mn>0</mml:mn>
                            <mml:mn>1</mml:mn>
                            <mml:msub>
                              <mml:mi>y</mml:mi>
                              <mml:mi>i</mml:mi>
                            </mml:msub>
                            <mml:mi>f</mml:mi>
                            <mml:mrow>
                              <mml:mo>(</mml:mo>
                              <mml:mo>)</mml:mo>
                              <mml:msub>
                                <mml:mi>X</mml:mi>
                                <mml:mi>i</mml:mi>
                              </mml:msub>
                            </mml:mrow>
                          </mml:mrow>
                          <mml:mo>=</mml:mo>
                          <mml:mo>max</mml:mo>
                          <mml:mo>+</mml:mo>
                          <mml:mo fence="false">‖</mml:mo>
                          <mml:munderover>
                            <mml:mo>∑</mml:mo>
                            <mml:mrow>
                              <mml:mi>i</mml:mi>
                              <mml:mo>=</mml:mo>
                              <mml:mn>1</mml:mn>
                            </mml:mrow>
                            <mml:mi>n</mml:mi>
                          </mml:munderover>
                          <mml:mfrac>
                            <mml:mi>λ</mml:mi>
                            <mml:mn>2</mml:mn>
                          </mml:mfrac>
                          <mml:mi>w</mml:mi>
                          <mml:msup>
                            <mml:mo fence="false">‖</mml:mo>
                            <mml:mn>2</mml:mn>
                          </mml:msup>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:math>
                </disp-formula>
              
              <p>where, the first term ensures correct classification, and the second term is the regularization term with hyperparameter <inline-formula>
  <mml:math id="m351rqp55i">
    <mml:mi>λ</mml:mi>
  </mml:math>
</inline-formula> to avoid overfitting.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.5. Model configuration and training</title>
          
          <p>The hyperparameters in <xref ref-type="table" rid="table_2">Table 2</xref> represent the various configurations used to train the InceptionResNetV2 model and the SVM model for this research on automated identification of insect pests. Batch sizes of 32 were used for both models, a standard choice that balances memory efficiency and model convergence. 50 epochs were trained to provide enough time for the models to learn from the data and to avoid underfitting. To avoid overfitting, early stopping with patience 5 was used, which means training would be halted if validation loss does not improve for five consecutive epochs. Optimization-wise, InceptionResNetV2 utilizes the Adam optimizer, which is known for its adaptive learning rate as well as speed of convergence at a learning rate of 0.0001.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Detailed hyperparameters configured in the Inception ResNetV2 and SVM models</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Parameters</p></td><td colspan="1" rowspan="1"><p>Inception_ResNetV2 Value</p></td><td colspan="1" rowspan="1"><p>SVM Value</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image size</p></td><td colspan="1" rowspan="1"><p>299×299</p></td><td colspan="1" rowspan="1"><p>64×64</p></td></tr><tr><td colspan="1" rowspan="1"><p>Batch size</p></td><td colspan="1" rowspan="1"><p>32</p></td><td colspan="1" rowspan="1"><p>32</p></td></tr><tr><td colspan="1" rowspan="1"><p>Epoch</p></td><td colspan="1" rowspan="1"><p>50</p></td><td colspan="1" rowspan="1"><p>50</p></td></tr><tr><td colspan="1" rowspan="1"><p>Patience</p></td><td colspan="1" rowspan="1"><p>5</p></td><td colspan="1" rowspan="1"><p>5</p></td></tr><tr><td colspan="1" rowspan="1"><p>Learning rate</p></td><td colspan="1" rowspan="1"><p>0.0001</p></td><td colspan="1" rowspan="1"><p>0.0001</p></td></tr><tr><td colspan="1" rowspan="1"><p>Optimizer</p></td><td colspan="1" rowspan="1"><p>Adam</p></td><td colspan="1" rowspan="1"><p>SGD</p></td></tr><tr><td colspan="1" rowspan="1"><p>Loss</p></td><td colspan="1" rowspan="1"><p>Categorical cross-entropy</p></td><td colspan="1" rowspan="1"><p>Squared hinge</p></td></tr><tr><td colspan="1" rowspan="1"><p>SVM kernel</p></td><td colspan="1" rowspan="1"><p>NA</p></td><td colspan="1" rowspan="1"><p>Linear</p></td></tr><tr><td colspan="1" rowspan="1"><p>SVM probability</p></td><td colspan="1" rowspan="1"><p>NA</p></td><td colspan="1" rowspan="1"><p>True</p></td></tr><tr><td colspan="1" rowspan="1"><p>Training split</p></td><td colspan="1" rowspan="1"><p>0.8</p></td><td colspan="1" rowspan="1"><p>0.8</p></td></tr><tr><td colspan="1" rowspan="1"><p>Validation split</p></td><td colspan="1" rowspan="1"><p>0.1</p></td><td colspan="1" rowspan="1"><p>0.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>Test split</p></td><td colspan="1" rowspan="1"><p>0.1</p></td><td colspan="1" rowspan="1"><p>0.1</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>The SVM model, in contrast, uses Stochastic Gradient Descent (SGD), a commonly used optimization technique, for large-scale ML problems. Categorical cross-entropy was used as the loss function for InceptionResNetV2, suitable for multi-class classification, whereas squared hinge loss was employed for SVM, which is designed for margin-based classification. In terms of input size, InceptionResNetV2 accepts images of size 299 × 299, providing more intricate feature extraction at the cost of increased computational requirements. The SVM model, however, accepts images of size 64 × 64, where computational efficiency and processing speed are accorded greater priority, and thus the model is comparatively more viable for web-based real-time systems. The train, validation, and test splits were kept at 80:10:10 to ensure a well-balanced dataset for the adequate training, validation, and evaluation purposes of the models. The SVM classifier uses a linear kernel, which works well with high-dimensional data. In addition, probability estimates were set to true to allow confidence-based predictions.</p>
        </sec>
      
      
        <sec>
          
            <title>3.6. Model evaluation</title>
          
          <p>Accuracy, precision, recall, and F1-score were utilized in this research as performance metrics for the classification of rice insect pests into their classes. Accuracy is the general percentage correct in classifying, yet it can be misleading if the dataset isn't sufficiently balanced like in the case where one species of an insect is a lot larger than the rest. Precision measures the proportion of true positives, i.e., pest species correctly identified to all positive predictions, which is critical in preventing false positives, e.g., incorrectly classifying a harmless insect as a pest. Recall verifies the model's ability to predict actual rice disease, which is critical in pest control, where misidentification of harmful species could lead to significant agricultural loss. The F1-score provides a balance between precision and recall and is especially useful in the case of imbalanced datasets. Eqs. (9)-(12) depict the mathematical form of accuracy, precision, recall, and F1-score [<xref ref-type="bibr" rid="ref_28">28</xref>], [<xref ref-type="bibr" rid="ref_29">29</xref>].</p>
          
            <disp-formula>
              <label>(9)</label>
              <mml:math id="m0xtdz8tzz">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mtext>Accuracy</mml:mtext>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>T</mml:mi>
                          <mml:mi>P</mml:mi>
                          <mml:mi>T</mml:mi>
                          <mml:mi>N</mml:mi>
                          <mml:mo>+</mml:mo>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>T</mml:mi>
                          <mml:mi>P</mml:mi>
                          <mml:mi>T</mml:mi>
                          <mml:mi>N</mml:mi>
                          <mml:mi>F</mml:mi>
                          <mml:mi>P</mml:mi>
                          <mml:mi>F</mml:mi>
                          <mml:mi>N</mml:mi>
                          <mml:mo>+</mml:mo>
                          <mml:mo>+</mml:mo>
                          <mml:mo>+</mml:mo>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(10)</label>
              <mml:math id="mb85ctvu3e">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mtext>Precision</mml:mtext>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>T</mml:mi>
                          <mml:mi>P</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>T</mml:mi>
                          <mml:mi>P</mml:mi>
                          <mml:mi>F</mml:mi>
                          <mml:mi>P</mml:mi>
                          <mml:mo>+</mml:mo>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(11)</label>
              <mml:math id="mu7omtnj0y">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mtext>Recall</mml:mtext>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>T</mml:mi>
                          <mml:mi>P</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>T</mml:mi>
                          <mml:mi>P</mml:mi>
                          <mml:mi>F</mml:mi>
                          <mml:mi>N</mml:mi>
                          <mml:mo>+</mml:mo>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(12)</label>
              <mml:math id="md3rmy6myi">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mi>F</mml:mi>
                      <mml:mn>1</mml:mn>
                      <mml:mo>−</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mtext>score</mml:mtext>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                          <mml:mo>×</mml:mo>
                          <mml:mo>×</mml:mo>
                          <mml:mtext>Precision</mml:mtext>
                          <mml:mtext>Recall</mml:mtext>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mtext>Precision</mml:mtext>
                          <mml:mtext>Recall</mml:mtext>
                          <mml:mo>+</mml:mo>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
        </sec>
      
      
        <sec>
          
            <title>3.7. Model deployment</title>
          
          <p>Since the expected end users of this system are farmers who may have little or no programming skills, it becomes necessary to develop a friendly user interface that can ease the classification of any captured rice pest images for effective and early pest control. This study used a user-friendly methodology to create a web-based application that uses ML to classify rice insect pests in real time. The web interface, which was built using HTML/CSS/JavaScript on the frontend and the Streamlit framework on the backend, was designed to allow researchers to upload and categorize images of rice insect pests into groups related to various rice diseases, such as brown plant hopper, Asiatic rice borer, rice leaf roller, and rice leaf caterpillar. The real-time display of the classification results was made possible by the application's dynamic and responsive frontend. The following is an outline of the phases:</p><p>a) Configuring the web interface</p><p>Flask serves as the web application's first backend framework, handling user requests and controlling ML model interaction. To build a responsive and tidy layout, HTML and CSS were used to create the frontend. The layout included a middle file upload button to allow users to upload their rice insect pest images and a space to show the classification output.</p><p>b) Uploading and displaying rice insect images</p><p>The users can upload a rice insect pest from their personal workstation by clicking the upload image button. The users can see a file dialog box where they may search for and choose the image they wish to categorize. Following the selection of an image, the Pillow library was used for preprocessing and scaling. The online interface displayed the image so that the user could examine it thoroughly before classifying it.</p><p>c) Displaying results of classification</p><p>The model output showed the predicted class of rice disease and was presented on the web interface, allowing users to easily identify what disease the model predicted.</p>
        </sec>
      
      
        <sec>
          
            <title>3.8. System specification</title>
          
          <p>The simulation was conducted on a Zinox computer that has an Intel Core i7-6700 processor clocked at 3.40 GHz, 16 GB of random access memory (RAM), a dedicated 64 MB graphics card, and 64-bit Windows 10 Professional. The models were developed and configured in the Spyder Integrated Development Environment (IDE) using libraries, including TensorFlow, Keras, matplotlib, numpy, pandas, and tkinter. Because of the absence of a dedicated Graphic Processing Unit (GPU) for ML processing, training models was extremely slow with the configurations in <xref ref-type="table" rid="table_2">Table 2</xref>. The length of training time was primarily due to the size of the images (299 × 299 for Inception_ResNetV2) and the complex model architectures that require vast amounts of processing power. Performance was also restricted by the use of 16 GB of RAM, especially while backpropagating and splitting the dataset into training, validation, and testing. Therefore, every training experiment took hours to complete.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Result and discussion</title>
      <p>This section discusses the obtained results from the deployment of the developed ML-driven web platform for automated insect detection. This entails the results obtained from a comparative analysis of SVM and Inception_ResNetV2 models. Accuracy, precision, recall, F1-score, loss, latency inference time, energy consumption, computational time (non-GPU system), bandwidth, and efficiency as evaluation metrics were equally discussed. Equally, the results obtained from the comparative analysis were validated with the most recent related research.</p>
      
        <sec>
          
            <title>4.1. Globally retrieved dataset of insect pest and preprocessed dataset</title>
          
          <p>This subsection discusses the retrieved benchmark insect pest dataset as well as the communication requirement in the preprocessing of the data using bits as performance metrics.</p><p>The benchmark insect pest dataset called the IP102 Dataset has been majorly adopted by several researchers owing to its large image sizes totaling 75,000 images of 102 pest species, making it an essential resource for research in agricultural monitoring and pest management. Although this dataset's main goal is to construct a viable and solid model that accurately labels rice pests, it is essential to evaluate the dataset's data transmission, preprocessing, and performance from the communications aspects of the dataset, which adds consideration about effective and reliable use of the dataset with data integrity. The accessing and downloading of the dataset, as described in Section 3.1, allude to both understanding and encoding techniques, bandwidth limitations, data transmission. This is summarized in <xref ref-type="table" rid="table_3">Table 3</xref>.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Downloading the IP102 benchmark dataset</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>S/N</p></td><td colspan="1" rowspan="1"><p>Metrics</p></td><td colspan="1" rowspan="1"><p>Methods</p></td></tr><tr><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>Comprehension and encoding techniques</p></td><td colspan="1" rowspan="1"><p>Lossy compression (JPEG), lossless compression (PNG), LZMA (7-Zip)</p></td></tr><tr><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>Bandwidth usage</p></td><td colspan="1" rowspan="1"><p>Caching and resume support and concurrent downloads</p></td></tr><tr><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p>Data transmission protocols</p></td><td colspan="1" rowspan="1"><p>HTTPS protocols, Content Delivery Networks (CDNs), and Transport Layer Security (TLS)</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>a) Comprehension techniques</p><p>The IP102 dataset relies on the inherent structure of image files to convey information. The images in the dataset are in JPEG or PNG. Therefore, lossy and lossless compression were used, respectively, while ensuring high fidelity. To ease downloading and storage, the dataset was compressed into ZIP and TAR archive formats and facilitated downloading and storage on the computer with specifications captured in Section 3.8, employing algorithms like the Lempel–Ziv–Markov chain algorithm (LZMA) for efficient size reduction. LZMA is a lossless data compression technique that was created by Igor Pavlov. It is used in the 7z (7-Zip) compression format and is well known for its ability to achieve superior compression ratios compared to algorithms like DEFLATE.</p><p>b) Bandwidth usage</p><p>Bandwidth usage is a critical aspect when downloading large datasets like IP102. The IP102 pest dataset contains images of various pests affecting various crops, while this research focused on the pest peculiar to rice disease alone; it is paramount to download the entire dataset before extraction of the rice pest images. The IP102 dataset, with a total size of 7.8 GB, consumed network resources amidst the usage of effective compression like LZMA, reduced the size of the dataset and minimized bandwidth consumption during downloads. In addition, Kaggle supports multiple simultaneous downloads, which can optimize time but may increase overall bandwidth demands for large datasets. The caching and download resumption mechanisms inherent in Kaggle enabled the usage of download managers in downloading the dataset, thereby reducing redundant data transfers in case of network interruptions. </p><p>c) Data transmission protocols</p><p>Data transmission on Kaggle was secured through the adoption of the Hypertext Transfer Protocol Secure (HTTPS), which provides a robust method for safeguarding data in transit against unauthorized alterations, efficiently transferring either sensitive or non-sensitive data. This can mostly help speed distribution of the dataset via CDNs around the world, thus reducing latency and allowing faster data downloads for all users, regardless of location. In addition, Kaggle employs the Secure Socket Layer or Transport Layer Security to encrypt data in transit to protect against interception. Kaggle also provides Application Programming Interfaces (APIs) that allow users to download the datasets programmatically. APIs make it easier and more efficient for users to access the datasets using HTTP methods (GET and POST), enabling easier scripting and automation of processes.</p>
        </sec>
      
      
        <sec>
          
            <title>4.2. Evaluation of the models' performance</title>
          
          <p>The evaluation of Inception_ResNetV2 and SVM for effective rice insect pest classification was assessed using their computational efficiency, predictive accuracy, and communication effectiveness in real-time systems. The classification performance of these models was quantitatively assessed using standard performance metrics (accuracy, precision, recall, and F1-score), which collectively offer insights into the models’ reliability in a practical deployment setting. This subsection discusses the results using communication metrics and performance metrics.</p>
          
            <sec>
              
                <title>4.2.1 Communication performance metrics</title>
              
              <p>From the communication system’s perspective, the performance of the Inception_ResNetV2 and SVM models was analyzed using energy consumption, bandwidth efficiency, and latency, which are important when deploying these models in edge computing or Internet of Things (IoT)-based smart agriculture systems. The comparative performance of Inception_ResNetV2 and SVM using the communication performance metric is presented in <xref ref-type="table" rid="table_4">Table 4</xref>.</p>
              
                <table-wrap id="table_4">
                  <label>Table 4</label>
                  <caption>
                    <title>Comparative analysis of Inception_ResNetV2 and SVM using communication performance metrics</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="1"><p>S/N</p></td><td colspan="1" rowspan="1"><p>Performance Metrics</p></td><td colspan="1" rowspan="1"><p>Inception_ResNetV2</p></td><td colspan="1" rowspan="1"><p>SVM</p></td></tr><tr><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>Latency (inference time)</p></td><td colspan="1" rowspan="1"><p>70 ms per image</p></td><td colspan="1" rowspan="1"><p>54 ms per image</p></td></tr><tr><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>Energy consumption</p></td><td colspan="1" rowspan="1"><p><mml:math id="m4684wni0f">
  <mml:mo>∼</mml:mo>
</mml:math> 3.2 W per inference</p></td><td colspan="1" rowspan="1"><p><mml:math id="m859hke8ki">
  <mml:mo>∼</mml:mo>
</mml:math> 1.4 W per inference</p></td></tr><tr><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p>Computational time (non-GPU)</p></td><td colspan="1" rowspan="1"><p>1 hr 15 minutes</p></td><td colspan="1" rowspan="1"><p>0 hr 49 minutes</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>Bandwidth efficiency</p></td><td colspan="1" rowspan="1"><p>Higher</p></td><td colspan="1" rowspan="1"><p>High</p></td></tr></tbody></table>
                </table-wrap>
              
              <p>From <xref ref-type="table" rid="table_4">Table 4</xref>, the Inception_ResNetV2 model exhibits a higher computational cost owing to its deep architecture, and this increases energy consumption. In addition, the convolutional layers in Inception_ResNetV2 require more data transmission bandwidth but can be optimized using compressed feature maps instead of raw images. SVM, on the other hand, operates on lower-dimensional handcrafted features and is more bandwidth-efficient with lower computational time. For low-power devices like IOT edge devices, SVM may be preferable due to its lower consumption and faster inference time. However, communication parameters alone cannot be used to ascertain the best model among the two. Therefore, the models were further subjected to statistical evaluation.</p>
            </sec>
          
          
            <sec>
              
                <title>4.2.2 Performance metrics in terms of accuracy and loss</title>
              
              <p>To evaluate how well the model is learning over time, the training and validation accuracy and loss graphs of both models were generated. The training accuracy measures how well the model performs on the training dataset, whereas the validation accuracy measures how well the model generalizes to unseen validation data. These graphs plot accuracy and loss values against epochs (iterations over the entire dataset) to visualize the model’s performance, as seen in <xref ref-type="table" rid="table_5">Table 5</xref> and <xref ref-type="fig" rid="fig_5">Figure 5</xref> and <xref ref-type="fig" rid="fig_6">Figure 6</xref>.</p>
              
                <table-wrap id="table_5">
                  <label>Table 5</label>
                  <caption>
                    <title>Training and validation accuracy and loss comparative analysis of SVM and Inception_ResNetV2</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="2"><p>Metrics</p></td><td colspan="2" rowspan="1"><p>SVM</p></td><td colspan="2" rowspan="1"><p>Inception_ResNetV2</p></td></tr><tr><td colspan="1" rowspan="1"><p>Training (%)</p></td><td colspan="1" rowspan="1"><p>Validation (%)</p></td><td colspan="1" rowspan="1"><p>Training (%)</p></td><td colspan="1" rowspan="1"><p>Validation (%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>99.64</p></td><td colspan="1" rowspan="1"><p>99.97</p></td><td colspan="1" rowspan="1"><p>99.97</p></td><td colspan="1" rowspan="1"><p>86.47</p></td></tr><tr><td colspan="1" rowspan="1"><p>Loss</p></td><td colspan="1" rowspan="1"><p>0.0237</p></td><td colspan="1" rowspan="1"><p>0.0098</p></td><td colspan="1" rowspan="1"><p>0.0098</p></td><td colspan="1" rowspan="1"><p>0.8360</p></td></tr></tbody></table>
                </table-wrap>
              
              
                <fig id="fig_5">
                  <label>Figure 5</label>
                  <caption>
                    <title>Training and validation loss and accuracy of SVM</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_9f1e47NE_7gZjaeS.png"/>
                </fig>
              
              
                <fig id="fig_6">
                  <label>Figure 6</label>
                  <caption>
                    <title>Training and validation loss and accuracy of Inception_ResNetV2</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_RhihjBhuT16MmY3R.png"/>
                </fig>
              
              <p>As stated in <xref ref-type="table" rid="table_2">Table 2</xref>, the epoch was set at 50. But during the model training phase, it automatically terminated at Epoch 10 and Epoch 16, as shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref> and <xref ref-type="fig" rid="fig_6">Figure 6</xref>, respectively. The reason hinged on the early stop mechanism introduced into the model. This helps the model to overcome overfitting, early convergence, and computational efficiency; hence, the model is prevented from memorizing noise and unnecessary details in the dataset and terminates when no further improvements are observed, leading to an effective reduction in the number of epochs. It is seen that the training and validation loss decreases and terminates at a low-value rate in both models. While the training loss terminates at 0.0237, the validation loss, on the other hand, terminates at 0.8360 in the SVM model, and the training and validation loss of Inception_ResNetV2 terminate at 0.0098 and 0.7250, respectively. Inception_ResNetV2 achieves a training accuracy of 99.97 and a validation accuracy of 88.25, while the SVM achieves a training accuracy of 99.64 and a validation accuracy of 86.47. A balance of bias and variance can be seen as the gap between the training and validation performance indicates a well-generalized model.</p><p>To ascertain the robustness and effectiveness of the models, the accuracy, precision, recall, and F1-score were used to evaluate the performances of the models. To effectively evaluate the percentage improvement of the models, Eq. (13) was used.</p>
              
                <disp-formula>
                  <label>(13)</label>
                  <mml:math id="mfzka6n56t">
                    <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                      <mml:mtr>
                        <mml:mtd>
                          <mml:mi>%</mml:mi>
                          <mml:mtext>Improvement</mml:mtext>
                          <mml:mo>=</mml:mo>
                          <mml:mo>×</mml:mo>
                          <mml:mfrac>
                            <mml:mtext>ModelAP-ModelBP</mml:mtext>
                            <mml:mtext>ModelAP</mml:mtext>
                          </mml:mfrac>
                          <mml:mn>100</mml:mn>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:math>
                </disp-formula>
              
              <p>The results obtained from the simulations of the two models are presented in <xref ref-type="table" rid="table_6">Table 6</xref>.</p>
              
                <table-wrap id="table_6">
                  <label>Table 6</label>
                  <caption>
                    <title>Comparative results between Inception_ResNetV2 and SVM</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="1"><p>Models</p></td><td colspan="1" rowspan="1"><p>Accuracy (%)</p></td><td colspan="1" rowspan="1"><p>Precision (%)</p></td><td colspan="1" rowspan="1"><p>Recall (%)</p></td><td colspan="1" rowspan="1"><p>F1-Score (%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Inception_ResNetV2</p></td><td colspan="1" rowspan="1"><p>99.97</p></td><td colspan="1" rowspan="1"><p>99.46</p></td><td colspan="1" rowspan="1"><p>99.81</p></td><td colspan="1" rowspan="1"><p>99.53</p></td></tr><tr><td colspan="1" rowspan="1"><p>SVM</p></td><td colspan="1" rowspan="1"><p>99.64</p></td><td colspan="1" rowspan="1"><p>99.34</p></td><td colspan="1" rowspan="1"><p>99.40</p></td><td colspan="1" rowspan="1"><p>99.37</p></td></tr><tr><td colspan="1" rowspan="1"><p>%Improvement</p></td><td colspan="1" rowspan="1"><p>0.33</p></td><td colspan="1" rowspan="1"><p>0.12</p></td><td colspan="1" rowspan="1"><p>0.41</p></td><td colspan="1" rowspan="1"><p>0.16</p></td></tr></tbody></table>
                </table-wrap>
              
              <p>From <xref ref-type="table" rid="table_6">Table 6</xref>, it is seen that Inception_ResNetV2 outperforms SVM with an improvement of 0.33%, 0.12%, 0.41%, and 0.16% in accuracy, precision, recall, and F1-score, respectively. This shows that Inception_ResNetV2 demonstrates its capability to learn complex hierarchical patterns in pest images and ensure that fewer pest cases go undetected, which is critical in real-time pest monitoring and decision support systems. In addition, the result demonstrates a balanced trade-off between precision and recall. For an effective real-time and high-accuracy pest classification system, Inception_ResNetV2 is the superior choice due to its higher recall and precision. However, in low-resource settings, like IoT edge devices, SVM may be preferable due to its lower power consumption and faster inference time. Hence, the choice of the best algorithm hinges on the trade-off and the expectation of the system. Inception_ResNetV2 was adopted in this research owing to its higher accuracy and detection rate, which are critical for the agricultural sector to ease farmers in early detection of diseases for effective measures. Inception_ResNetV2’s performance was further evaluated using the confusion matrix to compare the predicted pest classes with the actual ones. The result obtained is presented in <xref ref-type="fig" rid="fig_7">Figure 7</xref>.</p>
              
                <fig id="fig_7">
                  <label>Figure 7</label>
                  <caption>
                    <title>Inception_ResNetV2 confusion matrix</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_R5stKiJxuXfcI7ka.png"/>
                </fig>
              
              <p>From <xref ref-type="fig" rid="fig_7">Figure 7</xref>, the confusion matrix reveals key misclassifications among two rice pests like paddy stem maggot and rice leaf caterpillar, and such errors can lead to incorrect pest control measures, impacting crop yield and increasing costs. Enhancing classification accuracy is crucial for effective and targeted pest management.</p><p>As for both models, the ultimate goal is to apply them in practical applications like autonomous pest monitoring systems. Real-time communication in this scenario is extremely critical, as the image data needs to be sent in real time from sensors (drones, cameras, etc.) to the model on a processing unit (e.g., server or edge device). This is an instance of a communication system in which the “message” (in this example, the identification of the pest) is communicated from the sensors to the model, and the model responds with a classification or a suggestion back to the user or control system.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>4.3. Deployment of an inception_resnetv2 model on a web platform using streamlit</title>
          
          <p>The Inception_ResNetV2 model was adopted due to its superior performance and was deployed in Python IDLE using Streamlit, serving as a bridge between ML, web technologies, and real-time communication systems. This allowed a high-performance DL model with an interactive and accessible web application for real-time insect pest classification, making it usable for agricultural stakeholders. To deploy the server online, the command “streamlit run $&lt;<inline-formula>
  <mml:math id="m70bdpnjwo">
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>&gt;<inline-formula>
  <mml:math id="mxiw3hesk0">
    <mml:mo>”</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>&lt;<inline-formula>
  <mml:math id="m0n9odtjos">
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>&gt;$ refers to the default model. The developed model was named Inception_ResNetV2PestClassifier.py; thus, the Streamlit run Inception_ResNetV2PestClassifier.py was used to start the server, as seen in <xref ref-type="fig" rid="fig_8">Figure 8</xref>.</p>
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>Starting Streamlit</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_bNbuMYuppX7FkIMM.png"/>
            </fig>
          
          <p>The developed web application can be accessed either locally or remotely through any installed browser on the host and client computers. The host system allows access through the URL http://localhost:8501, while the connected client system uses the network URL http://192.168.44.167:8501, as shown in <xref ref-type="fig" rid="fig_8">Figure 8</xref>. The user interface of the developed model can be accessed through any web browser by entering the server address. Upon establishing a successful connection, users are directed to an intuitive and user-friendly interface, as illustrated in <xref ref-type="fig" rid="fig_9">Figure 9</xref>.</p>
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>
                <title>Developed user-friendly interface</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_eoWO02LdDz2GoG8H.png"/>
            </fig>
          
          <p>The user-friendly interface allows farmers, researchers, and agronomists to upload images of insect pests as well as real-time pest image acquisition using live camera input support, as presented in <xref ref-type="fig" rid="fig_9">Figure 9</xref>. The browse files button allows the user to navigate to the directory where the pest image is, while the drag-and-drop option allows the user to drag the image directly into the file field. The loaded image is visualized on the browser, as shown in <xref ref-type="fig" rid="fig_10">Figure 10</xref>.</p>
          
            <fig id="fig_10">
              <label>Figure 10</label>
              <caption>
                <title>Verifying a pest image</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_QfzIg1BTpk5qk82R.png"/>
            </fig>
          
          <p>In <xref ref-type="fig" rid="fig_10">Figure 10</xref>, the pest image is successfully uploaded and processed through the user interface. Once the image is loaded, the system activates the pre-trained model, saved in the .pkl file format, to analyze and predict the pest in the uploaded image. This cooperative coordination of the user interface and model allows effective and accurate detection and identification of the pests in order to enable farmers to get real-time and meaningful information for effective pest management. The prediction output is transmitted from the model to the user interface, as indicated in <xref ref-type="fig" rid="fig_11">Figure 11</xref>.</p>
          
            <fig id="fig_11">
              <label>Figure 11</label>
              <caption>
                <title>Pest prediction</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_hvD2hXStK_jQOw5X.png"/>
            </fig>
          
          <p>As illustrated in <xref ref-type="fig" rid="fig_11">Figure 11</xref>, the visualization dashboard presents the predicted pest as a rice leaf caterpillar, with a confidence level of 99.50%, a response time of 5.70 seconds, a data rate of 8.48 KB/s, and a file size of 48.38 KB. At the sight of the specific pest, the farmers are able to take appropriate and immediate actions. The rice leaf caterpillar feeds on rice leaves, which may lead to decreased photosynthesis and yield if uncontrolled. This means, with a confidence level of 99.50%, that the model is very sure of the fact that the pest in question is the rice leaf caterpillar. This represents the model's confidence in making the prediction. Thus, the high confidence ensures that the farmers believe the prediction and follow it accordingly without doubting the result. The particular information provides farmers with valuable knowledge regarding the specific pest affecting their crops and enables them to know the magnitude of damage that it can cause to their farms. By providing accurate and timely predictions, the dashboard allows farmers to make informed decisions and adopt the most effective pest control measures unique to the outlined pest, hence minimizing crop loss and optimizing agricultural output. Further analysis was performed to predict the remaining twelve classes. The findings obtained are presented in <xref ref-type="table" rid="table_7">Table 7</xref>.</p>
          
            <table-wrap id="table_7">
              <label>Table 7</label>
              <caption>
                <title>Developed system performance</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>S/N</p></td><td colspan="1" rowspan="1"><p>Pest Name</p></td><td colspan="1" rowspan="1"><p>Data Rate (KB/s)</p></td><td colspan="1" rowspan="1"><p>File Size (KB)</p></td><td colspan="1" rowspan="1"><p>Response Time (seconds)</p></td><td colspan="1" rowspan="1"><p>Confidence</p><p>(%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>Rice leaf roller</p></td><td colspan="1" rowspan="1"><p>7.21</p></td><td colspan="1" rowspan="1"><p>43.25</p></td><td colspan="1" rowspan="1"><p>5.57</p></td><td colspan="1" rowspan="1"><p>99.89</p></td></tr><tr><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>Rice leaf caterpiller</p></td><td colspan="1" rowspan="1"><p>8.48</p></td><td colspan="1" rowspan="1"><p>48.38</p></td><td colspan="1" rowspan="1"><p>5.70</p></td><td colspan="1" rowspan="1"><p>99.50</p></td></tr><tr><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p>Paddy stem maggot</p></td><td colspan="1" rowspan="1"><p>9.56</p></td><td colspan="1" rowspan="1"><p>51.27</p></td><td colspan="1" rowspan="1"><p>6.23</p></td><td colspan="1" rowspan="1"><p>99.72</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>Asiatic rice borer</p></td><td colspan="1" rowspan="1"><p>5.10</p></td><td colspan="1" rowspan="1"><p>40.72</p></td><td colspan="1" rowspan="1"><p>5.21</p></td><td colspan="1" rowspan="1"><p>99.45</p></td></tr><tr><td colspan="1" rowspan="1"><p>5</p></td><td colspan="1" rowspan="1"><p>Yellow rice borer</p></td><td colspan="1" rowspan="1"><p>4.98</p></td><td colspan="1" rowspan="1"><p>37.88</p></td><td colspan="1" rowspan="1"><p>5.04</p></td><td colspan="1" rowspan="1"><p>99.23</p></td></tr><tr><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p>Rice gall midge</p></td><td colspan="1" rowspan="1"><p>9.34</p></td><td colspan="1" rowspan="1"><p>67.34</p></td><td colspan="1" rowspan="1"><p>9.45</p></td><td colspan="1" rowspan="1"><p>99.67</p></td></tr><tr><td colspan="1" rowspan="1"><p>7</p></td><td colspan="1" rowspan="1"><p>Brown planthopper</p></td><td colspan="1" rowspan="1"><p>10.07</p></td><td colspan="1" rowspan="1"><p>80.04</p></td><td colspan="1" rowspan="1"><p>98.83</p></td><td colspan="1" rowspan="1"><p>98.12</p></td></tr><tr><td colspan="1" rowspan="1"><p>8</p></td><td colspan="1" rowspan="1"><p>Rice stem fly</p></td><td colspan="1" rowspan="1"><p>09.13</p></td><td colspan="1" rowspan="1"><p>70.30</p></td><td colspan="1" rowspan="1"><p>99.02</p></td><td colspan="1" rowspan="1"><p>89.45</p></td></tr><tr><td colspan="1" rowspan="1"><p>9</p></td><td colspan="1" rowspan="1"><p>Rice water weevil</p></td><td colspan="1" rowspan="1"><p>06.59</p></td><td colspan="1" rowspan="1"><p>74.41</p></td><td colspan="1" rowspan="1"><p>98.67</p></td><td colspan="1" rowspan="1"><p>99.49</p></td></tr><tr><td colspan="1" rowspan="1"><p>10</p></td><td colspan="1" rowspan="1"><p>Rice leaf hopper</p></td><td colspan="1" rowspan="1"><p>06.8</p></td><td colspan="1" rowspan="1"><p>65.80</p></td><td colspan="1" rowspan="1"><p>98.89</p></td><td colspan="1" rowspan="1"><p>99.12</p></td></tr><tr><td colspan="1" rowspan="1"><p>11</p></td><td colspan="1" rowspan="1"><p>Rice shell pest</p></td><td colspan="1" rowspan="1"><p>08.60</p></td><td colspan="1" rowspan="1"><p>55.07</p></td><td colspan="1" rowspan="1"><p>97.46</p></td><td colspan="1" rowspan="1"><p>98.67</p></td></tr><tr><td colspan="1" rowspan="1"><p>12</p></td><td colspan="1" rowspan="1"><p>Thrips</p></td><td colspan="1" rowspan="1"><p>07.44</p></td><td colspan="1" rowspan="1"><p>53.37</p></td><td colspan="1" rowspan="1"><p>96.23</p></td><td colspan="1" rowspan="1"><p>98.50</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>From the evaluation of the developed system, the proposed system has a higher confidence level in the effective classification of the pest images into their respective classes at a shorter data rate and response time.</p>
        </sec>
      
      
        <sec>
          
            <title>4.4. Comparative analysis with existing models and discussion</title>
          
          <p>This section provides the comparative analysis of the deployed model with the closest related work in precision, recall, accuracy, F1-score, and deployment mode. </p>
          
            <table-wrap id="table_8">
              <label>Table 8</label>
              <caption>
                <title>Comparative analysis of the developed model with existing models</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Reference</p></td><td colspan="1" rowspan="1"><p>Classifier</p></td><td colspan="1" rowspan="1"><p>Dataset Size</p></td><td colspan="1" rowspan="1" colwidth="155"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>Precision</p></td><td colspan="1" rowspan="1"><p>Recall</p></td><td colspan="1" rowspan="1"><p>F1-Score</p></td><td colspan="1" rowspan="1"><p>Deployment</p></td></tr><tr><td colspan="1" rowspan="1"><p>[<xref ref-type="bibr" rid="ref_18">18</xref>]</p></td><td colspan="1" rowspan="1"><p>FVBC</p></td><td colspan="1" rowspan="1"><p>2627</p></td><td colspan="1" rowspan="1" colwidth="155"><p>97.60%</p></td><td colspan="1" rowspan="1"><p>97.85%</p></td><td colspan="1" rowspan="1"><p>97.70%</p></td><td colspan="1" rowspan="1"><p>97.78%</p></td><td colspan="1" rowspan="1"><p>No</p></td></tr><tr><td colspan="1" rowspan="1"><p>[<xref ref-type="bibr" rid="ref_23">23</xref>]</p></td><td colspan="1" rowspan="1"><p>AlexNet</p></td><td colspan="1" rowspan="1"><p>1600</p></td><td colspan="1" rowspan="1" colwidth="155"><p>94%</p></td><td colspan="1" rowspan="1"><p>94%</p></td><td colspan="1" rowspan="1"><p>94%</p></td><td colspan="1" rowspan="1"><p>94%</p></td><td colspan="1" rowspan="1"><p>Mobile application</p></td></tr><tr><td colspan="1" rowspan="1"><p>Developed</p><p>Model</p></td><td colspan="1" rowspan="1"><p>Inception_ResNetV2</p></td><td colspan="1" rowspan="1"><p>7736</p></td><td colspan="1" rowspan="1" colwidth="155"><p>99.97%</p></td><td colspan="1" rowspan="1"><p>99.46%</p></td><td colspan="1" rowspan="1"><p>99.81%</p></td><td colspan="1" rowspan="1"><p>99.53%</p></td><td colspan="1" rowspan="1"><p>Web application</p></td></tr></tbody></table>
            </table-wrap>
          
          <p><xref ref-type="table" rid="table_8">Table 8</xref> presents a comparison of the proposed model with other models, showing significant enhancement in most of the evaluation metrics and deployment factors. The proposed model with the Inception_ResNetV2 classifier was trained on 7,736 images and achieved a remarkable accuracy of 99.97%. This is a very big improvement compared to AlexNet, which has 94% accuracy, and the FVBC, which has 97.60% accuracy. In addition, the model outperforms FVBC with +1.61% precision, +2.11% recall, and +1.75% F1-score. It also outperforms AlexNet with +5.46% precision, +5.81% recall, and +5.53% F1-score. These improvements suggest that the model can reduce false positives and negatives, thus ensuring the reliability of pest management. The improved precision, recall, and F1-score have direct real-world applications for pest management. Therefore, high precision reduces false positives, enabling crops not to be misclassified as infected, thus saving farmers time and resources. High recall ensures most pest infestations are detected, precluding late-stage damage from concealed pests. The enhanced F1-score, balancing precision with recall, ensures enhanced and more reliable pest detection regardless of the conditions. Moreover, the deployment of the developed model as a web application offers farmers increased accessibility and ease of use over previous models not deployed or limited to mobile applications. This online deployment enables real-time monitoring of pests in multiple agricultural setups. The larger training set of 7,736 samples also makes the model more robust and generalizable, thus enabling it to handle changing environmental conditions and pest appearance more efficiently.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>This research presents an improved ML-driven web platform for automated rice pest identification. It is necessary to resolve the inherent issue of the traditional approach in rice pest detection as well as the limitations of the classical ML algorithms and DL models in the effective detection of rice pests. No doubt that the traditional approach of rice pest detection and diagnosis entails the evaluation of pests and their effect on the farmland rate by an expert for effective pest control and removal. This approach is time-consuming, and an inaccurate understanding of some pest types can lead to poor control strategies. To alleviate this problem, modern technologies such as cloud computing, IOT, and artificial intelligence are integrated into agricultural practices to enhance crop monitoring for precise personality management, intelligent production control, quantitative decision-making, and effective crop harvesting. This research aims to improve the classification of insect pests in order to improve the performance of the training outcome. The collection of rice pest datasets and image preprocessing were first discussed. The rice pest images were globally retrieved from a benchmark insect pest dataset known as IP102. The IP102 dataset contains 102 classes of different agricultural insect pests with over 77,000 images. For the purpose of this study, 12 classes of the insect pest with about 7,736, which were peculiar to rice, were extracted from the IP102 dataset and were preprocessed to remove blur, corrupt, and incomplete images and resize images to suit the architectures of the models considered. Two ML models were considered for this research, namely, SVM and inception_ResNetv2, which were configured from scratch with a learning rate of 0.0001 and trained with 50 epochs to accurately classify the rice insect pest. The performance of the aforementioned ML models was documented. Judged by using the accuracy, precision, recall, and F1-score, inception_ResNetv2 was at the top of the performance metrics as it achieved an accuracy of 99.97%, a precision of 99.46%, a recall of 99.81%, and an F1-score of 99.53%. Finally, based on performance, inception_ResNetv2 was deployed into a web application. The outcome of this research highlights the potential of machines in the field of agriculture, particularly in reducing the reliance on traditional methods and mitigating the loss associated with rice farming, thereby improving pest detection, leading to higher crop yields, and strengthening food security.</p><p>Throughout the research, a number of limitations were found. The primary constraint is that deployment is currently limited to a web platform, which may be inaccessible to users in rural or remote regions with unstable internet connectivity. For future research, a hybrid deployment strategy is recommended—incorporating both web and mobile applications. A mobile app with offline functionality would enhance accessibility, allowing farmers to use the pest identification system without requiring a constant internet connection. This hybrid system could further integrate with other agricultural tools already present on farmers’ devices, although challenges such as model compression, cross-platform optimization, and offline capability must be addressed through further development. Another limitation is the extended training time for the Inception_ResNetV2 model, primarily due to the lack of a dedicated GPU suitable for DL applications. Future work should utilize systems with high-performance GPUs and apply optimization methods such as mixed-precision training to reduce computational demands and enhance training efficiency. To improve the suitability of the model for real-time applications, future work should also focus on optimizing the Inception_ResNetV2 architecture. Techniques such as model pruning, quantization, and knowledge distillation can significantly reduce energy consumption and inference latency while preserving model performance. Additionally, the scalability and long-term maintenance of the web platform were not addressed in this study. Future research should explore strategies for scaling the platform to accommodate increasing user demand and maintaining model accuracy over time. This includes implementing mechanisms for periodic model retraining with newly collected data, and continuously updating the platform to integrate emerging technologies and ensure its relevance and effectiveness in real-world farming environments.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The image dataset supporting our research results are deposited in Kaggle, which does not issue DOIs. The data can be accessed at https://www.kaggle.com/datasets/hungt1/ip102-dataset.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>1065</page-range>
          <issue>7</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Wei</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Teng Fi</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Xiao Yu</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>Jian Zhang</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Jun</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/agriculture12071065</pub-id>
          <article-title>Recommending advanced deep learning models for efficient insect pest detection</article-title>
          <source>Agriculture</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>73019-73032</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ullah</surname>
              <given-names>Nadeem</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>Junaid A.</given-names>
            </name>
            <name>
              <surname>Alharbi</surname>
              <given-names>Laila A.</given-names>
            </name>
            <name>
              <surname>Raza</surname>
              <given-names>Ahmad</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>Waqar</given-names>
            </name>
            <name>
              <surname>Ahmad</surname>
              <given-names>Irfan</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2022.3189676</pub-id>
          <article-title>An efficient approach for crops pests recognition and classification based on novel DeepPestNet deep learning model</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>741</page-range>
          <issue>6</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mohidem</surname>
              <given-names>Nur Atikah</given-names>
            </name>
            <name>
              <surname>Hashim</surname>
              <given-names>Norhashila</given-names>
            </name>
            <name>
              <surname>Shamsudin</surname>
              <given-names>Rosnah</given-names>
            </name>
            <name>
              <surname>Che Man</surname>
              <given-names>Hasfalina</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/agriculture12060741</pub-id>
          <article-title>Rice for food security: Revisiting its production, diversity, rice milling process and nutrient content</article-title>
          <source>Agriculture</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>24</volume>
          <page-range>386-394</page-range>
          <issue>4</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ali</surname>
              <given-names>M. Abbas</given-names>
            </name>
            <name>
              <surname>Abdellah</surname>
              <given-names>I. M.</given-names>
            </name>
            <name>
              <surname>Eletmany</surname>
              <given-names>M. R.</given-names>
            </name>
          </person-group>
          <article-title>Towards sustainable management of insect pests: Protecting food security through ecological intensification</article-title>
          <source>Int. J. Chem. Biochem. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>138-145</page-range>
          <issue>2</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bashir</surname>
              <given-names>Yahaya</given-names>
            </name>
            <name>
              <surname>Sani</surname>
              <given-names>Zainab</given-names>
            </name>
            <name>
              <surname>Ashafa</surname>
              <given-names>Abubakar</given-names>
            </name>
            <name>
              <surname>Mijinyawa</surname>
              <given-names>Abubakar</given-names>
            </name>
            <name>
              <surname>Sufiyanu</surname>
              <given-names>Sani</given-names>
            </name>
            <name>
              <surname>Abdulazeez</surname>
              <given-names>Umar</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.57233/ijsgs.v10i2.656</pub-id>
          <article-title>Morphological, biochemical and molecular characterization of the causal agent of bacterial panicle blight disease of rice (BPB) in selected rice production zones of Zamfara State</article-title>
          <source>Int. J. Sci. Glob. Sustain.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>e1384</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Ke</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Xiao</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Bing Kai</given-names>
            </name>
            <name>
              <surname>Ge</surname>
              <given-names>Cheng Xin</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>You Hua</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Li</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.7717/peerj-cs.1384</pub-id>
          <article-title>Diagnosis and application of rice diseases based on deep learning</article-title>
          <source>PeerJ Comput. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>328-341</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Doan</surname>
              <given-names>T. N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.12720/jait.14.2.328-341</pub-id>
          <article-title>Large-scale insect pest image classification</article-title>
          <source>J. Adv. Inf. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>184-199</page-range>
          <issue>3</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mommoh</surname>
              <given-names>Joshua S.</given-names>
            </name>
            <name>
              <surname>Obetta</surname>
              <given-names>James L.</given-names>
            </name>
            <name>
              <surname>John</surname>
              <given-names>Samuel N.</given-names>
            </name>
            <name>
              <surname>Okokpujie</surname>
              <given-names>Kennedy</given-names>
            </name>
            <name>
              <surname>Omoruyi</surname>
              <given-names>Osemwegie N.</given-names>
            </name>
            <name>
              <surname>Awelewa</surname>
              <given-names>Ayokunle A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/ida030304</pub-id>
          <article-title>Detection of fruit ripeness and defectiveness using convolutional neural networks</article-title>
          <source>Inf. Dyn. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>100431</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mandal</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Yadav</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Panme</surname>
              <given-names>F. A.</given-names>
            </name>
            <name>
              <surname>Devi</surname>
              <given-names>K. M.</given-names>
            </name>
            <name>
              <surname>Kumar</surname>
              <given-names>S. M. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.atech.2024.100431</pub-id>
          <article-title>Adaption of smart applications in agriculture to enhance production</article-title>
          <source>Smart Agric. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>377</page-range>
          <issue>4</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Botero-Valencia</surname>
              <given-names>Juan</given-names>
            </name>
            <name>
              <surname>García-Pineda</surname>
              <given-names>Vanessa</given-names>
            </name>
            <name>
              <surname>Valencia-Arias</surname>
              <given-names>Alejandro</given-names>
            </name>
            <name>
              <surname>Valencia</surname>
              <given-names>Jackeline</given-names>
            </name>
            <name>
              <surname>Reyes-Vera</surname>
              <given-names>Erick</given-names>
            </name>
            <name>
              <surname>Mejia-Herrera</surname>
              <given-names>Mateo</given-names>
            </name>
            <name>
              <surname>Hernández-García</surname>
              <given-names>Ruber</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/agriculture15040377</pub-id>
          <article-title>Machine learning in sustainable agriculture: Systematic review and research perspectives</article-title>
          <source>Agriculture</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>100416</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mana</surname>
              <given-names>A. A.</given-names>
            </name>
            <name>
              <surname>Allouhi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Hamrani</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Rehman</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Jamaoui</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Jayachandran</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.atech.2024.100416</pub-id>
          <article-title>Sustainable AI-based production agriculture: Exploring AI applications and implications in agricultural practices</article-title>
          <source>Smart Agric. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-5</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sri</surname>
              <given-names>M. K.</given-names>
            </name>
            <name>
              <surname>Saikrishna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Kumar</surname>
              <given-names>V. V.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.2139/ssrn.3558355</pub-id>
          <article-title>Classification of ripening of banana fruit using convolutional neural networks</article-title>
          <source>Proceedings of the 4th International Conference: Innovative Advancement in Engineering &amp; Technology (IAET), Rajasthan, India</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>230</volume>
          <page-range>109938</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Castillo-Girones</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Munera</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Martínez-Sober</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Blasco</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Cubero</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Gómez-Sanchis</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compag.2025.109938</pub-id>
          <article-title>Artificial neural networks in agriculture, the core of artificial intelligence: What, when, and why</article-title>
          <source>Comput. Electron. Agric.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>57</volume>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>El Sakka</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Mothe</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Ivanovici</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/22797254.2024.2352386</pub-id>
          <article-title>Images and CNN applications in smart agriculture</article-title>
          <source>Eur. J. Remote Sens.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>540</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Albahar</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/agriculture13030540</pub-id>
          <article-title>Survey on deep learning and its impact on agriculture: Challenges and opportunities</article-title>
          <source>Agriculture</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Deng</surname>
              <given-names>Ruo Ling</given-names>
            </name>
            <name>
              <surname>Tao</surname>
              <given-names>Ming</given-names>
            </name>
            <name>
              <surname>Xing</surname>
              <given-names>Hang</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Xiu Li</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Chuang</given-names>
            </name>
            <name>
              <surname>Liao</surname>
              <given-names>Kai Feng</given-names>
            </name>
            <name>
              <surname>Qi</surname>
              <given-names>Long</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fpls.2021.701038</pub-id>
          <article-title>Automatic diagnosis of rice diseases using deep learning</article-title>
          <source>Front. Plant Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-5</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bhartiya</surname>
              <given-names>V. P.</given-names>
            </name>
            <name>
              <surname>Janghel</surname>
              <given-names>R. R.</given-names>
            </name>
            <name>
              <surname>Rathore</surname>
              <given-names>Y. K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/icpc2t53885.2022.9776692</pub-id>
          <article-title>Rice leaf disease prediction using machine learning</article-title>
          <source>2022 Second International Conference on Power, Control and Computing Technologies (ICPC2T), Raipur, India</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <issue>8692</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Naresh Kumar</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Sakthivel</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-025-87800-3</pub-id>
          <article-title>Rice leaf disease classification using a fusion vision approach</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>2139</page-range>
          <issue>8</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Deng</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Lei</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Ye</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zeng</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/agronomy13082139</pub-id>
          <article-title>Deep-learning-based rice disease and insect pest detection on a mobile phone</article-title>
          <source>Agronomy</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>179</volume>
          <page-range>105809</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ayan</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Erbay</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Varçın</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compag.2020.105809</pub-id>
          <article-title>Crop pest classification with a genetic algorithm-based weighted ensemble of deep convolutional neural networks</article-title>
          <source>Comput. Electron. Agric.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>20</volume>
          <page-range>321-328</page-range>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pattnaik</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Parvathy</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.12928/TELKOMNIKA.v20i2.19740</pub-id>
          <article-title>Machine learning-based approaches for tomato pest classification</article-title>
          <source>TELKOMNIKA Telecommun. Comput. Electron. Control</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>446-457</page-range>
          <issue>3</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kasinathan</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Singaraju</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Uyyala</surname>
              <given-names>S. R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.inpa.2020.09.006</pub-id>
          <article-title>Insect classification and detection in field crops using modern machine learning techniques</article-title>
          <source>Inf. Process. Agric.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>1279-1285</page-range>
          <issue>5</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Adi</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Widodo</surname>
              <given-names>C. E.</given-names>
            </name>
            <name>
              <surname>Widodo</surname>
              <given-names>A. P.</given-names>
            </name>
            <name>
              <surname>Setiadi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Setiawan</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/mmep.110517</pub-id>
          <article-title>Identification of rice plant diseases using convolutional neural network method</article-title>
          <source>Math. Model. Eng. Probl.</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>218</volume>
          <page-range>2026-2040</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Singh</surname>
              <given-names>S. P.</given-names>
            </name>
            <name>
              <surname>Pritamdas</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Devi</surname>
              <given-names>K. J.</given-names>
            </name>
            <name>
              <surname>Devi</surname>
              <given-names>S. D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.procs.2023.01.179</pub-id>
          <article-title>Custom convolutional neural network for detection and classification of rice plant diseases</article-title>
          <source>Procedia Comput. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-6</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Nayem</surname>
              <given-names>Z. H.</given-names>
            </name>
            <name>
              <surname>Jahan</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Rakib</surname>
              <given-names>A. A.</given-names>
            </name>
            <name>
              <surname>Mia</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/iccece51049.2023.10084936</pub-id>
          <article-title>Detection and identification of rice pests using memory efficient convolutional neural network</article-title>
          <source>2023 International Conference on Computer, Electrical &amp; Communication Engineering (ICCECE), Kolkata, India</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="webpage">
          <article-title>IP102 dataset</article-title>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>2020</volume>
          <page-range>1-14</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>X. C.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Q. Q.</given-names>
            </name>
            <name>
              <surname>Fang</surname>
              <given-names>N. R.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>P. C.</given-names>
            </name>
            <name>
              <surname>Ding</surname>
              <given-names>L. T.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>S. Q.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2020/7534970</pub-id>
          <article-title>A hybrid model for prediction in asphalt pavement performance based on support vector machine and grey relation analysis</article-title>
          <source>J. Adv. Transp.</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>119-128</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Okokpujie</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Imhade  Okokpujie</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Odumuyiwa  Ayomikun</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Abidemi  Orimogunje</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Adebayo  Ogundipe</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/mmep.100113</pub-id>
          <article-title>Development of a web and mobile applications-based cassava disease classification interface using convolutional neural network</article-title>
          <source>Math. Model. Eng. Probl.</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Okokpujie</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>John</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Ndujiuba</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Noma-Osaghae</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Development of an adaptive trait-aging invariant face recognition system using convolutional neural networks</article-title>
          <source>Information Science and Applications, Lecture Notes in Electrical Engineering</source>
          <publisher-name>Springer, Singapore</publisher-name>
          <year>2020</year>
          <pub-id pub-id-type="doi">10.1007/978-981-15-1465-4_41</pub-id>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>