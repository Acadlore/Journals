<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-t7yyHYdGGUYhAROfOjSaEXOVGMmqpFXy</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml050102</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Decision-Level Multimodal Fusion for Non-Invasive Diagnosis of Endometriosis: Strategies, Calibration, and Net Clinical Benefit</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-7197-2615</contrib-id>
          <name>
            <surname>Fatade</surname>
            <given-names>Oluwayemisi B.</given-names>
          </name>
          <email>fatadeo@babcock.edu.ng</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0008-2693-0851</contrib-id>
          <name>
            <surname>Ajiboye</surname>
            <given-names>Oyebimpe F.</given-names>
          </name>
          <email>bfajiboye@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5794-3657</contrib-id>
          <name>
            <surname>Sanusi</surname>
            <given-names>Funmilayo A.</given-names>
          </name>
          <email>sanusifu@babcock.edu.ng</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0944-1497</contrib-id>
          <name>
            <surname>Okesola</surname>
            <given-names>Kikelomo I.</given-names>
          </name>
          <email>okesolak@babcock.edu.ng</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0006-0216-6911</contrib-id>
          <name>
            <surname>Okorie</surname>
            <given-names>Grace C.</given-names>
          </name>
          <email>nenyeokorie90@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-4825-4936</contrib-id>
          <name>
            <surname>Opateye</surname>
            <given-names>Goodness O.</given-names>
          </name>
          <email>opateye0426@pg.babcock.edu.ng</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-7129-4899</contrib-id>
          <name>
            <surname>Famodimu</surname>
            <given-names>Oluwasefunmi B.</given-names>
          </name>
          <email>famodimuo@babcock.edu.ng</email>
        </contrib>
        <aff id="aff_1">Department of Computer Science, Babcock University, 121003 Ilishan Remo, Nigeria</aff>
        <aff id="aff_2">Wosler Diagnostics, T2H 2B2 Calgary, Canada</aff>
        <aff id="aff_3">Department of Software Engineering, Babcock University, 121003 Ilishan Remo, Nigeria</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>18</day>
        <month>01</month>
        <year>2026</year>
      </pub-date>
      <volume>5</volume>
      <issue>1</issue>
      <fpage>11</fpage>
      <lpage>19</lpage>
      <page-range>11-19</page-range>
      <history>
        <date date-type="received">
          <day>04</day>
          <month>11</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>07</day>
          <month>01</month>
          <year>2026</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2026 by the author(s)</copyright-statement>
        <copyright-year>2026</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Endometriosis remains underdiagnosed due to reliance on invasive laparoscopy. Artificial Intelligence (AI) using either imaging or structured clinical data have shown promise, but single modality approaches face limitations in sensitivity, calibration, and clinical reliability. This work seeks to evaluate whether decision-level multimodal fusion of Magnetic Resonance Imaging (MRI)-based and clinical data-based AI systems improves diagnostic performance, calibration, and net clinical benefit, compared with single-modality models. Two previously validated models were combined with retrospective data from 1,208 patients with suspected endometriosis: a Dual U-Net trained on pelvic MRI with Gradient-weighted Class Activation Mapping (Grad-CAM) interpretability and a dense neural network trained on structured clinical features with SHapley Additive exPlanations (SHAP). This study tested weighted averaging, stacking via logistic regression, and confidence-gating. Performance was assessed using accuracy, precision, recall, F1-score, and area under the curve (AUC). Calibration was evaluated using the Brier score, expected calibration error (ECE), and reliability diagrams. Clinical utility was quantified with decision curve analysis (DCA). Statistical significance was tested with McNemar’s test for accuracy and DeLong’s test for AUC. Multimodal fusion outperformed both single modality models. Weighted averaging accuracy was 0.89, precision was 0.89, recall was 0.87, and F1-score was 0.86, thus improving on either modality alone. Stacking further enhanced calibration (ECE reduction from 0.8 to 0.04) and yielded higher net benefit across clinically relevant probability thresholds (20 to 60%). DCA indicated fusion would avoid 12 to 18 unnecessary surgical investigations per 100 patients, compared with single modality strategies. Confidence-gating maintained performance under simulated distribution shifts to support robustness. Decision-level multimodal fusion enhanced non-invasive diagnosis of endometriosis by improving accuracy, calibration, and clinical utility. These results demonstrated the value of integrative AI gynecological care and justify prospective validation in real-world clinical settings.</p></abstract>
      <kwd-group>
        <kwd>Endometriosis</kwd>
        <kwd>Multimodal fusion</kwd>
        <kwd>Gradient-weighted Class Activation Mapping</kwd>
        <kwd>Neural U-Net</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="7"/>
        <fig-count count="2"/>
        <table-count count="3"/>
        <ref-count count="22"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Endometriosis is a chronic gynecological condition affecting approximately 10% female of reproductive age, yet diagnosis is often delayed by 7 to 10 years due to the reliance on invasive laparoscopy as the reference standard [<xref ref-type="bibr" rid="ref_1">1</xref>]. According to World Health Organization (WHO), endometriosis cannot be prevented and there is no known cure; however, awareness and early diagnosis may be able to stop the natural course of the disease and lessen the long-term effects of its symptoms. Still, diagnosing endometriosis and figuring out the factors affecting its course and related symptoms remains extremely difficult even now. The prolonged diagnostic process of endometriosis frequently results in years of frustration and anxiety for sufferers. Treatment and care for these women are therefore postponed, sometimes for as long as 7 to 10 years after the onset of symptoms. Available evidence and data now support patients’ active participation in the detection and diagnosis of the illness. Relying on diagnosis through pain itself is insufficient to identify endometriosis. Aside from the intricacy of the diagnosis and the prognosis of symptoms, another concern is the percentage of women who show improvement following the surgery. The prognosis of reproductive issues in endometriosis-affected women is another urgent topic, thereby confirming that endometriosis has a significant impact on several aspects of women’s lives [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>].</p><p>Non-invasive methods, particularly those leveraging machine learning on imaging and clinical data, offer a promising pathway to earlier intervention and improved quality of life [<xref ref-type="bibr" rid="ref_4">4</xref>]. Recent advances have demonstrated the potential of artificial intelligence (AI) for endometriosis diagnosis in single-modality settings. Convolutional neural networks trained on Abdominal and Pelvic Magnetic Resonance Imaging (MRI) have shown strong lesion localization and diagnostic performance when paired with explainable AI (XAI) techniques such as Gradient-weighted Class Activation Mapping (Grad-CAM) for visual interpretability [<xref ref-type="bibr" rid="ref_5">5</xref>]. Similarly, dense neural networks applied to structured clinical data, augmented with SHapley Additive exPlanations (SHAP), have yielded competitive performance while offering feature-level transparency [<xref ref-type="bibr" rid="ref_6">6</xref>]. While effective in isolation, these features reflect structural manifestations, whereas clinical data reflect patients’ history and symptomatology.</p><p>Integrating multimodal data addresses the limitations of single-modality systems by leveraging complementary information. Decision-level fusion, which combines output from independently trained models, offers a practical strategy with reduced complexity compared with feature-level fusion. Despite its promise, multimodal fusion for diagnosis of endometriosis has not yet been systematically evaluated with respect to calibration, robustness, and clinical benefit.</p>
    </sec>
    <sec sec-type="">
      <title>2. Related works</title>
      <p>Non-invasive diagnosis for endometriosis has greatly been improved via AI and Machine Learning (ML), offering up to 90% sensitivity and specificity in research settings [<xref ref-type="bibr" rid="ref_7">7</xref>]. Models have been developed using diverse data sources which include clinical features, self-reported symptoms [<xref ref-type="bibr" rid="ref_8">8</xref>], imaging (ultrasound or MRI), genetic, and proteomic data.</p><p>Stolz et al. [<xref ref-type="bibr" rid="ref_9">9</xref>] found that a fused 3D T1/T2 MRI protocol achieved \( &lt; \)94% accuracy in diagnosis and improved inter-reader reproductivity compared with standard 2D MRI sets. In the AI domain, a study applied AI-segmentation (fuzzy C-means clustering) MRI for ovarian endometriosis; this showed improved sensitivity and reduced processing time. However, these efforts remain largely single-modality and rarely integrate structured clinical data or metrics for decision analysis. Parallel to image-only approaches, models of structured clinical data have gained traction in 2025. Fatade et al. [<xref ref-type="bibr" rid="ref_6">6</xref>] worked on explainable AI using SHAP-features for interpretation of clinical variables for endometriosis and demonstrated interpretable risk prediction. Yet models based purely on clinical features often underperform in capturing spatial and anatomical cues available in MRI, and they suffer from calibration biases.</p><p>There is more evidence to support multimodal fusion methods that integrate sources of heterogeneous data for improved diagnosis. Surveys of HER + imaging fusion underline consistent performance gains of multimodal systems over single modalities. Mohsen et al. [<xref ref-type="bibr" rid="ref_10">10</xref>] similarly reviewed multimodal medical image fusion (MMF), outlined pixel-, feature-, and decision-level fusion strategies, and emphasized the transition toward deep learning-based networks. Zubair et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] explored specific image fusion studies such as MRI + Computed Tomography (CT) fusion using GANs and interval-gradient convolutional neural network (CNN) fusion methods further demonstrated the benefits of combining anatomical and functional imaging. Yet in gynecologic AI research, application of decision-level multimodal fusion remains scarce.</p><p>Calibration and evaluation of clinical utility have emerged as essential topics in predictive modelling. The methodology of decision curve analysis (DCA) has been recommended for assessing net clinical benefit of diagnostic models beyond discrimination alone [<xref ref-type="bibr" rid="ref_12">12</xref>]. Moreover, studies applying DCA to calibrated CNN illustrated that improvements of calibration translate into meaningfully clinical benefit [<xref ref-type="bibr" rid="ref_13">13</xref>]. Although many endometriosis models report discrimination (area under the curve (AUC), and accuracy), few incorporates calibration curves, expected calibration error (ECE)/Brier metrics or DCA.</p><p>Compared with pixel-level and feature-level fusion strategies, decision-level multimodal fusion offers several advantages for clinical deployment. By combining independently optimized modality-specific models at the output stage, decision-level fusion reduces the need for extensive cross modal data harmonization and lowers computational overhead. Importantly, it preserves the interpretability of the model by allowing modality specific explanations for example imaging heatmaps and clinical feature attributions to be presented separately, in order to align with clinical decision-making workflows [<xref ref-type="bibr" rid="ref_14">14</xref>], [<xref ref-type="bibr" rid="ref_15">15</xref>].</p><p>In this study, we investigated decision-level fusion strategies that combined on an MRI-based U-Net model with a structured clinical-data neural network. We comparatively evaluated weighted averaging, stacking, and confidence-gating approaches to examine how different fusion strategies influenced diagnostic performance, probability calibration, and clinical net benefit. Beyond conventional accuracy metrics, this work focused on elucidating the trade-offs between predictive performance, robustness, interpretability, and implementation complexity, with the aim of informing the selection of fusion strategies that are most suitable for real-world clinical decision-support settings.</p>
    </sec>
    <sec sec-type="methods">
      <title>3. Methods</title>
      <p>This study adopted both structured and unstructured data types which were obtained from patients with indications of endometriosis. These datasets included Abdominal and Pelvic MRI, symptoms, findings from physical examination, and patients’ clinical history. In this study, we endeavored to address the gaps by comparing decision-level fusion strategies (weighted averaging, stacking, and confidence-gating), assessing calibration via ECE Brier score, and quantifying net clinical benefit through DCA.</p>
      
        <sec>
          
            <title>3.1. Data and preprocessing</title>
          
          <p>This retrospective study included records for 1208 patients who underwent Abdominal and Pelvic MRI for suspected endometriosis at Crestview Radiology Center across their branches in four cities in Nigeria (Lagos, Kano, Ibadan, and Ilorin), where these reports were anonymized. Diagnostic labels were derived from a combination of radiological reports and clinical documentation recorded in the electronic medical record. Specifically, endometriosis-positive cases were identified based on a consensus by gynecologists and radiologists following the interpretation of MRI and clinical evaluation. While laparoscopic confirmation was considered the reference standard, such data were not available in this cohort. The reliance on MRI-supported clinical diagnosis reflects real-world diagnostic pathways and aligns with the current trend towards reducing invasive approach adopted in previous imaging-based AI studies for endometriosis and other gynecological disorders [<xref ref-type="bibr" rid="ref_16">16</xref>], [<xref ref-type="bibr" rid="ref_17">17</xref>]. Due to the retrospective nature of our dataset, there was no a priori calculation of the sample size. The size of the final cohort was determined by the availability of data during the study period.</p>
          
            <sec>
              
                <title>3.1.1 Mri data</title>
              
              <p>Abdominal-Pelvic examinations were acquired using 1.5T and 3T scanners under standardized protocols that included T2-weighted fast pin echo sequences. Lesions were segmented semi-automatically using 3D Slicer (open-source platform, www.slicer.org), with manual refinements performed by experienced radiologists. The resulting lesion masks were used both to support the training of the model and to evaluate the localization performance of Grad-CAM heatmaps.</p>
            </sec>
          
          
            <sec>
              
                <title>3.1.2 Clinical data</title>
              
              <p>Structured clinical data included demographics (age, body mass index (BMI), and parity), gynecological and surgical history, and profiles of symptoms (severity of pain, dysmenorrhea, and dyspareunia). Categorical features were one-hot encoded, while continuous features were Z-scored normalized. Missing values were imputed using the median (continuous) or most frequent category. </p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Base predictive models</title>
          
          
            <sec>
              
                <title>3.2.1 Mri model</title>
              
              <p>A dual-attention U-Net was employed for segmentation and classification of lesion. The architecture incorporated spatial and channel attention blocks to enhance the representation of features [<xref ref-type="bibr" rid="ref_5">5</xref>]. Training employed the Adam Optimizer with a learning rate of 1e-4, Dice loss for segmentation, and binary cross-entropy for classification. To enhance generalization, data augmentation included rotations, flips, and intensity scaling. For interpretability, Grad-CAM maps were generated from the classification head to highlight image regions most influential to predictions [<xref ref-type="bibr" rid="ref_5">5</xref>].</p>
            </sec>
          
          
            <sec>
              
                <title>3.2.2 Model of clinical data</title>
              
              <p>A densely feedforward neural network was trained using structured clinical features. Selection of features was guided by SHAP importance ranking to reduce dimensionality and mitigate overfitting [<xref ref-type="bibr" rid="ref_6">6</xref>]. The network comprised three hidden layers (128-64-32 neurons) with ReLU activation and dropout regularization (<inline-formula>
  <mml:math id="mp5v85opph">
    <mml:mi>p</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mn>0.3</mml:mn>
  </mml:math>
</inline-formula>). The Adam Optimizer (learning rate of 1e-3) was used with binary cross-entropy loss. Probabilistic output indicated the likelihood of endometriosis.</p>
            </sec>
          
          
            <sec>
              
                <title>3.2.3 Model training and data splitting</title>
              
              <p>The dataset was randomly partitioned into training (80%) and test (20%) sets at the patient level to prevent leakage of data. Randomization was performed using a fixed random seed to ensure reproducibility. The training set was used for model fitting and internal tuning, while the test set was held out and used exclusively for evaluation of final performance. Both MRI-based and clinical models were trained independently using the same data split, to ensure fair comparison and consistent evaluation.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. Fusion strategies</title>
          
          <p>Three decision-level fusion strategies were evaluated:</p>
          <p>a. Weighted Averaging (baseline): probabilities from both models were combined as:</p><p style="text-align: center"><inline-formula>
  <mml:math id="m48n828gcx">
    <mml:msub>
      <mml:mrow>
        <mml:mi>P</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mrow>
          <mml:mi data-mjx-variant="-tex-mathit">fusion</mml:mi>
        </mml:mrow>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mrow>
        <mml:mi>P</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mrow>
          <mml:mi data-mjx-variant="-tex-mathit">MRI</mml:mi>
        </mml:mrow>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mrow>
        <mml:mi>P</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mrow>
          <mml:mi data-mjx-variant="-tex-mathit">clinical</mml:mi>
        </mml:mrow>
      </mml:mrow>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mo>+</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>α</mml:mi>
    <mml:mi>α</mml:mi>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula></p><p>where, <inline-formula>
  <mml:math id="m9u19oog64">
    <mml:mi>α</mml:mi>
  </mml:math>
</inline-formula> was tuned on the validation set. The weighting parameter was selected on the validation set with a discrete grid search over values between 0 and 1. The value yielding the best validation performance was fixed and applied to the test set to avoid leakage of information.</p>
          <p>b. Stacking Meta-Leaners: A logistic regression model was trained on the probabilistic output of the two base models. Five-fold cross-validation was applied to prevent overfitting.</p>
          <p>c. Confidence-Gating: For each case, the output of the model with higher prediction confidence (probability farther from 0.5) was selected as the fused output.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Evaluation protocol</title>
          
          <p>To thoroughly assess the performance of the proposed multimodal fusion framework, we evaluated models along three complementary dimensions: predictive accuracy, probability calibration, and clinical utility. Predictive accuracy captures the ability of the models to correctly classify cases, while calibration reflects the reliability of predicted probabilities in reflecting the risk of the disease. Clinical utility was quantified using decision curve analysis to determine whether fused models provided a measurable benefit in realistic diagnostic decision-making scenarios. Statistical testing was performed to determine whether observed improvements in fusion were significant compared with single modality baselines.</p><p>The subsection that follows describes the metrics, calibration methods, decision curve framework, and statistical analysis plan.</p>
          
            <sec>
              
                <title>3.4.1 Performance metrics</title>
              
              <p>The performance metrics for the model was derived using accuracy, precision, recall, F1-score, and AUC.</p>
            </sec>
          
          
            <sec>
              
                <title>3.4.2 Calibration metrics</title>
              
              <p>This was quantified using Brier score, ECE, and reliability diagram. To improve the probability further, two post-hoc recalibration methods were applied on the validation set. Platt scaling which fits regression model to map raw prediction scores into well-calibrated probabilities, making it effective when miscalibration follows a sigmoid pattern [<xref ref-type="bibr" rid="ref_18">18</xref>]. Isotonic regression, however, is a non-parametric monotonic mapping that can flexibly correct irregular calibration error [<xref ref-type="bibr" rid="ref_19">19</xref>], [<xref ref-type="bibr" rid="ref_20">20</xref>]. Using both approaches allowed us to compare a simple parametric correction with a more flexible, data-driven alternative.</p>
            </sec>
          
          
            <sec>
              
                <title>3.4.3 Clinical utility</title>
              
              <p>DCA was conducted to assess net clinical benefit across threshold probability from 0.2–0.6, a range relevant for gynecological triage decisions.</p><p> The probability range of 0.2–0.6 for the threshold was selected to reflect clinically plausible triage scenarios in the non-invasive assessment of suspected endometriosis. Lower thresholds (around 20%) correspond to settings where clinicians prioritize sensitivity to avoid missed diagnoses, while higher thresholds (up to 60%) reflect scenarios requiring greater diagnostic confidence before recommending further investigation or referral from specialists. This range captures the decision space where risk-based stratification is most likely to influence clinical management.</p>
            </sec>
          
          
            <sec>
              
                <title>3.4.4 Statistical analysis</title>
              
              <p>Bootstrapping with 1,000 iterations was used to compute 95% confidence intervals for performance metrics. Paired comparisons of accuracy were conducted using McNemar’s test, while AUCs were compared using the DeLong test. A $p$-value \( &lt; \)0.05 was considered statistically significant.</p>
            </sec>
          
        </sec>
      
    </sec>
    <sec sec-type="results">
      <title>4. Results</title>
      
        <sec>
          
            <title>4.1. Overall performance</title>
          
          <p><xref ref-type="table" rid="table_1">Table 1</xref> summarizes the performance of the single modality and fusion models. The dual-attention U-Net trained on MRI achieved an accuracy of 0.87, precision of 0.85, recall of 0.88, and F1-score of 0.86, while the dense neural network trained on structured clinical data achieved an accuracy of 0.83, precision of 0.82, recall of 0.8, and F1-score of 0.81.</p><p>All fusion strategies outperformed the single-modality baselines. Weighted average achieved an accuracy of 0.89, precision of 0.89, and recall of 0.87. Stacking with logistic regression further improved the AUC to 0.92, representing a statistically significant increase compared with single-modality model (DeLong test, <inline-formula>
  <mml:math id="mzh4mquzhc">
    <mml:mi>p</mml:mi>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mn>0.05</mml:mn>
  </mml:math>
</inline-formula>). Confidence-gating maintained competitive performance while prioritizing robustness under varying prediction of the confidence level.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Performance of single-modality and fusion models</title>
              </caption>
              <table><tr><th >Model</th><th >Accuracy</th><th >Precision</th><th >Recall</th><th >F1-Score</th><th >AUC</th></tr><tr><td >MRI only (U-Net)</td><td >0.87</td><td >0.85</td><td >0.88</td><td >0.86</td><td >0.90</td></tr><tr><td >Clinical only (Dense NN)</td><td >0.83</td><td >0.82</td><td >0.80</td><td >0.81</td><td >0.86</td></tr><tr><td >Fusion-Weighted Average</td><td >0.89</td><td >0.89</td><td >0.87</td><td >0.86</td><td >0.91</td></tr><tr><td >Fusion-Stacking</td><td >0.90</td><td >0.90</td><td >0.88</td><td >0.89</td><td >0.92</td></tr><tr><td >Fusion-Confidence-Gating</td><td >0.88</td><td >0.87</td><td >0.86</td><td >0.86</td><td >0.90</td></tr></table>
            </table-wrap>
          
          <p>Although confidence intervals were not explicitly tabulated for all metrics, consistent performance improvements across fusion strategies and statistically significant comparative tests support the robustness of the reported results. </p>
        </sec>
      
      
        <sec>
          
            <title>4.2. Calibration performance</title>
          
          <p>It is seen in <xref ref-type="fig" rid="fig_1">Figure 1</xref> (Reliability diagram) that both single models were modestly mis-calibrated, thus tending to overestimate probabilities at high confidence. The MRI model had an ECE of 0.08 and the clinical model was 0.1. Fusion through stacking demonstrated the best calibration, with an ECE of 0.04 and the lowest Brier score of 0.09. Weighted averaging also improved calibration compared to single modality models, with an ECE of 0.05.</p><p>There is a distinct calibration pattern across the models from the reliability diagram. The MRI only model exhibited mild over-confidence in higher predicted probability bins, with predicted risks exceeding observed outcome frequencies. In contrast, the clinical only model tended to underestimate risk in lower probability intervals, thus reflecting conservative probability output. The stacked fusion model demonstrated improved alignment between the predicted and observed probabilities across most bins, indicating more stable calibration behavior across the full probability range.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>Reliability diagram of MRI (orange), clinical (blue), and fusion models (green). The stacking-based fusion model demonstrated the closest alignment between predicted and observed probabilities</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/0/img_zjkbWHXfDEC16H_l.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.3. Clinical utility</title>
          
          <p>Decision curve analysis in <xref ref-type="fig" rid="fig_2">Figure 2</xref> shows that fusion models consistently achieved higher net benefit across clinically relevant probability thresholds (0.2–0.6). At a threshold of 0.4, stacking avoided approximately 12 to 18 unnecessary invasive procedures per 100 patients, compared with the best-performing single modality model.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Decision curve analysis comparing net clinical benefit of single-modality and fusion models across the threshold probability</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/0/img_JLk3R73JT-gZm8jw.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.4. Robustness analysis</title>
          
          <p>Experiments of robustness indicated that the performance of single-modality models degraded under simulated distribution shifts (<xref ref-type="table" rid="table_2">Table 2</xref>). For example, under intensity-scaled MRI data, the accuracy of U-Net dropped from 0.87 to 0.82. In contrast, the confidence-gating fusion approach maintained stable performance, with reduction of less than 3% (from 0.88 to 0.86) in accuracy. Scanner-level cross-validation confirmed that fusion models generalized better across 1.5T and 3T subsets than single-modality baselines.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Results of robustness under distribution shifts and cross-scanner validation</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p>Accuracy Baseline</p></th><th colspan="1" rowspan="1"><p>Accuracy under Shift</p></th><th colspan="1" rowspan="1"><p><mml:math id="m90577f8m2">
  <mml:mi>Δ</mml:mi>
</mml:math> Accuracy</p></th></tr><tr><td colspan="1" rowspan="1"><p>MRI only</p></td><td colspan="1" rowspan="1"><p>0.87</p></td><td colspan="1" rowspan="1"><p>0.82</p></td><td colspan="1" rowspan="1"><p>-0.05</p></td></tr><tr><td colspan="1" rowspan="1"><p>Clinical only</p></td><td colspan="1" rowspan="1"><p>0.83</p></td><td colspan="1" rowspan="1"><p>0.78</p></td><td colspan="1" rowspan="1"><p>-0.05</p></td></tr><tr><td colspan="1" rowspan="1"><p>Fusion-Weighted</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>-0.03</p></td></tr><tr><td colspan="1" rowspan="1"><p>Fusion-Stacking</p></td><td colspan="1" rowspan="1"><p>0.90</p></td><td colspan="1" rowspan="1"><p>0.87</p></td><td colspan="1" rowspan="1"><p>-0.03</p></td></tr><tr><td colspan="1" rowspan="1"><p>Fusion-Gating</p></td><td colspan="1" rowspan="1"><p>0.88</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>-0.02</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>4.5. Ablation study</title>
          
          <p>The result of the ablation study as shown in <xref ref-type="table" rid="table_3">Table 3</xref> shows the contribution of attention mechanisms and feature selection. When dual attention from U-Net was removed, there was a decrease in accuracy from 0.87 to 0.82. Excluding the selection of SHAP-based features could reduce the performance of the clinical model from 0.83 to 0.79. Fusion models incorporating these components consistently achieved the strongest results.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Ablation study evaluating the impact of model components</title>
              </caption>
              <table><tr><th >Model Variant</th><th >Accuracy</th><th >F1-Score</th><th >AUC</th></tr><tr><td >U-Net (No attention)</td><td >0.82</td><td >0.81</td><td >0.86</td></tr><tr><td >Dense NN (All features, no SHAP)</td><td >0.79</td><td >0.78</td><td >0.83</td></tr><tr><td >Fusion (Full model)</td><td >0.90</td><td >0.89</td><td >0.92</td></tr></table>
            </table-wrap>
          
        </sec>
      
    </sec>
    <sec sec-type="discussion">
      <title>5. Discussion</title>
      <p>Demonstrated in this study is the fact decision-level multimodal fusion of MRI-based and clinical data-based models significantly improved the non-invasive diagnosis of endometriosis, when compared with single-modality systems. Across the accuracy, calibration and clinical utility metrics, fusion strategies consistently outperformed individual models. Stacking fusion achieved the best overall performance, with an AUC of 0.92, an expected calibration error of 0.04, and the highest net clinical benefit in decision curve analysis.</p><p>Beyond performance gains, model interpretability plays a critical role in supporting the decision making of clinicians. Grad-CAM visualizations from the MRI-based model highlight image regions contributing most strongly to predictions, thus enabling radiologists to verify whether model attention aligns with known anatomical sites involving endometriosis. This could increase confidence in the output of the model and assist in identifying subtle imaging patterns that may warrant closer inspection. Similarly, SHAP-based explanations from the clinical model provide feature-level attribution, allowing clinicians to understand how profiles of symptoms, medical history, and demographic factors contribute to estimates of individual risk. When used in conjunction with clinical judgement, these interpretability output could support transparent risk stratification, facilitate clinician-AI collaboration, and improve trust in AI-assisted non-invasive diagnostic workflows.</p>
      
        <sec>
          
            <title>5.1. Interpretation of findings</title>
          
          <p>The results highlight the complementary strengths of imaging and clinical data in characterizing endometriosis. MRI features capture structural manifestations of the disease, while clinical variables reflect symptoms burden and medical history. Their integration yielded synergistic gains, particularly in calibration, hence suggesting that fused models generate more reliability estimates. This is clinically meaningful, as calibrated probabilities better align with real-world decision thresholds. Importantly, decision curve analysis indicated that fusion models could prevent 12 to 18 unnecessary invasive procedures per 100 patients, compared with MRI or clinical models alone. Thus, the potential of meaningful patient impact was underscored. </p>
        </sec>
      
      
        <sec>
          
            <title>5.2. Comparison with prior works</title>
          
          <p>Previous studies largely focused on unimodal AI approaches for endometriosis, including convolutional networks applied to MRI [<xref ref-type="bibr" rid="ref_5">5</xref>], and dense neural network applied to structured data [<xref ref-type="bibr" rid="ref_6">6</xref>], to mention a few examples. While these models achieved competitive performance in isolation, our findings showed that decision-level fusion offered incremental benefit. These results align with broader multimodal AI literature, where fusion has improved the reliability of diagnosis of oncology, cardiology, and neuroimaging tasks [<xref ref-type="bibr" rid="ref_21">21</xref>], [<xref ref-type="bibr" rid="ref_22">22</xref>].</p>
        </sec>
      
      
        <sec>
          
            <title>5.3. Clinical implications</title>
          
          <p>The improved calibration of the fusion models is especially relevant in gynecological practice, where decisions of treatment often hinge on threshold probability. A well-calibrated system reduces the risk of over- or under-treatment, and it enables clinicians to use model output as reliable adjuncts in triage and surgical planning. </p>
        </sec>
      
      
        <sec>
          
            <title>5.4. Limitations</title>
          
          <p>This study has several limitations. First, diagnostic labels were derived from MRI reports and clinical consensus rather than laparoscopic or histopathological confirmation. While this reflects real-world diagnostic practices, it may introduce label noise. Second, MRI segmentation was performed using semi-automatic approach with experts’ manual correction, but formal inter-observers’ agreement metrics (e.g., Dice similarity coefficient) were not evaluated. While expert-guided segmentation reflects realistic workflows of clinical annotation, future studies could incorporate independent multi-rater annotations and quantitative agreement analysis to further strengthen the reliability of segmentation. Third, data were obtained from a single institution, which may limit generalizability. Fourth, while we explored decision-level fusion methods, feature-level or representation-level fusion strategies may offer further gains and should be investigated. Fifth, although missingness was limited, simple imputation strategies may attenuate associations for certain features, and future work could explore more advanced imputation methods. </p><p>Finally, it is important to mention the likely imitation that comes with single diagnostic center. This may limit the generalizability of the findings to other institutions with differing patient populations, imaging protocols, and clinical workflows. Variations in scanner vendors, acquisition parameters, and practices of documentation when deployed, could affect the performance of the model. However, the employment of decision-level fusion partially mitigates these challenges by allowing independent optimization of modality-specific models and reducing dependencies on tightly coupled cross-model feature representations. Future work will focus on external validation across multiple centers, including settings with heterogeneous imaging infrastructure, availability of varying levels of clinical data as well as prospective evaluation to assess robustness under conditions of real-world deployment. </p>
        </sec>
      
      
        <sec>
          
            <title>5.5. Future work</title>
          
          <p>Future work should pursue prospective and multi-center validation of multimodal fusion models, incorporating both clinical and imaging data streams. Additional research is needed to explore dynamic weighing strategies, integrate temporal clinical data, and evaluate feature-level fusion approaches. Importantly, embedding these models in clinical-facing decision support systems and assessing their applicability, trust, and interpretability in real-world workflows would be critical for translation into practice. </p>
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>6. Conclusions</title>
      <p>Decision-level multimodal fusion of MRI-based and clinical data-based AI models enhance the non-invasive diagnosis of endometriosis by improving diagnostic accuracy, probability calibration, and clinical utility, compared with single-modality approaches. Among the evaluated strategies, stacking-based fusion achieved the strongest overall performance and calibration, indicating high potentially clinical value. These findings support the integration of multimodal AI into gynecological decision support systems and highlight the demand for prospective and multi-center validation to enable translation into clinical practice. </p><p>Despite superior performance, stacking-based fusion entails greater complexity in implementation, including coordinated integration of the model and adaptation of the workflow. Simpler fusion strategies, such as weighted averaging and confidence-gating, may offer advantages in case of deployment, computational efficiency, and clinical acceptance, particularly in resource-constrained settings while maintaining competitive performance. Collectively, these findings underscore the importance of balancing diagnostic accuracy with clinical reliability, interpretability, and feasibility when translating multimodal AI systems into routine practice. </p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>Conceptualization, study design, software implementation, formal analysis, data curation, visualization, literature review, and writing—original draft preparation, O.B.F.; radiology analysis, MRI lesion annotation, manual segmentation review, and provision of clinical expertise, O.F.A.; methodology development and preparation of the methodology section, F.A.S. and K.I.O.; research documentation, data organization, and compilation of study materials, K.I.O. and G.C.O.; manuscript preparation, section integration, and overall structuring of content, G.O.O. and O.B.F.; abstract writing, manuscript refinement, and intellectual review support, O.B.F. and F.A.S. All authors have read and agreed to the published version of the manuscript.</p>
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="webpage">
          <article-title>Endometriosis</article-title>
          <source>, https://www.who.int/news-room/fact-sheets/detail/endometriosis</source>
          <year>2025</year>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>356</page-range>
          <issue>2</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gkrozou</surname>
              <given-names>Fotini</given-names>
            </name>
            <name>
              <surname>Tsonis</surname>
              <given-names>Orestis</given-names>
            </name>
            <name>
              <surname>Sorrentino</surname>
              <given-names>Francesco</given-names>
            </name>
            <name>
              <surname>Nappi</surname>
              <given-names>Luigi</given-names>
            </name>
            <name>
              <surname>Vatopoulou</surname>
              <given-names>Anastasia</given-names>
            </name>
            <name>
              <surname>Skentou</surname>
              <given-names>Christina</given-names>
            </name>
            <name>
              <surname>Pandey</surname>
              <given-names>Shilpa</given-names>
            </name>
            <name>
              <surname>Paschopoulos</surname>
              <given-names>Minas</given-names>
            </name>
            <name>
              <surname>Daniilidis</surname>
              <given-names>Alexandros</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/jcm13020356</pub-id>
          <article-title>Endometriosis predictive models based on self-assessment questionnaire, clinical examination, or imaging findings: A narrative review</article-title>
          <source>J. Clin. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>59</volume>
          <page-range>499</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Szubert</surname>
              <given-names>Maria</given-names>
            </name>
            <name>
              <surname>Rycerz</surname>
              <given-names>Aleksander</given-names>
            </name>
            <name>
              <surname>Wilczyński</surname>
              <given-names>Jacek Radoslaw</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/medicina59030499</pub-id>
          <article-title>How to improve non-invasive diagnosis of endometriosis with advanced statistical methods</article-title>
          <source>Medicina</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>245</volume>
          <page-range>437-447</page-range>
          <issue>5</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hudson</surname>
              <given-names>Quanah Joseph</given-names>
            </name>
            <name>
              <surname>Perricos</surname>
              <given-names>Alexandra</given-names>
            </name>
            <name>
              <surname>Wenzl</surname>
              <given-names>Rene</given-names>
            </name>
            <name>
              <surname>Yotova</surname>
              <given-names>Iveta</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1177/1535370220903270</pub-id>
          <article-title>Challenges in uncovering non-invasive biomarkers of endometriosis</article-title>
          <source>Exp. Biol. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>4</volume>
          <page-range>97-108</page-range>
          <issue>2</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kuyoro</surname>
              <given-names>Afolashade Oluwakemi</given-names>
            </name>
            <name>
              <surname>Fatade</surname>
              <given-names>Oluwayemisi Boye</given-names>
            </name>
            <name>
              <surname>Onuiri</surname>
              <given-names>Ernest Enyinnaya</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/ataiml040203</pub-id>
          <article-title>Enhancing non-invasive diagnosis of endometriosis through explainable artificial intelligence: A Grad-CAM approach</article-title>
          <source>Acadlore Trans. AI Mach. Learn.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Fatade</surname>
              <given-names>Oluwayemisi B.</given-names>
            </name>
            <name>
              <surname>Kuyoro</surname>
              <given-names>Afolashade O.</given-names>
            </name>
            <name>
              <surname>Onuiri</surname>
              <given-names>Ernest E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.51244/IJRSI.2025.12030070</pub-id>
          <article-title>Explainable AI for endometriosis diagnosis: A dense neural network approach with SHAP interpretation</article-title>
          <source>Int. J. Res. Stud. Inf. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>639</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bendifallah</surname>
              <given-names>Sofiane</given-names>
            </name>
            <name>
              <surname>Puchar</surname>
              <given-names>Anne</given-names>
            </name>
            <name>
              <surname>Suisse</surname>
              <given-names>Stephane</given-names>
            </name>
            <name>
              <surname>Delbos</surname>
              <given-names>Lea</given-names>
            </name>
            <name>
              <surname>Poilblanc</surname>
              <given-names>Mathieu</given-names>
            </name>
            <name>
              <surname>Descamps</surname>
              <given-names>Philippe</given-names>
            </name>
            <name>
              <surname>Golfier</surname>
              <given-names>Francois</given-names>
            </name>
            <name>
              <surname>Touboul</surname>
              <given-names>Cyril</given-names>
            </name>
            <name>
              <surname>Dabi</surname>
              <given-names>Yohann</given-names>
            </name>
            <name>
              <surname>Darai</surname>
              <given-names>Emile</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-021-04637-2</pub-id>
          <article-title>Machine learning algorithms as a screening approach for patients with endometriosis</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>5499</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Goldstein</surname>
              <given-names>Anat</given-names>
            </name>
            <name>
              <surname>Cohen</surname>
              <given-names>Shani</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-023-32761-8</pub-id>
          <article-title>Self-report symptom-based endometriosis prediction using machine learning</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>187</volume>
          <page-range>112091</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Stolz</surname>
              <given-names>Alexandre</given-names>
            </name>
            <name>
              <surname>Pupulim</surname>
              <given-names>Lawrence Fabian</given-names>
            </name>
            <name>
              <surname>Rojas Soldado</surname>
              <given-names>Maria</given-names>
            </name>
            <name>
              <surname>Chabloz</surname>
              <given-names>Patrick</given-names>
            </name>
            <name>
              <surname>Kinkel</surname>
              <given-names>Karen</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ejrad.2025.112091</pub-id>
          <article-title>Fusion 3D T1/T2 MRI for diagnosing pelvic deep infiltrating endometriosis: A non-inferiority study</article-title>
          <source>Eur. J. Radiol.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>17981</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mohsen</surname>
              <given-names>Farida</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>Hazrat</given-names>
            </name>
            <name>
              <surname>El Hajj</surname>
              <given-names>Nady</given-names>
            </name>
            <name>
              <surname>Shah</surname>
              <given-names>Zubair</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-022-22514-4</pub-id>
          <article-title>Artificial intelligence-based methods for fusion of electronic health records and imaging data</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>2505</volume>
          <page-range>14715</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zubair</surname>
              <given-names>Muhammad</given-names>
            </name>
            <name>
              <surname>Hussain</surname>
              <given-names>Muzammil</given-names>
            </name>
            <name>
              <surname>Al-Bashrawi</surname>
              <given-names>Mousa Ahmad</given-names>
            </name>
            <name>
              <surname>Bendechache</surname>
              <given-names>Malika</given-names>
            </name>
            <name>
              <surname>Owais</surname>
              <given-names>Muhammad</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2505.14715</pub-id>
          <article-title>A comprehensive review of techniques, algorithms, advancements, challenges, and clinical applications of multi-modal medical image fusion for improved diagnosis</article-title>
          <source>arXiv</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>1643-1648</page-range>
          <issue>10</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Vickers</surname>
              <given-names>Andrew James</given-names>
            </name>
            <name>
              <surname>Holland</surname>
              <given-names>Fiona</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.spinee.2021.02.024</pub-id>
          <article-title>Decision curve analysis to evaluate the clinical benefit of prediction models</article-title>
          <source>Spine J.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>30</volume>
          <page-range>6867-6876</page-range>
          <issue>12</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Deniffel</surname>
              <given-names>Dominik</given-names>
            </name>
            <name>
              <surname>Abraham</surname>
              <given-names>Nabila</given-names>
            </name>
            <name>
              <surname>Namdar</surname>
              <given-names>Khashayar</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>Xin</given-names>
            </name>
            <name>
              <surname>Salinas</surname>
              <given-names>Emmanuel</given-names>
            </name>
            <name>
              <surname>Milot</surname>
              <given-names>Laurent</given-names>
            </name>
            <name>
              <surname>Khalvati</surname>
              <given-names>Farzad</given-names>
            </name>
            <name>
              <surname>Haider</surname>
              <given-names>Masoom Ali</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00330-020-07030-1</pub-id>
          <article-title>Using decision curve analysis to benchmark MRI-based deep learning models for prostate cancer risk assessment</article-title>
          <source>Eur. Radiol.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>022001</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cui</surname>
              <given-names>Can</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Hai Chun</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Yao Hong</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Shi Lin</given-names>
            </name>
            <name>
              <surname>Asad</surname>
              <given-names>Zuhayr</given-names>
            </name>
            <name>
              <surname>Coburn</surname>
              <given-names>Lori Ann</given-names>
            </name>
            <name>
              <surname>Wilson</surname>
              <given-names>Keith Thomas</given-names>
            </name>
            <name>
              <surname>Landman</surname>
              <given-names>Bennett Alan</given-names>
            </name>
            <name>
              <surname>Huo</surname>
              <given-names>Yuankai</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1088/2516-1091/acc2fe</pub-id>
          <article-title>Deep multimodal fusion of image and non-image data in disease diagnosis and prognosis: A review</article-title>
          <source>Prog. Biomed. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>136</page-range>
          <issue>1</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Huang</surname>
              <given-names>S. C.</given-names>
            </name>
            <name>
              <surname>Pareek</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Seyyedi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Banerjee</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Lungren</surname>
              <given-names>M. P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41746-020-00341-z</pub-id>
          <article-title>Fusion of medical imaging and electronic health records using deep learning: A systematic review and implementation guidelines</article-title>
          <source>NPJ Digit. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>23</volume>
          <page-range>426</page-range>
          <issue>4</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kido</surname>
              <given-names>Aki</given-names>
            </name>
            <name>
              <surname>Himoto</surname>
              <given-names>Yuki</given-names>
            </name>
            <name>
              <surname>Moribata</surname>
              <given-names>Yusaku</given-names>
            </name>
            <name>
              <surname>Kurata</surname>
              <given-names>Yasuhisa</given-names>
            </name>
            <name>
              <surname>Nakamoto</surname>
              <given-names>Yuji</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3348/kjr.2021.0405</pub-id>
          <article-title>MRI in the diagnosis of endometriosis and related diseases</article-title>
          <source>Korean J. Radiol.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>108</volume>
          <page-range>886-894</page-range>
          <issue>6</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bazot</surname>
              <given-names>Marc</given-names>
            </name>
            <name>
              <surname>Darai</surname>
              <given-names>Emile</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.fertnstert.2017.10.026</pub-id>
          <article-title>Diagnosis of deep endometriosis: Clinical examination, ultrasonography, magnetic resonance imaging, and other techniques</article-title>
          <source>Fertil. Steril.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="conf-paper">
          <page-range>12182-12204</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gupta</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Ramdas</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Online Platt scaling with recalibration</article-title>
          <source>Proceedings of the 40th International Conference on Machine Learning, Honolulu, Hawaii, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1972-1980</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Berta</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Bach</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Jordan</surname>
              <given-names>M. I.</given-names>
            </name>
          </person-group>
          <article-title>Classifier calibration with ROC-Regularized Isotonic Regression</article-title>
          <source>27th International Conference on Artificial Intelligence and Statistics, Valence, Spain</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>127343-127352</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Huang</surname>
              <given-names>Liang</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Jun</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Bo</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Hao</given-names>
            </name>
            <name>
              <surname>Broucke</surname>
              <given-names>Seppe</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3008150</pub-id>
          <article-title>An experimental investigation of calibration techniques for imbalanced data</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>64</volume>
          <page-range>149-187</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Yu Dong</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>Zheng Chao</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Shui Hua</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>Xiang</given-names>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names>Xu Jing</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Qing Hua</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>Hua</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Min</given-names>
            </name>
            <name>
              <surname>Jiménez-Mesa</surname>
              <given-names>Carmen</given-names>
            </name>
            <name>
              <surname>Ramirez</surname>
              <given-names>Javier</given-names>
            </name>
            <name>
              <surname>et.al</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.inffus.2020.07.006</pub-id>
          <article-title>Advances in multimodal data fusion in neuroimaging: Overview, challenges, and novel orientations</article-title>
          <source>Inf. Fus.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>40</volume>
          <page-range>1095-1110</page-range>
          <issue>10</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lipkova</surname>
              <given-names>Jana</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Richard Jonathan</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Bowen</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>Ming Yu</given-names>
            </name>
            <name>
              <surname>Barbieri</surname>
              <given-names>Matteo</given-names>
            </name>
            <name>
              <surname>Shao</surname>
              <given-names>Daniel</given-names>
            </name>
            <name>
              <surname>Vaidya</surname>
              <given-names>Anurag Jayant</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Chengkuan</given-names>
            </name>
            <name>
              <surname>Zhuang</surname>
              <given-names>Luoting</given-names>
            </name>
            <name>
              <surname>Williamson</surname>
              <given-names>Drew</given-names>
            </name>
            <name>
              <surname>et.al</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ccell.2022.09.012</pub-id>
          <article-title>Artificial intelligence for multimodal data integration in oncology</article-title>
          <source>Cancer Cell</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>