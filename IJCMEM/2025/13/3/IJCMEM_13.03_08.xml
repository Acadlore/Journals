<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IJCMEM</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>International Journal of Computational Methods and Experimental Measurements</journal-title>
        <abbrev-journal-title abbrev-type="issn">Int. J. Comput. Methods Exp. Meas.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IJCMEM</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2046-0554</issn>
      <issn publication-format="print">2046-0546</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-eRI4ZTx89yk_ZnxlSdG_e3S5n0dB1AwP</article-id>
      <article-id pub-id-type="doi">10.56578/ijcmem130308</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Adaptive Multi-Scale Gated Convolution and Context-Aware Attention Network for Accurate Small Object Detection</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1,2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-3328-4818</contrib-id>
          <name>
            <surname>Wang</surname>
            <given-names>Jia-Chi</given-names>
          </name>
          <email>jiachi430@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-3369-757X</contrib-id>
          <name>
            <surname>Jung</surname>
            <given-names>Min-Po</given-names>
          </name>
          <email>minpo@ysu.ac.kr</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5367-1372</contrib-id>
          <name>
            <surname>Yin</surname>
            <given-names>Shoulin</given-names>
          </name>
          <email>yslin@synu.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1230-4007</contrib-id>
          <name>
            <surname>Li</surname>
            <given-names>Hang</given-names>
          </name>
          <email>lihangsoft@163.com</email>
        </contrib>
        <aff id="aff_1">Department of Computer and Information Engineering, Graduate School Youngsan University, 612-022 Busan, South Korea</aff>
        <aff id="aff_2">College of Artificial Intelligence, Shenyang Normal University, 110034 Shenyang, China</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>17</day>
        <month>10</month>
        <year>2025</year>
      </pub-date>
      <volume>13</volume>
      <issue>3</issue>
      <fpage>576</fpage>
      <lpage>587</lpage>
      <page-range>576-587</page-range>
      <history>
        <date date-type="received">
          <day>06</day>
          <month>08</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>23</day>
          <month>09</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Small object detection in complex scenes remains a challenging task due to background clutter, scale variation, and weak feature representation. Conventional deep learning–based detectors are prone to false positives and missed detections when dealing with dense or low-contrast objects. To address these limitations, this paper proposes an Adaptive Multi-Scale Gated Convolution and Context-Aware Attention Network (AGCAN) designed to enhance small object detection accuracy under complex visual conditions. The model introduces an improved Multi-Scale Gated Convolution Module (MGCM) to replace standard U-Net convolutional blocks, enabling comprehensive extraction of fine-grained object features across multiple scales. A Multi-Information Fusion Enhancement Module (MFEM) is incorporated at skip connections by integrating improved dilated convolution and hybrid residual window attention to minimize information loss and optimize cross-layer feature fusion. Furthermore, the Distance-IoU (DIoU) loss replaces the conventional Smooth L1 loss to accelerate model convergence and improve localization precision. Contextual cues are adaptively integrated into region-of-interest classification to strengthen small-object discrimination. Experimental evaluations on the DIOR and NWPU VHR-10 datasets demonstrate that the proposed network achieves superior performance compared with state-of-the-art methods, effectively reducing false detections and improving robustness in complex environments.</p></abstract>
      <kwd-group>
        <kwd>Deep learning</kwd>
        <kwd>Small object detection</kwd>
        <kwd>Adaptive multi-scale gated convolution</kwd>
        <kwd>Context-aware attention model</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="4"/>
        <fig-count count="8"/>
        <table-count count="6"/>
        <ref-count count="38"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>In recent years, with the acceleration of urbanization, the demand for small target detection algorithms in complex scenarios has been increasing day by day. Especially in tasks such as autonomous driving [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>], traffic monitoring [<xref ref-type="bibr" rid="ref_3">3</xref>], and unmanned aerial vehicle detection [<xref ref-type="bibr" rid="ref_4">4</xref>], [<xref ref-type="bibr" rid="ref_5">5</xref>], its importance has become increasingly prominent [<xref ref-type="bibr" rid="ref_6">6</xref>]. In the field of optical remote sensing images, Ahmed et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] systematically analyzed the advantages and disadvantages of traditional and deep learning methods, he pointed out the potential of the Transformer algorithm in the fine-grained recognition of ships. The researches show that deep learning methods significantly improve the detection accuracy in complex backgrounds through end-to-end feature learning, but the characterization of small target features and background interference remain core challenges [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>]. Factors such as noisy background interference, illumination changes, and occlusion between objects seriously affect the accuracy of small target detection. Therefore, achieving efficient detection of small targets in complex environments has important theoretical and application value [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>].</p><p>At present, the target detection algorithms based on convolutional neural networks (CNN) are mainly classified into two types: single-stage algorithms and two-stage algorithms. Initially, methods such as R-CNN [<xref ref-type="bibr" rid="ref_12">12</xref>], Fast RCNN [<xref ref-type="bibr" rid="ref_13">13</xref>], Faster R-CNN [<xref ref-type="bibr" rid="ref_14">14</xref>], Mask R-CNN [<xref ref-type="bibr" rid="ref_15">15</xref>], Cascade RCNN [<xref ref-type="bibr" rid="ref_16">16</xref>], etc., which belong to the two-stage series, led the development of target detection technology. The characteristic of the two-stage algorithms is to first generate candidate boxes and then identify the objects within the boxes. However, these methods, due to the need to generate a large number of redundant bounding boxes and the resulting excessive computational load, bring problems such as slow running speed and reduced detection efficiency, making it difficult to meet the real-time requirements brought about by the rapid development of current technology. Single-stage algorithms, such as RetinaNet, GFL, TOOD, and the YOLO series, enable the model to directly complete end-to-end object detection, reducing the number of calculation steps, addressing the shortcomings of the two-stage algorithms in terms of real-time performance, achieving effective real-time detection of targets, and achieving a more ideal balance between accuracy and speed. It is worth noting that recent researches have made significant progress in lightweight network. Li et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] proposed an optimization scheme based on the attention mechanism for the YOLOv4-Tiny algorithm, providing an effective solution to the problem of balancing accuracy and speed in lightweight models under complex scenarios. Yin and Ding [<xref ref-type="bibr" rid="ref_18">18</xref>] proposed a lightweight model YOLOv8-VSC for the detection of surface defects on steel strips, providing a new approach for small target detection in industrial scenarios. However, both single-stage and two-stage algorithms rely on the Non-Maximum Suppression (NMS) technique to eliminate redundant bounding boxes, which to some extent affects the inference speed and robustness of the model.</p><p>To address the issues of missdetection and false detection in multi-scale image small target detection, Zheng et al. [<xref ref-type="bibr" rid="ref_19">19</xref>] introduced the DIoU (Distance Intersection over Union) loss function to make the prediction boxes of YOLOv3 more accurate. Huang et al. [<xref ref-type="bibr" rid="ref_20">20</xref>] incorporated deformable convolution into RetinaNet to enable the algorithm to adaptively adjust the receptive field, thereby improving the detection accuracy in complex environmental backgrounds and when objects are small. Wang and Wang [<xref ref-type="bibr" rid="ref_21">21</xref>] introduced the self-attention mechanism into the YOLOv5 algorithm to address the issue of dense distribution of small targets. Li and Wu [<xref ref-type="bibr" rid="ref_22">22</xref>] introduced frequency channel attention into the YOLOv5 algorithm, thereby improving the detection performance of the algorithm for small targets. Wang et al. [<xref ref-type="bibr" rid="ref_23">23</xref>] introduced the convolutional attention module into the YOLOv5 algorithm, aiming to enhance the target features to improve the detection accuracy of small targets. Su et al. [<xref ref-type="bibr" rid="ref_24">24</xref>] addressed the issue of insufficient pixel and feature information for small targets in images, proposing an algorithm that integrated the attention mechanism, local receptive field, and refined feature pyramid to enhance the contextual information and feature expression ability of effective features, thereby improving the detection effect of small targets. For the problem of missdetection and false detection of multi-scale targets in images, Wang et al. [<xref ref-type="bibr" rid="ref_25">25</xref>] proposed a feature fusion FMSSD (Feature-Merged Single-Shot Detection) algorithm based on the SSD algorithm. By adopting the dilated spatial feature pyramid module, regional weighted cost function, and optimized loss calculation method, etc., it enhanced the detection accuracy of multi-scale targets. Baig et al. [<xref ref-type="bibr" rid="ref_26">26</xref>] addressed the issues of uneven target distribution and drastic size variations, and proposed a rotation target detection algorithm that integrated convolutional channel attention. Bian et al. [<xref ref-type="bibr" rid="ref_27">27</xref>] addressed the problem of poor multi-scale object detection and proposed a cross-scale connection operation to enhance the feature extraction capability of the algorithm. To address the above problems, this paper proposes a novel adaptive multi-scale gated convolution and context-aware attention model for small object detection. The main contributions are as follows.</p><p>(1) By replacing the traditional convolution blocks with the multi-scale gate convolution module (MGCM), this approach enables the model to have a larger receptive field through multi-scale feature extraction, which can adapt to lesion regions of different sizes and shapes. At the same time, it combines spatial and channel attention as well as an improved attention gate mechanism to redistribute the weights of spatial and channel features, enhancing the response of lesion features to distinguish the target from the background regions, thereby more accurately locating and segmenting the target regions.</p><p>(2) To fully utilize the rich image feature information of the U-shaped network, a multi-information fusion enhancement module (MFEM) is introduced at the jump connection. By improving the dense dilated convolution, the abundant semantic information at the bottom of the U-shaped network is fully extracted, and a mixed-enhanced residual window attention is constructed to capture the local and global features of the current layer of the U-shaped network.</p><p>(3) We design a local context enhancement function (LCE), which uses parameter fine-tuning to better focus on the context information required for different categories and scales of small targets, enhance the background information, and reduce false detection.</p>
    </sec>
    <sec sec-type="">
      <title>2. Proposed method for small object detection</title>
      <p>The traditional object segmentation models often suffer from insufficient segmentation accuracy and are prone to be affected by the background region. In this paper, a object segmentation model that integrates multi-scale gated convolution and window attention is proposed. The overall architecture of the network model is shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>. The proposed model replaces the traditional convolution blocks with MGCM to fully extract the feature information of objects, and changes the number of channels of the input image to 32. Secondly, the maximum pooling [<xref ref-type="bibr" rid="ref_28">28</xref>], [<xref ref-type="bibr" rid="ref_29">29</xref>] is used for downsampling operation to reduce the information loss at the skip connection and to utilize the rich image semantic information contained in the data at the bottom of the model. A MFEM is introduced at the skip connection to mine the feature information of the current layer of the U-shaped network and to enhance the features using the data at the bottom of the model. Finally, the result of the last decoding layer is input into a 1 <inline-formula>
  <mml:math id="meai38n2ec">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 1 convolution with a Sigmoid activation function to obtain the final predicted object segmentation image, thereby achieving precise detection of the object.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Proposed model</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_hfxpbWVZX22UMXM2.jpeg"/>
        </fig>
      
      
        <sec>
          
            <title>2.1. Multi-scale gated convolution module (mgcm)</title>
          
          <p>The traditional convolutional blocks have difficulty in specifically training the target pixels during the training process, which may lead to challenges when adapting to targets of different sizes, shapes, and with lower contrast. Inspired by reference [<xref ref-type="bibr" rid="ref_30">30</xref>], this paper designs the MGCM to address these issues. It mainly utilizes the multi-scale gated squeeze-excitation module (MGSM) to achieve more accurate recognition and modeling of targets with different sizes and shapes. At the same time, the double residual attention module (DRAM) and the double attention gate module (DAGM) are adopted to enhance the response of the target features, further distinguishing the target from the surrounding background areas. Compared with traditional networks that capture single-scale features using a single branch, MGSM adopts a parallel branch design consisting of four convolutional blocks of different sizes. At the final stage of the four branches, it references the compressed activation blocks and the attention gate mechanism to design a gate squeeze-excitation block (Gate-SE Block). Multiscale processing can not only increase the receptive field but also capture feature information at different scales, thereby better identifying and covering various shapes, sizes and positions of the lesion areas in the feature map. The compressed activation block dynamically adjusts the weights of the feature maps by learning the correlation dependencies between the channels, so that more important features receive larger weights, which helps to enhance the expressive power of the feature representation. The attention gate can automatically learn and focus on the relevant parts, thereby enhancing the attention to important information. Therefore, in this paper, combining the compression incentive block with the attention gate can enable the network to no longer simply integrate all channel information, but to flexibly emphasize the most significant and important features in the current task and suppress the unimportant ones. At the same time, adding the Gate-SE Block in multiple scales can selectively enhance or weaken the features at specific scales. The network is more likely to learn the universal features and adapt to different sizes and shapes, avoiding the occurrence of overfitting.</p><p>In the MGSM module, it is assumed that the input feature map is <inline-formula>
  <mml:math id="mq0gbydg9m">
    <mml:mtext>A</mml:mtext>
    <mml:mo>∈</mml:mo>
    <mml:msup>
      <mml:mtext>R</mml:mtext>
      <mml:mrow>
        <mml:mtext>C</mml:mtext>
        <mml:mtext>H</mml:mtext>
        <mml:mtext>W</mml:mtext>
        <mml:mo>×</mml:mo>
        <mml:mo>×</mml:mo>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula>, where C represents the channel number of the input image. H and W denote the pixel size of the height and width of the input image respectively. First, the input A is sent to the 4 paths for different-sized convolution operations to generate the feature map <inline-formula>
  <mml:math id="mp3i3k8xnh">
    <mml:mrow>
      <mml:mover>
        <mml:mtext>A</mml:mtext>
        <mml:mo>^</mml:mo>
      </mml:mover>
    </mml:mrow>
  </mml:math>
</inline-formula> for each path. Then, A and <inline-formula>
  <mml:math id="mbu96qrbvz">
    <mml:mrow>
      <mml:mover>
        <mml:mtext>A</mml:mtext>
        <mml:mo>^</mml:mo>
      </mml:mover>
    </mml:mrow>
  </mml:math>
</inline-formula> are input into the Gate-SE block. In the Gate-SE Block module, <inline-formula>
  <mml:math id="m51ppdcskm">
    <mml:mrow>
      <mml:mover>
        <mml:mtext>A</mml:mtext>
        <mml:mo>^</mml:mo>
      </mml:mover>
    </mml:mrow>
  </mml:math>
</inline-formula> is processed through the SE Block and 1 <inline-formula>
  <mml:math id="mj0cwc142y">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 1 convolution, and then a matrix addition and non-linear activation (ReLU) operation is performed with the 1 <inline-formula>
  <mml:math id="mgeijc2i35">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 1 convolution processed A to generate the attention coefficient. Then, the attention coefficient is passed through the SE Block and combined with the attention coefficient that undergoes 1 <inline-formula>
  <mml:math id="mrumm7mr4v">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 1 convolution and Sigmoid operation, making the aligned weights in the attention coefficient larger. A is connected with the generated attention coefficient for output. Finally, the feature maps output by the 4 paths through the Gate-SE block are concatenated, and then a 1×1 convolution is used to change the channel dimension to generate a new feature map <inline-formula>
  <mml:math id="mxqq79wjc6">
    <mml:mtext>B</mml:mtext>
    <mml:mo>∈</mml:mo>
    <mml:msup>
      <mml:mtext>R</mml:mtext>
      <mml:mrow>
        <mml:mtext>C</mml:mtext>
        <mml:mtext>H</mml:mtext>
        <mml:mtext>W</mml:mtext>
        <mml:mo>×</mml:mo>
        <mml:mo>×</mml:mo>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula>.</p>
        </sec>
      
      
        <sec>
          
            <title>2.2. Multi-information fusion enhancement module</title>
          
          <p>Although replacing the traditional convolutional blocks with MGCM can alleviate the problem of gradient vanishing and fully extract the target feature information, during the encoding and decoding process, direct skip connections will inevitably introduce a certain degree of noise interference, thereby causing the phenomenon of missed segmentation. This paper takes into account that the local and global feature information of the current layer in the U-Net model have not been effectively utilized, and the bottom layer integrates spatially and contains a considerable amount of image semantic information. In order to reduce the phenomenon of missed points, this paper, based on the dense hole convolution block and the Swin Transformer [<xref ref-type="bibr" rid="ref_31">31</xref>], we design the MFEM.</p><p>For the detection of small targets at multiple scales, it is also necessary to pay more attention to the contextual information of small targets at different scales. Because for small targets, it is difficult to accurately identify them merely by their appearance. On the contrary, if we want to successfully identify these small targets, it often relies on background information. The surrounding environment can provide valuable clues about the shape, direction and other features of the targets. In the left of <xref ref-type="fig" rid="fig_2">Figure 2</xref>, due to the appearance characteristics of the target object within the red box, the algorithm might mistakenly identify it as a runway or a basketball court, but in fact, this target is not the object that needs to be detected. In the right side of <xref ref-type="fig" rid="fig_2">Figure 2</xref>, the true label of the object is an airplane. However, due to the presence of other objects in the yellow box, the algorithm may miss detecting this airplane. The occurrence of these erroneous detections is mainly because the algorithm only considers the limited contextual information near the object. Therefore, a context enhancement function that can better focus on the contextual information of different categories and scales of small objects is needed.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Contextual information diagrams required for different types of targets</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_K1bz_kvxyq-Bqj8h.jpeg"/>
            </fig>
          
          <p>In the fine-grained query-aware sparse attention module, the final fine-grained attention employs the LCE function, which can enhance the local context information. The calculation formula of the LCE function can be expressed as shown in Eq. (1).</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="mqn4ip3exu">
                <mml:mtext> LCE = DWConv(V) </mml:mtext>
              </mml:math>
            </disp-formula>
          
          <p>Here, V represents the value vector. DWConv denotes the depthwise convolution operation, which is capable of retaining more fine-grained and shallow details. The size of the convolution kernel in this operation is set to 5.</p><p>In order to effectively utilize the local and global features of the current layer of the U-shaped network, a better feature extraction is achieved by introducing the Swin Transformer Block (ST Block) and combining it with the channel attention (CAB) and spatial attention (SAB) of the DRAM module. Firstly, the ST Block's conventional window multi-head self-attention (W-MSA) is utilized to divide the input features into local windows, and calculate the self-attention within each small window. Then, the channel attention weights of the input features are calculated through CAB, and the two are added together to enhance the important features of each small window and suppress the non-important ones, thereby capturing the feature information of the local region. However, due to the division of the windows, the information interaction among different windows is impossible. Therefore, the Shifted Window Multi-Head Self-Attention (SW-MSA) is utilized to enable the feature information to be transmitted between adjacent windows. At the same time, SAB is added to retain the feature information in the original space, further enhancing the feature representation of the key areas. Finally, it is connected with the input features of the current layer through a residual connection, thereby better capturing the global structure and local details in the image. The mathematical definition is as follows:</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="m86kxvosft">
                <mml:msup>
                  <mml:mrow>
                    <mml:mover>
                      <mml:mrow>
                        <mml:mi>z</mml:mi>
                      </mml:mrow>
                      <mml:mo>^</mml:mo>
                    </mml:mover>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi>l</mml:mi>
                    </mml:mrow>
                  </mml:mrow>
                </mml:msup>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>z</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mtext>l</mml:mtext>
                    <mml:mo>−</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mrow>
                  <mml:mi>W</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi data-mjx-auto-op="false">MSA</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mrow>
                    <mml:mi data-mjx-auto-op="false">LN</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mi>z</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mtext>l</mml:mtext>
                        <mml:mo>−</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msup>
                  </mml:mrow>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi data-mjx-auto-op="false">CAB</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mi>z</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext>l</mml:mtext>
                      <mml:mo>−</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msup>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="mq8rygdsm1">
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>z</mml:mi>
                  </mml:mrow>
                  <mml:mtext>l</mml:mtext>
                </mml:msup>
                <mml:msup>
                  <mml:mrow>
                    <mml:mover>
                      <mml:mrow>
                        <mml:mi>z</mml:mi>
                      </mml:mrow>
                      <mml:mo>^</mml:mo>
                    </mml:mover>
                  </mml:mrow>
                  <mml:mtext>l</mml:mtext>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mi>MLP</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mrow>
                    <mml:mi data-mjx-auto-op="false">LN</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mover>
                          <mml:mrow>
                            <mml:mi>z</mml:mi>
                          </mml:mrow>
                          <mml:mo>^</mml:mo>
                        </mml:mover>
                      </mml:mrow>
                      <mml:mtext>l</mml:mtext>
                    </mml:msup>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="mgmr3emzeg">
                <mml:msup>
                  <mml:mrow>
                    <mml:mover>
                      <mml:mrow>
                        <mml:mi>z</mml:mi>
                      </mml:mrow>
                      <mml:mo>^</mml:mo>
                    </mml:mover>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi>l</mml:mi>
                    </mml:mrow>
                    <mml:mo>+</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>z</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi>l</mml:mi>
                    </mml:mrow>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mrow>
                  <mml:mi data-mjx-auto-op="false">SW</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi data-mjx-auto-op="false">MSA</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mrow>
                    <mml:mi data-mjx-auto-op="false">LN</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mi>z</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mi>l</mml:mi>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:msup>
                  </mml:mrow>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi data-mjx-auto-op="false">SAB</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mi>z</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mi>l</mml:mi>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:msup>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="mzt2h3vrh4">
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>z</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi>l</mml:mi>
                    </mml:mrow>
                    <mml:mo>+</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:msup>
                  <mml:mrow>
                    <mml:mover>
                      <mml:mrow>
                        <mml:mi>z</mml:mi>
                      </mml:mrow>
                      <mml:mo>^</mml:mo>
                    </mml:mover>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi>l</mml:mi>
                    </mml:mrow>
                    <mml:mo>+</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mrow>
                  <mml:mi data-mjx-auto-op="false">MLP</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mrow>
                    <mml:mi data-mjx-auto-op="false">LN</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mover>
                          <mml:mrow>
                            <mml:mi>z</mml:mi>
                          </mml:mrow>
                          <mml:mo>^</mml:mo>
                        </mml:mover>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mi>l</mml:mi>
                        </mml:mrow>
                        <mml:mo>+</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msup>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(6)</label>
              <mml:math id="mn9poqtwre">
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>x</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>z</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi>l</mml:mi>
                    </mml:mrow>
                    <mml:mo>+</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msup>
                <mml:msup>
                  <mml:mrow>
                    <mml:mi>z</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi>l</mml:mi>
                    </mml:mrow>
                    <mml:mo>−</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msup>
              </mml:math>
            </disp-formula>
          
          <p> where, <inline-formula>
  <mml:math id="mbs4ykwgiw">
    <mml:msup>
      <mml:mrow>
        <mml:mover>
          <mml:mrow>
            <mml:mi>z</mml:mi>
          </mml:mrow>
          <mml:mo>^</mml:mo>
        </mml:mover>
      </mml:mrow>
      <mml:mn>1</mml:mn>
    </mml:msup>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mhu6mszz6b">
    <mml:msup>
      <mml:mrow>
        <mml:mi>z</mml:mi>
      </mml:mrow>
      <mml:mn>1</mml:mn>
    </mml:msup>
  </mml:math>
</inline-formula> represent the output features of the SW-MSA module and the MLP module respectively. W-MSA and SW-MSA represent the use of conventional and shifted window multi-head self-attention respectively. LN indicates the layer normalization operation. CAB and SAB represent channel attention and spatial attention respectively. <inline-formula>
  <mml:math id="mic7uzw4jx">
    <mml:msup>
      <mml:mtext>z</mml:mtext>
      <mml:mrow>
        <mml:mtext>l</mml:mtext>
        <mml:mo>−</mml:mo>
        <mml:mn>1</mml:mn>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> represents the input image at the jump connection. <inline-formula>
  <mml:math id="mxl5k2ia8p">
    <mml:msub>
      <mml:mrow>
        <mml:mi>x</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mrow>
          <mml:mi>i</mml:mi>
        </mml:mrow>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> represents the output result after passing through the hybrid-enhanced residual window attention module (HERWM).</p><p>Finally, in order to integrate the rich semantic information of the bottom data with the feature information of the current layer, as shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>, L is processed through 1 <inline-formula>
  <mml:math id="mrsln50hvi">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 1 convolution, average pooling, and Softmax activation function to generate a spatial weight map. This spatial weight map is then multiplied with the output result of HERWM, and finally added to J to output the feature map <inline-formula>
  <mml:math id="mqyaitiu1x">
    <mml:mtext>M</mml:mtext>
    <mml:mo>∈</mml:mo>
    <mml:msup>
      <mml:mtext>R</mml:mtext>
      <mml:mrow>
        <mml:mtext>C</mml:mtext>
        <mml:mtext>H</mml:mtext>
        <mml:mtext>W</mml:mtext>
        <mml:mo>×</mml:mo>
        <mml:mo>×</mml:mo>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula>.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>HERWM module</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_HPeMp9cMYmt17DR9.jpeg"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>2.3. Loss function</title>
          
          <p>The original network uses SmoothL1 as the loss function. When multiple detection boxes have the same SmoothL1 loss value, there may be significant differences in their Intersection-over-Union (IoU) with the real boxes. The actual evaluation of the regression indicators of the detection boxes uses IoU. At this time, calculating the SmoothL1 loss value cannot reflect the degree of overlap between the detection box and the real box.</p><p>This paper uses the DIoU loss, which is an extension of the IoU loss by adding a penalty term. It minimizes and normalizes the distance between the centers of the two bounding boxes, thereby accelerating the convergence process. The <inline-formula>
  <mml:math id="m8i4wccb9j">
    <mml:msub>
      <mml:mrow>
        <mml:mi>R</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mtext>DIoU </mml:mtext>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> is defined as follows:</p>
          
            <disp-formula>
              <label>(7)</label>
              <mml:math id="mg3nkvxjmv">
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>R</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">DIoU</mml:mi>
                    </mml:mrow>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msup>
                      <mml:mi>ρ</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msup>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mrow>
                        <mml:mi>b</mml:mi>
                      </mml:mrow>
                      <mml:msup>
                        <mml:mrow>
                          <mml:mtext> </mml:mtext>
                          <mml:mi>b</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mi data-mjx-auto-op="false">gt</mml:mi>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:msup>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:msup>
                    <mml:mtext>l</mml:mtext>
                    <mml:mn>2</mml:mn>
                  </mml:msup>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>Here, b and <inline-formula>
  <mml:math id="mn581xhaqq">
    <mml:msup>
      <mml:mtext>b</mml:mtext>
      <mml:mrow>
        <mml:mrow>
          <mml:mi data-mjx-auto-op="false">gt</mml:mi>
        </mml:mrow>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> represent the center points of the predicted box and the true box respectively. <inline-formula>
  <mml:math id="mwzftymae4">
    <mml:msup>
      <mml:mi>ρ</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msup>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mtext>b</mml:mtext>
      <mml:msup>
        <mml:mtext>b</mml:mtext>
        <mml:mrow>
          <mml:mtext>g</mml:mtext>
          <mml:mtext>t</mml:mtext>
        </mml:mrow>
      </mml:msup>
    </mml:mrow>
  </mml:math>
</inline-formula> represents the Euclidean distance between the two points. l represents the diagonal length of the minimum bounding box that contains the two boxes. Therefore, the loss function of <inline-formula>
  <mml:math id="mbvedk8x6u">
    <mml:msub>
      <mml:mrow>
        <mml:mi>L</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mtext>DIoU </mml:mtext>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> can be defined as:</p>
          
            <disp-formula>
              <label>(8)</label>
              <mml:math id="mzeplj9v1j">
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>L</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">DIoU</mml:mi>
                    </mml:mrow>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>R</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">DIoU</mml:mi>
                    </mml:mrow>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mn>1</mml:mn>
                <mml:mrow>
                  <mml:mi data-mjx-auto-op="false">IoU</mml:mi>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>The schematic diagram of <inline-formula>
  <mml:math id="mle04za4e6">
    <mml:msub>
      <mml:mrow>
        <mml:mi>L</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mtext>DIoU</mml:mtext>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> is shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>. Here, <inline-formula>
  <mml:math id="mcla4mb9ea">
    <mml:mtext>d</mml:mtext>
    <mml:mo>=</mml:mo>
    <mml:mi>ρ</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mtext>b</mml:mtext>
      <mml:msup>
        <mml:mtext>b</mml:mtext>
        <mml:mrow>
          <mml:mtext>g</mml:mtext>
          <mml:mtext>t</mml:mtext>
        </mml:mrow>
      </mml:msup>
    </mml:mrow>
  </mml:math>
</inline-formula> represents the distance between the centers of the two bounding boxes.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>$\mathrm{L}_{\text {DIoU }}$ calculation principle</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_rHNJkfo3tsU_tdKe.jpeg"/>
            </fig>
          
          <p>Compared to SmoothL1, <inline-formula>
  <mml:math id="mwedt60s7s">
    <mml:msub>
      <mml:mrow>
        <mml:mi>L</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mtext>DIoU </mml:mtext>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> can better minimize the distance between the predicted box and the real box. When there are cases such as containment, perpendicularity, and parallelism between two bounding boxes, <inline-formula>
  <mml:math id="m3h6g3wpkf">
    <mml:msub>
      <mml:mrow>
        <mml:mi>L</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mtext>DIoU </mml:mtext>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> enables the predicted box to regress more quickly.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Results and discussion</title>
      
        <sec>
          
            <title>3.1. Dataset</title>
          
          <p>This experiment utilizes four publicly available small object detection datasets: VisDrone2019, KITTI, TinyPerson, and DOTAv1.0. The VisDrone2019 dataset is a large-scale dataset collected by the AISKYEYE team from Tianjin University. It is shot in 14 different cities across China, under various scenarios and with different lighting conditions, covering different environments such as urban and rural areas. The data is highly diverse and complex. It includes ten categories such as pedestrians, cars, trucks, buses, tricycles, bicycles, and motorcycles. The size distribution of the targets shows a distinct small target characteristic, with most targets having small pixel sizes, mainly concentrated within the range of 1501 pixels in size. The training set uses 6471 images, the validation set has 548 images, and the test set employs 1610 images.</p><p>The KITTI dataset [<xref ref-type="bibr" rid="ref_32">32</xref>] is jointly created by the Karlsruhe Institute of Technology in Germany and the Toyota Technical Institute in the United States. It is sourced from urban environments and includes scenes with busy traffic and overlapping targets, covering driving data under various weather and urban conditions. The target categories include cars, trucks, pedestrians, cyclists, etc., with most targets being small-sized targets with dimensions concentrated within the range of 30 to 150 pixels. The dataset is divided into training images (5236), validation images (748), and test images (1497).</p><p>The TinyPerson dataset [<xref ref-type="bibr" rid="ref_33">33</xref>] is a benchmark dataset specifically focused on small object detection. It contains 1610 labeled images, among which there are 1261 training images, 158 validation images, and 158 test images.</p><p>The images in the DOTAv1.0 dataset are sourced from aerial images collected by various sensors and platforms, covering a wide range of real-world scenarios, thus making the dataset comprehensive.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Experimental environment</title>
          
          <p>The hardware experiments in this paper are all conducted on the Ubuntu 20.04 operating system, using an RTX 3090 GPU with 24GB of memory and a 14-core Intel(R) Xeon(R) Platinum 8362 CPU operating at 2.80GHz. The experimental environment employs Python 3.8.19 and Pytorch 1.11.0 with CUDA version 11.3. To ensure the fairness of the experiments, the initial learning rate is uniformly set to 0.0001 during the training stage, using the AdamW optimizer with a weight decay value of 0.0001. The batch size and the number of worker processes are both set to 4. Additionally, all experiments do not use pre-trained weights. Depending on the characteristics of different datasets, the number of training epochs is different.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Evaluation index</title>
          
          <p>In order to conduct a comprehensive and in-depth assessment of the detection capabilities and performance of the model, this paper carefully selects a series of scientific and effective evaluation indicators. Mean Average Precision (mAP), including mAP@0.5 and mAP@0.5:0.95, is used to evaluate the accuracy of the model in detecting and classifying targets. Precision (P) is used to measure the proportion of positive samples that the model predicts as positive and are actually positive. Recall (R) reflects the proportion of positive samples that are actually positive and are correctly predicted as positive. Parameters (Params) are a key indicator for measuring the complexity of the model. Computational cost (GFLOPs) can effectively evaluate the computational complexity during the model's operation. The following are the calculation formulas for Recall, Precision, and Average Precision.</p>
          
            <disp-formula>
              <label>(9)</label>
              <mml:math id="mghjcp77ud">
                <mml:mrow>
                  <mml:mi data-mjx-auto-op="false">mAP</mml:mi>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mn>1</mml:mn>
                  <mml:mrow>
                    <mml:mtext> </mml:mtext>
                    <mml:mi>N</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
                <mml:munderover>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi>N</mml:mi>
                    </mml:mrow>
                  </mml:mrow>
                </mml:munderover>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi data-mjx-auto-op="false">AP</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:mrow>
                  </mml:mrow>
                </mml:msub>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(10)</label>
              <mml:math id="m0ruq71hkc">
                <mml:mtext> Precision </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi data-mjx-auto-op="false">TP</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">TP</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">FP</mml:mi>
                    </mml:mrow>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(11)</label>
              <mml:math id="msopqhthlb">
                <mml:mtext> Recall </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi data-mjx-auto-op="false">TP</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">TP</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">FN</mml:mi>
                    </mml:mrow>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>TP (True Positive) refers to the number of true positive examples, which is the quantity of positive samples accurately predicted by the model. FP (False Positive) refers to the number of false positive examples, which is the quantity of negative samples misjudged by the model as positive. FN (False Negative) refers to the number of false negative examples, representing the quantity of positive samples misjudged by the model as negative. If T is the time (in seconds) required to process one image, then the FPS calculation formula is:</p>
          
            <disp-formula>
              <label>(12)</label>
              <mml:math id="m8jtfqh6i4">
                <mml:mrow>
                  <mml:mi data-mjx-auto-op="false">FPS</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>/</mml:mo>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>T</mml:mi>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mn>1</mml:mn>
              </mml:math>
            </disp-formula>
          
        </sec>
      
      
        <sec>
          
            <title>3.4. Ablation experiment</title>
          
          <p>This paper employs multiple nodules to improve the proposed model. To verify the effectiveness of the proposed modules, ablation experiments are conducted on the public VisDrone2019 small target dataset. The specific data is shown in <xref ref-type="table" rid="table_1">Table 1</xref>. The experimental results show that the MGCM module demonstrates significant advantages. Compared to the baseline model, it has improved by 1.1% in the mAP@0.5, and by 0.5% in the mAP@0.5:0.95. The parameter count and computational cost have been reduced by 27.1% and 13.2% respectively. The MFEM module has improved by 0.1% in mAP@0.5 and mAP@0.5:0.95, and by 0.7% and 0.8% respectively in mAP@0.5:0.95 compared to the baseline model. LCE has improved by 0.7% and 0.8% in mAP@0.5 and mAP@0.5:0.95 compared to the baseline model.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Ablation experiment results on the VisDrone2019 dataset</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>No.</p></th><th colspan="1" rowspan="1"><p>MGCM</p></th><th colspan="1" rowspan="1"><p>MFEM</p></th><th colspan="1" rowspan="1"><p>LCE</p></th><th colspan="1" rowspan="1"><p>P/%</p></th><th colspan="1" rowspan="1"><p>R/%</p></th><th colspan="1" rowspan="1"><p>mAP@0.5/%</p></th><th colspan="1" rowspan="1"><p>mAP@0.5:0.95/%</p></th><th colspan="1" rowspan="1"><p>GFLOPs</p></th><th colspan="1" rowspan="1"><p>Params</p></th><th colspan="1" rowspan="1"><p>FPS</p></th></tr><tr><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p><mml:math id="m4c0jfmteb">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="m08nkh8cp5">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="mfo0u4wpd9">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1"><p>62.3</p></td><td colspan="1" rowspan="1"><p>46.7</p></td><td colspan="1" rowspan="1"><p>48.0</p></td><td colspan="1" rowspan="1"><p>29.4</p></td><td colspan="1" rowspan="1"><p>57.0</p></td><td colspan="1" rowspan="1"><p>19.9</p></td><td colspan="1" rowspan="1"><p>71.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p><mml:math id="mjf790rk41">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="m6ju131vqx">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="mq647xyn89">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1"><p>61.7</p></td><td colspan="1" rowspan="1"><p>47.9</p></td><td colspan="1" rowspan="1"><p>49.1</p></td><td colspan="1" rowspan="1"><p>29.9</p></td><td colspan="1" rowspan="1"><p>49.5</p></td><td colspan="1" rowspan="1"><p>14.5</p></td><td colspan="1" rowspan="1"><p>51.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p><mml:math id="mfo087y9fq">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="mkzrdrly6o">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="mv72jnlmz4">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1"><p>62.2</p></td><td colspan="1" rowspan="1"><p>46.4</p></td><td colspan="1" rowspan="1"><p>48.1</p></td><td colspan="1" rowspan="1"><p>29.7</p></td><td colspan="1" rowspan="1"><p>57.1</p></td><td colspan="1" rowspan="1"><p>20.0</p></td><td colspan="1" rowspan="1"><p>74.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p><mml:math id="mcyoxldsil">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="mlam2sq8zy">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="mcem6joks9">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p>62.2</p></td><td colspan="1" rowspan="1"><p>46.9</p></td><td colspan="1" rowspan="1"><p>48.7</p></td><td colspan="1" rowspan="1"><p>30.2</p></td><td colspan="1" rowspan="1"><p>57.0</p></td><td colspan="1" rowspan="1"><p>19.9</p></td><td colspan="1" rowspan="1"><p>71.7</p></td></tr><tr><td colspan="1" rowspan="1"><p>5</p></td><td colspan="1" rowspan="1"><p><mml:math id="mdcg3eafmj">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="mmt9j70mcp">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="mn6852wss6">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1"><p>62.6</p></td><td colspan="1" rowspan="1"><p>48.3</p></td><td colspan="1" rowspan="1"><p>49.6</p></td><td colspan="1" rowspan="1"><p>30.5</p></td><td colspan="1" rowspan="1"><p>49.6</p></td><td colspan="1" rowspan="1"><p>14.6</p></td><td colspan="1" rowspan="1"><p>52.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p><mml:math id="m3yn4xbwux">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="ml6xd103kr">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="mt1vu92sb3">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p>62.4</p></td><td colspan="1" rowspan="1"><p>48.5</p></td><td colspan="1" rowspan="1"><p>50.0</p></td><td colspan="1" rowspan="1"><p>30.9</p></td><td colspan="1" rowspan="1"><p>49.5</p></td><td colspan="1" rowspan="1"><p>14.5</p></td><td colspan="1" rowspan="1"><p>53.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>7</p></td><td colspan="1" rowspan="1"><p><mml:math id="m0g41tmwuu">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="mimrb97cvk">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="mo6ci0cyiw">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p>62.0</p></td><td colspan="1" rowspan="1"><p>47.3</p></td><td colspan="1" rowspan="1"><p>49.0</p></td><td colspan="1" rowspan="1"><p>30.4</p></td><td colspan="1" rowspan="1"><p>57.1</p></td><td colspan="1" rowspan="1"><p>20.0</p></td><td colspan="1" rowspan="1"><p>76.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>8</p></td><td colspan="1" rowspan="1"><p><mml:math id="mnl2h7puew">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="mwe6aevwok">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="mrc2k5b3i2">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p>64.3</p></td><td colspan="1" rowspan="1"><p>48.8</p></td><td colspan="1" rowspan="1"><p>50.8</p></td><td colspan="1" rowspan="1"><p>31.7</p></td><td colspan="1" rowspan="1"><p>49.6</p></td><td colspan="1" rowspan="1"><p>14.6</p></td><td colspan="1" rowspan="1"><p>54.6</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>Then we conduct an ablation experiment on the loss functions. To fully verify the effectiveness of the <inline-formula>
  <mml:math id="m7fsb8sj1b">
    <mml:msub>
      <mml:mrow>
        <mml:mi>L</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mrow>
          <mml:mi data-mjx-auto-op="false">DIoU</mml:mi>
        </mml:mrow>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> loss function, an ablation experiment is conducted by comparing it with GIoU and other commonly used IoU (such as CIoU, DloU, InnerloU, MPDIoU) on the VisDrone2019 dataset. The experimental results shown in <xref ref-type="table" rid="table_2">Table 2</xref> indicate that the mAP@50 and mAP@50:95 of the <inline-formula>
  <mml:math id="mavk1wa8jv">
    <mml:msub>
      <mml:mrow>
        <mml:mi>L</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mrow>
          <mml:mi data-mjx-auto-op="false">DIoU</mml:mi>
        </mml:mrow>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> loss function have reached 48.7% and 30.2% respectively, which are significantly higher than those of the traditional IoU. This loss function has greatly improved the accuracy of boundary positioning. This refined optimization mechanism is particularly suitable for small object detection. Since the proportion of pixels for small objects is low, the impact of boundary offset on IoU is more sensitive. <inline-formula>
  <mml:math id="mdf6fb0l7u">
    <mml:msub>
      <mml:mrow>
        <mml:mi>L</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mrow>
          <mml:mi data-mjx-auto-op="false">DIoU</mml:mi>
        </mml:mrow>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> achieves dual enhancements of sensitivity to the boundaries of small targets and consistency of internal features through the multi-point distance penalty of MPD-IoU and the internal region constraint of Inner-IoU. This forces the model to fit the target boundaries more strictly while suppressing background noise. This effect fully validates the superiority and effectiveness of the loss function of the improved model.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>IoU ablation experiment results</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>IoU</p></th><th colspan="1" rowspan="1"><p>GIoU</p></th><th colspan="1" rowspan="1"><p>CIoU</p></th><th colspan="1" rowspan="1"><p>DIoU</p></th><th colspan="1" rowspan="1"><p>InnerIoU</p></th><th colspan="1" rowspan="1"><p>MPDIoU</p></th><th colspan="1" rowspan="1"><p>L<mml:math id="meigk6p8b8">
  <mml:msub>
    <mml:mi/>
    <mml:mrow>
      <mml:mtext>DIoU </mml:mtext>
    </mml:mrow>
  </mml:msub>
</mml:math></p></th></tr><tr><td colspan="1" rowspan="1"><p>mAP@ 50</p></td><td colspan="1" rowspan="1"><p>48</p></td><td colspan="1" rowspan="1"><p>47.9</p></td><td colspan="1" rowspan="1"><p>47.6</p></td><td colspan="1" rowspan="1"><p>47.6</p></td><td colspan="1" rowspan="1"><p>47.5</p></td><td colspan="1" rowspan="1"><p>48.7</p></td></tr><tr><td colspan="1" rowspan="1"><p>mAP@ 50: 95</p></td><td colspan="1" rowspan="1"><p>29.4</p></td><td colspan="1" rowspan="1"><p>29.5</p></td><td colspan="1" rowspan="1"><p>29.3</p></td><td colspan="1" rowspan="1"><p>29.2</p></td><td colspan="1" rowspan="1"><p>28.8</p></td><td colspan="1" rowspan="1"><p>30.2</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>This paper calculated the results of the VisDrone2019 test-dev for three independent experiments as shown in <xref ref-type="table" rid="table_3">Table 3</xref>. Here SD = standard deviation.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Test-dev results</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Index</p></th><th colspan="1" rowspan="1"><p>Average</p></th><th colspan="1" rowspan="1"><p>SD</p></th><th colspan="1" rowspan="1"><p>95% CI (t-distribution, df = 2 )</p></th></tr><tr><td colspan="1" rowspan="1"><p>mAP@0.5</p></td><td colspan="1" rowspan="1"><p>0.507</p></td><td colspan="1" rowspan="1"><p>0.0018</p></td><td colspan="1" rowspan="1"><p>[50.2%, 51.2%]</p></td></tr><tr><td colspan="1" rowspan="1"><p>mAP@0.5: 0.95</p></td><td colspan="1" rowspan="1"><p>0.316</p></td><td colspan="1" rowspan="1"><p>0.0021</p></td><td colspan="1" rowspan="1"><p>[31.1%, 32.1%]</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>Taking VisDrone2019 as an example, this paper conducted a Welch's t-test between the 3-run results and DEIM, RT-DETR, and YOLOv11 as shown in <xref ref-type="table" rid="table_4">Table 4</xref>.</p>
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Confidence interval comparison with the mainstream models</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p><mml:math id="mcffg3ezzy">
  <mml:mi>Δ</mml:mi>
  <mml:mtext>mAP@0.5/%</mml:mtext>
</mml:math></p></th><th colspan="1" rowspan="1"><p>95% CI of <mml:math id="mtdjaf0ynt">
  <mml:mi>Δ</mml:mi>
</mml:math></p></th><th colspan="1" rowspan="1"><p>p Value</p></th></tr><tr><td colspan="1" rowspan="1"><p>DEIM</p></td><td colspan="1" rowspan="1"><p>0.3</p></td><td colspan="1" rowspan="1"><p>[$-<mml:math id="mr04qnd9a2">
  <mml:mn>0.1</mml:mn>
  <mml:mo>,</mml:mo>
</mml:math>+<mml:math id="m7jo7xvhv8">
  <mml:mn>0.7</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>0.063</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>2.8</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mo>]</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>[</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>R</mml:mi>
  <mml:mi>T</mml:mi>
  <mml:mi>D</mml:mi>
  <mml:mi>E</mml:mi>
  <mml:mi>T</mml:mi>
  <mml:mi>R</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
</mml:math>+<mml:math id="m9iz7olck9">
  <mml:mn>2.4</mml:mn>
  <mml:mo>,</mml:mo>
</mml:math>+<mml:math id="m5fg15flh0">
  <mml:mn>3.2</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mo>]</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
</mml:math>&lt;<mml:math id="mrd2ygbsmd">
  <mml:mn>0.001</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>11</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>6.8</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>[</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>Y</mml:mi>
  <mml:mi>O</mml:mi>
  <mml:mi>L</mml:mi>
  <mml:mi>O</mml:mi>
  <mml:mi>v</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
</mml:math>+<mml:math id="mxleef81n3">
  <mml:mn>6.3</mml:mn>
  <mml:mo>,</mml:mo>
</mml:math>+<mml:math id="mhqmpywmbv">
  <mml:mn>7.3</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mo>]</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
</mml:math>&lt;<mml:math id="mtipe8jxtk">
  <mml:mn>0.001</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>8.8</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>[</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>P</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
</mml:math>+<mml:math id="mmen7xmbkd">
  <mml:mn>7.9</mml:mn>
  <mml:mo>,</mml:mo>
</mml:math>+<mml:math id="mpeyvtxm1o">
  <mml:mn>8.5</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mo>]</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
</mml:math>&lt;$ 0.001</p></td></tr></tbody></table>
            </table-wrap>
          
          <p> <xref ref-type="fig" rid="fig_5">Figure 5</xref> shows the testing error. From the above results, we can know that the statistical results of this method are also superior to those of other methods.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Testing error</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_9NZf0xr4wLN89g26.jpeg"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.5. Comparison experiments</title>
          
          <p>To further verify the superiority of the proposed model in the detection of small targets, we select the VisDrone2019 and KITTI datasets, and compare our algorithm with numerous mainstream baseline algorithms as well as other advanced algorithms (EMRT-DETR [<xref ref-type="bibr" rid="ref_34">34</xref>], SCP-DETR [<xref ref-type="bibr" rid="ref_35">35</xref>], DMU-YOLO [<xref ref-type="bibr" rid="ref_36">36</xref>], DEIM [<xref ref-type="bibr" rid="ref_37">37</xref>], RT-DETR [<xref ref-type="bibr" rid="ref_38">38</xref>]). Results are shown in <xref ref-type="table" rid="table_5">Table 5</xref>, <xref ref-type="table" rid="table_6">Table 6</xref>, <xref ref-type="fig" rid="fig_6">Figure 6</xref>, <xref ref-type="fig" rid="fig_7">Figure 7</xref>.</p>
          
            <table-wrap id="table_5">
              <label>Table 5</label>
              <caption>
                <title>Comparison results on VisDrone2019</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p>P/%</p></th><th colspan="1" rowspan="1"><p>R/%</p></th><th colspan="1" rowspan="1"><p>mAP@0.5/%</p></th><th colspan="1" rowspan="1"><p>mAP@0.5:0.95/%</p></th><th colspan="1" rowspan="1"><p>GFLOPs</p></th><th colspan="1" rowspan="1"><p>Params</p></th><th colspan="1" rowspan="1"><p>FPS</p></th></tr><tr><td colspan="1" rowspan="1"><p>Faster RCNN</p></td><td colspan="1" rowspan="1"><p>49.5</p></td><td colspan="1" rowspan="1"><p>36.6</p></td><td colspan="1" rowspan="1"><p>39.1</p></td><td colspan="1" rowspan="1"><p>23.3</p></td><td colspan="1" rowspan="1"><p>208.0</p></td><td colspan="1" rowspan="1"><p>41.4</p></td><td colspan="1" rowspan="1"><p>31.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>Cascade RCNN</p></td><td colspan="1" rowspan="1"><p>50.1</p></td><td colspan="1" rowspan="1"><p>36.5</p></td><td colspan="1" rowspan="1"><p>39.5</p></td><td colspan="1" rowspan="1"><p>23.9</p></td><td colspan="1" rowspan="1"><p>236.0</p></td><td colspan="1" rowspan="1"><p>69.3</p></td><td colspan="1" rowspan="1"><p>25.7</p></td></tr><tr><td colspan="1" rowspan="1"><p>RetinaNet</p></td><td colspan="1" rowspan="1"><p>47.7</p></td><td colspan="1" rowspan="1"><p>38.0</p></td><td colspan="1" rowspan="1"><p>37.0</p></td><td colspan="1" rowspan="1"><p>22.2</p></td><td colspan="1" rowspan="1"><p>210.0</p></td><td colspan="1" rowspan="1"><p>36.5</p></td><td colspan="1" rowspan="1"><p>33.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>GFL</p></td><td colspan="1" rowspan="1"><p>45.0</p></td><td colspan="1" rowspan="1"><p>35.9</p></td><td colspan="1" rowspan="1"><p>34.4</p></td><td colspan="1" rowspan="1"><p>20.3</p></td><td colspan="1" rowspan="1"><p>206.0</p></td><td colspan="1" rowspan="1"><p>32.3</p></td><td colspan="1" rowspan="1"><p>31.9</p></td></tr><tr><td colspan="1" rowspan="1"><p>TOOD</p></td><td colspan="1" rowspan="1"><p>48.8</p></td><td colspan="1" rowspan="1"><p>37.6</p></td><td colspan="1" rowspan="1"><p>37.2</p></td><td colspan="1" rowspan="1"><p>22.1</p></td><td colspan="1" rowspan="1"><p>199.0</p></td><td colspan="1" rowspan="1"><p>32.0</p></td><td colspan="1" rowspan="1"><p>25.0</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv5m</p></td><td colspan="1" rowspan="1"><p>52.1</p></td><td colspan="1" rowspan="1"><p>41.1</p></td><td colspan="1" rowspan="1"><p>41.7</p></td><td colspan="1" rowspan="1"><p>25.2</p></td><td colspan="1" rowspan="1"><p>64.0</p></td><td colspan="1" rowspan="1"><p>25.1</p></td><td colspan="1" rowspan="1"><p>120.4</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv51</p></td><td colspan="1" rowspan="1"><p>55.8</p></td><td colspan="1" rowspan="1"><p>41.7</p></td><td colspan="1" rowspan="1"><p>43.3</p></td><td colspan="1" rowspan="1"><p>26.5</p></td><td colspan="1" rowspan="1"><p>134.7</p></td><td colspan="1" rowspan="1"><p>53.1</p></td><td colspan="1" rowspan="1"><p>72.9</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv8m</p></td><td colspan="1" rowspan="1"><p>53.0</p></td><td colspan="1" rowspan="1"><p>41.1</p></td><td colspan="1" rowspan="1"><p>42.0</p></td><td colspan="1" rowspan="1"><p>25.7</p></td><td colspan="1" rowspan="1"><p>78.7</p></td><td colspan="1" rowspan="1"><p>25.9</p></td><td colspan="1" rowspan="1"><p>103.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv81</p></td><td colspan="1" rowspan="1"><p>55.1</p></td><td colspan="1" rowspan="1"><p>42.8</p></td><td colspan="1" rowspan="1"><p>44.1</p></td><td colspan="1" rowspan="1"><p>27.2</p></td><td colspan="1" rowspan="1"><p>164.9</p></td><td colspan="1" rowspan="1"><p>43.6</p></td><td colspan="1" rowspan="1"><p>69.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv9m</p></td><td colspan="1" rowspan="1"><p>55.1</p></td><td colspan="1" rowspan="1"><p>41.3</p></td><td colspan="1" rowspan="1"><p>43.4</p></td><td colspan="1" rowspan="1"><p>26.6</p></td><td colspan="1" rowspan="1"><p>76.5</p></td><td colspan="1" rowspan="1"><p>20.0</p></td><td colspan="1" rowspan="1"><p>79.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv9c</p></td><td colspan="1" rowspan="1"><p>56.4</p></td><td colspan="1" rowspan="1"><p>42.3</p></td><td colspan="1" rowspan="1"><p>44.1</p></td><td colspan="1" rowspan="1"><p>26.8</p></td><td colspan="1" rowspan="1"><p>102.4</p></td><td colspan="1" rowspan="1"><p>25.3</p></td><td colspan="1" rowspan="1"><p>68.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv10m</p></td><td colspan="1" rowspan="1"><p>54.1</p></td><td colspan="1" rowspan="1"><p>40.8</p></td><td colspan="1" rowspan="1"><p>42.0</p></td><td colspan="1" rowspan="1"><p>25.8</p></td><td colspan="1" rowspan="1"><p>63.5</p></td><td colspan="1" rowspan="1"><p>16.5</p></td><td colspan="1" rowspan="1"><p>80.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv101</p></td><td colspan="1" rowspan="1"><p>55.2</p></td><td colspan="1" rowspan="1"><p>42.4</p></td><td colspan="1" rowspan="1"><p>44.2</p></td><td colspan="1" rowspan="1"><p>27.4</p></td><td colspan="1" rowspan="1"><p>126.4</p></td><td colspan="1" rowspan="1"><p>25.7</p></td><td colspan="1" rowspan="1"><p>61.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv11m</p></td><td colspan="1" rowspan="1"><p>53.7</p></td><td colspan="1" rowspan="1"><p>42.5</p></td><td colspan="1" rowspan="1"><p>43.8</p></td><td colspan="1" rowspan="1"><p>26.8</p></td><td colspan="1" rowspan="1"><p>67.7</p></td><td colspan="1" rowspan="1"><p>20.0</p></td><td colspan="1" rowspan="1"><p>95.4</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv111</p></td><td colspan="1" rowspan="1"><p>55.3</p></td><td colspan="1" rowspan="1"><p>42.3</p></td><td colspan="1" rowspan="1"><p>44.0</p></td><td colspan="1" rowspan="1"><p>27.2</p></td><td colspan="1" rowspan="1"><p>86.6</p></td><td colspan="1" rowspan="1"><p>25.3</p></td><td colspan="1" rowspan="1"><p>63.4</p></td></tr><tr><td colspan="1" rowspan="1"><p>EMRT-DETR</p></td><td colspan="1" rowspan="1"><p>62.0</p></td><td colspan="1" rowspan="1"><p>46.6</p></td><td colspan="1" rowspan="1"><p>48.8</p></td><td colspan="1" rowspan="1"><p>30.5</p></td><td colspan="1" rowspan="1"><p>68.1</p></td><td colspan="1" rowspan="1"><p>13.8</p></td><td colspan="1" rowspan="1"><p>-</p></td></tr><tr><td colspan="1" rowspan="1"><p>SCP-DETR</p></td><td colspan="1" rowspan="1"><p>63.2</p></td><td colspan="1" rowspan="1"><p>47.3</p></td><td colspan="1" rowspan="1"><p>49.4</p></td><td colspan="1" rowspan="1"><p>30.2</p></td><td colspan="1" rowspan="1"><p>52.4</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>-</p></td></tr><tr><td colspan="1" rowspan="1"><p>DMU-YOLO</p></td><td colspan="1" rowspan="1"><p>58.6</p></td><td colspan="1" rowspan="1"><p>47.4</p></td><td colspan="1" rowspan="1"><p>48.1</p></td><td colspan="1" rowspan="1"><p>29.7</p></td><td colspan="1" rowspan="1"><p>89.5</p></td><td colspan="1" rowspan="1"><p>25.4</p></td><td colspan="1" rowspan="1"><p>-</p></td></tr><tr><td colspan="1" rowspan="1"><p>DEIM</p></td><td colspan="1" rowspan="1"><p>57.2</p></td><td colspan="1" rowspan="1"><p>49.1</p></td><td colspan="1" rowspan="1"><p>50.5</p></td><td colspan="1" rowspan="1"><p>30.9</p></td><td colspan="1" rowspan="1"><p>56.4</p></td><td colspan="1" rowspan="1"><p>19.2</p></td><td colspan="1" rowspan="1"><p>80.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>RT-DETR</p></td><td colspan="1" rowspan="1"><p>62.3</p></td><td colspan="1" rowspan="1"><p>46.7</p></td><td colspan="1" rowspan="1"><p>48.0</p></td><td colspan="1" rowspan="1"><p>29.4</p></td><td colspan="1" rowspan="1"><p>57.0</p></td><td colspan="1" rowspan="1"><p>19.9</p></td><td colspan="1" rowspan="1"><p>71.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>Proposed</p></td><td colspan="1" rowspan="1"><p>64.3</p></td><td colspan="1" rowspan="1"><p>48.8</p></td><td colspan="1" rowspan="1"><p>50.8</p></td><td colspan="1" rowspan="1"><p>31.7</p></td><td colspan="1" rowspan="1"><p>49.6</p></td><td colspan="1" rowspan="1"><p>14.6</p></td><td colspan="1" rowspan="1"><p>54.6</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <table-wrap id="table_6">
              <label>Table 6</label>
              <caption>
                <title>Comparison results on KITTI dataset</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p>P/%</p></th><th colspan="1" rowspan="1"><p>R/%</p></th><th colspan="1" rowspan="1"><p>mAP@0.5/%</p></th><th colspan="1" rowspan="1"><p>mAP@0.5:0.95%</p></th><th colspan="1" rowspan="1"><p>GFLOPs</p></th><th colspan="1" rowspan="1"><p>Params</p></th><th colspan="1" rowspan="1"><p>FPS</p></th></tr><tr><td colspan="1" rowspan="1"><p>Faster RCNN</p></td><td colspan="1" rowspan="1"><p>91.2</p></td><td colspan="1" rowspan="1"><p>85.7</p></td><td colspan="1" rowspan="1"><p>92.8</p></td><td colspan="1" rowspan="1"><p>67.4</p></td><td colspan="1" rowspan="1"><p>119.0</p></td><td colspan="1" rowspan="1"><p>41.4</p></td><td colspan="1" rowspan="1"><p>45.6</p></td></tr><tr><td colspan="1" rowspan="1"><p>Cascade RCNN</p></td><td colspan="1" rowspan="1"><p>93.0</p></td><td colspan="1" rowspan="1"><p>84.5</p></td><td colspan="1" rowspan="1"><p>92.9</p></td><td colspan="1" rowspan="1"><p>69.4</p></td><td colspan="1" rowspan="1"><p>147.0</p></td><td colspan="1" rowspan="1"><p>69.2</p></td><td colspan="1" rowspan="1"><p>33.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>RetinaNet</p></td><td colspan="1" rowspan="1"><p>86.7</p></td><td colspan="1" rowspan="1"><p>77.7</p></td><td colspan="1" rowspan="1"><p>85.4</p></td><td colspan="1" rowspan="1"><p>58.4</p></td><td colspan="1" rowspan="1"><p>113.0</p></td><td colspan="1" rowspan="1"><p>36.4</p></td><td colspan="1" rowspan="1"><p>50.0</p></td></tr><tr><td colspan="1" rowspan="1"><p>GFL</p></td><td colspan="1" rowspan="1"><p>88.9</p></td><td colspan="1" rowspan="1"><p>85.1</p></td><td colspan="1" rowspan="1"><p>90.9</p></td><td colspan="1" rowspan="1"><p>66.7</p></td><td colspan="1" rowspan="1"><p>112.0</p></td><td colspan="1" rowspan="1"><p>32.3</p></td><td colspan="1" rowspan="1"><p>47.7</p></td></tr><tr><td colspan="1" rowspan="1"><p>TOOD</p></td><td colspan="1" rowspan="1"><p>92.3</p></td><td colspan="1" rowspan="1"><p>85.2</p></td><td colspan="1" rowspan="1"><p>92.1</p></td><td colspan="1" rowspan="1"><p>67.0</p></td><td colspan="1" rowspan="1"><p>108.0</p></td><td colspan="1" rowspan="1"><p>32.0</p></td><td colspan="1" rowspan="1"><p>33.7</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv5m</p></td><td colspan="1" rowspan="1"><p>93.9</p></td><td colspan="1" rowspan="1"><p>88.6</p></td><td colspan="1" rowspan="1"><p>93.7</p></td><td colspan="1" rowspan="1"><p>75.3</p></td><td colspan="1" rowspan="1"><p>64.0</p></td><td colspan="1" rowspan="1"><p>25.0</p></td><td colspan="1" rowspan="1"><p>111.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv51</p></td><td colspan="1" rowspan="1"><p>93.5</p></td><td colspan="1" rowspan="1"><p>89.9</p></td><td colspan="1" rowspan="1"><p>94.0</p></td><td colspan="1" rowspan="1"><p>77.0</p></td><td colspan="1" rowspan="1"><p>134.7</p></td><td colspan="1" rowspan="1"><p>53.1</p></td><td colspan="1" rowspan="1"><p>72.0</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv8m</p></td><td colspan="1" rowspan="1"><p>95.1</p></td><td colspan="1" rowspan="1"><p>88.9</p></td><td colspan="1" rowspan="1"><p>93.8</p></td><td colspan="1" rowspan="1"><p>76.6</p></td><td colspan="1" rowspan="1"><p>78.7</p></td><td colspan="1" rowspan="1"><p>25.8</p></td><td colspan="1" rowspan="1"><p>106.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv81</p></td><td colspan="1" rowspan="1"><p>94.0</p></td><td colspan="1" rowspan="1"><p>89.3</p></td><td colspan="1" rowspan="1"><p>94.2</p></td><td colspan="1" rowspan="1"><p>77.7</p></td><td colspan="1" rowspan="1"><p>164.8</p></td><td colspan="1" rowspan="1"><p>43.6</p></td><td colspan="1" rowspan="1"><p>61.4</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv9m</p></td><td colspan="1" rowspan="1"><p>94.6</p></td><td colspan="1" rowspan="1"><p>90.0</p></td><td colspan="1" rowspan="1"><p>94.1</p></td><td colspan="1" rowspan="1"><p>77.0</p></td><td colspan="1" rowspan="1"><p>76.5</p></td><td colspan="1" rowspan="1"><p>20.0</p></td><td colspan="1" rowspan="1"><p>77.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv9c</p></td><td colspan="1" rowspan="1"><p>93.9</p></td><td colspan="1" rowspan="1"><p>91.1</p></td><td colspan="1" rowspan="1"><p>94.4</p></td><td colspan="1" rowspan="1"><p>77.8</p></td><td colspan="1" rowspan="1"><p>102.3</p></td><td colspan="1" rowspan="1"><p>25.3</p></td><td colspan="1" rowspan="1"><p>67.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv10m</p></td><td colspan="1" rowspan="1"><p>93.7</p></td><td colspan="1" rowspan="1"><p>85.7</p></td><td colspan="1" rowspan="1"><p>92.8</p></td><td colspan="1" rowspan="1"><p>74.7</p></td><td colspan="1" rowspan="1"><p>63.4</p></td><td colspan="1" rowspan="1"><p>16.5</p></td><td colspan="1" rowspan="1"><p>89.6</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv101</p></td><td colspan="1" rowspan="1"><p>94.9</p></td><td colspan="1" rowspan="1"><p>88.1</p></td><td colspan="1" rowspan="1"><p>93.5</p></td><td colspan="1" rowspan="1"><p>75.4</p></td><td colspan="1" rowspan="1"><p>126.4</p></td><td colspan="1" rowspan="1"><p>25.7</p></td><td colspan="1" rowspan="1"><p>62.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv11m</p></td><td colspan="1" rowspan="1"><p>94.4</p></td><td colspan="1" rowspan="1"><p>89.7</p></td><td colspan="1" rowspan="1"><p>93.9</p></td><td colspan="1" rowspan="1"><p>76.3</p></td><td colspan="1" rowspan="1"><p>67.7</p></td><td colspan="1" rowspan="1"><p>20.0</p></td><td colspan="1" rowspan="1"><p>88.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv111</p></td><td colspan="1" rowspan="1"><p>94.1</p></td><td colspan="1" rowspan="1"><p>88.7</p></td><td colspan="1" rowspan="1"><p>93.3</p></td><td colspan="1" rowspan="1"><p>75.7</p></td><td colspan="1" rowspan="1"><p>86.6</p></td><td colspan="1" rowspan="1"><p>25.3</p></td><td colspan="1" rowspan="1"><p>65.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>RT-DETR</p></td><td colspan="1" rowspan="1"><p>93.6</p></td><td colspan="1" rowspan="1"><p>90.7</p></td><td colspan="1" rowspan="1"><p>94.2</p></td><td colspan="1" rowspan="1"><p>75.2</p></td><td colspan="1" rowspan="1"><p>57.0</p></td><td colspan="1" rowspan="1"><p>19.9</p></td><td colspan="1" rowspan="1"><p>74.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>Proposed</p></td><td colspan="1" rowspan="1"><p>95.7</p></td><td colspan="1" rowspan="1"><p>89.6</p></td><td colspan="1" rowspan="1"><p>94.8</p></td><td colspan="1" rowspan="1"><p>75.6</p></td><td colspan="1" rowspan="1"><p>49.6</p></td><td colspan="1" rowspan="1"><p>14.6</p></td><td colspan="1" rowspan="1"><p>56.0</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>The visualized results of Table 5</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_7MfhiKl3BjuW7v8_.jpeg"/>
            </fig>
          
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>The visualized results of Table 6</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_uh6HpSdadzNyxH-_.jpeg"/>
            </fig>
          
          <p>To conduct a thorough analysis of the detection performance with the proposed model, the small targets detection results obtained using the proposed model in this paper are presented, as shown in <xref ref-type="fig" rid="fig_8">Figure 8</xref>. As can be seen from the figure, vehicles in dimly lit areas are also successfully detected. This indicates that the proposed method significantly enhances the detection accuracy, especially in complex backgrounds and the identification of small targets. The experiment further verifies that even small targets that are obscured or at a long distance, the proposed model can still successfully detect them, fully demonstrating its effectiveness in detecting small targets under complex scenarios.</p>
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>Detection results with proposed model</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_OxXeEFGCCUEPe9eO.jpeg"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>4. Conclusions</title>
      <p>To address the problems of severe background interference and insufficient feature expression in small targets in complex scenes, this paper proposes a novel adaptive multi-scale Gated convolution and context-aware attention model for small object detection, which significantly reduces background interference and improves feature expression capabilities. The experimental results show that proposed model has improved the mAP@0.5 and mAP@0.5.0.95 performance on the VisDrone2019 dataset by 2.8% and 2.3% respectively. The parameter quantity and computational cost have been reduced by 26.6% and 13% respectively, making it more advantageous compared to other advanced algorithms. Moreover, it has also been verified for generalization on the KITTI dataset. This further verifies the effectiveness and robustness of the model, and it has significant practical significance and application value.</p><p>However, proposed model still has the following limitations. Its performance on small-scale datasets needs improvement, and the model has difficulty converging on small-scale datasets. Real-time detection needs to be enhanced. Although the proposed method achieves a mAP of 50.8% on VisDrone2019, the following limitations still need to be noted. (1) Data scale sensitivity: When the number of training images is less than 1000, the multi-scale gated branch of MGCM suffers from overfitting, and the mAP of the validation set fluctuates by <inline-formula>
  <mml:math id="mnr1sjwn82">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula> 1.6% (n = 5). (2) Failure in extreme scales: When the short side of the target is less than 8 pixel, the window attention of MFEM degenerates to a single point, and the recall rate drops by 9.2%. Future work may focus on the following directions: optimizing the model architecture to reduce the limitations imposed by data size; combining lightweight design with model compression techniques (such as pruning and knowledge distillation) to further improve the processing efficiency of real-time detection, and potentially providing more efficient solutions for small target detection in complex scenarios.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p>This researchwas funded by Liaoning Provincial Science and Technology Plan Project (Grant No.: 2024JH2/102600106).</p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>2509.02011</volume>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhou</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Kong</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2509.02011</pub-id>
          <article-title>Generalizing unsupervised lidar odometry model from normal to snowy weather conditions</article-title>
          <source>arXiv</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Deng</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Meng</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Su</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.dt.2025.09.006</pub-id>
          <article-title>Autonomous dispatch trajectory planning of carrier-based vehicles: An iterative safe dispatch corridor framework</article-title>
          <source>Defence Technology</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>28</volume>
          <page-range>429–440</page-range>
          <issue>3</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pichamuthu</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Sengodan</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Matheswaran</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Srinivasan</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.6180/jase.202503_28(3).0001</pub-id>
          <article-title>Energy-efficient hybrid protocol with optimization inference model for WBANs</article-title>
          <source>J. Appl. Sci. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yin</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3759254</pub-id>
          <article-title>LKAFormer: A lightweight Kolmogorov-Arnold transformer model for image semantic segmentation</article-title>
          <source>ACM Transactions on Intelligent Systems and Technology</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>174394-174409</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pawe lczyk</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Wojtyra</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/access.2020.3026192</pub-id>
          <article-title>Real world object detection dataset for quadcopter unmanned aerial vehicle detection</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <page-range>3443-3458</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yin</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Laghari</surname>
              <given-names>A. A.</given-names>
            </name>
            <name>
              <surname>Teng</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Gadekallu</surname>
              <given-names>T. R.</given-names>
            </name>
            <name>
              <surname>Almadhor</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ojcoms.2024.3520562</pub-id>
          <article-title>FLSN-MVO: Edge computing and privacy protection based on federated learning Siamese network with multi-verse optimization algorithm for Industry 5.0</article-title>
          <source>IEEE Open Journal of the Communications Society</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>56</volume>
          <page-range>13521-13617</page-range>
          <issue>11</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ahmed</surname>
              <given-names>S. F.</given-names>
            </name>
            <name>
              <surname>Alam</surname>
              <given-names>M. S. B.</given-names>
            </name>
            <name>
              <surname>Hassan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Rozbu</surname>
              <given-names>M. R.</given-names>
            </name>
            <name>
              <surname>Ishtiak</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Rafa</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Mofijur</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Shawkat Ali</surname>
              <given-names>A. B. M.</given-names>
            </name>
            <name>
              <surname>Gandomi</surname>
              <given-names>A. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10462-023-10466-8</pub-id>
          <article-title>Deep learning modelling techniques: Current progress, applications, advantages, and challenges</article-title>
          <source>Artificial Intelligence Review</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>26</volume>
          <page-range>1555–1561</page-range>
          <issue>11</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.6180/jase.202311_26(11).0005</pub-id>
          <article-title>MRCNNAM: Mask region convolutional neural network model based on attention mechanism and Gabor feature for pedestrian detection</article-title>
          <source>J. Appl. Sci. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>28</volume>
          <page-range>391–398</page-range>
          <issue>2</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zheng</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.6180/jase.202502_28(2).0017</pub-id>
          <article-title>Single shot multibox detector-based feature fusion model for building object detection</article-title>
          <source>J. Appl. Sci. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>45</volume>
          <page-range>13467–13488</page-range>
          <issue>11</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cheng</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zeng</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Han</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/tpami.2023.3290594</pub-id>
          <article-title>Towards large-scale small object detection: Survey and benchmarks</article-title>
          <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>36</volume>
          <page-range>6283-6303</page-range>
          <issue>12</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wei</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Cheng</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00521-024-09422-6</pub-id>
          <article-title>A review of small object detection based on deep learning</article-title>
          <source>Neural Computing and Applications</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <page-range>580-587</page-range>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Girshick</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Donahue</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Darrell</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Malik</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/cvpr.2014.81</pub-id>
          <article-title>Rich feature hierarchies for accurate object detection and semantic segmentation</article-title>
          <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>39</volume>
          <page-range>1137-1149</page-range>
          <issue>6</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ren</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Girshick</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/tpami.2016.2577031</pub-id>
          <article-title>Faster R-CNN: Towards real-time object detection with region proposal networks</article-title>
          <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>5862-5871</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yin</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/jstars.2020.3025582</pub-id>
          <article-title>Hot region selection based on selective search and modified fuzzy C-means in remote sensing images</article-title>
          <source>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>29300</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yin</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Teng</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Laghari</surname>
              <given-names>A. A.</given-names>
            </name>
            <name>
              <surname>Almadhor</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Gregus</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Sampedro</surname>
              <given-names>G. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-024-78566-1</pub-id>
          <article-title>Brain CT image classification based on mask RCNN and attention mechanism</article-title>
          <source>Scientific Reports</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>4277-4292</page-range>
          <issue>19</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Dong</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1049/gtd2.12948</pub-id>
          <article-title>An improved cascade RCNN detection method for key components and defects of transmission lines</article-title>
          <source>IET Generation, Transmission &amp;amp; Distribution</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>e1288</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Han</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Ren</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.7717/peerj-cs.1288</pub-id>
          <article-title>Improved YOLOv4-tiny based on attention mechanism for skin detection</article-title>
          <source>PeerJ Computer Science</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>19</volume>
          <page-range>1–6</page-range>
          <issue>2</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yin</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Ding</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.2174/0126662558315127241210053411</pub-id>
          <article-title>Lightweight research on fatigue driving face detection based on YOLOv8</article-title>
          <source>Recent Advances in Computer Science and Communications</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="conf-paper">
          <page-range>12993-13000</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zheng</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Ye</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Ren</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1609/aaai.v34i07.6999</pub-id>
          <article-title>Distance-IoU loss: Faster and better learning for bounding box regression</article-title>
          <source>Proceedings of the AAAI Conference on Artificial Intelligence</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1325602</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Huang</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1117/12.3037924</pub-id>
          <article-title>Object detection algorithm based on residual deformable convolution</article-title>
          <source>Proceedings of the International Conference on Computer Vision and Pattern Analysis (ICCPA)</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>138</volume>
          <page-range>109377</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.patcog.2023.109377</pub-id>
          <article-title>Cascaded feature fusion with multi-level self-attention mechanism for object detection</article-title>
          <source>Pattern Recognition</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>1673</page-range>
          <issue>11</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/electronics11111673</pub-id>
          <article-title>Improved YOLO v5 wheat ear detection algorithm based on attention mechanism</article-title>
          <source>Electronics</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>6645</page-range>
          <issue>11</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app13116645</pub-id>
          <article-title>Driver attention detection based on improved YOLOv5</article-title>
          <source>Applied Sciences</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>17799</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Su</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Qin</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Jia</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-024-68934-2</pub-id>
          <article-title>MPE-YOLO: Enhanced small target detection in aerial imaging</article-title>
          <source>Scientific Reports</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>58</volume>
          <page-range>3377-3390</page-range>
          <issue>5</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Diao</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/tgrs.2019.2954328</pub-id>
          <article-title>FMSSD: Feature-merged single-shot detection for multiscale objects in large-scale remote sensing imagery</article-title>
          <source>IEEE Transactions on Geoscience and Remote Sensing</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>4</volume>
          <page-range>4442-4451</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Baig</surname>
              <given-names>N. A.</given-names>
            </name>
            <name>
              <surname>Malik</surname>
              <given-names>M. B.</given-names>
            </name>
            <name>
              <surname>Zeeshan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>M. Z. U.</given-names>
            </name>
            <name>
              <surname>Ajaz</surname>
              <given-names>M. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/access.2016.2598965</pub-id>
          <article-title>Efficient target detection and joint estimation of target parameters with a two-element rotating antenna</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>164</volume>
          <page-range>105297</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bian</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Ling</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Lv</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.dsp.2025.105297</pub-id>
          <article-title>A refined methodology for small object detection: Multi-scale feature extraction and cross-stage feature fusion network</article-title>
          <source>Digital Signal Processing</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <volume>29</volume>
          <page-range>171–178</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.6180/jase.20260129(1).0017</pub-id>
          <article-title>Artificial intelligence-based Bayesian optimization and transformer model for tennis motion recognition</article-title>
          <source>J. Appl. Sci. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>29402-29411</page-range>
          <issue>18</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yin</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Laghari</surname>
              <given-names>A. A.</given-names>
            </name>
            <name>
              <surname>Gadekallu</surname>
              <given-names>T. R.</given-names>
            </name>
            <name>
              <surname>Sampedro</surname>
              <given-names>G. A.</given-names>
            </name>
            <name>
              <surname>Almadhor</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/jiot.2024.3353337</pub-id>
          <article-title>An anomaly detection model based on deep auto-encoder and capsule graph convolution via sparrow search algorithm in 6G internet of everything</article-title>
          <source>IEEE Internet of Things Journal</source>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>1765-1781</page-range>
          <issue>4</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Teng</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Qiao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yin</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.2298/csis240314057T</pub-id>
          <article-title>Underwater image denoising based on curved wave filtering and two-dimensional variational mode decomposition</article-title>
          <source>Computer Science and Information Systems</source>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="journal">
          <volume>2025</volume>
          <page-range>39</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yao</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Su</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s13634-025-01249-0</pub-id>
          <article-title>TFSWA-ResUNet: Music source separation with time–frequency sequence and shifted window attention-based ResUNet</article-title>
          <source>EURASIP Journal on Advances in Signal Processing</source>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>48–53</page-range>
          <issue>8</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Al-refai</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Al-refai</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.14569/ijacsa.2020.0110807</pub-id>
          <article-title>Road object detection using YOLOv3 and Kitti dataset</article-title>
          <source>International Journal of Advanced Computer Science and Applications</source>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>1697</page-range>
          <issue>9</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Han</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Han</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Jiao</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/electronics13091697</pub-id>
          <article-title>NRPerson: A non-registered multi-modal benchmark for tiny person detection and localization</article-title>
          <source>Electronics</source>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>32342</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ferdous</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Islam</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Mahboubi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Islam</surname>
              <given-names>M. Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-025-17647-1</pub-id>
          <article-title>A novel technique for ransomware detection using image based dynamic features and transfer learning to address dataset limitations</article-title>
          <source>Scientific Reports</source>
        </element-citation>
      </ref>
      <ref id="ref_35">
        <label>35.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>32348</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liang</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Lv</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-025-18263-9</pub-id>
          <article-title>Object detection model of vehicle-road cooperative autonomous driving based on improved YOLO11 algorithm</article-title>
          <source>Scientific Reports</source>
        </element-citation>
      </ref>
      <ref id="ref_36">
        <label>36.</label>
        <element-citation publication-type="journal">
          <volume>81</volume>
          <page-range>1301</page-range>
          <issue>14</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Xu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Xing</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11227-025-07788-5</pub-id>
          <article-title>Enhancing UAV object detection through multi-scale deformable convolutions and adaptive fusion attention</article-title>
          <source>The Journal of Supercomputing</source>
        </element-citation>
      </ref>
      <ref id="ref_37">
        <label>37.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>1–28</page-range>
          <issue>5</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kıratlı</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Eroğlu</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11554-025-01758-z</pub-id>
          <article-title>Real-time multi-object detection and tracking in UAV systems: Improved YOLOv11-EFAC and optimized tracking algorithms</article-title>
          <source>Journal of Real-Time Image Processing</source>
        </element-citation>
      </ref>
      <ref id="ref_38">
        <label>38.</label>
        <element-citation publication-type="journal">
          <volume>184</volume>
          <page-range>113811</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Qin</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Xia</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.asoc.2025.113811</pub-id>
          <article-title>A lightweight network enhanced by attention-guided cross-scale interaction for underwater object detection</article-title>
          <source>Applied Soft Computing</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>