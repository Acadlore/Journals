<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IJCMEM</journal-id>
      <journal-id journal-id-type="doi">10.18280</journal-id>
      <journal-title-group>
        <journal-title>International Journal of Computational Methods and Experimental Measurements</journal-title>
        <abbrev-journal-title abbrev-type="issn">Int. J. Comput. Methods Exp. Meas.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IJCMEM</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2046-0554</issn>
      <issn publication-format="print">2046-0546</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-SsWDyU1nfE2yt5_zoWyRz7J4jcW9YuI3</article-id>
      <article-id pub-id-type="doi">10.18280/ijcmem.120111</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Revealing Hidden Patterns: A Deep Learning Approach to Camouflage Detection</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3373-0734</contrib-id>
          <name>
            <surname>Kamble</surname>
            <given-names>Rita</given-names>
          </name>
          <email>reeta.kamble12@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7383-2615</contrib-id>
          <name>
            <surname>Rajarajeswari</surname>
            <given-names>Pothuraju</given-names>
          </name>
          <email/>
        </contrib>
        <aff id="aff_1">Department of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, Vaddeswaram 522502, India</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>30</day>
        <month>03</month>
        <year>2024</year>
      </pub-date>
      <volume>12</volume>
      <issue>1</issue>
      <fpage>97</fpage>
      <lpage>105</lpage>
      <page-range>97-105</page-range>
      <history>
        <date date-type="received">
          <day>30</day>
          <month>09</month>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <day>21</day>
          <month>03</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>Â©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>In military defence and wildlife conservation operations, detecting camouflage in images poses a significant challenge. This research investigates the efficacy of deep learning techniques, including Convolutional Neural Networks (CNN), Artificial Neural Networks (ANN), and Long Short-Term Memory (LSTM), in addressing this challenge. The study meticulously evaluates each model's performance using metrics such as average accuracy, validation accuracy, and loss measures across well-known benchmark datasets comprising camouflaged and non-camouflaged images. Notably, the CNN + ANN Pipeline model emerges as the most effective, achieving a remarkable average accuracy of 91.37%. This model, together with the standalone CNN, outperforms the ANN and LSTM models in terms of camouflage detection. The discoveries advance the state-of-the-art in image analysis while also having practical implications for real-world applications. In military settings, good camouflage detection can improve situational awareness and danger detection capabilities. Similarly, automated camouflage detection helps monitor and protect endangered species by detecting hidden creatures or potential threats. Overall, this study highlights the ability of deep learning techniques to greatly improve visual analytic tasks across a variety of domains.</p></abstract>
      <kwd-group>
        <kwd>CNNs</kwd>
        <kwd>LSTM</kwd>
        <kwd>ANNs</kwd>
        <kwd>Pipeline</kwd>
        <kwd>Camouflage</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="2"/>
        <fig-count count="8"/>
        <table-count count="5"/>
        <ref-count count="24"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Future generations will be based on advanced technologies such as artificial intelligence (AI), hash AI, Explainable AI (XAI), etc. [<xref ref-type="bibr" rid="ref_1">1</xref>] , [<xref ref-type="bibr" rid="ref_2">2</xref>]. It may use unmanned systems to strike and detect the targets [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>], [<xref ref-type="bibr" rid="ref_5">5</xref>]. It also leads to the development of camouflage technology, in which military devices blend in with the environment so that they cannot be detected with human observation as shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref> [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>]. This camouflage technique has posed significant challenges to UAV's military target striking [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>]. Machine learning (ML) techniques can play an important role in detecting various types of targets, and researchers have proposed numerous algorithms that use ML. However, there has been little consideration paid to military camouflaged target detection.</p><p>All of the offered ways using deep learning are grouped into two regions [<xref ref-type="bibr" rid="ref_10">10</xref>]. To begin with, prospective regions were taken into consideration such as SPP-Net, ReCNN, etc. The next one is for ideas that involve regression, like SSD, YOLO, etc. The above-mentioned standard algorithms perform badly in camouflaged target detection since they fall into the trap of the edges and backdrop of camouflaged targets because the real battlefield conditions may include desert, woods, snow, etc. It is critical in the introduction to explain why specific models such as Convolutional Neural Networks (CNN), Artificial Neural Networks (ANN), and Long Short-Term Memory (LSTM) were chosen to address the highlighted issues in camouflage target detection. These models were chosen based on their distinct capabilities and aptitude for addressing the issues given by camouflage technology. CNNs are ideal for image processing jobs because of their capacity to capture spatial hierarchies and learn complex patterns.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Camouflage images</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_GMWd9w5C2WU6MfK5.jpeg"/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_YIwBakhg7oZaXpJ5.jpeg"/>
        </fig>
      
      <p>In the context of camouflage detection, CNNs can rapidly extract features from photos, allowing the detection of small deviations and anomalies that may signal the existence of disguised targets in a variety of situations such as the desert, woodlands, or snow-covered terrain. ANNs provide flexibility in modelling nonlinear interactions within data, making them useful for analysing complicated and heterogeneous datasets related to camouflage detection. ANN designs enable the detection system to learn complicated patterns and correlations between multiple features, improving its capacity to accurately identify camouflaged targets in a variety of backgrounds and environmental situations. LSTMs, with their ability to identify temporal dependencies in sequential data, are useful in analysing dynamic circumstances in which camouflage targets may move or alter over time. By adding LSTM networks into the detection framework, the system can successfully track and analyse temporal patterns associated with camouflaged objects, hence boosting overall detection performance in dynamic combat settings. Furthermore, it is critical to highlight how the incorporation of these unique deep learning models overcomes the shortcomings of standard algorithms, which frequently struggle with edge identification and background noise in camouflage target recognition. The suggested approach intends to solve these issues by exploiting the enhanced capabilities of CNNs, ANNs, and LSTMs, hence improving the accuracy and reliability of camouflage detection systems in real-world military applications. Incorporating these additional insights into the introduction will provide a thorough understanding of the rationale for selecting specific deep learning models and how they are expected to handle the stated issues in camouflage target detection.</p>
      
        <sec>
          
            <title>1.1. Problem description</title>
          
          <p>There are a number of problems that must be solved before it will be possible to military camouflaged people in images as follows:</p><p><p>Due to the environment's constantly changing dynamics, it can be challenging to distinguish the texture, shape, and location of targeted camouflaged objects. For instance, the edges of targets in the desert, the desert sands, and the forest are easily absorbed into the surroundings.</p><p>It is quite challenging to get the labelled data necessary to train and evaluate a deep learning model during the data set collection phase.</p><p>The well-known datasets, ImageNet and VOC, cannot be used to detect and identify military targets that have been camouflaged.</p></p>
        </sec>
      
      
        <sec>
          
            <title>1.2. Overview of proposed methodology</title>
          
          <p>The Proposed method for detecting camouflage photographs makes use of a variety of deep learning models, including CNN, ANN, LSTM, and the CNN+ANN pipeline. These models were chosen for their distinct capabilities and aptitude for solving specific issues found in camouflage detecting tasks. The CNN is well-suited for extracting detailed spatial hierarchies and patterns from images, but the ANN is flexible in modelling nonlinear interactions within the data. LSTM, on the other hand, is very good at identifying temporal dependencies, which is important for analysing sequential data like image sequences or video streams. Furthermore, the CNN+ANN pipeline takes advantage of the complementing strengths of both models, combining robust feature extraction with sophisticated connection learning. The methodology seeks to solve the complexity of camouflage detection in a complete manner by utilising these several models. The models are evaluated using a variety of criteria, including average accuracy, validation accuracy, and average loss. Notably, the CNN+ANN pipeline outperforms all datasets, highlighting its effectiveness in this domain. Furthermore, the methodology's usefulness goes beyond military and defence contexts to include a wide range of disciplines, such as wildlife conservation. This study highlights the potential of deep learning techniques to improve camouflage detection operations and establishes the framework for future advances in this sector.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p>A variety of approaches for detecting camouflage in images proposed by investigators have been discussed in this section along with the advantages over the existing terminologies [<xref ref-type="bibr" rid="ref_11">11</xref>], [<xref ref-type="bibr" rid="ref_12">12</xref>], [<xref ref-type="bibr" rid="ref_13">13</xref>], [<xref ref-type="bibr" rid="ref_14">14</xref>].</p><p>Liu et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] have proposed semi-supervised search identification network (Semi-SINet) based camouflaged military people detection system, in which, the camouflaged object detection dataset (COD10K) was taken into the consideration. It was observed that the proposed approach performance better than the existing approaches. But the obtained accuracy was low.</p><p>Ren et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] suggested texture aware refinement module that separates the background in the camouflaged image, so that object can be easily detected. They applied the covariance matrices to get the texture information. In the performance analysis, COD10K and CAMO dataset were taken into the consideration. It was observed that the proposed model shows the superior performance, but still lacking to detect the dynamic camouflaged objects in the images.</p><p>Chen et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] proposed the Context-aware Cross-level Fusion Network (C2F-Net), in which an information-based module was developed as a coefficient to detect camouflaged objects. A Dual-branch Global Context Module (DGCM) was also proposed to improve the informative-based features. The proposed C2F-Net performed better than state-of-the-art algorithms in the results, but a large dataset should be looked at for rigorous testing and analysis.</p><p>Lin et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] addressed the issue of context aggregation strategies, where COD detection is difficult. They proposed a frequency-based context aggregation methodology known as FACA, which suppresses high frequency information. Furthermore, a gradient weighted loss function was developed to provide deep inside the contour details. During the experimentation and result analysis, it was discovered that the proposed mechanism outperforms the existing state-of-the-art techniques. But, still power of hybrid deep learning technique can be opted to enhance the accuracy.</p><p>For the COD, Fan et al. [<xref ref-type="bibr" rid="ref_19">19</xref>] proposed the SINet search identification network. They also gathered a dataset of 10,000 images known as COD10K, which serves as a benchmark dataset for testing various ML and deep learning-based models. During the experiments, it was discovered that the proposed SINet outperformed the existing works.</p><p>Shen et al. [<xref ref-type="bibr" rid="ref_20">20</xref>] proposed a COD mechanism using polarization imaging and deep learning. First, the polarization specificity is determined using the Stokes-vector-based parameter image in this methodology. The authors then suggested using the Otsu segmentation algorithm and morphological operations to highlight the target in the camouflaged-based images. In the end, 80% of the images were correctly identified, but accuracy still needs to be improved in order to be used in military and wildlife applications.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Literature review</title>
          </caption>
          <table><tbody><tr><td><p>References</p></td><td><p>Title</p></td><td><p>Description</p></td><td><p>Advantages</p></td><td><p>Limitations</p></td></tr><tr><td><p>[<xref ref-type="bibr" rid="ref_15">15</xref>]</p></td><td><p>Camouflaged people detection based on a semi-supervised search identification network.</p></td><td><p>The authors presented a semi-supervised search identification network (Semi-SINet) for detecting camouflaged military personnel.</p></td><td><p>Camouflage techniques and strategies evolve with time, requiring adaptability.</p></td><td><p>Dependence on the quantity and quality of unlabeled data.</p></td></tr><tr><td><p>[<xref ref-type="bibr" rid="ref_16">16</xref>]</p></td><td><p>Deep texture-aware features for camouflaged object detection.</p></td><td><p>The author's suggested texture-aware refining module separates the backdrop in the camouflaged image, allowing the object to be easily spotted.</p></td><td><p>These traits are effective at distinguishing camouflaged items from complicated and variable backgrounds, hence improving detection accuracy.</p></td><td><p>The technique may be vulnerable to noise or artefacts in the input data, affecting the quality of derived features and subsequent detection accuracy.</p></td></tr><tr><td><p>[<xref ref-type="bibr" rid="ref_17">17</xref>]</p></td><td><p>Camouflaged object detection via context-aware cross-level fusion.</p></td><td><p>The author presented the Context-aware Cross-level Fusion Network (C2F-Net), in which an information-based module was created as a coefficient for detecting camouflaged items.</p></td><td><p>The merging of contextual information enables the model to adapt to a wide range of environmental conditions and camouflage patterns, making it suitable for a variety of scenarios.</p></td><td><p>Integrating information at several levels of abstraction complicates the detection pipeline, potentially increasing computational overhead and necessitating careful optimisation.</p></td></tr><tr><td><p>[<xref ref-type="bibr" rid="ref_18">18</xref>]</p></td><td><p>Frequency-aware camouflaged object detection. </p></td><td><p>The authors discuss context aggregation ways in which COD detection is challenging.</p></td><td><p>Frequency-aware approaches gather significant information across many frequency domains, resulting in a rich feature representation that improves object detection accuracy.</p></td><td><p>Extracting frequency-aware characteristics may require complex signal processing techniques, which may increase computing overhead and necessitate specialised knowledge for implementation.</p></td></tr><tr><td><p>[<xref ref-type="bibr" rid="ref_19">19</xref>]</p></td><td><p>Camouflaged object detection. </p></td><td><p>The author proposes. For the COD, they proposed the SINet search identification network.</p></td><td><p>The proceedings provide the most recent research and breakthroughs in camouflaged object identification, offering significant insights into cutting-edge techniques and methodologies.</p></td><td><p>The proceedings may be influenced by publication bias, in which only successful or notable research results are accepted for presentation, thus leading to an incomplete depiction of the challenges and limitations in camouflaged object detection.</p></td></tr><tr><td><p>[<xref ref-type="bibr" rid="ref_20">20</xref>]</p></td><td><p>Rapid detection of camouflaged artificial target based on polarization imaging and deep learning.</p></td><td><p>The authors presented a COD method that incorporates polarisation imaging and deep learning.</p></td><td><p>The combination of polarisation imaging and deep learning algorithms can considerably improve the accuracy of detecting camouflaged targets by leveraging light polarisation features that standard imaging systems may not detect.</p></td><td><p>Implementing polarisation imaging systems may necessitate specialised hardware, which can be expensive and not always available in all locations or applications.</p></td></tr><tr><td><p>[<xref ref-type="bibr" rid="ref_21">21</xref>]</p></td><td><p>Deep gradient learning for efficient camouflaged object detection. </p></td><td><p>The investigator proposed the COD employing the deep gradient network known as DGNet.</p></td><td><p>Deep gradient learning algorithms can successfully capture gradient-based information required for high-accuracy detection of camouflaged objects, particularly in complicated and cluttered backdrops.</p></td><td><p>Deep gradient learning approaches may have lower representation power than more complex deep learning models, resulting in inferior performance in instances involving very intricate camouflage patterns or backdrops.</p></td></tr><tr><td><p>[<xref ref-type="bibr" rid="ref_22">22</xref>]</p></td><td><p>Cascade and fusion: a deep learning approach for camouflaged object sensing.</p></td><td><p>The researcher suggested the COD mechanism, which solved the problem of previous algorithms, namely the difficulty in extracting informative sections such as characteristics with low signal to noise ratio.</p></td><td><p>The ability to cascade and fuse input from several stages or modalities allows the model to adapt to complicated scenarios with varying background clutter and occlusions, resulting in more accurate detection performance.</p></td><td><p>Implementing cascade and fusion techniques may increase computing complexity, particularly when combining data from various stages or modalities, thereby limiting scalability and real-time performance in resource-constrained contexts.</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>In the study [<xref ref-type="bibr" rid="ref_21">21</xref>], the COD was proposed by the investigator using a deep gradient network known as DGNet. Decoupling has occurred in context and a texture encoder in this mechanism. Furthermore, soft grouping chose texture and context features. In the simulation, DGNet outperforms state-of-the-art algorithms and can be used for COD after accuracy is improved.</p><p>Huang et al. [<xref ref-type="bibr" rid="ref_22">22</xref>] proposed the COD mechanism, in which the drawback of traditional algorithms was addressed, namely, the difficulty in extracting informative parts such as features with low signal to noise ratio. They recommended the Cascade and Feedback Fusion approaches. The proposed terminology outperformed recent state-of-the-art methods in the obtained results. The state-of-Art of literature review as shown in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
      
        <sec>
          
            <title>2.1. Advantage of proposed scheme over existing works</title>
          
          <p>List of merits of the proposed work as follows:</p><p><p>The proposed deep learning models can accurately detect camouflage in images, which is an important challenge in military, defense, and wildlife conservation operations.</p><p>The use of a combination of CNN and ANN pipeline shows promising results and can be further explored in other image detection tasks.</p><p>The study shows that deep learning techniques can be useful in detecting camouflage in images, which can potentially save time and resources in manual detection.</p><p>The study can contribute to the development of more effective and efficient techniques for detecting camouflage in images, leading to better security and conservation efforts.</p><p>The proposed models are evaluated on well-known benchmark datasets, which can aid in benchmarking and comparing with other models in the future.</p></p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Preliminaries</title>
      <p>In this section, description of notations and abbreviations along with overview of deep learning models are presented.</p>
      
        <sec>
          
            <title>3.1. Notations and abbreviations</title>
          
          <p><xref ref-type="table" rid="table_2">Table 2</xref> represents the notations and various abbreviations used in this research work.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Description of notations and abbreviations</title>
              </caption>
              <table><tbody><tr><td><p>Abbreviations</p></td><td><p>Description</p></td></tr><tr><td><p>CNNs</p></td><td><p>Convolutional Neural Networks</p></td></tr><tr><td><p>ANNs</p></td><td><p>Artificial Neural Networks</p></td></tr><tr><td><p>LSTM</p></td><td><p>Long Short-Term Memory</p></td></tr><tr><td><p>DL</p></td><td><p>Deep Learning</p></td></tr><tr><td><p>ML</p></td><td><p>Machine Learning</p></td></tr><tr><td><p>ReLU</p></td><td><p>Rectified Linear Unit</p></td></tr><tr><td><p>RNNs</p></td><td><p>Recurrent Neural Networks</p></td></tr><tr><td><p>AI</p></td><td><p>Artificial Intelligence</p></td></tr><tr><td><p>Adam</p></td><td><p>Adaptive Moment Estimation</p></td></tr><tr><td><p>COD</p></td><td><p>Co-salient Object Detection</p></td></tr><tr><td><p>DGCM</p></td><td><p>Dual-branch Global Context Module</p></td></tr><tr><td><p>C2F-Net</p></td><td><p>Context-aware Cross-level Fusion Network</p></td></tr><tr><td><p>FACA</p></td><td><p>Frequency-Based Context Aggregation</p></td></tr><tr><td><p>DGNet</p></td><td><p>Domain Guided Network</p></td></tr><tr><td><p>SINet</p></td><td><p>Salient Object Detection Identification Network</p></td></tr><tr><td><p>VOC</p></td><td><p>Visual Object Classes</p></td></tr><tr><td><p>SSD</p></td><td><p>Single Shot MultiBox Detector</p></td></tr><tr><td><p>ReCNN</p></td><td><p>Recurrent Convolutional Neural Network</p></td></tr><tr><td><p>UAV</p></td><td><p>Unmanned Aerial Vehicle</p></td></tr><tr><td><p>XAI</p></td><td><p>Explainable AI</p></td></tr><tr><td><p>SPP-Net</p></td><td><p>Spatial Pyramid Pooling Network</p></td></tr><tr><td><p>YOLO</p></td><td><p>You Only Look Once</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Overview of deep learning models</title>
          
          <p>In this section brief description of DL models used is given:</p>
          
            <sec>
              
                <title>3.2.1 Convolutional neural networks (cnns)</title>
              
              <p>In computer vision applications, notably in picture classification and object recognition, CNNs are a common form of deep neural network. The layers of neurons that make up a CNN each learn to extract a particular set of characteristics from the input data. The input picture is typically processed by a convolutional layer, which creates a collection of feature maps by applying a number of filters. These feature maps show where specific visual patterns, such corners or edges, are prevalent across the input picture.</p><p>The pooling layer down samples the feature maps by picking the most significant values from the output of the convolutional layer. The retrieved features' dimensionality is decreased as a result of this operation, which also improves the effectiveness of the network. The final classification decision is made by a sequence of fully linked layers after the pooled characteristics have been flattened. The CNN learns its weights by a technique known as backpropagation, in which the network modifies its settings to reduce the discrepancy between the training data expected and actual labels.</p>
            </sec>
          
          
            <sec>
              
                <title>3.2.2 Artificial neural networks (anns)</title>
              
              <p>A group of algorithms known as ANNs are modelled after the structure and operation of the human brain. These networks may be used for a variety of tasks, including image categorization, and are built to mimic the behaviour of organic neurons. ANNs are made up of several layers of synthetic neurons that process incoming data and provide predictions. Weights that are changed during training to improve the model's performance link the layers of neurons.</p><p>ANNs are a class of algorithms that are modelled after how the human brain functions. These networks are designed to resemble the action of biological neurons and may be utilised for a range of tasks, including picture classification. Multiple layers of artificial neurons make up ANNs, which process incoming data and offer predictions. The layers of neurons are connected by weights that are altered throughout training to enhance the model's performance.</p>
            </sec>
          
          
            <sec>
              
                <title>3.2.3 Long short-term memory (lstm)</title>
              
              <p>A special kind of recurrent neural network (RNN) called LSTM is made to deal with long-term dependencies in sequential input. In order to maintain information over time, LSTMs employ memory cells. This enables them to selectively forget or keep information depending on the input at each time step. The input gate, forget gate, and output gate are three different types of gates that regulate the memory cells. The memory cell's input gate selects how much fresh input should be fed to it, the forget gate chooses which data should be removed from the memory cell, and the output gate decides how much of the memory cell should be transmitted to the following layer.</p><p>LSTMs are advantageous for tasks requiring sequential input, like as voice recognition and natural language processing, where the network must retain context from earlier portions of the sequence. This is because LSTMs employ memory cells and gates. Widely employed in both business and academics, LSTMs have been demonstrated to perform better than conventional RNNs on a number of tasks. Sequential information can be useful in finding patterns in the photos, and this research uses LSTMs for the job of detecting camouflage. The pictorial representation of LSTM circuit is demonstrated in <xref ref-type="fig" rid="fig_2">Figure 2</xref>.</p>
              
                <fig id="fig_2">
                  <label>Figure 2</label>
                  <caption>
                    <title>Pictorial representation of LSTM model</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_9Fcd7IeQQPKyKxhT.jpeg"/>
                </fig>
              
            </sec>
          
          
            <sec>
              
                <title>3.2.4 Cnn+ann</title>
              
              <p>The combination of CNN and ANN in a pipeline is a powerful deep learning approach used for image classification tasks. CNNs are effective in extracting features from images through convolutional and pooling layers. Meanwhile, ANNs excel in making predictions based on the features extracted by the CNNs. In this pipeline approach, the output of the convolutional layers from the CNN is fed as input to the fully connected layers of the ANN, which then produce the final classification output. Thus, this combination allows the model to leverage both the feature extraction capabilities of CNNs and the prediction capabilities of ANNs, making it a powerful tool for image classification tasks.</p><p>In comparison to using CNNs or ANNs alone, the pipeline approach has demonstrated improved performance in many image classification tasks. The pipeline technique is able to discover more complex patterns in the input data, producing more accurate classification results. This is accomplished by integrating the strengths of both CNNs and ANNs. The pipeline technique is also quite adaptable and can be tailored to match various image categorization jobs.</p>
            </sec>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Proposed methodology</title>
      <p>In this section, brief description of model architecture, Adam optimizer, categorical cross entropy loss function, training procedure and proposed algorithm is presented.</p>
      
        <sec>
          
            <title>4.1. Model architecture</title>
          
          <p>In this study, we trained and tested deep-learning models to classify camouflage images. The general architecture of the models consisted of a CNN followed by an ANN. The CNN portion of the model consisted of two convolutional layers, each with a 3x3 kernel size and ReLU activation function, followed by max-pooling layers with a 2Ã2 pool size. The output of the second max pooling layer was flattened and fed into the ANN portion of the model. The ANN consisted of three dense layers with a ReLU activation function and a final SoftMax activation function in the output layer. Dropout regularization with a rate of 0.5 was applied to the first dense layer of the ANN to prevent overfitting. The proposed model architecture is presented graphically in <xref ref-type="fig" rid="fig_3">Figure 3</xref>.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Proposed model architecture</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_mGlLxjlzcLXFsumb.jpeg"/>
            </fig>
          
          <p>The models were trained using the Adam optimizer, a stochastic gradient descent optimization algorithm that calculates adaptive learning rates for each parameter. The categorical cross-entropy loss function was used to measure the difference between the predicted and actual class labels. This is a common loss function used for multi-class classification tasks. The accuracy metric was used to evaluate the performance of the models. It represents the percentage of correctly classified images out of all images in the test set. The description of various sub-methodologies such as Adam optimizer, categorical cross-entropy and training procedure is provided in next subsections.</p>
          
            <sec>
              
                <title>4.1.1 Adam optimizer</title>
              
              <p>A well-liked optimisation approach called Adam (Adaptive Moment Estimation) is used in deep learning to adjust the neural network's weights as it is being trained. It is a stochastic gradient descent optimisation technique that brings together the advantages of momentum and RMSProp techniques. Based on the estimated first and second moments of the gradients, Adam adapts the learning rate for each parameter. In comparison to other optimisation techniques, this adaptive learning rate aids in faster convergence and better performance.</p>
            </sec>
          
          
            <sec>
              
                <title>4.1.2 Categorical cross-entropy loss function</title>
              
              <p>A typical loss function in multi-class classification issues is categorical cross-entropy. It calculates the discrepancy between the target classes' actual probability distribution and the projected probability distribution. The SoftMax activation function is used to the last layer of the neural network to produce the anticipated probability distribution. The categorical cross-entropy loss function penalises the model when it assigns a low probability to the right class in an effort to reduce the gap between these two distributions.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>4.2. Training procedure</title>
          
          <p>The models were trained using a stochastic gradient descent optimizer with a learning rate of 0.001 and a batch size of 32. The number of epochs varied depending on the model architecture and dataset but typically ranged from 50 to 100 epochs. Regularization techniques such as dropout and weight decay were applied to prevent overfitting.</p><p>The training and validation sets were split randomly with a ratio of 80:20, where 80% of the data was used for training and 20% was used for validation. The models were evaluated during training by monitoring the training loss and validation loss, as well as the training accuracy and validation accuracy. The categorical cross-entropy loss function was used as the primary metric for evaluating the model's performance. Additionally, the accuracy metric was used to measure the percentage of correctly classified images in both the training and validation sets. The models were trained until convergence was reached, as determined by the absence of further improvements in the validation loss and accuracy.</p>
        </sec>
      
      
        <sec>
          
            <title>4.3. Performance evaluation: accuracy</title>
          
          <p>Accuracy is a metric used to evaluate the performance of a classification model. It measures the percentage of correctly predicted samples out of the total number of samples. Mathematically, accuracy is defined as the ratio of the number of correctly predicted samples to the total number of samples. It is a useful metric when the classes are balanced, meaning that each class has roughly the same number of samples.</p>
        </sec>
      
      
        <sec>
          
            <title>4.4. Proposed algorithm</title>
          
          <p>In this section, proposed algorithm to reveal the hidden pattern in images is presented in Algorithm 1 and its graphical flow is shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>.</p><table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Algorithm 1: A Deep Learning Approach to Camouflage Detection</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Data: Input datasets camo-covo and Camo-v</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Result: Prediction accuracy of proposed deep learning model</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Step 1: Split the dataset into 80:20 ratio, where 80% of dataset considered as training and rest for testing</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Step 2: Apply the deep learning models, CNN, LSTM and CNN+ANN</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Step 3: Monitor the models performance using training and validation loss</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Step 4: Monitor the number of correctly classified images using the accuracy metrics</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Step 5: Repeat step 2 to 4, until the convergence will reach</p></td></tr></tbody></table>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Graphical flow of proposed algorithm</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_EKJnHYmOS-9W_Rk_.jpeg"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>5. Result and analysis</title>
      <p>In this section, detailed description of simulation environment and obtained results of proposed deep learning model is provided in descriptive, tabular and graphical form.</p>
      
        <sec>
          
            <title>5.1. Dataset description</title>
          
          <p>A camouflage dataset typically contains images of animals, insects, or other objects that have developed adaptations to merge in with their surroundings [<xref ref-type="bibr" rid="ref_23">23</xref>] , [<xref ref-type="bibr" rid="ref_24">24</xref>]. It consist of 10,000 colored images. The objective of training models on such a dataset is to enable the model to detect and identify camouflaged objects in their natural environment. The dataset may contain examples of animals that utilize coloration, texture, or other physical characteristics to merge in with their environment.</p><p>A non-camouflage dataset contains images of objects that are not camouflaged and can be distinguished from their surroundings. These datasets are used to train and evaluate models for general object detection and recognition tasks, without the additional difficulty of camouflaged objects.</p>
        </sec>
      
      
        <sec>
          
            <title>5.2. Experimental analysis</title>
          
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>The simulation parameters</title>
              </caption>
              <table><tbody><tr><td><p>Method</p></td><td><p>Dense</p></td><td><p>Max-Pooling</p></td><td><p>Convolutional</p></td><td><p>Flatten</p></td><td><p>Activation</p><p>Funct.</p></td><td><p>Epochs</p></td><td><p>Batch Size</p></td><td><p>Loss Funct.</p></td><td><p>Optimizer</p></td></tr><tr><td><p>LSTM</p></td><td><p>2</p></td><td></td><td></td><td><p>1</p></td><td><p>Relu, Softmax</p></td><td><p>50</p></td><td><p>64</p></td><td><p>sparse_categorical_crossentropy</p></td><td><p>Adam</p></td></tr><tr><td><p>CNN</p></td><td><p>2</p></td><td><p>4</p></td><td><p>4</p></td><td><p>1</p></td><td><p>Relu, Softmax</p></td><td><p>50</p></td><td><p>64</p></td><td><p>categorical_crossentropy</p></td><td><p>Adam</p></td></tr><tr><td><p>ANN</p></td><td><p>3</p></td><td></td><td></td><td><p>1</p></td><td><p>Relu, Softmax</p></td><td><p>50</p></td><td><p>64</p></td><td><p>categorical_crossentropy</p></td><td><p>Adam</p></td></tr><tr><td><p>ANN+CNN</p></td><td><p>4</p></td><td><p>2</p></td><td><p>2</p></td><td><p>1</p></td><td><p>Relu, Softmax</p></td><td><p>50</p></td><td><p>64</p></td><td><p>categorical_crossentropy</p></td><td><p>Adam</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>The experiment involved using Python on VS Code to execute code through a Jupiter notebook extension. A computer equipped with an Intel 2.30 GHz Ryzen-7 processor and 16.0 GB of RAM was utilized for the experiment. The simulation parameters considered in the proposed work are shown in <xref ref-type="table" rid="table_3">Table 3</xref>.</p>
        </sec>
      
      
        <sec>
          
            <title>5.3. Results</title>
          
          <p>The results obtained from the deep learning models trained on camouflage and non-camouflage images are summarized in the <xref ref-type="table" rid="table_4">Table 4</xref> and depicted in <xref ref-type="fig" rid="fig_5">Figure 5-7</xref>. In addition, identified camouflage in images are also presented in <xref ref-type="fig" rid="fig_8">Figure 8</xref>. The models were trained using three different architectures: CNN, ANN, and LSTM, as well as a CNN + ANN pipeline.</p><p>For the camouflage images, the CNN + ANN pipeline achieved the highest accuracy of 0.9014, while the CNN model performed the best for non-camouflage images with an accuracy of 0.9257. These results suggest that combining both CNN and ANN models can lead to better performance when detecting camouflage and non-camouflage images.</p><p>In terms of individual model performances, the CNN + ANN pipeline models generally outperformed the CNN, ANN, and LSTM models across all image types, indicating that spatial information is an important factor in detecting camouflage.</p>
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Details of dataset with model and accuracy</title>
              </caption>
              <table><tbody><tr><td><p>Dataset</p></td><td><p>Image Type</p></td><td><p>DL Model</p></td><td><p>Average Accuracy</p></td></tr><tr><td><p>Camo Covo</p></td><td><p>Camouflage</p></td><td><p>CNN</p><p>ANN</p><p>LSTM</p><p>CNN+ANN Pipeline</p></td><td><p>0.8802222146</p><p>0.7855555548</p><p>0.7957222031</p><p>0.9014444351</p></td></tr><tr><td></td><td><p>Non-Camouflage</p></td><td><p>CNN</p><p>ANN</p><p>LSTM</p><p>CNN+ANN Pipeline</p></td><td><p>0.9146666659</p><p>0.7979999781</p><p>0.7873333295</p><p>0.9257777863</p></td></tr><tr><td><p>Camo v</p></td><td><p>Camouflage</p></td><td><p>CNN</p><p>ANN</p><p>LSTM</p><p>CNN+ANN Pipeline</p></td><td><p>0.7853999913</p><p>0.801000011</p><p>0.8063000023</p><p>0.9137000024</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Obtained accuracy of DL models on camouflage in images using camo-covo dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_LwkUCIga5eGUJcMY.jpeg"/>
            </fig>
          
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Obtained accuracy of DL models on camouflage in images using Camo-V dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_GNJjkcloiYmZtmhw.jpeg"/>
            </fig>
          
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>Obtained accuracy of DL models on non-camouflage in images using camo-covo dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_H7PDBJ0vRwwgl6AF.jpeg"/>
            </fig>
          
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>Identified camouflage in images</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_e8BUlWnsQ2qoYfv-.jpeg"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_mMsQU76EsVTlY5Ey.jpeg"/>
            </fig>
          
          <p>The LSTM model showed relatively lower accuracy on both camouflage and non-camouflage images, suggesting that the temporal information captured by the LSTM architecture might not be as relevant for this particular task.</p>
        </sec>
      
      
        <sec>
          
            <title>5.4. Comparative analysis with existing methodologies</title>
          
          <p>In this subsection, a proposed work is compared with the existing terminologies [<xref ref-type="bibr" rid="ref_15">15</xref>], [<xref ref-type="bibr" rid="ref_16">16</xref>], [<xref ref-type="bibr" rid="ref_17">17</xref>], as shown in <xref ref-type="table" rid="table_5">Table 5</xref>. It was observed that the proposed deep learning model based on CNN and ANN outperformed over the recent simulated works when tested on the datasetâs camo-covo and camo-v. The highest obtained accuracy to classify camouflaged in images was 91.37% in the proposed model, whereas, 90% in C^2 Fnet, 90.57% in TARM and 89.2% in Semi-SINet.</p>
          
            <table-wrap id="table_5">
              <label>Table 5</label>
              <caption>
                <title>Comparative analysis</title>
              </caption>
              <table><tbody><tr><td><p>Reference</p></td><td><p>Proposed Year</p></td><td><p>Methodology</p></td><td><p>Accuracy</p></td></tr><tr><td><p>[<xref ref-type="bibr" rid="ref_15">15</xref>]</p></td><td><p>2023</p></td><td><p>Semi-SNet</p></td><td><p>89.2</p></td></tr><tr><td><p>[<xref ref-type="bibr" rid="ref_16">16</xref>]</p></td><td><p>2021</p></td><td><p>TARM</p></td><td><p>90.57</p></td></tr><tr><td><p>[<xref ref-type="bibr" rid="ref_17">17</xref>]</p></td><td><p>2021</p></td><td><p>C^2.Fnet</p></td><td><p>90</p></td></tr><tr><td><p>Proposed Model</p></td><td><p>2024</p></td><td><p>CNN+ANN</p></td><td><p>91.37</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>6. Conclusion and future research direction</title>
      <p>This section presents the findings and a summary of the proposed methodology, as well as the direction for future research.</p>
      
        <sec>
          
            <title>6.1. Conclusion and summary</title>
          
          <p>The study goes thoroughly into deep learning approaches, with a particular emphasis on Convolutional Neural Networks (CNN) and Artificial Neural Networks (ANN), which are formidable tools for handling the complex challenge of detecting camouflaged images. Through comprehensive investigation and rigorous comparison, the study reveals the extraordinary performance of the CNN + ANN pipeline model, which emerges as the unchallenged leader with the highest average accuracy across all scrutinised datasets. This robustness not only demonstrates the efficacy of the hybrid approach, which strategically combines CNN for feature extraction and ANN for classification, but also highlights its enormous potential across a wide range of practical applications, from military defence to wildlife conservation operations. While the abstract hinted at this extraordinary outcome, a more exact expression of specific accuracy values would surely strengthen the conclusions drawn and provide a more detailed understanding of the model's capabilities in compared to its competitors. Surprisingly, the accuracy of this hybrid model, CNN+ANN, is 91.37%, confirming its status as a powerful rival in the field of deep learning-based image analysis and pattern identification. This high level of accuracy not only demonstrates the model's efficacy, but also emphasises its dependability in real-world settings where precision is critical.</p>
        </sec>
      
      
        <sec>
          
            <title>6.2. Future findings</title>
          
          <p>Furthermore, the study suggests intriguing avenues for future research, urging the investigation of more complicated scenarios, such as real-time video surveillance, to determine the models' flexibility and efficacy in dynamic settings. Furthermore, the optimisation of these models through the integration of varied datasets and rigorous fine-tuning of hyperparameters provides a fertile ground for improving their performance and robustness under difficult conditions. By condensing these complex insights and reaffirming major findings, the summary not only gives a thorough overview of the study's achievements, but it also lays the groundwork for future research in the field of deep learning-based image analysis and pattern recognition.</p>
        </sec>
      
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation/>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation/>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation/>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation/>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation/>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation/>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation/>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation/>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation/>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation/>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation/>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation/>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation/>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation/>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation/>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation/>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation/>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation/>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation/>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation/>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation/>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation/>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation/>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation/>
      </ref>
    </ref-list>
  </back>
</article>