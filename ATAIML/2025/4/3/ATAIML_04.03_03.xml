<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-pBYowHNWUoQsBtMTLCR07Q9-TnFIgNns</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml040303</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Interpretable Deep Learning Framework for Early Classification of Tomato and Grapevine Leaf Diseases</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-4712-556X</contrib-id>
          <name>
            <surname>Edara</surname>
            <given-names>Geethika Ramaiah</given-names>
          </name>
          <email/>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3315-8739</contrib-id>
          <name>
            <surname>Reddy</surname>
            <given-names>Aluru Ranganadha</given-names>
          </name>
          <email>hodbioinformatics@vignan.ac.in</email>
        </contrib>
        <aff id="aff_1">Department of Bioinformatics, Vignan’s Foundation for Science, Technology and Research, 522213 Guntur, India</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>17</day>
        <month>08</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>3</issue>
      <fpage>174</fpage>
      <lpage>185</lpage>
      <page-range>174-185</page-range>
      <history>
        <date date-type="received">
          <day>02</day>
          <month>07</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>14</day>
          <month>08</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>The integration of artificial intelligence (AI) in precision agriculture has facilitated significant advancements in crop health monitoring, particularly in the early identification and classification of foliar diseases. Accurate and timely diagnosis of plant diseases is critical for minimizing crop loss and enhancing agricultural sustainability. In this study, an interpretable deep learning model—referred to as the Multi-Crop Leaf Disease (MCLD) framework—was developed based on a Convolutional Neural Network (CNN) architecture, tailored for the classification of tomato and grapevine leaf diseases. The model architecture was derived from the Visual Geometry Group Network (VGGNet), optimized to improve computational efficiency while maintaining classification accuracy. Leaf image datasets comprising healthy and diseased samples were employed to train and evaluate the model. Performance was assessed using multiple statistical metrics, including classification accuracy, sensitivity, specificity, precision, recall, and F1-score. The proposed MCLD framework achieved a detection accuracy of 98.40% for grapevine leaf diseases and a classification accuracy of 95.71% for tomato leaf conditions. Despite these promising results, further research is required to address limitations such as generalizability across variable environmental conditions and the integration of field-acquired images. The implementation of such interpretable AI-based systems is expected to substantially enhance precision agriculture by supporting rapid and accurate disease management strategies.</p></abstract>
      <kwd-group>
        <kwd>Plant disease detection</kwd>
        <kwd>Deep learning</kwd>
        <kwd>Convolutional neural network</kwd>
        <kwd>Artificial intelligence</kwd>
        <kwd>Leaf disease</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="2"/>
        <fig-count count="11"/>
        <table-count count="3"/>
        <ref-count count="27"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Plant diseases have been known to threaten agricultural productivity and subsequently lead to the unavailability of foodstuffs in the absence of early detection. Rice and maize, however, are cores of agricultural productivity and food accessibility. Management and mitigation of plant diseases in agriculture depend on early detection as well as forecasting. In rural areas in developing countries, agricultural specialists still rely on visual diagnosis for the identification of plant diseases. The traditional method calls for expert monitoring quite often; thus, it becomes expensive for large-scale farms [<xref ref-type="bibr" rid="ref_1">1</xref>]. On the other hand, farmers in remote locations have to travel long distances to get expert consultation from an expert which increases costs and delays timely disease management. Hence, the traditional method has limitations in scope and scalability.</p><p>Therefore, research on automated plant disease detection is significant. Automation can efficiently survey vast agricultural landscapes and promptly detect symptoms of diseases in plant foliage [<xref ref-type="bibr" rid="ref_2">2</xref>]. Systems for identifying plant diseases that are rapid, automatic, cost-effective, and reliable are needed. Many studies have used classifiers, such as K-nearest neighbor (KNN), support vector machine (SVM), Fisher linear discriminant (FLD), artificial neural network (ANN) and random forest (RF), to classify plant images into healthy and infected. The leaves show the earliest indications of plant diseases [<xref ref-type="bibr" rid="ref_3">3</xref>]. Feature extraction methods such as the seven Hu invariant moments, scale-invariant feature transform (SIFT), Gabor transform, global-local singular value decomposition, and sparse representation have been employed in classical approaches for segmenting infected areas. Such a design in hand-crafted features demands an expert intervention that can be subjective and create problems in feature selection. Moreover, under tricky field conditions, existing segmentation methods rarely identify diseased leaf regions with sufficient accuracy, making the automatic detection of plant diseases challenging. Deep learning, especially CNNs, has recently shown great success in tackling these problems. CNNs are particularly effective in image categorization for both small- and large-scale applications [<xref ref-type="bibr" rid="ref_4">4</xref>]. For example, Mohanty et al. [<xref ref-type="bibr" rid="ref_5">5</xref>] achieved a stunning accuracy of 99.35% using a trained CNN model to distinguish between 14 crop species and 26 diseases. Using deep CNNs, Ma et al. [<xref ref-type="bibr" rid="ref_6">6</xref>] accurately identified downy mildew, anthracnose, powdery mildew, and target leaf spots on cucumbers with an accuracy of 93.4%. Although these studies show excellent results, the image datasets used in them are hardly diverse-spotting under controlled laboratory conditions instead of agricultural settings. CNNs have not exhibited much progress in identifying disease symptoms on leaves in field images with complex backgrounds. Because of the large number of trainable parameters, CNNs require large datasets with labeled examples, which are difficult to collect. Despite these limitations, deep learning research has shown promise. To address the limitations of classical CNNs, transfer learning techniques are useful. Fine-tuning only the final classification layers of pretrained networks is one such approach [<xref ref-type="bibr" rid="ref_7">7</xref>].</p><p>This work employs deep transfer learning within CNN architectures toward enhanced detection of slight symptom expressions of diseases with a lesser computation burden. In the proposed approach, the pretrained CNN module plays the role of basic feature extraction, while an auxiliary module is tasked with multi-scale feature representations for improved detection. Particularly, the addition of the Inception module enhances the VGGNet architecture. The VGGNet architectural modification includes changing the last convolutional layer to a 3 × 3 × 512 layer and incorporating batch normalization and Swish activation functions in place of the Rectified Linear Unit (ReLU). Two Inception modules follow that extract multi-scale features from convolutional outputs. Global average pooling replaces fully connected layers for feature map dimensionality reduction. A fully connected Softmax layer is added at the end and customized to the number of disease classes, thus making INC-VGGN, a variant CNN for plant disease classification.</p><p>Identification of plant diseases is very important in maintaining agricultural productivity and economic stability. Early diagnosis of diseases in plants reduces crop loss due to diseases, assures food supply for the growing population, and helps preserve farmers' revenues. The following part describes some recent applications of machine learning for identifying plant diseases. Ferentinos et al. [<xref ref-type="bibr" rid="ref_8">8</xref>] used the CNN architectures AlexNet, AlexNetOWTBn, GoogleNet, Overfeat, and Visual Geometry Group (VGG) to classify plant diseases. The models were trained on 87,848 images of 25 plant species and 58 plant-disease pairs. The best model classified damaged and healthy plants with an accuracy of 99.53%. Mohanty et al. [<xref ref-type="bibr" rid="ref_5">5</xref>] used 54,306 images of diseased and healthy leaves in an open-source dataset. Their model used deep CNN (AlexNet and GoogleNet) for multicategory classification of 14 crops and 26 diseases with almost 99.35% accuracy on a different test set. Mehedi et al. [<xref ref-type="bibr" rid="ref_9">9</xref>] pre-trained EfficientNetV2L, MobileNetV2, and ResNet152V2 for transfer learning. The model diagnosed 38 leaf diseases across 14 plant species using data from Kaggle. The best-performing model achieved an accuracy of 99.63%, which was the case for EfficientNetV2L. Local Interpretable Model-Agnostic Explanations (LIME), a tool for explainable artificial intelligence (XAI), helped explain the predictions made by the models. Ramesh et al. [<xref ref-type="bibr" rid="ref_10">10</xref>] classified healthy and diseased leaves using the RF classifier. After creating a dataset and extracting features using Histogram of Oriented Gradients (HOG), the classifier was trained and classification was conducted. The model predicted approximately 70% accuracy on 160 images of papaya leaves.</p><p>Jasim and Al-Tuwaijari [<xref ref-type="bibr" rid="ref_11">11</xref>] showed that deep learning models are superior to the typical machine learning algorithms in the detection and categorization of early plant diseases. For this, 20,636 Plant Village images were used, with tomato, pepper, and potato crops selected for their economic significance. A model with an accuracy of 98.03% was developed based on CNN, which can be further enhanced by adding more training data. Harakannanavar et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] applied machine learning with image processing to identify diseases in tomatoes. A commendable strategy was proposed for the preliminary detection of diseases by using SVM, KNN, and CNN as classifiers. The model achieved accuracies of 88%, 97%, and 99.6% with each approach, respectively. The CNN-based model proposed by Benito Fernández et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] was tuned to XAI methods such as LIME (2016), SHapley Additive exPlanations (SHAP) (2017), and Gradient-weighted Class Activation Mapping (Grad-CAM) (2017), enhancing the network prediction transparency. Kinger and Kulkarni [<xref ref-type="bibr" rid="ref_14">14</xref>] discussed several approaches and aimed to make deep models more interpretable in the context of plant disease recognition. The accuracy obtained using the VGG16 architecture was 98.15%. Grad-CAM was utilized to provide visual explanations that are interpretable to humans for the decisions made by the model.</p><p>Khattak et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] proposed a CNN-based approach to tackle the diseases of citrus fruits. The model distinguished between healthy and unhealthy citrus fruits and leaves with an accuracy of 94.55%. The identification of the disease in citrus fruit increased the accuracy to 95.65%, thus proving the model to be a reliable tool for managing diseases among farmers. Nahiduzzaman et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] proposed a CNN-XAI architecture for the classification of mulberry leaf diseases. A lightweight CNN framework was employed, which achieved remarkable accuracies of 95.05 ± 2.86% for three-class classification and 96.06 ± 3.01% for binary classification. In comparison to the traditional deep transfer learning models, the proposed model achieved better accuracy with a reduced number of parameters, layers, and computational complexity than those models. Additionally, SHAP brought more transparency to the model.</p><p>Arsenovic et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] addressed a crucial problem by applying deep learning for precise classification of plant diseases. Traditional augmentation techniques and generative adversarial networks were used to build a huge dataset comprising 79,265 images of leaves. Experimental results showed that the model was able to detect plant diseases with an accuracy of 93.67% under varied conditions. Singh et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] applied computer vision to reduce the agricultural loss in India caused by diseases of plants. Hand-labeling 2,598 samples from 13 plant species and 17 disease categories required 300 hours. The validation of the dataset was done by training three classification models, which achieved 31% improvement over the existing datasets. Many studies have used deep learning models to detect and diagnose plant diseases, comparing multiple architectures and visualization techniques. However, these studies have also highlighted vulnerabilities in the research, particularly the reliance on small datasets that do not reflect the diversity found in nature.</p>
    </sec>
    <sec sec-type="">
      <title>2. Methodology</title>
      <p> <xref ref-type="fig" rid="fig_1">Figure 1</xref> illustrates the proposed methodology for identification and diagnosis of diseases affecting the leaves of several crops. After images of diseased plant leaves were captured and categorized, the dataset was processed to eliminate noise, convert the images to grayscale, enhance image quality, and resize them. Data augmentation was used to improve the dataset by introducing new image samples through rotation, translation, and random variations. The augmented images improved both the dataset and the model’s performance. The model was trained with processed images in the next step. Then the model predicted diseases in new images, resulting in accurate diagnosis and classification of plant diseases.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Flow chart of the proposed methodology</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_MNT3D9Kx4qfUje1o.png"/>
        </fig>
      
      <p> <xref ref-type="fig" rid="fig_2">Figure 2</xref> shows the field-collected healthy and diseased tomato leaves. The disease symptoms differ in pattern, site, and color in each of the images. The diseases of the tomato leaves, as shown by their symptoms, include grey spots, leaf mold and bacterial wilt [<xref ref-type="bibr" rid="ref_19">19</xref>]. <xref ref-type="fig" rid="fig_3">Figure 3</xref> shows photos of grapevine leaves taken in Nashik, Maharashtra, India. The dataset consists of leaf blight (1,076 images), healthy leaves (423 images), black measles (1,383 images), and black rot (1,180 images). Adjustments were made for categories A to D in terms of brightness and color for dataset enhancement. The second batch of tomato samples included the following diseases: tomato yellow leaf curl (3,209 cases), early blight (1,000 cases), leaf mold (952 cases), spider mite infestation (1,676 cases), bacterial spot (2,127 cases), Septoria leaf spot (1,771 cases), late blight (1,909 cases), unspecified spot disease (1,404 cases), and mosaic virus (373 cases). Deep learning greatly improved computer vision, particularly in image recognition and categorization. The generally proposed AI-based agricultural approaches for identifying and classifying leaf diseases in crops using CNNs involve stages of data acquisition, preprocessing, image segmentation, classification, and feature extraction. Feature extraction, image processing and classification were performed on the Google Colaboratory platform.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Tomato leaf sample images depicting: A) Mosaic Virus, B) Healthy Leaf, C) Target Spot, D) Late Blight, E) Bacterial Spot, F) Septoria Spot, G) Spider Mite, H) Leaf Mold, I) Early Blight, and J) Yellow Leaf</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_GfWQKzlCBuU1GDiW.png"/>
        </fig>
      
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>Grape leaf sample images depicting: A) Black Rot, B) Esca, C) Healthy, D) Leaf Blight (Isariopsis Leaf Spot)</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_vAigl3dXuVVEk4BH.png"/>
        </fig>
      
      
        <sec>
          
            <title>2.1. Image augmentation</title>
          
          <p>Larger datasets enhance the efficiency of learning algorithms and also reduce overfitting. It is difficult and time-consuming to obtain real-time training datasets. Data augmentation introduces variety in training data for deep learning models. Some of the augmentation techniques include flipping, cropping, rotation, color changes, color augmentation based on Principal Component Analysis (PCA), noise reduction, Generative Adversarial Network (GAN), and Neural Style Transfer (NST) [<xref ref-type="bibr" rid="ref_20">20</xref>]. The Faster Dual-Region Integrated Attention Convolutional Neural Network (Faster DR-IACNN) model is a deep learning-based framework designed for the rapid and accurate identification of disease spots on grape leaves. From the original set of images, 4,449 images were used, and 62,286 additional images were generated through data augmentation methods.</p><p>Feature extraction of images was conducted during segmentation when fixed-length feature vectors were formed. The system was used to evaluate the color, texture, and shape of the images. Color properties were obtained from the Hue, Saturation, and Value (HSV) and Red, Green, and Blue (RGB) color spaces using methods such as means, confidence intervals, and smoothness. Texture features were captured from color images using the gray-level co-occurrence matrix (GLCM). This approach is essential for detecting diseases in plants. </p>
        </sec>
      
      
        <sec>
          
            <title>2.2. Transfer learning</title>
          
          <p>Deep learning models require time, computational resources, and especially advanced GPUs along with massive training data to train and tune the models. However, these challenges can be easily resolved through transfer learning. Transfer learning in deep learning uses a pre-trained CNN for one task to exploit its knowledge for other tasks [<xref ref-type="bibr" rid="ref_21">21</xref>].</p><p>A multi-crop image dataset of 224 × 224 was used. The ResNet architecture was modified to accommodate the dataset. In almost all the topologies of ResNet, the layer preceding the softmax activation is a 7 × 7 average pooling layer. Smaller sizes of pooling allow the network to process smaller images. Transfer learning requires that the images be preprocessed to fit the models of multi-crop datasets.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Results and discussion</title>
      
        <sec>
          
            <title>3.1. Cnn</title>
          
          <p>CNNs contain convolutional, pooling, fully connected, and dense layers, as shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>. Below is a detailed description of each layer.</p><p>Convolutional layers are primarily used to obtain features from the input images. The effectiveness of feature extraction is enhanced by applying these layers repeatedly [<xref ref-type="bibr" rid="ref_22">22</xref>]. The process of CNN feature extraction through several layers is given by the following equation:</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="moiato5enl">
                <mml:msub>
                  <mml:mi>H</mml:mi>
                  <mml:mi>i</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mi>φ</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>+</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>H</mml:mi>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mo>−</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>W</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>b</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mh75hpd6k6">
    <mml:msub>
      <mml:mi>H</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is the feature map, <inline-formula>
  <mml:math id="mbbttu698n">
    <mml:msub>
      <mml:mi>W</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is the weight, <inline-formula>
  <mml:math id="m97t1htnls">
    <mml:msub>
      <mml:mi>b</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is the offset, and <inline-formula>
  <mml:math id="mf085ym52o">
    <mml:mi>φ</mml:mi>
  </mml:math>
</inline-formula> is the ReLU.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Schematic of the proposed CNN architecture</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_8ej3Lw7Dlz6Pt49j.png"/>
            </fig>
          
          <p>By combining feature information, pooling layers in CNNs eliminate the dimensionality of the feature map and help to optimize computation efficiency in picture processing. Max and average pooling are principal forms of these layers. In max pooling, the maximum value is selected from an image region, whereas in average pooling its mean is computed. Dropout layers are a form of regularization in training models, which helps improve the performance of the model. This reduces the dependence on particular neurons and thus prevents overfitting. A scaling factor makes this approach systematic for all activation functions [<xref ref-type="bibr" rid="ref_23">23</xref>]. The flatten layer reduces the pooled feature maps while preserving the channel information. It reshapes the data into a one-dimensional vector for the fully connected, dense layers.</p><p>A particular role of the fully connected layers is to help to classify the features extracted in the image. The softmax function predicts the properties from the previous layers and outputs multiclass classification by activating the output layers. In two-class classification problems, Multilayer Perceptron (MLP) models serve as classifiers within the layers of neural networks. Nonlinearity is introduced by the ReLU activation function in fully connected vectors. This architecture enables the implementation of complex decision boundaries. Some of the basic principles of SVMs are as follows:</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="mmnywmtg7k">
                <mml:mrow>
                  <mml:mover>
                    <mml:mrow>
                      <mml:mi>Minimize</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>n</mml:mi>
                    </mml:mrow>
                  </mml:mover>
                </mml:mrow>
                <mml:msup>
                  <mml:mrow/>
                  <mml:mrow>
                    <mml:mn>1</mml:mn>
                    <mml:mn>2</mml:mn>
                    <mml:mrow>
                      <mml:mo>/</mml:mo>
                    </mml:mrow>
                  </mml:mrow>
                </mml:msup>
                <mml:msup>
                  <mml:mi>W</mml:mi>
                  <mml:mn>2</mml:mn>
                </mml:msup>
                <mml:munderover>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>j</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>N</mml:mi>
                </mml:munderover>
                <mml:munderover>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>j</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>ξ</mml:mi>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:munderover>
                <mml:msub>
                  <mml:mrow/>
                  <mml:mn>1</mml:mn>
                </mml:msub>
                <mml:mn>1</mml:mn>
                <mml:mo>+</mml:mo>
                <mml:mi>C</mml:mi>
              </mml:math>
            </disp-formula>
          
          <p>where, $C<inline-formula>
  <mml:math id="me3y5tjo1k">
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>y_{i^{\prime}}(\bar{W} \cdot \bar{X}+b) \cdot \geq 1-\zeta_j \cdot(j=1,2,3, \ldots, N)<inline-formula>
  <mml:math id="m0jgx7qtgh">
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
  </mml:math>
</inline-formula>\gamma=1<inline-formula>
  <mml:math id="mlfkvfuaj1">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>C=1$. The depth characterizes the ConvNet architecture. It deepens the network with more convolutional layers and hence improves the architecture. The accuracy in the recognition task is enhanced with the use of small 3 × 3 convolutional filters in all layers. These refined ConvNet models thus succeed in achieving state-of-the-art classification on datasets and localization, performing quite well across image recognition datasets even in very simple processing pipelines [<xref ref-type="bibr" rid="ref_24">24</xref>]. The ConvNets were trained on 224 × 224 RGB images. Pre-processing was limited to mean RGB value subtraction, calculated from the training dataset. The image went through convolutional layers and filters with a 3 × 3 receptive field. In one configuration, 1 × 1 convolutional filters were used to perform a linear transformation on the input channels followed by a non-linear function. To preserve spatial resolution, 3 × 3 filters require a stride and padding of 1 pixel each. Spatial pooling was performed by five max-pooling layers, each following specific convolutional layers. Not every convolutional layer was immediately shadowed by max pooling. A 2 × 2 pixel window with a stride of 2 was used to reduce the spatial dimensions while preserving salient features.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Vgg16</title>
          
          <p>A pre-trained VGG16 CNN model was used to classify healthy and unhealthy crop images to enhance the classification accuracy. The pre-trained VGG16 network helped the model determine the conditions of crop leaves. Besides, the CNN model also learned to detect and classify plant diseases from photographs taken in new fields [<xref ref-type="bibr" rid="ref_25">25</xref>].</p><p>The upgraded VGG model was implemented in configurations containing either 11 or 5 convolutional layers, each employing a uniform filter size of 3 × 3. The input image size was secured at 224 × 224. The images were pre-processed before being passed through the 3 × 3 convolutional layer. This layer applied a linear transformation to the input channel using a 1 × 1 filter. Max pooling was performed using a 2 × 2 filter with a stride of 2. Each fully connected layer consisted of 4,096 units, maintaining consistent dimensionality across layers.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Performance evaluation</title>
          
          <p>The F1-score, Receiver Operating Characteristic (ROC) curve, accuracy matrix, and Area Under the Curve (AUC) were applied for segmentation performance assessment. Evaluation metrics also define the effectiveness of classifiers.</p>
          <p>An overall assessment of the performance of the model on each class was conducted. Accuracy was calculated by dividing the number of correct predictions by the total number of predictions made. For a complete assessment, recall, F1-score and precision were also calculated. A mathematical representation of accuracy is given below.</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="mo0h69p0cj">
                <mml:mi>A</mml:mi>
                <mml:mi>C</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>T</mml:mi>
                    <mml:mi>N</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mi>T</mml:mi>
                    <mml:mi>N</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="m62bd4z9vp">
    <mml:mi>T</mml:mi>
    <mml:mi>P</mml:mi>
  </mml:math>
</inline-formula> is the true positive, indicating the correctly identified positive cases; <inline-formula>
  <mml:math id="mq5uvl4mgx">
    <mml:mi>T</mml:mi>
    <mml:mi>N</mml:mi>
  </mml:math>
</inline-formula> is the true negative, indicating the correctly identified negative cases; <inline-formula>
  <mml:math id="mmzubipwre">
    <mml:mi>F</mml:mi>
    <mml:mi>P</mml:mi>
  </mml:math>
</inline-formula> is the false positive, indicating the incorrectly classified negative cases as positive; and <inline-formula>
  <mml:math id="m6tbm0zxs6">
    <mml:mi>F</mml:mi>
    <mml:mi>N</mml:mi>
  </mml:math>
</inline-formula> is the false negative, indicating the incorrectly classified positive cases as negative.</p><p>Following are the classifier performance measures using evaluation metrics:</p>
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="m62ne5fo47">
                <mml:mtable displaystyle="true" columnspacing="1em" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mi>TPR</mml:mi>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mtext> Sensitivity </mml:mtext>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>T</mml:mi>
                          <mml:mi>P</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>T</mml:mi>
                          <mml:mi>P</mml:mi>
                          <mml:mi>F</mml:mi>
                          <mml:mi>N</mml:mi>
                          <mml:mo>+</mml:mo>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mi>TNR</mml:mi>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mtext> Specificity </mml:mtext>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>T</mml:mi>
                          <mml:mi>N</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>T</mml:mi>
                          <mml:mi>N</mml:mi>
                          <mml:mi>F</mml:mi>
                          <mml:mi>P</mml:mi>
                          <mml:mo>+</mml:mo>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="mjwg4yxuvz">
                <mml:mtable displaystyle="true" columnspacing="1em" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mi data-mjx-auto-op="false">FPR</mml:mi>
                      </mml:mrow>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>F</mml:mi>
                          <mml:mi>P</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>+</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mi>F</mml:mi>
                          <mml:mi>P</mml:mi>
                          <mml:mi>T</mml:mi>
                          <mml:mi>N</mml:mi>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mlxacz044t">
    <mml:mi>T</mml:mi>
    <mml:mi>P</mml:mi>
    <mml:mi>R</mml:mi>
  </mml:math>
</inline-formula> is the true positive rate, <inline-formula>
  <mml:math id="mnnkqpzfw5">
    <mml:mi>T</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>R</mml:mi>
  </mml:math>
</inline-formula> is the true negative rate, and <inline-formula>
  <mml:math id="mq9xb6y1c4">
    <mml:mi>F</mml:mi>
    <mml:mi>P</mml:mi>
    <mml:mi>R</mml:mi>
  </mml:math>
</inline-formula> is the false positive rate.</p>
          
            <disp-formula>
              <label>(6)</label>
              <mml:math id="md6wc180s4">
                <mml:mtable displaystyle="true" columnspacing="1em" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mtext> Precision </mml:mtext>
                      <mml:mtext> Recall </mml:mtext>
                      <mml:mo>=</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>T</mml:mi>
                          <mml:mi>P</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>+</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mi>T</mml:mi>
                          <mml:mi>P</mml:mi>
                          <mml:mi>F</mml:mi>
                          <mml:mi>P</mml:mi>
                        </mml:mrow>
                      </mml:mfrac>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>T</mml:mi>
                          <mml:mi>P</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>+</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mi>T</mml:mi>
                          <mml:mi>N</mml:mi>
                          <mml:mi>F</mml:mi>
                          <mml:mi>N</mml:mi>
                        </mml:mrow>
                      </mml:mfrac>
                      <mml:mstyle scriptlevel="0">
                        <mml:mspace width="1em"/>
                      </mml:mstyle>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(7)</label>
              <mml:math id="me6rep3vv6">
                <mml:mtable displaystyle="true" columnspacing="1em" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mi>G</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>M</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>n</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:munderover>
                          <mml:mo>∏</mml:mo>
                          <mml:mrow>
                            <mml:mrow>
                              <mml:mi>K</mml:mi>
                            </mml:mrow>
                            <mml:mo>=</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                          <mml:mi>m</mml:mi>
                        </mml:munderover>
                        <mml:msub>
                          <mml:mi>Recall</mml:mi>
                          <mml:mrow>
                            <mml:mrow>
                              <mml:mi>k</mml:mi>
                            </mml:mrow>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>/</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>m</mml:mi>
                      </mml:mrow>
                      <mml:mo>−</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>where, $m<inline-formula>
  <mml:math id="m9pgtx90x2">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>G$ denotes the accuracy ratio of the true negative rate to the false positive rate.</p><p>The mean average precision (mAP) of the algorithm measures precision, recall, and mean. mAP is used to evaluate image processing tasks and detection tasks. In terms of results, accuracy assesses the ratio of appropriately classified examples, while recall measures the proportion of correctly identified instances to the total relevant cases.</p><p>The F1-score is yet another crucial performance measure, since it provides a balance between precision and recall which can be said to be more informative. It can be calculated as follows:</p>
          
            <disp-formula>
              <label>(8)</label>
              <mml:math id="md4n5z25f0">
                <mml:mtable displaystyle="true" columnspacing="1em" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mtext> F1 Score </mml:mtext>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mn>2</mml:mn>
                          <mml:mo>×</mml:mo>
                          <mml:mo>×</mml:mo>
                          <mml:mtext> Precision </mml:mtext>
                          <mml:mtext> Recall </mml:mtext>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>+</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mtext> Precision </mml:mtext>
                          <mml:mtext> Recall </mml:mtext>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>The ROC curve is useful in assessing classification performance and addressing issues related to computational modeling. <xref ref-type="fig" rid="fig_5">Figure 5</xref> represents the connection between the false positive rate and the true positive rate at different thresholds.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>ROC curve: False positive rate vs. true positive rate</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_lKWeoPyE0ZePwJPT.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.4. Experimental setup and results</title>
          
          <p>The model with the highest true negative rate detected the bad cases, whereas the model with the highest true positive rate classified the healthy cases. To reduce both training and testing times, an overall assessment was made using the Matthews Correlation Coefficient (MCC). MCC classifies only tough datasets reliably. Unlike accuracy, which can be misleading in imbalanced datasets, MCC takes into account all classification results, i.e., true negatives, true positives, false negatives, and false positives. Its values range between -1 (lowest classification) and +1 (perfect classification), and 0 denotes random predictions. The combination of hidden layers, number of epochs and hidden nodes, dropout rate, activation functions, learning rate and batch size impacts model optimization. Hyperparameter tuning – changing epochs, learning rate, hidden layers and activation functions in a systematic way – increases efficiency and performance. The model was adjusted to improve its accuracy and reduce the average loss.</p>
          <p>An experimental analysis was conducted on Google Colaboratory using the research tools developed by Google. This environment was equipped with Python programming and several pre-installed research libraries. Python 3, running on Google Compute Engine GPUs with 12.72 GB of RAM and 68.40 GB of disk space, was utilized for the experiment. The dataset was accessed by mounting Google Drive, and the platform’s robust computational resources were utilized to train the model. This setup facilitated the execution of a Python program that converted images into arrays and retrieved them from the designated directory. All label images were binarized using Scikit-learn’s label binarization function and retrieved from a designated folder for processing. The train-test-split function in Python was used to split the dataset as training and testing datasets. The model parameters were set as shown in <xref ref-type="table" rid="table_1">Table 1</xref>. The deep learning CNN was optimized using Adam, thereby overcoming sparse gradient noise.</p><p>The input network processed batches of 224 × 224 images containing 30 grapes and 25 tomatoes. The model was tested over epochs by varying the batch size and the learning rate. A max-pooling operation of 2 × 2 with ReLU activation was applied after each layer. In the final layer, multi-crop predictions were made using softmax activation. These training hyperparameters were tuned for performance as well.</p><p>The model attained an average accuracy of 98.40% and 95.71% for grapes and tomatoes, respectively. Learning rates were optimized through validation on a large multi-crop image dataset to enhance performance metrics. Accuracy increased with batch size and epoch configuration.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Comprehensive parameter settings for the trained model</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Hyperparameter</p></th><th colspan="1" rowspan="1"><p>Value/Setting</p></th></tr><tr><td colspan="1" rowspan="1"><p>Crops</p></td><td colspan="1" rowspan="1"><p>Grapes &amp;amp; Tomatoes</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image size</p></td><td colspan="1" rowspan="1"><p>224 × 224 × 3</p></td></tr><tr><td colspan="1" rowspan="1"><p>Convolutional layers</p></td><td colspan="1" rowspan="1"><p>13</p></td></tr><tr><td colspan="1" rowspan="1"><p>Max pooling layers</p></td><td colspan="1" rowspan="1"><p>5</p></td></tr><tr><td colspan="1" rowspan="1"><p>Activation functions</p></td><td colspan="1" rowspan="1"><p>ReLU, Softmax</p></td></tr><tr><td colspan="1" rowspan="1"><p>Dropout rate</p></td><td colspan="1" rowspan="1"><p>0.15 / 0.25 / 0.50</p></td></tr><tr><td colspan="1" rowspan="1"><p>Learning rate</p></td><td colspan="1" rowspan="1"><p>0.00001 / 0.0001</p></td></tr><tr><td colspan="1" rowspan="1"><p>Epochs</p></td><td colspan="1" rowspan="1"><p>20 / 25 / 30 / 40 / 45</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>Crop leaf images were employed to train the model to identify and classify disease types using transfer learning techniques, including VGG16. The dataset was distributed into 80:10:10 ratio for training, validation, and testing. Changes in model training parameters were evaluated by monitoring training and validation accuracy. These experiments were conducted on Google Colaboratory, equipped with 12.50 GB of RAM. In these experiments, the learning rate, epochs, dropout rate and the number of images were set to different values. These parameters influence accuracy, training loss, validation loss, and accuracy. The grape and tomato leaf samples were used to test the model. The experiments for the grape and tomato datasets are described in <xref ref-type="table" rid="table_2">Table 2</xref> and <xref ref-type="table" rid="table_3">Table 3</xref>. The model was trained, tested, and validated on the leaves of grapes and tomatoes. <xref ref-type="fig" rid="fig_6">Figure 6</xref> and <xref ref-type="fig" rid="fig_7">Figure 7</xref> reflect the training along with validation accuracy and loss for both types of plants. <xref ref-type="fig" rid="fig_8">Figure 8</xref> shows the confusion matrix for classifying grape and tomato leaves. <xref ref-type="fig" rid="fig_9">Figure 9</xref> and <xref ref-type="fig" rid="fig_10">Figure 10</xref> show the confusion matrix heat maps and normalized confusion matrix heat maps for the tomato and grape datasets. <xref ref-type="fig" rid="fig_11">Figure 11</xref> presents a comparison of different models based on accuracy with the proposed VGG16 model for the grape and tomato datasets, respectively. The proposed process was assessed on a real-field image dataset with diverse backgrounds and lighting variations.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Test results of classification</title>
              </caption>
              <table><tr><th >No. of Epochs</th><th >Learning Rate</th><th >Dropout Rate</th><th >No. of Images</th><th >Training Loss</th><th >Training Accuracy</th><th >Validation Loss</th><th >Validation Accuracy</th></tr><tr><td >40</td><td >0.00001</td><td >0.25</td><td >450</td><td >0.08</td><td >0.98</td><td >0.04</td><td >0.98</td></tr><tr><td >30</td><td >0.0001</td><td >0.50</td><td >450</td><td >0.11</td><td >0.95</td><td >0.05</td><td >0.98</td></tr><tr><td >45</td><td >0.00001</td><td >0.25</td><td >400</td><td >0.09</td><td >0.97</td><td >0.05</td><td >0.98</td></tr><tr><td >45</td><td >0.0001</td><td >0.25</td><td >750</td><td >0.08</td><td >0.96</td><td >0.05</td><td >0.98</td></tr><tr><td >30</td><td >0.0001</td><td >0.50</td><td >450</td><td >0.13</td><td >0.95</td><td >0.06</td><td >0.98</td></tr><tr><td >40</td><td >0.001</td><td >0.30</td><td >600</td><td >0.11</td><td >0.96</td><td >0.05</td><td >0.98</td></tr></table>
            </table-wrap>
          
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Experimental results of the grape model for setting different parameters</title>
              </caption>
              <table><tr><th >No. of Epochs</th><th >Learning Rate</th><th >Dropout Rate</th><th >No. of Images</th><th >Training Loss</th><th >Training Accuracy</th><th >Validation Loss</th><th >Validation Accuracy</th></tr><tr><td >22</td><td >0.0001</td><td >0.25</td><td >200</td><td >0.16</td><td >0.95</td><td >0.26</td><td >0.94</td></tr><tr><td >35</td><td >0.00001</td><td >0.20</td><td >180</td><td >0.22</td><td >0.92</td><td >0.31</td><td >0.90</td></tr><tr><td >30</td><td >0.00001</td><td >0.15</td><td >200</td><td >0.26</td><td >0.90</td><td >0.33</td><td >0.89</td></tr><tr><td >30</td><td >0.00001</td><td >0.25</td><td >200</td><td >0.30</td><td >0.88</td><td >0.37</td><td >0.88</td></tr><tr><td >30</td><td >0.00001</td><td >0.25</td><td >180</td><td >0.48</td><td >0.83</td><td >0.41</td><td >0.86</td></tr><tr><td >30</td><td >0.00001</td><td >0.50</td><td >200</td><td >0.52</td><td >0.82</td><td >0.45</td><td >0.85</td></tr></table>
            </table-wrap>
          
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Training and validation performance: (a) accuracy and (b) loss curves for the VGG16 model on the grape dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_G9g1TNGp8pPF1zU_.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_G9g1TNGp8pPF1zU_.png"/>
            </fig>
          
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>Training and validation performance: (a) accuracy and (b) loss curves of the VGG16 model on the tomato dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_xmNs0U6hWt69qmWZ.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_kK77r0MrVDOtL_-r.png"/>
            </fig>
          
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>Confusion matrix results: (a) tomato dataset and (b) grape dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_2P0pHJW-MYTmWKaN.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_HUao1tdfub09tFGU.png"/>
            </fig>
          
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>
                <title>Confusion matrix heat map: (a) tomato dataset and (b) grape dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_IV91B65mmxdOozsx.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_aoTw8Icv1SYSCYVZ.png"/>
            </fig>
          
          
            <fig id="fig_10">
              <label>Figure 10</label>
              <caption>
                <title>Normalized confusion matrix heat map: (a) tomato dataset and (b) grape dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_lhz2Zd3BQpZe18pC.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_OUHZ0wWwie_AkXY7.png"/>
            </fig>
          
          
            <fig id="fig_11">
              <label>Figure 11</label>
              <caption>
                <title>Comparison of different models based on accuracy with the proposed VGG16 model for (a) the grape dataset and (b) the tomato dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_fKPs2lHIThigDLkD.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/7/img_MRSSFbd9HleAhhMx.png"/>
            </fig>
          
          <p>Data augmentation techniques such as random rotation, flipping, and scaling were used along with pre-processing to make the sample images more diverse and combat overfitting. These augmented the training dataset and thus strengthened the model. These processes are described as follows:</p><p>a) Image resizing: All images were resized to 224 × 224 pixels to be compatible with the model. Diversity was increased by data augmentation of at least 200 healthy and sick images.</p><p>b) Image preprocessing: This kept proportional ratios and yet maintained the structural information. The method gave more clarity to the image while reducing distortion.</p><p>c) Dataset partitioning and training: Random selection of images was used.</p><p>d) Validation and testing: The model was tested on both old and new images.</p><p>Results acquired were paralleled with the actual classes and hence the model’s performance was evaluated on control effectiveness. A modified Residual Dense Network (RDN) model used sets of residual blocks along with a Densely Connected Convolutional Network (DenseNet) to detect diseases in tomato leaves. After image normalization and convolutional residual modules, the dense layer achieved 95% accuracy in classifying tomato disease images on the disease dataset [<xref ref-type="bibr" rid="ref_26">26</xref>]. The Inception-ResNet-v2 model with the ReLU activation function obtained an accuracy of 86.1% in the AI Challenger Competition 2018 [<xref ref-type="bibr" rid="ref_27">27</xref>]. Under cluttered background conditions, the VGGNet model scored an accuracy of 91.83%. Using INC-VGGN, "Phaeosphaeria spot" and "maize eyespot" diseases were detected with an accuracy of 80.38% [<xref ref-type="bibr" rid="ref_27">27</xref>] ( <xref ref-type="fig" rid="fig_11">Figure 11</xref>).</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Conclusion</title>
      <p>A dataset comprising diseased leaves from two crop types was collected and prepared for this research. The VGG16 model, based on CNNs, was used for data augmentation, dataset preprocessing, training, and testing. The results were improved by designing the model and testing it against already available datasets and methods. The classification accuracy was 98.40% for grape diseases and 95.71% for tomato diseases. Identification of diseased leaves in field crops must be under agri-research development. The proposed system outperformed under all conditions and thus can contribute to advancements in agricultural practices. The purpose of this research is to enhance the agricultural sector and food security through high-quality production. Future work will involve gathering high-quality datasets with deep learning applications on images of crop leaves: more detailed analysis of diseases in crops using CNN models based on Inception V3 and ResNet principles. This research will empower the farmers by increasing their income, resulting in an increase in the national GDP.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>31-38</page-range>
          <issue>1</issue>
          <year>2011</year>
          <person-group person-group-type="author">
            <name>
              <surname>Al-Hiary</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Bani-Ahmad</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Reyalat</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Braik</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Al-Rahamneh</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.5120/2183-2754</pub-id>
          <article-title>Fast and accurate detection and classification of plant diseases</article-title>
          <source>Int. J. Comput. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>267-275</page-range>
          <issue>2</issue>
          <year>2011</year>
          <person-group person-group-type="author">
            <name>
              <surname>Al Bashish</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Braik</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Bani-Ahmad</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3923/itj.2011.267.275</pub-id>
          <article-title>Detection and classification of leaf diseases using K-means-based segmentation and neural-networks-based classification</article-title>
          <source>Inf. Technol. J.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>660</page-range>
          <issue>1</issue>
          <year>2013</year>
          <person-group person-group-type="author">
            <name>
              <surname>Arnal Barbedo</surname>
              <given-names>J. G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/2193-1801-2-660</pub-id>
          <article-title>Digital image processing techniques for detecting, quantifying and classifying plant diseases</article-title>
          <source>SpringerPlus</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>147</volume>
          <page-range>70-90</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kamilaris</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Prenafeta-Boldú</surname>
              <given-names>F. X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compag.2018.02.016</pub-id>
          <article-title>Deep learning in agriculture: A survey</article-title>
          <source>Comput. Electron. Agric.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>1419</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mohanty</surname>
              <given-names>S. P.</given-names>
            </name>
            <name>
              <surname>Hughes</surname>
              <given-names>D. P.</given-names>
            </name>
            <name>
              <surname>Salathé</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fpls.2016.01419</pub-id>
          <article-title>Using deep learning for image-based plant disease detection</article-title>
          <source>Front. Plant Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>154</volume>
          <page-range>18-24</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ma</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Du</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Gong</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compag.2018.08.048</pub-id>
          <article-title>A recognition method for cucumber diseases using leaf symptom images based on deep convolutional neural network</article-title>
          <source>Comput. Electron. Agric.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>7</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bensoltane</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Zaki</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s13278-021-00794-4</pub-id>
          <article-title>Towards Arabic aspect-based sentiment analysis: A transfer learning-based approach</article-title>
          <source>Soc. Netw. Anal. Min.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>145</volume>
          <page-range>311–318</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ferentinos</surname>
              <given-names>K. P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compag.2018.01.009</pub-id>
          <article-title>Deep learning models for plant disease detection and diagnosis</article-title>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="conf-paper">
          <page-range>166–170</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mehedi</surname>
              <given-names>M. H. K.</given-names>
            </name>
            <name>
              <surname>Hosain</surname>
              <given-names>A. K. M. S.</given-names>
            </name>
            <name>
              <surname>Ahmed</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Promita</surname>
              <given-names>S. T.</given-names>
            </name>
            <name>
              <surname>Muna</surname>
              <given-names>R. K.</given-names>
            </name>
            <name>
              <surname>Hasan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Reza</surname>
              <given-names>M. T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/IEMCON56893.2022.9946513</pub-id>
          <article-title>Plant leaf disease detection using transfer learning and explainable AI</article-title>
          <source>2022 IEEE 13th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="conf-paper">
          <page-range>41-45</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ramesh</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Hebbar</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Niveditha</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Pooja</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Shashank</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Vinod</surname>
              <given-names>P. V.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICDI3C.2018.00017</pub-id>
          <article-title>Plant disease detection using machine learning</article-title>
          <source>2018 International conference on design innovations for 3Cs compute communicate control (ICDI3C)</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="conf-paper">
          <page-range>259–265</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jasim</surname>
              <given-names>M. A.</given-names>
            </name>
            <name>
              <surname>Al-Tuwaijari</surname>
              <given-names>J. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CSASE48920.2020.9142097</pub-id>
          <article-title>Plant leaf diseases detection and classification using image processing and deep learning techniques</article-title>
          <source>2020 International Conference on Computer Science and Software Engineering (CSASE)</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>305–310</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Harakannanavar</surname>
              <given-names>S. S.</given-names>
            </name>
            <name>
              <surname>Rudagi</surname>
              <given-names>J. M.</given-names>
            </name>
            <name>
              <surname>Puranikmath</surname>
              <given-names>V. I.</given-names>
            </name>
            <name>
              <surname>Siddiqua</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Pramodhini</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">https://doi.org/10.1016/j.gltp.2022.03.016</pub-id>
          <article-title>Plant leaf disease detection using computer vision and machine learning algorithms</article-title>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="conf-paper">
          <page-range>417-428</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Benito Fernández</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Martínez</surname>
              <given-names>D. L.</given-names>
            </name>
            <name>
              <surname>González-Briones</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Chamoso</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Corchado</surname>
              <given-names>E. S.</given-names>
            </name>
          </person-group>
          <article-title>Evaluation of XAI models for interpretation of deep learning techniques’ results in automated plant disease diagnosis</article-title>
          <source>Sustainable Smart Cities and Territories International Conference, Cham</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="conf-paper">
          <page-range>209-216</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kinger</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Kulkarni</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3474124.3474154</pub-id>
          <article-title>Explainable AI for deep learning-based disease detection</article-title>
          <source>Proceedings of the 2021 Thirteenth International Conference on Contemporary Computing</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>112942-112954</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Khattak</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Asghar</surname>
              <given-names>M. U.</given-names>
            </name>
            <name>
              <surname>Batool</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Asghar</surname>
              <given-names>M. Z.</given-names>
            </name>
            <name>
              <surname>Ullah</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Al-Rakhami</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Gumaei</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2021.3096895</pub-id>
          <article-title>Automatic detection of citrus fruit and leaves diseases using deep neural network model</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>1175515</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Nahiduzzaman</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Chowdhury</surname>
              <given-names>M. E. H.</given-names>
            </name>
            <name>
              <surname>Salam</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Nahid</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Ahmed</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Al-Emadi</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Ayari</surname>
              <given-names>M. A.</given-names>
            </name>
            <name>
              <surname>Khandakar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Haider</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fpls.2023.1175515</pub-id>
          <article-title>Explainable deep learning model for automatic mulberry leaf disease classification</article-title>
          <source>Front. Plant Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>939</page-range>
          <issue>7</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Arsenovic</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Karanovic</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Sladojevic</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Anderla</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Stefanovic</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/sym11070939</pub-id>
          <article-title>Solving current limitations of deep learning based approaches for plant disease detection</article-title>
          <source>Symmetry</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="conf-paper">
          <page-range>249–253</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Singh</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Jain</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Jain</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Kayal</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Kumawat</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Batra</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3371158.3371196</pub-id>
          <article-title>PlantDoc: A dataset for visual plant disease detection</article-title>
          <source>Proceedings of the 7th ACM IKDD CoDS and 25th COMAD</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>3565–3573</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Paymode</surname>
              <given-names>A. S.</given-names>
            </name>
            <name>
              <surname>Malode</surname>
              <given-names>V. B.</given-names>
            </name>
            <name>
              <surname>Shinde</surname>
              <given-names>U. B.</given-names>
            </name>
          </person-group>
          <article-title>Artificial intelligence in agriculture for leaf disease detection and prediction: A review</article-title>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="conf-paper">
          <page-range>199–204</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pandian</surname>
              <given-names>J. A.</given-names>
            </name>
            <name>
              <surname>Geetharamani</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Annette</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/IACC48062.2019.8971580</pub-id>
          <article-title>Data augmentation on plant leaf disease image dataset using image manipulation and deep learning techniques</article-title>
          <source>2019 IEEE 9th International Conference on Advanced Computing (IACC)</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>163</volume>
          <page-range>104859</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Nevavuori</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Narra</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Lipping</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compag.2019.104859</pub-id>
          <article-title>Crop yield prediction with deep convolutional neural networks</article-title>
          <source>Comput. Electron. Agric.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>173</volume>
          <page-range>105393</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Nanehkaran</surname>
              <given-names>Y. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compag.2020.105393</pub-id>
          <article-title>Using deep transfer learning for image-based plant disease identification</article-title>
          <source>Comput. Electron. Agric.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>14-15</page-range>
          <issue>3</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>S. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/MITP.2020.2986121</pub-id>
          <article-title>Artificial intelligence (AI) in agriculture</article-title>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="conf-paper">
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Simonyan</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zisserman</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1409.1556</pub-id>
          <article-title>Very deep convolutional networks for large-scale image recognition</article-title>
          <source>3rd International Conference on Learning Representations, ICLR 2015- Conference Track Proceedings</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>787-794</page-range>
          <issue>2</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Alencastre-Miranda</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Johnson</surname>
              <given-names>R. M.</given-names>
            </name>
            <name>
              <surname>Krebs</surname>
              <given-names>H. I.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TII.2020.2992229</pub-id>
          <article-title>Convolutional neural networks and transfer learning for quality inspection of different sugarcane varieties</article-title>
          <source>IEEE Trans. Ind. Inform.</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>28822-28831</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhou</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Xing</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2021.3058947</pub-id>
          <article-title>Tomato leaf disease identification by restructured deep residual dense network</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>171686-171693</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ai</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Tie</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Cai</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3025325</pub-id>
          <article-title>Research on recognition model of crop diseases and insect pests based on deep learning in harsh environments</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>