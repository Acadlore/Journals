<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-0HORy6Qm7LwuDdG23eTtgJdf2lXI8s9M</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml040405</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Multimodal Audio Violence Detection: Fusion of Acoustic Signals and Semantics</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <name>
            <surname>Nadar</surname>
            <given-names>Shivwani</given-names>
          </name>
          <email>nadarshivwani@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <name>
            <surname>Gandhi</surname>
            <given-names>Disha</given-names>
          </name>
          <email>its.dishagandhi@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9982-5218</contrib-id>
          <name>
            <surname>Jawale</surname>
            <given-names>Anupama</given-names>
          </name>
          <email>anupama.jawale26@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <name>
            <surname>Pawar</surname>
            <given-names>Shweta</given-names>
          </name>
          <email>shwetabpawar@nmcce.ac.in</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <name>
            <surname>Prabhu</surname>
            <given-names>Ruta</given-names>
          </name>
          <email>ruta.prabhu@nmcce.ac.in</email>
        </contrib>
        <aff id="aff_1">Department of Information Technology, Narsee Monjee College of Commerce and Economics, 400056 Mumbai, India</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>23</day>
        <month>12</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>4</issue>
      <fpage>301</fpage>
      <lpage>311</lpage>
      <page-range>301-311</page-range>
      <history>
        <date date-type="received">
          <day>22</day>
          <month>10</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>17</day>
          <month>12</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>When public safety is considered to be of paramount importance, the capacity to detect violent situations through audio monitoring has become increasingly indispensable. This paper proposed a hybrid audio text violence detection system that combines text-based information with frequency-based features to improve accuracy and reliability. The two core models of the system include a frequency-based model, Random Forest (RF) classifier, and a natural language processing (NLP) model called Bidirectional Encoder Representations from Transformers (BERT). RF classifier was trained on Mel-Frequency Cepstral Coefficients (MFCCs) and other spectrum features, whereas BERT identified violent content in transcribed speech. BERT model was improved through task-specific fine-tuning on a curated violence-related text dataset and balanced with class-weighting strategies to address category imbalance. This adaptation enhanced its ability to capture subtle violent language patterns beyond general purpose embeddings. Furthermore, a meta-learner ensemble model using eXtreme Gradient Boosting (XGBoost) classifier model could combine the probability output of the two base models. The ensemble strategy proposed in this research differed from conventionally multimodal fusion techniques, which depend on a single strategy, either NLP or audio. The XGBoost fusion model possessed the qualities derived from both base models to improve classification accuracy and robustness by creating an ideal decision boundary. The proposed system was supported by a Graphical User Interface (GUI) for multiple purposes, such as smart city applications, emergency response, and security monitoring with real-time analysis. The proposed XGBoost ensemble model attained an overall accuracy of over 97.37%, demonstrating the efficacy of integrating machine learning-based decision.</p></abstract>
      <kwd-group>
        <kwd>Audio</kwd>
        <kwd>BERT</kwd>
        <kwd>Graphical User Interface</kwd>
        <kwd>MFCC</kwd>
        <kwd>Stacking ensemble</kwd>
        <kwd>Violence</kwd>
        <kwd>XGBoost</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="5"/>
        <fig-count count="4"/>
        <table-count count="5"/>
        <ref-count count="27"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Traditional surveillance systems mostly employ visual data to monitor security concerns. Owing to issues like inadequate lighting, occlusions, or limited camera angles, visual-only methods are frequently less effective [<xref ref-type="bibr" rid="ref_1">1</xref>]. On the other hand, audio-based analysis could offer a different viewpoint or crucial insights into violent events [<xref ref-type="bibr" rid="ref_2">2</xref>] in situations where visual monitoring is impractical or challenging. This study proposed a hybrid audio violence detection system combining acoustic and linguistic features for improved classification accuracy and robustness in order to get around challenges arising from background noise, different speech patterns, and changing acoustic environments [<xref ref-type="bibr" rid="ref_3">3</xref>]. Unlike traditional fusion systems that rely on simple probability averaging, the XGBoost-based fusion model learns how to intelligently weigh and combine predictions from both models, hence capturing complex decision patterns to improve accuracy, recall, and precision [<xref ref-type="bibr" rid="ref_4">4</xref>]. The final model becomes more adaptable and reliable in a range of real-world scenarios by minimizing the errors of the individual base models [<xref ref-type="bibr" rid="ref_5">5</xref>]. Traditional probability-averaging methods often suffer from inflated false positive rates; for instance, prior studies have shown that non-violent but emotionally intense audio (e.g., cheering or loud music) was frequently misclassified as violent due to over-reliance on acoustic probability spikes. These shortcomings motivate the use of a stacked fusion strategy, where the meta-learner could reduce such errors by learning context-specific weighting. To address the need of social safety and other real-world requirements, an user interface was developed and described in the methodology section. The stacking ensemble learning process that the system employed for violence detection consists of the following (<xref ref-type="table" rid="table_1">Table 1</xref>):</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Levels of ensemble learning process</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Model Level</p></td><td colspan="1" rowspan="1"><p>Description</p></td></tr><tr><td colspan="1" rowspan="2"><p>Base Model at Level 0</p></td><td colspan="1" rowspan="1"><p>A frequency-based model was trained using RF on MFCCs, Chroma Features, Mel-Spectrograms, and Spectral Contrast in order to capture the audio patterns of violent content.</p></td></tr><tr><td colspan="1" rowspan="1"><p>A text-based model that analyzes speech transcriptions and employs a revised BERT classifier to detect violent language cues [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>].</p></td></tr><tr><td colspan="1" rowspan="1"><p>Meta Learner Model at Level 1</p></td><td colspan="1" rowspan="1"><p>An XGBoost classifier is trained on the probability output from both base models (audio_prob and text_prob) in order to learn an optimal decision boundary, which enhances overall classification performance [<xref ref-type="bibr" rid="ref_8">8</xref>].</p></td></tr></tbody></table>
        </table-wrap>
      
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p>Advances in artificial intelligence and deep learning have sped up research on audio-based violence detection. Earlier methods combined manually generated audio features like MFCCs and Spectral Contrast with machine learning models like Support Vector Machines (SVMs) and RF classifiers. Conversely, more recent approaches employ deep learning models, which directly learn feature representations from raw audio data, significantly improving accuracy and generalization.</p><p>Classical models exhibit distinct trade-offs: SVMs achieve strong margin-based classification but scale poorly with large datasets; Extreme Learning Machines (ELMs) offer faster training times but often underfit complex acoustic patterns; RFs provide robustness to noisy data and strong generalization but require careful hyperparameter tuning. These limitations partly explain the shift toward ensemble and deep learning methods. Some approaches mentioned above are illustrated as follows:</p><p>1. Machine Learning-Based Approaches: Several researchers have looked into different machine learning techniques for audio-based violence detection. Durães et al. [<xref ref-type="bibr" rid="ref_3">3</xref>] demonstrated that deep learning outperformed conventional techniques when trained on a range of datasets in the assessment of deep learning models and data augmentation techniques. Mahalle and Rojatkar [<xref ref-type="bibr" rid="ref_9">9</xref>] further investigated the efficacy of ELM for audio-based violence categorization, emphasizing the faster computation time of the models compared to more traditional models.</p><p>Further studies have examined lightweight deep neural networks designed for real-world applications and demonstrated that they could increase the accuracy and efficiency of violence detection [<xref ref-type="bibr" rid="ref_10">10</xref>]. Multimodal approaches integrating audio with extra contextual information have been proposed to improve recognition performance in low-resource settings [<xref ref-type="bibr" rid="ref_11">11</xref>]. In the research paper, a cross-lingual model that used few-shot learning techniques was developed to detect violent speech in many languages in order to increase the effectiveness of the method in a range of linguistic settings [<xref ref-type="bibr" rid="ref_12">12</xref>].</p><p>2. Deep Learning and Multimodal Fusion: Deep learning has revolutionized the field of violence detection by enabling models to process both textual and auditory data simultaneously. Certain systems have been introduced to detect violent interactions by combining linguistic and speech features. Their results demonstrated that categorization accuracy could be increased by combining data from multiple sources. Similarly, multimodal fusion techniques that used meta-information and deep neural networks have been explored to enhance aggression detection in surveillance applications [<xref ref-type="bibr" rid="ref_13">13</xref>].</p><p>As multimodal techniques gain popularity, researchers are looking into how audio may be integrated with written or visual data. Weakly supervised learning methods that used hyperbolic space representations have been developed to improve aggression classification. Other studies have expanded on this concept by introducing a multimodal attention-enhanced feature fusion framework, combining multiple modalities to significantly boost model performance in difficult real-world scenarios [<xref ref-type="bibr" rid="ref_14">14</xref>].</p><p>3. NLP for Violence Detection: Beyond acoustic analysis, NLP has evolved into a powerful technique for identifying aggression and violence in speech. Conversational agents are classic examples of advances in NLP [<xref ref-type="bibr" rid="ref_15">15</xref>]. By employing BERT models to detect gender-based violence on social media, research has demonstrated how transformer-based language models may effectively detect harmful information [<xref ref-type="bibr" rid="ref_16">16</xref>], [<xref ref-type="bibr" rid="ref_17">17</xref>]. Building on this, further investigations have proposed a BERT-fasttext model that is more capable of detecting violent language than traditional NLP-based classifiers. Multimodal NLP-based systems combining lightweight neural architecture with specialized audio processing techniques have been introduced, thus facilitating real-time processing for security applications on edge devices. By exploring deeper learning techniques for detecting cases of domestic violence through audio monitoring, additional contributions have highlighted the usefulness of audio surveillance [<xref ref-type="bibr" rid="ref_18">18</xref>]. Other researchers contributed to this subject by showing how few-shot learning could improve NLP models in cross-lingual contexts, in order to boost their capacity to identify violent speech in multiple languages. Although audio-based violence detection has advanced significantly, certain challenges remain. One major obstacle that results in false positives is the inability to distinguish between actual violent episodes and intense but non-violent situations, such as cheering or loud music. Moreover, it is challenging to apply deep learning models in real-time applications due to computational constraints, thus necessitating optimization techniques to boost efficacy.</p><p>The techniques and datasets employed in previous works are summarized in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>Highlights of previous works related to the detection of violent speech</title>
          </caption>
          <table><tbody><tr><th colspan="1" rowspan="1"><p>Reference</p></th><th colspan="1" rowspan="1" colwidth="512"><p>Methods</p></th><th colspan="1" rowspan="1"><p>Dataset</p></th><th colspan="1" rowspan="1"><p>Results</p></th></tr><tr><td colspan="1" rowspan="1"><p>[<xref ref-type="bibr" rid="ref_19">19</xref>]</p></td><td colspan="1" rowspan="1" colwidth="512"><p>BERT-Based model and Large Language Model (LLM)</p></td><td colspan="1" rowspan="1"><p>Data from <mml:math id="mfya5d8g78">
  <mml:mo>∼</mml:mo>
</mml:math> 420 k Twitter posts spanning a 3-year duration (January 1, 2020 to February 1, 2023)</p></td><td colspan="1" rowspan="1"><p>F1 score of 0.69. In comparison, the detection of antiAsian hateful speech showed a higher effectiveness, with an F1 score of 0.89</p></td></tr><tr><td colspan="1" rowspan="1"><p>[<xref ref-type="bibr" rid="ref_20">20</xref>]</p></td><td colspan="1" rowspan="1" colwidth="512"><p>BERT, XGBoost, and RF</p></td><td colspan="1" rowspan="1"><p>Students’ Violent Speech (SVS) dataset with 7056 tagged tweets</p></td><td colspan="1" rowspan="1"><p>90% accuracy with BERT</p></td></tr><tr><td colspan="1" rowspan="1"><p>[<xref ref-type="bibr" rid="ref_21">21</xref>]</p></td><td colspan="1" rowspan="1" colwidth="512"><p>TensorFlow custom object detection and speech analysis</p></td><td colspan="1" rowspan="1"><p>Custom datasets for 6-image categories. Custom datasets for 2-speech categories.</p></td><td colspan="1" rowspan="1"><p>84% accuracy of the developed system</p></td></tr><tr><td colspan="1" rowspan="1"><p>[<xref ref-type="bibr" rid="ref_22">22</xref>]</p></td><td colspan="1" rowspan="1" colwidth="512"><p>MFCC and Stationary Wavelet Transform (SWT)-Based feature extraction. K-Nearest Neighbors (KNN) and Convolutional Neural Networks (CNNs) for classification.</p></td><td colspan="1" rowspan="1"><p>Vera Am Mittag (VAM) corpus German TV talk show recordings</p></td><td colspan="1" rowspan="1"><p>CNNs with feature selection achieved 97.21% classification accuracy</p></td></tr><tr><td colspan="1" rowspan="1"><p>[<xref ref-type="bibr" rid="ref_2">2</xref>]</p></td><td colspan="1" rowspan="1" colwidth="512"><p>BERT “Efficiently Learning an Encoder that Classifies Token Replacements Accurately” (ELECTRA) models for NLP</p></td><td colspan="1" rowspan="1"><p>Pre-training dataset collected by crawling 110 Bangla websites (27.5GB, 5.25 million documents). Evaluation dataset for the Violence Inciting Text Detection shared task (texts in the Bangla language).</p></td><td colspan="1" rowspan="1"><p>F1 score of 0.737</p></td></tr><tr><td colspan="1" rowspan="1"><p>[<xref ref-type="bibr" rid="ref_23">23</xref>]</p></td><td colspan="1" rowspan="1" colwidth="512"><p>Naive Bayes, SVM, Ensemble classifiers, and Artificial Neural Net</p></td><td colspan="1" rowspan="1"><p>Waseem and Hoyy (2016) English dataset; Fortuna (2017) Portuguese dataset</p></td><td colspan="1" rowspan="1"><p>Improvement of F1 score by 0.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>[<xref ref-type="bibr" rid="ref_10">10</xref>]</p></td><td colspan="1" rowspan="1" colwidth="512"><p>Mel-Spectrogram images of speech signals</p></td><td colspan="1" rowspan="1"><p>Image and acoustic data</p></td><td colspan="1" rowspan="1"><p>Improvement in F1 by 8%</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>As highlighted in the above table, there is an accelerating demand for robust models that could operate in multilingual and low-resource environments. The models discussed so far focus on standard news and tweets datasets or textual datasets. A natural spoken language has seldom been assessed in all these models. By emphasizing the benefits of few-shot learning for cross-lingual adaptability, the current research created new avenues for improving global violence detection systems. </p><p>In summary, the subject of audio-based violence detection has seen significant development, progressing from traditional machine learning models to advanced deep learning techniques that make use of multimodal data, NLP, and cross-lingual adaptation. Ongoing research into self-supervised learning, few-shot learning, and multimodal fusion will further enhance the accuracy and practicality of violence detection systems, thus boosting their effectiveness in a range of demanding scenarios.</p>
    </sec>
    <sec sec-type="">
      <title>3. Methodology</title>
      <p>This section explains the process of developing a powerful audio violence detection system that integrates a fusion mechanism to enhance the performance of linguistic and acoustic models. </p><p> <xref ref-type="fig" rid="fig_1">Figure 1</xref> exhibits how an audio input is processed in two parallel branches, one analyzing acoustic features with a RF classifier, and the other converting the audio to text for BERT-based classification. Both branches produce separate Violent/Non-Violent predictions, which are then combined in a decision fusion block. If the fusion score meets the threshold, the final outcome is classified as Violent or it will be Non-Violent. The result, along with intermediate probabilities, transcribed text, and confidence visualization (via a real‑time GUI built using Gradio), is displayed to the user. </p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Flowchart of methodology combined the two base models</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/11/img_L27UseYnHiKWwX_B.png"/>
        </fig>
      
      <p>The dataset used in this study was UBC-NLP/DetoxLLM-7B dataset [<xref ref-type="bibr" rid="ref_24">24</xref>], the latest dataset with a unique feature of a paraphrase detector. Another dataset where audio files were trained were extracted from various YouTube videos of web series and movies. Ground truth for these recorded videos was noted in a form of binary variable, violent or non-violent tone. A list of features extracted from these audio files is set out below.</p><p>1. MFCCs: 13 coefficients representing timbre in speech and music;</p><p>2. Chroma: 12 values (one for each note in the musical octave);</p><p>3. Mel: 128 Mel-Frequency bands, representing energy distribution over perceptual frequency scale; and</p><p>4. Spectral Contrast: 7 values describing the difference between peaks and valleys in the spectrum (captures brightness/timbre).</p><p>The three primary components of the system are: (A) Frequency-Based Model with optimized RF; (B) NLP-Based Model improved with BERT; and (C) Fusion Model for final prediction.</p><p>A. An Optimized RF Model</p><p>The frequency-based methodology focuses on extracting significant auditory features to identify violent patterns in audio. This approach leverages MFCCs, Spectral Contrast, Mel-Spectrogram, and Chroma as primary components for feature extraction. To ensure uniform analysis across all audio samples, a windowing technique was applied, where all input audio files were split into 10-second clips. In cases where the audio duration was less than 10 seconds, padding was added at both the beginning and end to maintain consistency. This preprocessing step ensured that all audio inputs were of a fixed length, which allowed the model to extract meaningful frequency-based features while minimizing bias introduced by varying clip durations. The extracted features were then standardized using scaling techniques before being fed into an optimized RF Model for classification. This model was fine-tuned with hyperparameter optimization, balancing between model complexity and generalization to ensure robust performance in violence detection.</p><p>(a) Extraction of Features: Since MFCCs record the short-term power spectrum of audio sources, they are helpful for tasks involving speech. The twelve different pitch classes that help identify the tonal properties of violent noises are displayed in Chroma Features. The Mel-Spectrogram provides a time-frequency representation of sound and displays the energy distribution across frequencies. In the present setup, 40 MFCCs were extracted per frame, thus yielding a feature matrix of 40 <inline-formula>
  <mml:math id="m9m2m8qsvm">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> $T<inline-formula>
  <mml:math id="m7ec8bfqfm">
    <mml:mo>(</mml:mo>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>T$ denotes the number of time frames). The Mel-Spectrogram was computed with 128 frequency bins, while Chroma features used 12 pitch classes and Spectral Contrast captured 7 sub-band contrasts per frame. By calculating the difference between the spectrum’s peaks and valleys, spectral contrast aids in the detection of dynamic sound variation. The MFCC calculation formula is presented in Eq. (1):</p>
      
        <disp-formula>
          <label>(1)</label>
          <mml:math id="musva76j37">
            <mml:mrow>
              <mml:mi data-mjx-auto-op="false">MFCC</mml:mi>
            </mml:mrow>
            <mml:mo>(</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:mo>⁡</mml:mo>
            <mml:mo>(</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:mo>⋅</mml:mo>
            <mml:mo>⁡</mml:mo>
            <mml:mo>(</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:mi>n</mml:mi>
            <mml:mi>log</mml:mi>
            <mml:mi>cos</mml:mi>
            <mml:munderover>
              <mml:mo>∑</mml:mo>
              <mml:mrow>
                <mml:mi>k</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mn>1</mml:mn>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>K</mml:mi>
              </mml:mrow>
            </mml:munderover>
            <mml:msub>
              <mml:mi>S</mml:mi>
              <mml:mrow>
                <mml:mi>k</mml:mi>
              </mml:mrow>
            </mml:msub>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>π</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mi>k</mml:mi>
                <mml:mo>(</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mn>0.5</mml:mn>
              </mml:mrow>
              <mml:mi>K</mml:mi>
            </mml:mfrac>
          </mml:math>
        </disp-formula>
      
      <p> where, $K<inline-formula>
  <mml:math id="m8r1ev5uxl">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>n<inline-formula>
  <mml:math id="miid64jatq">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>S_{\mathrm{k}}<inline-formula>
  <mml:math id="mboi01k14j">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>k$. <xref ref-type="fig" rid="fig_2">Figure 2</xref> shows the distribution of MFCC frequencies of important features.</p>
      <p><xref ref-type="fig" rid="fig_2">Figure 2</xref> and <xref ref-type="fig" rid="fig_3">Figure 3</xref> illustrates the importance of several audio elements in the XGBoost Fusion Model. Mel_119 (21.1%) and mfcc_3 (13.8%) are the most important traits, followed by contrast_7 (12.0%) and mel_118 (12.4%). The Mel and Contrast features, which are crucial for distinguishing between violent and non-violent sounds, illustrate how the model uses frequency-based analysis. The chart clearly displays a relative ranking of importance, according to which mel_119 is the most influential feature whereas mel_36 is the least influential among the top 10.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Chart of MFCC frequencies of important features</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/11/img_AfW6hmZxX1jF77by.png"/>
        </fig>
      
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>Distribution chart of feature importance</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/11/img_ZjNGHOA48q30E5Ud.png"/>
        </fig>
      
      <p>(b) Architecture of the Model: The Grid Search CV explored n_estimators <inline-formula>
  <mml:math id="mpclr9b3bi">
    <mml:mo>∈</mml:mo>
    <mml:mo fence="false">{</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo fence="false">}</mml:mo>
    <mml:mn>100</mml:mn>
    <mml:mn>200</mml:mn>
    <mml:mn>300</mml:mn>
  </mml:math>
</inline-formula>, max_depth <inline-formula>
  <mml:math id="m0wf6kfq0y">
    <mml:mo>∈</mml:mo>
    <mml:mo fence="false">{</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo fence="false">}</mml:mo>
    <mml:mn>10</mml:mn>
    <mml:mn>15</mml:mn>
    <mml:mn>20</mml:mn>
  </mml:math>
</inline-formula>, and min_samples_split <inline-formula>
  <mml:math id="mc7mozyuik">
    <mml:mo>∈</mml:mo>
    <mml:mo fence="false">{</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo fence="false">}</mml:mo>
    <mml:mn>2</mml:mn>
    <mml:mn>5</mml:mn>
    <mml:mn>10</mml:mn>
  </mml:math>
</inline-formula>. The optimization objective was to maximize the F1-score on the validation set, in order to ensure a balance between precision and recall rather than raw accuracy [<xref ref-type="bibr" rid="ref_9">9</xref>].</p><p>B. Fine-Tuned BERT, an NLP-Based Model</p><p>To detect violent content, the NLP-based approach employs speech-to-text transcription and an improved BERT model for text categorization.</p><p>(a) Audio Transcription: To convert spoken words into text, audio recordings are transcribed using Google’s Speech Recognition Application Programming Interface (API). This process enables the extraction of linguistic features for further analysis.</p><p>(b) Training with a dataset that has been associated with violence: BERT has been improved to distinguish between violent and non-violent words. The architecture of the model is: The input layer consists of tokenized text and attention masks. The contextual associations of the text are recorded by BERT encoder. The Classification Layer has softmax activation and is dense for binary classification. Softmax Activation Formula (used in the output layer of BERT) is shown in Eq. (2).</p>
      
        <disp-formula>
          <label>(2)</label>
          <mml:math id="m203ngspga">
            <mml:mi>P</mml:mi>
            <mml:mi>y</mml:mi>
            <mml:mi>i</mml:mi>
            <mml:mi>x</mml:mi>
            <mml:mo>(</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:mo>∣</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:msup>
                <mml:mi>e</mml:mi>
                <mml:mrow>
                  <mml:msub>
                    <mml:mi>z</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:msup>
              <mml:mrow>
                <mml:munderover>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>j</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>n</mml:mi>
                </mml:munderover>
                <mml:msup>
                  <mml:mi>e</mml:mi>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>z</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:msup>
              </mml:mrow>
            </mml:mfrac>
          </mml:math>
        </disp-formula>
      
      <p> where, <inline-formula>
  <mml:math id="mh31a1happ">
    <mml:msub>
      <mml:mi>z</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> represents the output logit for class <inline-formula>
  <mml:math id="mt9duoycx6">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula> is the total number of classes (2 in this case: violent or non-violent).</p><p>C. Final Prediction Using a Fusion Model</p><p>To improve the accuracy of violence detection in an ensemble learning fashion, predictions from the frequency-based RF Model and the NLP-Based Fine-Tuned BERT model are coupled using a meta-classifier [<xref ref-type="bibr" rid="ref_13">13</xref>]. </p><p>Stacking-Based Fusion Mechanism: Instead of averaging probability scores, the Fusion Mechanism Based on Stacking XGBoost is employed as a meta-classifier to determine the final classification. After being trained on the probability output from the RF and BERT fine-tuned models, this model determines the optimal decision boundary based on both predictions. Eq. (3) shows integration of the strengths of both models, resulting in enhanced accuracy, reduced false positives, and improved adaptability in real-world scenarios.</p>
      
        <disp-formula>
          <label>(3)</label>
          <mml:math id="mb192962j1">
            <mml:mtable displaystyle="true" columnspacing="1em" rowspacing="3pt">
              <mml:mtr>
                <mml:mtd>
                  <mml:msub>
                    <mml:mi>P</mml:mi>
                    <mml:mrow>
                      <mml:mi>a</mml:mi>
                      <mml:mi>u</mml:mi>
                      <mml:mi>d</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>o</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>=</mml:mo>
                  <mml:mtext> Probability of violence from the frequency-based model. </mml:mtext>
                </mml:mtd>
              </mml:mtr>
              <mml:mtr>
                <mml:mtd>
                  <mml:msub>
                    <mml:mi>P</mml:mi>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>x</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>=</mml:mo>
                  <mml:mtext> Probability of violence from the BERT fine tuned model. </mml:mtext>
                </mml:mtd>
              </mml:mtr>
              <mml:mtr>
                <mml:mtd>
                  <mml:mtext> A feature set is created using these probabilities: </mml:mtext>
                </mml:mtd>
              </mml:mtr>
              <mml:mtr>
                <mml:mtd>
                  <mml:msub>
                    <mml:mi>P</mml:mi>
                    <mml:mrow>
                      <mml:mi>f</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>n</mml:mi>
                      <mml:mi>a</mml:mi>
                      <mml:mi>l</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>P</mml:mi>
                    <mml:mrow>
                      <mml:mi>a</mml:mi>
                      <mml:mi>u</mml:mi>
                      <mml:mi>d</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>o</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>P</mml:mi>
                    <mml:mrow>
                      <mml:mi>t</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>x</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mo>=</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>+</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mtext> XGBoost </mml:mtext>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
      
      <p>Each component of this methodology, ranging from feature extraction with Librosa and the optimized RF classifier to the fine-tuning of BERT for NLP tasks and the XGBoost-based fusion mechanism, is designed to ensure robust and accurate detection of violent audio events, even in challenging environments.</p>
    </sec>
    <sec sec-type="">
      <title>4. Implementation</title>
      <p>The entire workflow was implemented and assessed on Google Colab; which provided high performance computing resources for model training and evaluation. </p><p>The four primary components for implementing the audio violence detection system are the Frequency-Based Model (Acoustic Analysis), NLP-Based Model (Text-Based Violence Detection), Meta-Learner Model (Ensemble Learning), and a GUI for real-time analysis. The developed system uses Python Libraries, such as Gradio for GUI development, XGBoost for ensemble learning, Transformers (Hugging Face) for NLP-Based Classification, Scikit-Learn for traditional machine learning, and Librosa for audio feature extraction. BERT-Base-Uncased model using the Hugging Face Transformers was used and finetuned for violent text recognition. The Frequency-Based Model distinguishes between violent and non-violent noises by analyzing unprocessed audio signals. Critical acoustic features, such as MFCCs, Chroma Features, Mel-Spectrogram, and Spectral Contrast, which aid in differentiating violent sounds (such as yelling and shouting) from non-violent ones, were extracted using the Librosa package. An optimized RF classifier, chosen for its resilience in processing noisy and high-dimensional audio data, received the standardized extracted features. Using Grid Search CV, hyperparameter tuning was carried out to optimize parameters like minimum samples per leaf (2), maximum depth (20), and number of estimators (100).</p><p>With a batch size of eight, ten training epochs, and a learning rate of 2 <inline-formula>
  <mml:math id="m9qjdrh7z8">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 10<inline-formula>
  <mml:math id="mh29frujn0">
    <mml:msup>
      <mml:mi/>
      <mml:mrow>
        <mml:mo>−</mml:mo>
        <mml:mn>5</mml:mn>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula>, the NLP-Based Model fared well in tests of violence classification. WordPiece Tokenization and pre-trained contextual embeddings were utilized to categorize violent and non-violent text segments extracted from transcribed speech using the improved BERT-Based model. The unequal distribution of classes was addressed during training by using class weighting techniques, which ensured that the model learned both violent and non-violent behaviors. Class weights were computed inversely proportional to class frequencies. Eq. (4) shows computation of class weights of BERT model:</p>
      
        <disp-formula>
          <label>(4)</label>
          <mml:math id="m65tpgsw15">
            <mml:msub>
              <mml:mi>W</mml:mi>
              <mml:mi>i</mml:mi>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mi>N</mml:mi>
              <mml:mrow>
                <mml:mi>C</mml:mi>
                <mml:mi>X</mml:mi>
                <mml:msub>
                  <mml:mi>n</mml:mi>
                  <mml:mi>i</mml:mi>
                </mml:msub>
              </mml:mrow>
            </mml:mfrac>
          </mml:math>
        </disp-formula>
      
      <p> where, $N<inline-formula>
  <mml:math id="m3au6kb4kl">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>C<inline-formula>
  <mml:math id="mb4njc564b">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>n_i<inline-formula>
  <mml:math id="m96xxkfq03">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>i$. This ensured that minority classes (violent or non-violent) contributed equally to the loss function.</p><p>The Google Speech Recognition API was applied to transcribe spoken text, which was preprocessed to remove stop words, punctuation, and noise before classification. To further increase prediction reliability, temperature scaling which altered logit distributions before softmax activation was used. Through the calibration of the model’s confidence scores, this technique improves categorization stability. Early pausing was implemented using a three-epoch patience window, in order to ensure that training would immediately terminate when validation loss stopped improving. The model successfully mitigated overfitting risks by using dropout regularization inside the hidden layers of BERT and self-attention processes, resulting in strong generalization performance across validation and test datasets.</p><p>A meta-learner (XGBoost classifier) was implemented; this was a powerful gradient boosting algorithm known for handling complex feature interactions and improving generalization [<xref ref-type="bibr" rid="ref_13">13</xref>]. The meta-learner training process involved feeding the probability scores from both base models into the XGBoost classifier, which learned the optimal weighting and decision boundaries between the two models. The ensemble model was fine-tuned using Bayesian Optimization, adjusting essential parameters like the learning rate (0.05), max depth (6), and number of boosting rounds (100).</p>
      
        <disp-formula>
          <label>(5)</label>
          <mml:math id="my370tmdvi">
            <mml:msub>
              <mml:mi>P</mml:mi>
              <mml:mrow>
                <mml:mi>f</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>l</mml:mi>
              </mml:mrow>
            </mml:msub>
            <mml:msub>
              <mml:mi>P</mml:mi>
              <mml:mrow>
                <mml:mi>a</mml:mi>
                <mml:mi>u</mml:mi>
                <mml:mi>d</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>o</mml:mi>
              </mml:mrow>
            </mml:msub>
            <mml:msub>
              <mml:mi>P</mml:mi>
              <mml:mrow>
                <mml:mi>t</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mi>t</mml:mi>
              </mml:mrow>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:mo>(</mml:mo>
            <mml:mo>+</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:mrow>
              <mml:mi data-mjx-auto-op="false">XGBoost</mml:mi>
            </mml:mrow>
          </mml:math>
        </disp-formula>
      
      <p>In Eq. (5), <inline-formula>
  <mml:math id="mrf560vxgj">
    <mml:msub>
      <mml:mi>P</mml:mi>
      <mml:mrow>
        <mml:mi>a</mml:mi>
        <mml:mi>u</mml:mi>
        <mml:mi>d</mml:mi>
        <mml:mi>i</mml:mi>
        <mml:mi>o</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mmh56a9quw">
    <mml:msub>
      <mml:mi>P</mml:mi>
      <mml:mrow>
        <mml:mi>t</mml:mi>
        <mml:mi>e</mml:mi>
        <mml:mi>x</mml:mi>
        <mml:mi>t</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> are the probability scores from RF and BERT models are represented by <inline-formula>
  <mml:math id="mhuxp84w62">
    <mml:msub>
      <mml:mi>P</mml:mi>
      <mml:mrow>
        <mml:mi>a</mml:mi>
        <mml:mi>u</mml:mi>
        <mml:mi>d</mml:mi>
        <mml:mi>i</mml:mi>
        <mml:mi>o</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="m39rgbuyf9">
    <mml:msub>
      <mml:mi>P</mml:mi>
      <mml:mrow>
        <mml:mi>t</mml:mi>
        <mml:mi>e</mml:mi>
        <mml:mi>x</mml:mi>
        <mml:mi>t</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula>, respectively. Experimental data showed that the XGBoost metaLearner outperformed both standalone models, thus achieving great precision, accuracy, and recall.</p><p>The system was evaluated using a diverse dataset that included both violent and non-violent audio samples. The dataset included a range of auditory environments to assess their robustness in real-world situations. An Indian movie was considered to capture the violent tones of characters. All things considered, the hybrid ensemble learning system integrated text-based and acoustic violence detection, as well as employing a revised RF classifier, a BERT-based NLP model, an improved feature extraction pipeline, and an XGBoost meta-learner to increase classification accuracy. The user-friendly GUI ensures that applications for public safety, emergency response, and surveillance are feasible. The scalability, high accuracy, and utility of this implementation contribute to the practicality of automated violence detection in real-time settings.</p>
    </sec>
    <sec sec-type="">
      <title>5. Results and discussion</title>
      <p>With the implementation of GUI, users can upload or record audio, which would subsequently be transcribed and processed in real-time to detect potential violence. It shows the classification results of the frequency-based and text-based models as well as probability scores and a confidence bar plot to facilitate understanding in a few seconds. The implemented GUI provides a user-friendly interface for the real-time audio violence detection system, in which audios could be uploaded or recorded with a real-time display of results. The processing time of a single audio file is reported to be 20-20 seconds. The GUI, shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>, showcases transcribed text, predictions of individual sections along with their confidence scores from the audio-based RF Model and BERT-Based Model.</p>
      
        <fig id="fig_4">
          <label>Figure 4</label>
          <caption>
            <title>GUI of the multimodel system</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/11/img_3C345z-obcDTT-8G.png"/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/11/img_De7XPYXnVovLfL74.png"/>
        </fig>
      
      <p>The GUI was tested on a CPU-only setup (Intel i7, 16GB RAM), achieving average inference latency of <inline-formula>
  <mml:math id="maosrwgf8i">
    <mml:mo>∼</mml:mo>
  </mml:math>
</inline-formula>1.2 seconds per 10-second clip. GPU acceleration (NVIDIA T4 on Google Colab) reduced latency to <inline-formula>
  <mml:math id="mla1n3ym6j">
    <mml:mo>∼</mml:mo>
  </mml:math>
</inline-formula>0.4 seconds. The lightweight design also allows deployment on edge devices such as Jetson Nano, with real-time processing supported under optimized batch settings.</p><p>The proposed Audio-Text Violence Detection System was evaluated using multiple models, including RF for audio features, BERT for text analysis, and an Ensemble Model (XGBoost) for fusion-based classification. The following subsections display a detailed performance analysis of these models.</p>
      
        <sec>
          
            <title>5.1. Rf model (audio-based classification)</title>
          
          <p>The RF classifier was trained on MFCCs, Chroma Features, Mel-Spectrograms, and Spectral Contrast to identify violent events based on their acoustic characteristics [<xref ref-type="bibr" rid="ref_3">3</xref>]. <xref ref-type="table" rid="table_3">Table 3</xref> represents the RF Model which achieved a test accuracy of 9836%, (averaging classwise accuracy from <xref ref-type="table" rid="table_3">Table 3</xref>) demonstrating its ability to classify violent and non-violent audio. However, text-based classification by BERT performed better, thus highlighting the importance of linguistic cues in violence detection. The best performance was achieved by the ensemble model, which combined both textual and acoustic features.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Classification report—RF model (MFCC alone)</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Class</p></th><th colspan="1" rowspan="1"><p>Precision (%)</p></th><th colspan="1" rowspan="1"><p>Recall (%)</p></th><th colspan="1" rowspan="1"><p>F1-Score (%)</p></th><th colspan="1" rowspan="1"><p>Support</p></th></tr><tr><td colspan="1" rowspan="1"><p>Non-violent (0.0)</p></td><td colspan="1" rowspan="1"><p>98.0</p></td><td colspan="1" rowspan="1"><p>98.0</p></td><td colspan="1" rowspan="1"><p>98.0</p></td><td colspan="1" rowspan="1"><p>341</p></td></tr><tr><td colspan="1" rowspan="1"><p>Violent (1.0)</p></td><td colspan="1" rowspan="1"><p>99.0</p></td><td colspan="1" rowspan="1"><p>99.0</p></td><td colspan="1" rowspan="1"><p>99.0</p></td><td colspan="1" rowspan="1"><p>515</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>5.2. Bert model (text-based classification)</title>
          
          <p>The BERT model was trained to analyze the textual transcriptions of audio clips; as represented in <xref ref-type="table" rid="table_4">Table 4</xref>, patterns of violent speech were effectively identified. The BERT model achieved Train Accuracy: 96.86%, Validation Accuracy: 88.19% and Test Accuracy: 90.37%. The performance of BERT indicates that text-based violence classification is highly effective in identifying harmful language.</p>
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Classification report—BERT model (text-based alone)</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Epoch</p></th><th colspan="1" rowspan="1"><p>Training Loss</p></th><th colspan="1" rowspan="1"><p>Validation Loss</p></th><th colspan="1" rowspan="1"><p>Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Precision (%)</p></th><th colspan="1" rowspan="1"><p>Recall (%)</p></th><th colspan="1" rowspan="1"><p>F1-Score (%)</p></th></tr><tr><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>0.1239</p></td><td colspan="1" rowspan="1"><p>0.3783</p></td><td colspan="1" rowspan="1"><p>88.19</p></td><td colspan="1" rowspan="1"><p>89.63</p></td><td colspan="1" rowspan="1"><p>95.26</p></td><td colspan="1" rowspan="1"><p>92.35</p></td></tr><tr><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>0.1818</p></td><td colspan="1" rowspan="1"><p>0.5030</p></td><td colspan="1" rowspan="1"><p>87.80</p></td><td colspan="1" rowspan="1"><p>87.40</p></td><td colspan="1" rowspan="1"><p>97.8</p></td><td colspan="1" rowspan="1"><p>92.30</p></td></tr><tr><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p>0.0232</p></td><td colspan="1" rowspan="1"><p>0.4752</p></td><td colspan="1" rowspan="1"><p>89.02</p></td><td colspan="1" rowspan="1"><p>91.46</p></td><td colspan="1" rowspan="1"><p>94.10</p></td><td colspan="1" rowspan="1"><p>92.76</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>0.0894</p></td><td colspan="1" rowspan="1"><p>0.0534</p></td><td colspan="1" rowspan="1"><p>88.73</p></td><td colspan="1" rowspan="1"><p>91.65</p></td><td colspan="1" rowspan="1"><p>93.45</p></td><td colspan="1" rowspan="1"><p>92.54</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>5.3. Fusion model (ensemble approach)</title>
          
          <p>To improve accuracy and robustness, this system integrates both linguistic (BERT) and acoustic (RF) features using XGBoost meta-learner [<xref ref-type="bibr" rid="ref_13">13</xref>]. The dataset shows class imbalance (Violent: 315,695 versus Non-violent: 116,622). To mitigate bias toward the dominant class, oversampling of minority instances and class-weight adjustments were applied during training to stabilize F1-score across both categories.</p><p>The model was trained and tested using an optimized dataset of violent and non-violent audio clips. The results below indicate that the ensemble learning approach significantly enhances classification performance as in <xref ref-type="table" rid="table_5">Table 5</xref>.</p>
          
            <table-wrap id="table_5">
              <label>Table 5</label>
              <caption>
                <title>Classification report—fusion model</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Class</p></th><th colspan="1" rowspan="1"><p>Precision (%)</p></th><th colspan="1" rowspan="1"><p>Recall (%)</p></th><th colspan="1" rowspan="1"><p>F1-Score (%)</p></th><th colspan="1" rowspan="1"><p>Support</p></th></tr><tr><td colspan="1" rowspan="1"><p>Non-violent</p></td><td colspan="1" rowspan="1"><p>97.0</p></td><td colspan="1" rowspan="1"><p>93.0</p></td><td colspan="1" rowspan="1"><p>95.0</p></td><td colspan="1" rowspan="1"><p>116,622</p></td></tr><tr><td colspan="1" rowspan="1"><p>Violent</p></td><td colspan="1" rowspan="1"><p>98.0</p></td><td colspan="1" rowspan="1"><p>99.0</p></td><td colspan="1" rowspan="1"><p>98.0</p></td><td colspan="1" rowspan="1"><p>315,695</p></td></tr><tr><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>97.37</p></td><td colspan="1" rowspan="1"><p>432,317</p></td></tr><tr><td colspan="1" rowspan="1"><p>Macro Average</p></td><td colspan="1" rowspan="1"><p>97.5</p></td><td colspan="1" rowspan="1"><p>96.0</p></td><td colspan="1" rowspan="1"><p>96.5</p></td><td colspan="1" rowspan="1"><p>-</p></td></tr><tr><td colspan="1" rowspan="1"><p>Weighted Average</p></td><td colspan="1" rowspan="1"><p>97.7</p></td><td colspan="1" rowspan="1"><p>97.6</p></td><td colspan="1" rowspan="1"><p>95.9</p></td><td colspan="1" rowspan="1"><p>-</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>The MFCC-Based model achieves the highest expected accuracy since violent speech often carries distinct pitch and tonal characteristics. However, text-based violence detection remains essential for practical deployment. The fusion model assembling outperformed single classifiers in distinguishing between violent and non-violent audio clips, achieved an overall accuracy of 97.37%, with both high precision and recall. This demonstrated that the integration of textual and frequency-based features improved classification robustness.</p>
        </sec>
      
    </sec>
    <sec sec-type="discussion">
      <title>6. Discussion</title>
      <p>The ensemble approach integrated in the Audio-Text Violence Detection System considerably improves the accuracy rate to 97.37% by combining the content analysis of speech with BERT and feature extraction using RF. This combination is beneficial because it allows greater scrutiny of possible violent incidents through the speech of perpetrators.</p><p>The BERT model shows strong performance in violent language classification, delivering a score of 90.37%. The primary shortcoming of BERT is its failure to efficiently recognize violence wrapped in non-verbal sounds or background noises [<xref ref-type="bibr" rid="ref_25">25</xref>]. However, the RF Model demonstrates competence in the analysis of acoustic features and obtains an accuracy rate of 98.36%. Its major problem comes from the inability to tell the difference between violence and non-violence sounds. This problem can be solved by the inclusion of features extracted from texts, thus necessitating an optimal approach to violence detection via both audio and textual analysis.</p><p>The findings in this study were in line with earlier research which suggested that multimodal fusion significantly enhanced categorization capacity. Future studies should investigate the merging of few-shot learning with self-supervised learning techniques, as suggested by Sankaran et al. [<xref ref-type="bibr" rid="ref_12">12</xref>], to improve adaptability in a variety of situations. In addition, real-time violence detection systems would be more beneficial in emergency response and security contexts, provided that models for edge deployment could be improved.</p>
    </sec>
    <sec sec-type="conclusions">
      <title>7. Conclusions</title>
      <p>This work introduced a novel ensemble-based audio violence detection technique to analyze both verbal and auditory data for increasing classification accuracy. By combining a RF classifier for frequency-based features and a refined BERT model for textual analysis, the fusion model could effectively capture the multi-modal nature of violent audio occurrences. An XGBoost meta-learner was added to enhance the decision-making process by balancing the contributions of both models to improve precision and recall. This ensemble learning approach outperformed standalone classifiers by addressing the shortcomings of traditional audio classification techniques and providing a more dependable solution for violence detection. Cross-validation and hyperparameter adjustment were adopted to improve the generalization and robustness of the model, in response to changes in speech patterns and background noise.</p><p>Further simplifying practical deployment, real-time GUI facilitates emergency response units, surveillance teams, and law enforcement organizations to engage with the proposed system. These upgrades enable the system to be used in practical safety applications, where it is essential to promptly and accurately identify violent situations.</p><p>Overall, this study advanced the field of automated violence detection by creating a scalable, highly accurate, and implementable method. To further improve performance, future research is recommended to investigate multilingual support by integrating variations of BERT models and using multilingual stopword support. Larger datasets and sophisticated deep learning architecture could also be explored in the future. Furthermore, multi-source sensor fusion and video-based violence detection could be added to the proposed system to greatly enhance its suitability for smart surveillance and monitoring public safety.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>Conceptualization, S.N. and R.P.; methodology, S.N.; software, D.G.; validation, A.J., S.P., and R.P.; formal analysis, A.J.; investigation, D.G.; resources, R.P.; data curation, D.G.; writing—original draft preparation, S.N.; writing—review and editing, S.N., A.J., and R.P.; visualization, S.P.; supervision, R.P.; project administration, R.P. All authors have read and agreed to the published version of the manuscript.</p>
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pham</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Lam</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Nguyen</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Schindler</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2407.03110</pub-id>
          <article-title>A toolchain for comprehensive audio/video analysis using deep learning based multimodal approach (A use case of riot or violent context detection)</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Page</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Mangalvedhekar</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Deshpande</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Chavan</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Sonawane</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2311.18778</pub-id>
          <article-title>Mavericks at BLP-2023 Task 1: Ensemble-based approach using language models for Violence Inciting Text Detection</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>72</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Durães</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Veloso</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Novais</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.9781/ijimai.2023.08.007</pub-id>
          <article-title>Violence detection in audio: Evaluating the effectiveness of deep learning models and data augmentation</article-title>
          <source>Int. J. Interact. Multimed. Artif. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2412.16455</pub-id>
          <article-title>Research on violent text detection system based on BERT-FastText model</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jain</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Phukan</surname>
              <given-names>O. C.</given-names>
            </name>
            <name>
              <surname>Buduru</surname>
              <given-names>A. B.</given-names>
            </name>
            <name>
              <surname>Sharma</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2406.06798</pub-id>
          <article-title>The reasonable effectiveness of speaker embeddings for violence detection</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Anwar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kanjo</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Anderez</surname>
              <given-names>D. O.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2206.11822</pub-id>
          <article-title>DeepSafety: Multi-level audio-text feature extraction and fusion approach for violence detection in conversations</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Peng</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wen</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>K. Yu</surname>
              <given-names>P. Yang</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2305.18797</pub-id>
          <article-title>Learning weakly supervised audio-visual violence detection in hyperbolic space</article-title>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="conf-paper">
          <page-range>204-208</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Soldevilla</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Flores</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICICSE52190.2021.9404127</pub-id>
          <article-title>Natural language processing through BERT for identifying gender-based violence messages on social media</article-title>
          <source>2021 IEEE International Conference on Information Communication and Software Engineering (ICICSE)</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>107-113</page-range>
          <issue>5</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mahalle</surname>
              <given-names>M. D.</given-names>
            </name>
            <name>
              <surname>Rojatkar</surname>
              <given-names>D. V.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.35940/ijrte.E5193.019521</pub-id>
          <article-title>Violence content detection based on audio using extreme learning machine</article-title>
          <source>Int. J. Recent Technol. Eng. IJRTE</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>222</volume>
          <page-range>244-251</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bakhshi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>García-Gómez</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Gil-Pita</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Chalup</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.procs.2023.08.162</pub-id>
          <article-title>Violence detection in real-life audio signals using lightweight deep neural networks</article-title>
          <source>Procedia Comput. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <page-range>2907-2925</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mohammed</surname>
              <given-names>A. L.</given-names>
            </name>
            <name>
              <surname>Swapnil</surname>
              <given-names>M. D.</given-names>
            </name>
            <name>
              <surname>Peris</surname>
              <given-names>I. H. N.</given-names>
            </name>
            <name>
              <surname>Nihal</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Matin</surname>
              <given-names>M. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/OJCOMS.2024.3520703</pub-id>
          <article-title>Multimodal deep learning for violence detection: VGGish and MobileViT integration with knowledge distillation on Jetson Nano</article-title>
          <source>IEEE Open J. Commun. Soc.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sankaran</surname>
              <given-names>A. N.</given-names>
            </name>
            <name>
              <surname>Farahbakhsh</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Crespi</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2412.01408</pub-id>
          <article-title>Towards cross-lingual audio abuse detection in low-resource settings with few-shot learning</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>211</volume>
          <page-range>118523</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jaafar</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Lachiri</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2022.118523</pub-id>
          <article-title>Multimodal fusion methods with deep neural networks and meta-information for aggression detection in surveillance</article-title>
          <source>Expert Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <page-range>129-140</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shin</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Miah</surname>
              <given-names>A. S. M.</given-names>
            </name>
            <name>
              <surname>Kaneko</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Hassan</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>H. S.</given-names>
            </name>
            <name>
              <surname>Jang</surname>
              <given-names>S. W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/OJCS.2024.3517154</pub-id>
          <article-title>Multimodal attention-enhanced feature fusion-based weakly supervised anomaly violence detection</article-title>
          <source>IEEE Open J. Comput. Soc.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>213</volume>
          <page-range>109638</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhu-Zhou</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Tejera-Berengué</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Gil-Pita</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Utrilla-Manso</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Rosa-Zurera</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.apacoust.2023.109638</pub-id>
          <article-title>Computationally constrained audio-based violence detection through transfer learning and data augmentation techniques</article-title>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="conf-paper">
          <page-range>64-71</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chhabra</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sangroya</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Anantaram</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Formalizing and verifying natural language system requirements using Petri Nets and context based reasoning</article-title>
          <source>MRC@IJCAI</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>150</volume>
          <page-range>729-738</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zuo</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Q.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.comcom.2019.11.053</pub-id>
          <article-title>Application of image retrieva based on convolutional neural networks and Hu invariant moment algorithm in computer telecommunications</article-title>
          <source>Comput. Commun.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bensakhria</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.13140/RG.2.2.36128.97280/1</pub-id>
          <article-title>Detecting domestic violence incidents using audio monitoring and deep learning techniques</article-title>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Verma</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Grover</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Mathew</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Kraemer</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>De Choudhury</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kumar</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2407.15227</pub-id>
          <article-title>A community-centric perspective for characterizing and detecting anti-Asian violence-provoking speech</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-8</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chnini</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Fredj</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>BenSaid</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Kacem</surname>
              <given-names>Y. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/AICCSA59173.2023.10479330</pub-id>
          <article-title>Violent speech detection in educational environments</article-title>
          <source>2023 20th ACS/IEEE International Conference on Computer Systems and Applications (AICCSA), Giza, Egypt</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-8</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kumari</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Memon</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Aslam</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Chowdhry</surname>
              <given-names>B. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/IMTIC58887.2023.10178618</pub-id>
          <article-title>An effective approach for violence detection using deep learning and natural language processing</article-title>
          <source>2023 7th International Multi-Topic ICT Conference (IMTIC), Jamshoro, Pakistan</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-6</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sekkate</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Chebbi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Adib</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Jebara</surname>
              <given-names>S. B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ISIVC61350.2024.10577928</pub-id>
          <article-title>A deep learning framework for offensive speech detection</article-title>
          <source>2024 IEEE 12th International Symposium on Signal, Image, Video and Communications (ISIVC)</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>610-619</page-range>
          <issue>5</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mehrotra</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Chaudhary</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Sharma</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.37896/YMER21.05/69</pub-id>
          <article-title>Violent speech detech in videos using natural language processing</article-title>
          <source>YMER Digit.</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <article-title>Dataset: Waseem Dataset</article-title>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Waseem</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Hovy</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.57702/QAFDBW7H</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="conf-paper">
          <page-range>94-104</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Fortuna</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Rocha Da Silva</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Soler-Company</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wanner</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Nunes</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18653/v1/W19-3510</pub-id>
          <article-title>A hierarchically-labeled Portuguese hate speech dataset</article-title>
          <source>Proceedings of the Third Workshop on Abusive Language Online</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="conf-paper">
          <page-range>19112-19139</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Khondaker</surname>
              <given-names>M. T. I.</given-names>
            </name>
            <name>
              <surname>Abdul-Mageed</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Lakshmanan</surname>
              <given-names>L. V. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18653/v1/2024.emnlp-main.1066</pub-id>
          <article-title>DetoxLLM: A framework for detoxification with explanations</article-title>
          <source>Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>1139-1147</page-range>
          <issue>3</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gautam</surname>
              <given-names>M. K.</given-names>
            </name>
            <name>
              <surname>Rajput</surname>
              <given-names>P. K.</given-names>
            </name>
            <name>
              <surname>Srivastava</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Kansal</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.22214/ijraset.2024.59027</pub-id>
          <article-title>Real time violence detection and alert system</article-title>
          <source>Int. J. Res. Appl. Sci. Eng. Technol.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>