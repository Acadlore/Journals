<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">HF</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Healthcraft Frontiers</journal-title>
        <abbrev-journal-title abbrev-type="issn">Healthcraft. Front.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">HF</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">3005-799X</issn>
      <issn publication-format="print">3005-7981</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-cyIvCiQ3vnsgd10yo3o2FayfpHutJcJd</article-id>
      <article-id pub-id-type="doi">10.56578/hf010102</article-id>
      <title-group>
        <article-title>Segmentation and Classification of Skin Cancer in Dermoscopy Images Using SAM-Based Deep Belief Networks</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Rahman</surname>
            <given-names>Syed Ziaur</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9007-6556</contrib-id>
          <email>syed.rahman@majancollege.edu.om</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="2">2</xref>
          <name>
            <surname>Singasani</surname>
            <given-names>Tejesh Reddy</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-6074-5584</contrib-id>
          <email>tsingasani19552@ucumberlands.edu</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="3">3</xref>
          <name>
            <surname>Shaik</surname>
            <given-names>Khaja Shareef</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1079-2665</contrib-id>
          <email>shareef.sk@mlrinstitutions.ac.in</email>
        </contrib>
        <aff id="1">Faculty of Information Technology, Majan University College, 710 Muscat, Oman</aff>
        <aff id="2">School of Computer and Information Sciences, University of Cumberlands, Louisville, 40769 KY, USA</aff>
        <aff id="3">Department of Information Technology, MLR Institute of Technology, 500043 Hyderabad, India</aff>
      </contrib-group>
      <year>2023</year>
      <volume>1</volume>
      <issue>1</issue>
      <fpage>15</fpage>
      <lpage>32</lpage>
      <page-range>15-32</page-range>
      <history>
        <date date-type="received">
          <month>10</month>
          <day>04</day>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <month>11</month>
          <day>23</day>
          <year>2023</year>
        </date>
        <date date-type="pub">
          <month>11</month>
          <day>30</day>
          <year>2023</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2023 by the author(s)</copyright-statement>
        <copyright-year>2023</copyright-year>
        <license>. Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the <a href='https://creativecommons.org/licenses/by/4.0/' target='_blank' class='text-yellow-700 hover:underline'>CC BY 4.0 license</a>.</license>
      </permissions>
      <abstract>In the field of computer-aided diagnostics, the segmentation and classification of biomedical images play a pivotal role. This study introduces a novel approach employing a Self-Augmented Multistage Deep Learning Network (SAMNetwork) and Deep Belief Networks (DBNs) optimized by Coot Optimization Algorithms (COAs) for the analysis of dermoscopy images. The unique challenges posed by dermoscopy images, including complex detection backgrounds and lesion characteristics, necessitate advanced techniques for accurate lesion recognition. Traditional methods have predominantly focused on utilizing larger, more complex models to increase detection accuracy, yet have often neglected the significant intraclass variability and inter-class similarity of lesion traits. This oversight has led to challenges in algorithmic application to larger models. The current research addresses these limitations by leveraging SAM, which, although not yielding immediate high-quality segmentation for medical image data, provides valuable masks, features, and stability scores for developing and training enhanced medical images. Subsequently, DBNs, aided by COAs to fine-tune their hyper-parameters, perform the classification task. The effectiveness of this methodology was assessed through comprehensive experimental comparisons and feature visualization analyses. The results demonstrated the superiority of the proposed approach over the current state-of-the-art deep learning-based methods across three datasets: ISBI 2017, ISBI 2018, and the PH2 dataset. In the experimental evaluations, the Multi-class Dilated D-Net (MD2N) model achieved a Matthew’s Correlation Coefficient (MCC) of 0.86201, the Deep convolutional neural networks (DCNN) model 0.84111, the standalone DBN 0.91157, the autoencoder (AE) model 0.88662, and the DBN-COA model 0.93291, respectively. These findings highlight the enhanced performance and potential of integrating SAM with optimized DBNs in the detection and classification of skin cancer in dermoscopy images, marking a significant advancement in the field of medical image analysis.</abstract>
      <kwd-group>
        <kwd>Skin cancer classification</kwd>
        <kwd>Dermoscopy image lesions</kwd>
        <kwd>Deep belief network</kwd>
        <kwd>Coot optimization algorithm</kwd>
        <kwd>Segmentation network</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors">3</count>
        <fig-count>3</fig-count>
        <table-count>6</table-count>
        <ref-count>25</ref-count>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec disp-level="level1" sec-type="intro">
      <title>1. Introduction</title>
      <p style="text-align: justify">Skin cancer, particularly its melanoma and non-melanoma forms, stands as a prevalent health concern compared to other cancer types. Melanoma, characterized by the uncontrolled proliferation of pigmented cells (melanocytes) (<xref ref-type="bibr" rid="ref_25">Zhang et al., 2020</xref>), has been observed to increasingly contribute to mortality rates annually (<xref ref-type="bibr" rid="ref_15">Razmjooy et al., 2020</xref>). This upward trend in melanoma incidences poses a significant public health risk, underlining the necessity of early detection measures. It is well-established that early-stage melanoma can be effectively treated, thereby rendering early detection methodologies (<xref ref-type="bibr" rid="ref_6">Daghrir et al., 2020</xref>) increasingly vital. In the realm of melanoma skin cancer imaging, distinguishing lesioned from non-lesioned areas presents a formidable challenge (<xref ref-type="bibr" rid="ref_10">Jakkulla et al., 2023</xref>), often necessitating specialized expertise. Consequently, divergent opinions among dermatologists are not uncommon. In response to this, an automated analysis method has been proposed (<xref ref-type="bibr" rid="ref_7">Dildar et al., 2021</xref>), aiming to facilitate rapid and accurate diagnostic decisions by dermatologists. Central to this automated analysis is the requirement for precise segmentation of the skin surrounding melanomas (<xref ref-type="bibr" rid="ref_4">Ali et al., 2021</xref>).</p><p style="text-align: justify">Recent technological advancements have heralded the widespread adoption of deep neural networks in computer vision tasks. Convolutional neural network (CNN) architectures, known for their adept feature extraction and classification capabilities (<xref ref-type="bibr" rid="ref_13">Macherla et al., 2023</xref>), have gained prominence in pattern recognition and segmentation tasks (<xref ref-type="bibr" rid="ref_24">Xu et al., 2020</xref>). In line with this, a number of studies have focused on the segmentation and classification of skin lesions, employing pre-trained CNN models augmented with transfer learning techniques (<xref ref-type="bibr" rid="ref_11">Kumar et al., 2020</xref>). Advanced deep learning methodologies, including UNet, Mask R-CNN, fully convolutional networks, feature pyramid networks, SegNet, and transformers, have been utilized to identify critical lesion locations at the pixel level (<xref ref-type="bibr" rid="ref_3">Ali et al., 2022</xref>; <xref ref-type="bibr" rid="ref_14">Pacheco &amp;amp; Krohling, 2020</xref>).</p><p style="text-align: justify">Building on the foundation of these prior studies (<xref ref-type="bibr" rid="ref_21">Toğaçar et al., 2021</xref>; <xref ref-type="bibr" rid="ref_20">Thomas et al., 2021</xref>), which predominantly revolve around lesion segmentation and classification in dermoscopy images (<xref ref-type="bibr" rid="ref_23">Wei et al., 2020</xref>), the current research introduces a multi-task learning network aimed at improving the segmentation and classification of skin lesions as melanoma. This network employs pre-processing techniques during segmentation, initially enhancing image clarity and removing hair details. Subsequently, SAMNet is utilized for lesion segmentation, followed by the application of a very deep super-resolution neural network to extract and enhance the quality of the segmented lesions. These high-resolution images then serve as inputs for the classifier model. The classifier model itself is a novel amalgamation of three robust, pre-trained deep models. Both the ISIC database and PH2 have been utilized to rigorously test each proposed method. Experimental results validate the superior performance of the developed classification system.</p><p style="text-align: justify">The advantages of the proposed deep network method are manifold:</p><ul><li><p style="text-align: justify">A very deep super-resolution neural network enhances the resolution of lesion images, refining them post-segmentation for input into the classifier model.</p></li><li><p style="text-align: justify">A SAMNetwork approach effectively localizes lesions in a diverse range of dermoscopy images, as evidenced by numerical and visual results from experimental tests.</p></li><li><p style="text-align: justify">The classifier model's design, based on an integration of diverse deep models, has demonstrated remarkable efficacy in melanoma classification, as verified through experimental studies.</p></li></ul><p style="text-align: justify">The study is organized as follows: Section 2 presents the literature review, Section 3 describes the proposed model, Section 4 details the experimental analysis, and Section 5 offers the conclusion.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>2. Related works</title>
      <p style="text-align: justify">In addressing the challenges of skin cancer detection and therapy planning, <xref ref-type="bibr" rid="ref_18">Subhashini &amp;amp; Chandrasekar (2023)</xref> proposed a distinctive deep learning-based framework. This approach incorporates preprocessing, segmenting the cancer regions, and Fast Adaptive Moth-Flame optimization with Cauchy mutation (FA-MFC) for data dimensionality reduction. The bidirectional Gated Recurrent Unit (BGRU)-quantum neural network (QNN) facilitates multi-class classification, while hidden features are extracted via the structural similarity with light U-structure (USSL). Tested on the Kaggle and ISIC-2019 datasets, this model demonstrated remarkable accuracy, achieving up to 96.458% on Kaggle and 94.238% on ISIC-2019. Such results indicate the potential of this hybrid deep learning methodology in enhancing skin cancer classification and, consequently, improving diagnostic, treatment, and survival outcomes.</p><p style="text-align: justify"><xref ref-type="bibr" rid="ref_5">Anand et al. (2023)</xref> explored the integration of Fully convolutional Neural Network as U-Net models for skin disease analysis. Their approach utilized U-Net for segmenting skin diseases in scans and introduced a convolutional model for multi-class classification of segmented images. The model was tested on the HAM10000 dataset, encompassing images across seven skin disease categories. Employing Adam and Adadelta optimizers for 20 epochs and a batch size of 32, the model demonstrated superior performance, especially with an Adadelta optimizer, achieving an accuracy of 97.96%.</p><p style="text-align: justify"><xref ref-type="bibr" rid="ref_12">Kumar et al. (2023)</xref> introduced the MD2N architecture for segmenting and classifying various types of skin cancer in screenings. The MD2N employs a downsampling ratio in its encoder phase to maintain feature information, crucial for differentiating small skin lesion patches. By utilizing dilated manifold parallel convolution, the network's receptive field is expanded, addressing the "grid issue" common in standard dilated convolutions. This approach enables the model to capture detailed feature information from regions with variably sized skin lesions. The system, powered by the proposed deep learning model, efficiently segments and identifies lesion locations from skin images provided by the International Skin Imaging Collaboration.</p><p style="text-align: justify"><xref ref-type="bibr" rid="ref_17">Singh et al. (2023)</xref> advocated for a modified deep learning model coupled with fuzzy logic-based image segmentation for skin cancer diagnosis. This model incorporates dermoscopy image enhancement techniques, including standard deviation methods, L-R fuzzy defuzzification, and mathematical logic infusion, aimed at augmenting lesion visibility by eliminating distractive artifacts such as hair follicles and dermoscopic scales. Following segmentation, the quality of the image is enhanced through histogram equalization before proceeding to detection. The modified model employs the You Look Only Once (YOLO) deep neural network approach for diagnosing melanoma lesions from digital and dermoscopic images. Enhancements to the YOLO model include an additional convolutional layer and residual connections within its DCNN layer sequence, alongside feature concatenation across layers. Training on 2,000 and 8,695 dermoscopy images from the ISIC 2017 and ISIC 2018 datasets, with the PH2 dataset for testing, revealed that YOLO surpasses other classifiers in accuracy and speed.</p><p style="text-align: justify"><xref ref-type="bibr" rid="ref_16">Sethanan et al. (2023)</xref> developed the skin cancer classification system (SC-CS), a highly accurate method for skin cancer classification. This system targets specific skin anomalies such as melanoma, benign keratosis, and other carcinomas and skin moles, utilizing a dual artificial multiple intelligence system (AMIS) ensemble model. The SC-CS combines image segmentation and CNN algorithms, optimizing the weighting of findings for high-quality outcomes. Evaluated using accuracy, precision, Area Under Curve (AUC), and F1-score metrics, along with feedback from 31 specialists, including dermatologists and physicians experienced in skin cancer diagnosis, the SC-CS demonstrated remarkable accuracy on the HAM10000 and malignant vs. benign datasets. With an accuracy exceeding 99.4%, it outperformed state-of-the-art models by 2.1% for larger sizes and 15.7% for smaller sizes. The System Usability Scale (SUS) of 96.85% indicates high user satisfaction and likelihood of recommendation. Data security measures were stringently applied to ensure patient confidentiality, with processed data and images being promptly destroyed post-upload.</p><p style="text-align: justify"><xref ref-type="bibr" rid="ref_1">Adla et al. (2023)</xref> developed a hyper-parameter-optimized full resolution convolutional network-based model, demonstrating significant accuracy in diagnosing skin cancer types in dermoscopy images. This model presents a potential advancement in computer-aided diagnostics, potentially leading to faster and more precise diagnoses. The incorporation of a dynamic graph cut technique in this model addresses the prevalent issues of over-segmentation and under-segmentation commonly seen in graph cut methods. Moreover, the model adeptly handles the challenge of incorrectly segmented small areas in the grab cut technique. The importance of data augmentation in training and testing was underscored, with the model showing enhanced results compared to using new images alone. In various trials against other transfer models, this proposed model excelled, achieving an accuracy rate of 97.986% for skin lesion classification.</p><p style="text-align: justify"><xref ref-type="bibr" rid="ref_8">Hong et al. (2022)</xref> introduced a weakly supervised semantic segmentation approach (CNN-SRR) for dermoscopy images, an innovative adaptation of the unsupervised superpixel technique and deep learning-based classifiers. This method leverages the abundance of labeled data at the image level for fine-tuning, focusing on lesion areas. The process involves training a repurposed CNN classifier and back-propagating the top layer’s peak values, followed by over-segmenting a test image into superpixels, which are then aggregated into region proposals. Lesion region responses are then identified via non-maximal suppression, leading to the selection of a segmented mask. Quantitative tests on the ISBI2017 and PH2 datasets demonstrated that the CNN-SRR method not only effectively discriminates lesion locations but also achieves competitive accuracy comparable to supervised segmentation algorithms. Specifically, on the ISBI2017 dataset, the CNN-SRR method outperformed the unsupervised superpixel segmentation algorithm by 12.4% on the Jaccard coefficient and 3.3% on segmentation accuracy.</p><p style="text-align: justify"><xref ref-type="bibr" rid="ref_2">Alenezi et al. (2023)</xref> reported on a multi-task learning approach for dermoscopic images. The approach begins with an efficient pre-processing strategy, employing max pooling, contrast, and shape filters to remove hair details and enhance images. These processed images are then segmented using a Fully Convolutional Network (FCN) layer architecture based on a Visual Geometry Group Network (VGGNet) model. Following segmentation, lesions are cropped and subjected to a very deep super-resolution neural network, aiming to adjust the cropped images to the classifier model's input size with minimal loss of detail. A deep learning network strategy, utilizing pre-trained convolutional neural networks, was then deployed for melanoma classification. Experiments conducted using the International available dermoscopy skin lesion dataset demonstrated impressive results. For lesion region segmentation, the method achieved 96.99% accuracy, 92.53% specificity, 97.65% precision, and 98.41% sensitivity. In classification tasks, it yielded 97.73% accuracy, 99.83% specificity, 99.83% precision, and 95.67% sensitivity.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>3. Proposed system</title>
      
        <sec disp-level="level2">
          
            <title>3.1. Dataset description</title>
          
          <p>The proposed model's efficacy was evaluated using three distinct dermoscopy skin lesion datasets (<xref ref-type="bibr" rid="ref_19">Thapar et al., 2022</xref>). The ISIC-2017 dataset comprises 2,000 dermoscopy images for training, 150 for validation, and 600 for testing. Each image in this dataset is accompanied by separate ground truths for segmentation, distinguishing between foreground and background, and classification into categories such as melanoma, nevus, and seborrheic keratosis. <xref ref-type="fig" rid="fig_1">Figure 1</xref><span style="color: rgb(0, 112, 192)"> </span>exemplifies images from these datasets.</p><p>The ISBI additional dataset includes 1,320 annotated dermoscopy images, curated from the largest known repository of skin lesions in the ISIC archive. This dataset provides a comprehensive collection for training and validation purposes.</p><p>The PH2 dataset contains a total of 200 dermoscopy images, comprising 160 nevi and 40 melanomas, with annotations for both segmentation and classification. The dataset is categorized into two distinct types: melanoma and nevus.</p><p>ISBI 2017: The International Symposium on Biomedical Imaging (ISBI) 2017 focuses on biomedical imaging, including medical image analysis and processing. This annual conference features challenges and competitions to foster the development of innovative algorithms and methodologies in biomedical imaging. The ISBI 2017 Challenge involved a specific biomedical imaging problem, encouraging researchers to develop and test algorithms using the provided dataset.</p><p>ISBI 2018: Similar to its predecessor, ISBI 2018 continued the tradition of hosting workshops, challenges, and sessions dedicated to advancing medical image analysis. The symposium serves as a platform for presenting the latest research and developments in the field.</p><p>PH2 dataset: The PH2 dataset, a cornerstone in dermatoscopic image analysis, is instrumental in skin cancer diagnosis research. Dermoscopy, the examination of the skin using a dermatoscope, is crucial for the early detection of skin cancer. The PH2 dataset includes high-quality dermoscopic images of various skin lesions, complete with clinical descriptions and ground truth annotations. This dataset is extensively used by researchers to develop and evaluate algorithms for image segmentation, feature extraction, and lesion classification.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>Sample dermoscopic images from (a) ISBI 2017, (b) PH2, and (c) ISBI 2018 datasets</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_p_JvHjqzUbA0WkPx.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_u1XdClC4D63wiKUp.png"/>
            </fig>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.2. Preprocessing</title>
          
          <p style="text-align: justify">In the preprocessing stage, the primary objective was to enhance the quality of dermoscopic images and reduce noise in skin lesion images. This phase is critical for developing an effective skin lesion segmentation and classification model. The initial step involved applying a hair removal technique to the lesion area, followed by an intensity-based image quality enhancement process on the hairless dermoscopy images. This two-stage preparation was essential for accurately isolating the region of interest (ROI) in the images.</p><p style="text-align: justify">The hair removal process, executed using morphological operations, targeted the precise identification of the ROI. Post hair removal, an intensity-based picture quality enhancement was performed, aimed at improving the pixel quality of the now hairless dermoscopic images. The resultant images, devoid of hair and enhanced in quality, proved instrumental in the accurate segmentation of the ROI.</p><p style="text-align: justify">The entropy of the processed images serves as a key metric for validation. Entropy, in this context, quantifies the degree of randomness or disorder within the image data. Mathematically, entropy is defined as:</p>
          
            <disp-formula>
              <label>(1)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>S</mi>
                <mi>k</mi>
                <mi>b</mi>
                <mi>ln</mi>
                <mi>W</mi>
                <mo>=</mo>
                <mo data-mjx-texclass="NONE">⁡</mo>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">where, <italic>kb</italic> represents the Boltzmann constant, measured at 1.38064852 1023 m<sup>2</sup> kgs<sup>2</sup> K<sup>1</sup>, and W denotes the microscopic distribution. The closer the entropy of the final processed image is to that of the original, the more effective the preprocessing is considered. The mask's preprocessing threshold is adjusted based on each entropy calculation, and it is maintained constant once an increase in entropy difference is observed.</p><p><xref ref-type="fig" rid="fig_2">Figure 2</xref> displays examples of pre-processed images, illustrating the outcomes of the HR-IQE method utilized in the preprocessing stage.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>Preprocessing outcomes: (a) original image and (b) preprocessed image</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_mJ-Y4BLM4_F1KYj-.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_iYm_ZsuKfBzQxOkB.png"/>
            </fig>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.3. Segmentation methodology</title>
          
          <p>The SAM mechanism, operated in grid prompt mode, was utilized to generate segmentation masks for an input image. In this mode, segmentation masks were created across all logical sections of the image, with each mask subsequently stored in a database. A segmentation prior map was then produced for each mask, using the stability score provided by SAM to determine the drawing level. In addition to the segmentation prior map, a boundary prior map was also generated based on the external borders of each mask from the list. Thus, for a given image <italic>x</italic>, two prior maps, namely prior seg and prior boundary, were created.</p>
          
            <sec disp-level="level3">
              
                <title>3.3.1 Augmenting input images</title>
              
              <p style="text-align: justify">Post-creation of the prior maps, these were employed to augment the input image <italic>x</italic>. The augmentation method chosen for this study involved adding the prior maps directly to the raw image. Medical image segmentation tasks often entail a three-class segmentation job: the background, ROIs, and boundaries between ROIs and the background. In the augmented image, the segmentation prior map was superimposed on the second channel, while the boundary prior map was placed on the third. For grayscale images, a three-channel image was constructed with the raw image, segmentation prior map, and boundary prior map as the first, second, and third channels, respectively. This process resulted in an augmented version <italic>x<sub>aug</sub> = Aug(prior<sub>seg</sub>, prior<sub>boundary</sub>, x)</italic> of each image <italic>x</italic> in the training set.</p>
            </sec>
          
          
            <sec disp-level="level3">
              
                <title>3.3.2 Model training with sam-augmented images</title>
              
              <p style="text-align: justify">The self-attention mechanism is an integral component of deep neural networks, notably in Transformer models, which have extended their applications beyond natural language processing to include computer vision. SAM is pivotal in capturing long-range dependencies and contextual information within images, especially for image segmentation tasks. It serves as an alternative to convolutional layers, effectively capturing spatial relationships between pixels or regions. The central element of SAM is the computation of attention weights, signifying the significance of various regions or pixels relative to a query. These weights are calculated using a similarity function, typically the dot product, between the query and key for all query-key pairs, followed by the application of a softmax function to yield the attention weights.</p><p style="text-align: justify">Once attention weights are determined, they are utilized to aggregate information from the values, proportionally to the relevance of each region to the query.</p><p style="text-align: justify">With the augmented training set <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">{</mo>
      <mo>,</mo>
      <mo>⋅</mo>
      <mo>⋅</mo>
      <mo data-mjx-texclass="CLOSE">}</mo>
      <mrow data-mjx-texclass="INNER">
        <mo data-mjx-texclass="OPEN">(</mo>
        <mo>,</mo>
        <mo data-mjx-texclass="CLOSE">)</mo>
        <msubsup>
          <mi>x</mi>
          <mn>1</mn>
          <mrow data-mjx-texclass="ORD">
            <mi>a</mi>
            <mi>u</mi>
            <mi>g</mi>
          </mrow>
        </msubsup>
        <msub>
          <mi>y</mi>
          <mn>1</mn>
        </msub>
      </mrow>
      <mrow data-mjx-texclass="INNER">
        <mo data-mjx-texclass="OPEN">(</mo>
        <mo>⋅</mo>
        <mo data-mjx-texclass="CLOSE">)</mo>
        <msubsup>
          <mi>x</mi>
          <mi>n</mi>
          <mrow data-mjx-texclass="ORD">
            <mtext>aug </mtext>
          </mrow>
        </msubsup>
        <msub>
          <mi>y</mi>
          <mi>n</mi>
        </msub>
      </mrow>
      <msub>
        <mrow data-mjx-texclass="INNER">
          <mo data-mjx-texclass="OPEN">(</mo>
          <mo>,</mo>
          <mo data-mjx-texclass="CLOSE">)</mo>
          <msubsup>
            <mi>x</mi>
            <mn>2</mn>
            <mrow data-mjx-texclass="ORD">
              <mtext>aug </mtext>
            </mrow>
          </msubsup>
          <msub>
            <mi>y</mi>
            <mn>2</mn>
          </msub>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mn>2</mn>
          <mo>…</mo>
          <mo>,</mo>
        </mrow>
      </msub>
    </mrow>
  </math>
</inline-formula>, where <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msubsup>
      <mi>x</mi>
      <mi>i</mi>
      <mrow data-mjx-texclass="ORD">
        <mi>a</mi>
        <mi>u</mi>
        <mi>g</mi>
      </mrow>
    </msubsup>
    <mo>∈</mo>
    <mo>,</mo>
    <mo>∈</mo>
    <mo fence="false" stretchy="false">{</mo>
    <mo>,</mo>
    <msup>
      <mi>R</mi>
      <mrow data-mjx-texclass="ORD">
        <mi>w</mi>
        <mi>h</mi>
        <mi>C</mi>
        <mo>×</mo>
        <mo>×</mo>
      </mrow>
    </msup>
    <msup>
      <mo fence="false" stretchy="false">}</mo>
      <mrow data-mjx-texclass="ORD">
        <mi>w</mi>
        <mi>h</mi>
        <mi>C</mi>
        <mo>×</mo>
        <mo>×</mo>
      </mrow>
    </msup>
    <msub>
      <mi>y</mi>
      <mi>i</mi>
    </msub>
    <mn>0</mn>
    <mn>1</mn>
  </math>
</inline-formula><sub> </sub>represents the annotation set, the learning objective was formulated considering the limits of <italic>M</italic>:</p>
              
                <disp-formula>
                  <label>(2)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <munderover>
                      <mo data-mjx-texclass="OP">∑</mo>
                      <mrow data-mjx-texclass="ORD">
                        <mi>i</mi>
                        <mo>=</mo>
                        <mn>1</mn>
                      </mrow>
                      <mi>n</mi>
                    </munderover>
                    <mi>loss</mi>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo>,</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mi>M</mi>
                      <mrow data-mjx-texclass="INNER">
                        <mo data-mjx-texclass="OPEN">(</mo>
                        <mo data-mjx-texclass="CLOSE">)</mo>
                        <msubsup>
                          <mi>x</mi>
                          <mi>i</mi>
                          <mrow data-mjx-texclass="ORD">
                            <mtext>aug </mtext>
                          </mrow>
                        </msubsup>
                      </mrow>
                      <msub>
                        <mi>y</mi>
                        <mi>i</mi>
                      </msub>
                    </mrow>
                  </math>
                </disp-formula>
              
              <p style="text-align: justify">This objective focused solely on training with augmented images. During model testing, the trained model received only SAM-augmented images. In scenarios where SAM did not provide plausible prior maps, consideration was given to training a segmentation model using both raw and SAM-augmented images. The new objective function for the model <italic>M</italic> was defined as:</p>
              
                <disp-formula>
                  <label>(3)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <munderover>
                      <mo data-mjx-texclass="OP">∑</mo>
                      <mrow data-mjx-texclass="ORD">
                        <mi>i</mi>
                        <mo>=</mo>
                        <mn>1</mn>
                      </mrow>
                      <mi>n</mi>
                    </munderover>
                    <mi>β</mi>
                    <mi>loss</mi>
                    <mi>λ</mi>
                    <mi>loss</mi>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo>,</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mi>M</mi>
                      <mrow data-mjx-texclass="INNER">
                        <mo data-mjx-texclass="OPEN">(</mo>
                        <mo data-mjx-texclass="CLOSE">)</mo>
                        <msub>
                          <mi>x</mi>
                          <mi>i</mi>
                        </msub>
                      </mrow>
                      <msub>
                        <mi>y</mi>
                        <mi>i</mi>
                      </msub>
                    </mrow>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo>,</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mi>M</mi>
                      <mrow data-mjx-texclass="INNER">
                        <mo data-mjx-texclass="OPEN">(</mo>
                        <mo data-mjx-texclass="CLOSE">)</mo>
                        <msubsup>
                          <mi>x</mi>
                          <mi>i</mi>
                          <mrow data-mjx-texclass="ORD">
                            <mtext>aug </mtext>
                          </mrow>
                        </msubsup>
                      </mrow>
                      <msub>
                        <mi>y</mi>
                        <mi>i</mi>
                      </msub>
                    </mrow>
                    <mo>+</mo>
                  </math>
                </disp-formula>
              
              <p style="text-align: justify">where, <italic>β</italic> and<italic> λ</italic> regulate the importance of training loss for samples with raw and augmented images, respectively. By default, both <italic>β</italic> and<italic> λ </italic>were set to 1. A Stochastic Gradient Descent (SGD) -optimization function was employed.</p>
            </sec>
          
          
            <sec disp-level="level3">
              
                <title>3.3.3 Model deployment with sam-augmented images</title>
              
              <p style="text-align: justify">In scenarios where the model is exclusively trained on SAM-augmented images, the deployment (testing) phase mandates the use of SAM-augmented images as input. The deployment of the model can be mathematically represented as:</p>
              
                <disp-formula>
                  <label>(4)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>y</mi>
                        <mo stretchy="false">^</mo>
                      </mover>
                    </mrow>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mi>M</mi>
                      <mrow data-mjx-texclass="INNER">
                        <mo data-mjx-texclass="OPEN">(</mo>
                        <mo data-mjx-texclass="CLOSE">)</mo>
                        <msup>
                          <mi>x</mi>
                          <mrow data-mjx-texclass="ORD">
                            <mi>a</mi>
                            <mi>u</mi>
                            <mi>g</mi>
                          </mrow>
                        </msup>
                      </mrow>
                    </mrow>
                    <mo>=</mo>
                    <mi>τ</mi>
                  </math>
                </disp-formula>
              
              <p style="text-align: justify">In this equation, τ denotes an output activation function, such as a sigmoid or softmax function, and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msup>
      <mi>x</mi>
      <mrow data-mjx-texclass="ORD">
        <mi>a</mi>
        <mi>u</mi>
        <mi>g</mi>
      </mrow>
    </msup>
  </math>
</inline-formula> is a SAM-augmented image. However, when the segmentation model <italic>M</italic> is trained on both raw and SAM-augmented images, additional opportunities emerge at inference time to harness the full potential of the trained model. A practical approach involves dual inference for each test sample: initially using the raw image <italic>x </italic>as input, followed by the SAM-augmented image. The final segmentation output is derived as an average ensemble of these two outputs, formally expressed as:</p>
              
                <disp-formula>
                  <label>(5)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>y</mi>
                        <mo stretchy="false">^</mo>
                      </mover>
                    </mrow>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo stretchy="false">(</mo>
                      <mo stretchy="false">)</mo>
                      <mo>+</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mi>M</mi>
                      <mi>x</mi>
                      <mi>M</mi>
                      <mrow data-mjx-texclass="INNER">
                        <mo data-mjx-texclass="OPEN">(</mo>
                        <mo data-mjx-texclass="CLOSE">)</mo>
                        <msup>
                          <mi>x</mi>
                          <mrow data-mjx-texclass="ORD">
                            <mi>a</mi>
                            <mi>n</mi>
                            <mi>g</mi>
                          </mrow>
                        </msup>
                      </mrow>
                    </mrow>
                    <mo>=</mo>
                    <mi>τ</mi>
                  </math>
                </disp-formula>
              
              <p>Another strategy for leveraging the two output candidates, <italic>M</italic>(<italic>x</italic>) and <italic>M</italic>(<italic>x<sup>aug</sup></italic>), is to select a plausible segmentation output from these candidates:</p>
              
                <disp-formula>
                  <label>(6)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>y</mi>
                        <mo stretchy="false">^</mo>
                      </mover>
                    </mrow>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mi>M</mi>
                      <mrow data-mjx-texclass="INNER">
                        <mo data-mjx-texclass="OPEN">(</mo>
                        <mo data-mjx-texclass="CLOSE">)</mo>
                        <msup>
                          <mi>x</mi>
                          <mo>∗</mo>
                        </msup>
                      </mrow>
                    </mrow>
                    <mo>=</mo>
                    <mi>τ</mi>
                  </math>
                </disp-formula>
              
              <p>where, <italic>x<sup>*</sup></italic> is chosen by solving the optimization problem:</p>
              
                <disp-formula>
                  <label>(7)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msup>
                      <mi>x</mi>
                      <mo>∗</mo>
                    </msup>
                    <mo>=</mo>
                    <msub>
                      <mi>argmin</mi>
                      <mrow data-mjx-texclass="ORD">
                        <msup>
                          <mi>x</mi>
                          <mrow data-mjx-texclass="ORD">
                            <mi data-mjx-alternate="1">′</mi>
                          </mrow>
                        </msup>
                        <mo>∈</mo>
                        <mrow data-mjx-texclass="INNER">
                          <mo data-mjx-texclass="OPEN">{</mo>
                          <mo>,</mo>
                          <mo data-mjx-texclass="CLOSE">}</mo>
                          <mi>x</mi>
                          <msup>
                            <mi>x</mi>
                            <mrow data-mjx-texclass="ORD">
                              <mi>a</mi>
                              <mi>v</mi>
                              <mi>g</mi>
                            </mrow>
                          </msup>
                        </mrow>
                      </mrow>
                    </msub>
                    <mi>Entropy</mi>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mi>τ</mi>
                      <mrow data-mjx-texclass="INNER">
                        <mo data-mjx-texclass="OPEN">(</mo>
                        <mo data-mjx-texclass="CLOSE">)</mo>
                        <mi>M</mi>
                        <mrow data-mjx-texclass="INNER">
                          <mo data-mjx-texclass="OPEN">(</mo>
                          <mo data-mjx-texclass="CLOSE">)</mo>
                          <msup>
                            <mi>x</mi>
                            <mrow data-mjx-texclass="ORD">
                              <mi data-mjx-alternate="1">′</mi>
                            </mrow>
                          </msup>
                        </mrow>
                      </mrow>
                    </mrow>
                  </math>
                </disp-formula>
              
              <p style="text-align: justify">This selection is based on the entropy, or prediction certainty, of the segmentation output. An output with lower entropy indicates a higher certainty in the model's prediction, which often correlates positively with segmentation accuracy (<xref ref-type="bibr" rid="ref_22">Wang et al., 2020</xref>). <xref ref-type="fig" rid="fig_3">Figure 3</xref> displays sample images for segmentation, showcasing the input and segmented images.</p>
              
                <fig id="fig_3">
                  <label>Figure 3</label>
                  <caption>(a) Sample input image and (b) segmented image</caption>
                  <abstract/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_9XQxqAMqgv2uGlNg.png"/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_SZOjRmTnNRTLwo6d.png"/>
                </fig>
              
            </sec>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.4. Proposed dbn for classification</title>
          
          <p style="text-align: justify">The implementation of DBNs in the proposed skin cancer detection system leverages recent advances in deep learning. DBNs, an energy-based model for generating probabilities, are constructed by stacking multiple Restricted Boltzmann Machines (RBMs). The DBN model operates in two phases: training and testing. In the training phase, the DBN layers are trained utilizing a series of RBM layers. These layers comprise neurons located in the hidden layer, situated above the input layer. Neurons between different layers communicate freely, whereas intra-layer communication is restricted. The architecture of the surrounding visible layer is represented as follows:</p>
          
            <disp-formula>
              <label>(8)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>v</mi>
                <mi>i</mi>
                <mi>s</mi>
                <mo>=</mo>
                <mo>;</mo>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">{</mo>
                  <mo>,</mo>
                  <mo>,</mo>
                  <mo>…</mo>
                  <mo>,</mo>
                  <mo>,</mo>
                  <mo>…</mo>
                  <mo>,</mo>
                  <mo data-mjx-texclass="CLOSE">}</mo>
                  <mi>v</mi>
                  <mi>i</mi>
                  <mi>v</mi>
                  <mi>i</mi>
                  <mi>v</mi>
                  <mi>i</mi>
                  <mi>v</mi>
                  <mi>i</mi>
                  <msub>
                    <mi>s</mi>
                    <mn>1</mn>
                  </msub>
                  <msub>
                    <mi>s</mi>
                    <mn>2</mn>
                  </msub>
                  <msub>
                    <mi>s</mi>
                    <mi>j</mi>
                  </msub>
                  <msub>
                    <mi>s</mi>
                    <mi>m</mi>
                  </msub>
                </mrow>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo>∈</mo>
                  <mo fence="false" stretchy="false">{</mo>
                  <mo>,</mo>
                  <mo fence="false" stretchy="false">}</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <mi>v</mi>
                  <mi>i</mi>
                  <msub>
                    <mi>s</mi>
                    <mi>j</mi>
                  </msub>
                  <mn>0</mn>
                  <mn>1</mn>
                </mrow>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">Similarly, the expression for any one of the hidden layers in the DBN is denoted as:</p>
          
            <disp-formula>
              <label>(9)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>h</mi>
                <mi>i</mi>
                <mi>d</mi>
                <mo>=</mo>
                <mo>;</mo>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">{</mo>
                  <mo>,</mo>
                  <mo>,</mo>
                  <mo>…</mo>
                  <mo>,</mo>
                  <mo>,</mo>
                  <mo>…</mo>
                  <mo>,</mo>
                  <mo data-mjx-texclass="CLOSE">}</mo>
                  <mi>h</mi>
                  <mi>i</mi>
                  <mi>h</mi>
                  <mi>i</mi>
                  <mi>h</mi>
                  <mi>i</mi>
                  <mi>h</mi>
                  <mi>i</mi>
                  <msub>
                    <mi>d</mi>
                    <mn>1</mn>
                  </msub>
                  <msub>
                    <mi>d</mi>
                    <mn>2</mn>
                  </msub>
                  <msub>
                    <mi>d</mi>
                    <mi>j</mi>
                  </msub>
                  <msub>
                    <mi>d</mi>
                    <mi>m</mi>
                  </msub>
                </mrow>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo>∈</mo>
                  <mo fence="false" stretchy="false">{</mo>
                  <mo>,</mo>
                  <mo fence="false" stretchy="false">}</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <mi>h</mi>
                  <mi>i</mi>
                  <msub>
                    <mi>d</mi>
                    <mi>j</mi>
                  </msub>
                  <mn>0</mn>
                  <mn>1</mn>
                </mrow>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">In this model, visible layers are denoted by 'vis', and hidden layers are indicated by 'hid'. The classification network (CN) technique, embedded within the DBN, determines the number of layers based on the pre-processed input result <italic>N</italic>(<italic>y,z</italic>). Adjusted characteristics are transmitted to the visible layer, which in turn communicates them to the hidden layer via a weight relationship, rather than direct connections. The primary parameters of this architecture are defined as = <italic>W,b,c</italic>, where <italic>W</italic> represents the weight matrix, <italic>B</italic> the bias in the hidden layer, and <italic>C</italic> the bias in the visible layer. vis<sub>j</sub> indicates the visible unit in the <italic>j</italic>-th layer and hid<sub>j</sub> denotes the hidden unit in the <italic>j</italic>-th layer. The RBM architecture, consisting of <italic>n </italic>hidden neurons and m input neurons, is mathematically structured as:</p>
          
            <disp-formula>
              <label>(10)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>W</mi>
                <mo>=</mo>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">{</mo>
                  <mo>∈</mo>
                  <mo data-mjx-texclass="CLOSE">}</mo>
                  <msub>
                    <mi>w</mi>
                    <mrow data-mjx-texclass="ORD">
                      <mi>j</mi>
                      <mi>k</mi>
                      <mo>,</mo>
                    </mrow>
                  </msub>
                  <msup>
                    <mi>S</mi>
                    <mrow data-mjx-texclass="ORD">
                      <mi>m</mi>
                      <mi>n</mi>
                      <mo>×</mo>
                    </mrow>
                  </msup>
                </mrow>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">where, <italic>W<sub>j,k</sub></italic> denotes the weight value between neurons in the <italic>j</italic>-th and <italic>k</italic>-th layers. The bias function for the hidden layer<italic> B</italic> is articulated as:</p>
          
            <disp-formula>
              <label>(11)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>B</mi>
                <mo>=</mo>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">{</mo>
                  <mo>∈</mo>
                  <mo data-mjx-texclass="CLOSE">}</mo>
                  <msub>
                    <mi>b</mi>
                    <mi>j</mi>
                  </msub>
                  <msup>
                    <mi>S</mi>
                    <mi>n</mi>
                  </msup>
                </mrow>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">The term <italic>b<sub>j</sub></italic> in the equation represents the bias threshold for the hidden neuron at the <italic>j</italic>-th position. A similar methodology is employed for determining the bias function of the visible layer:</p>
          
            <disp-formula>
              <label>(12)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>C</mi>
                <mo>=</mo>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">{</mo>
                  <mo>∈</mo>
                  <mo data-mjx-texclass="CLOSE">}</mo>
                  <msub>
                    <mi>c</mi>
                    <mi>k</mi>
                  </msub>
                  <msup>
                    <mi>S</mi>
                    <mi>m</mi>
                  </msup>
                </mrow>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">In this context, <italic>c<sub>k</sub></italic> signifies the bias function's threshold for the <italic>k</italic>-th observable neuron. The RBM model's energy function between the hidden and visible layers is employed to learn probability distributions, as calculated by:</p>
          
            <disp-formula>
              <label>(13)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>E</mi>
                <mi>v</mi>
                <mi>i</mi>
                <mi>s</mi>
                <mi>h</mi>
                <mi>i</mi>
                <mi>d</mi>
                <mi>θ</mi>
                <mi>v</mi>
                <mi>i</mi>
                <mi>h</mi>
                <mi>i</mi>
                <mi>v</mi>
                <mi>i</mi>
                <mi>h</mi>
                <mi>i</mi>
                <mo stretchy="false">(</mo>
                <mo>,</mo>
                <mo>∣</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <mo>−</mo>
                <mo>−</mo>
                <mo>−</mo>
                <munderover>
                  <mo data-mjx-texclass="OP">∑</mo>
                  <mrow data-mjx-texclass="ORD">
                    <mi>j</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                  <mi>m</mi>
                </munderover>
                <munderover>
                  <mo data-mjx-texclass="OP">∑</mo>
                  <mrow data-mjx-texclass="ORD">
                    <mi>k</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                  <mi>n</mi>
                </munderover>
                <munderover>
                  <mo data-mjx-texclass="OP">∑</mo>
                  <mrow data-mjx-texclass="ORD">
                    <mi>j</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                  <mi>m</mi>
                </munderover>
                <munderover>
                  <mo data-mjx-texclass="OP">∑</mo>
                  <mrow data-mjx-texclass="ORD">
                    <mi>k</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                  <mi>n</mi>
                </munderover>
                <msub>
                  <mi>b</mi>
                  <mi>j</mi>
                </msub>
                <msub>
                  <mi>s</mi>
                  <mi>j</mi>
                </msub>
                <msub>
                  <mi>c</mi>
                  <mi>k</mi>
                </msub>
                <msub>
                  <mi>d</mi>
                  <mi>k</mi>
                </msub>
                <msub>
                  <mi>s</mi>
                  <mi>j</mi>
                </msub>
                <msub>
                  <mi>W</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mi>j</mi>
                    <mi>k</mi>
                  </mrow>
                </msub>
                <msub>
                  <mi>d</mi>
                  <mi>k</mi>
                </msub>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">where, θ={<italic>W<sub>jk</sub>,b<sub>j</sub>,c<sub>k</sub></italic>} consists of the RBM model's parameters, and <italic>E</italic>, which defines the energy function between hidden and visible nodes. In the DBN architecture under consideration, the total count of neurons within the visible layer is denoted as <italic>m</italic>, while the aggregate of neurons across the hidden layers is represented as <italic>n</italic>. This configuration underpins the structural framework of the network.</p><p style="text-align: justify">Upon incorporating the exponential and regularization of the energy function, the probability function is defined, establishing the overlap between the probabilities of the RBM model's visible and hidden layers:</p>
          
            <disp-formula>
              <label>(14)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>P</mi>
                <mi>v</mi>
                <mi>i</mi>
                <mi>s</mi>
                <mi>h</mi>
                <mi>i</mi>
                <mi>d</mi>
                <mi>θ</mi>
                <mo stretchy="false">(</mo>
                <mo>,</mo>
                <mo>∣</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <mfrac>
                  <mn>1</mn>
                  <mrow>
                    <mi>Z</mi>
                    <mi>θ</mi>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                  </mrow>
                </mfrac>
                <msup>
                  <mi>e</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mo>−</mo>
                    <mo stretchy="false">(</mo>
                    <mo>,</mo>
                    <mo>∣</mo>
                    <mo stretchy="false">)</mo>
                    <mi>E</mi>
                    <mi>v</mi>
                    <mi>i</mi>
                    <mi>s</mi>
                    <mi>h</mi>
                    <mi>i</mi>
                    <mi>d</mi>
                    <mi>θ</mi>
                  </mrow>
                </msup>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">This equation is grounded in the Gibbs distribution function of the RBM model. From this, the partition function is derived:</p>
          
            <disp-formula>
              <label>(15)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>Z</mi>
                <mi>θ</mi>
                <mo stretchy="false">(</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <munder>
                  <mo data-mjx-texclass="OP">∑</mo>
                  <mrow data-mjx-texclass="ORD">
                    <mi>v</mi>
                    <mi>i</mi>
                    <mi>s</mi>
                    <mi>h</mi>
                    <mi>i</mi>
                    <mi>d</mi>
                    <mo>,</mo>
                  </mrow>
                </munder>
                <msup>
                  <mi>e</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mo>−</mo>
                    <mo stretchy="false">(</mo>
                    <mo>,</mo>
                    <mo>∣</mo>
                    <mo stretchy="false">)</mo>
                    <mi>E</mi>
                    <mi>v</mi>
                    <mi>i</mi>
                    <mi>s</mi>
                    <mi>h</mi>
                    <mi>i</mi>
                    <mi>d</mi>
                    <mi>θ</mi>
                  </mrow>
                </msup>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>Z</mi>
    <mi>θ</mi>
    <mo stretchy="false">(</mo>
    <mo stretchy="false">)</mo>
  </math>
</inline-formula> the function represents an energy state that is either uniformly distributed or normalized. This function encapsulates the total energy of all layers in the DBN, both visible and hidden. It is defined as the sum of energy evaluations conducted on these layers. The probability function in the DBN model is utilized to regulate the parameters' standards. The notation <italic>P</italic>(<italic>vis,hid</italic>|) represents the joint distribution of the visible and hidden layers, while <italic>P</italic>(<italic>vis</italic>|) symbolizes the marginal distribution function of the visible layer:</p>
          
            <disp-formula>
              <label>(16)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>P</mi>
                <mi>v</mi>
                <mi>i</mi>
                <mi>s</mi>
                <mi>θ</mi>
                <mo stretchy="false">(</mo>
                <mo>∣</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <mfrac>
                  <mn>1</mn>
                  <mrow>
                    <mi>Z</mi>
                    <mi>θ</mi>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                  </mrow>
                </mfrac>
                <munder>
                  <mo data-mjx-texclass="OP">∑</mo>
                  <mrow data-mjx-texclass="ORD">
                    <mtext>hid </mtext>
                  </mrow>
                </munder>
                <msup>
                  <mi>e</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mo>−</mo>
                    <mo stretchy="false">(</mo>
                    <mo>,</mo>
                    <mo>∣</mo>
                    <mo stretchy="false">)</mo>
                    <mi>E</mi>
                    <mi>v</mi>
                    <mi>i</mi>
                    <mi>s</mi>
                    <mi>h</mi>
                    <mi>i</mi>
                    <mi>d</mi>
                    <mi>θ</mi>
                  </mrow>
                </msup>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">The marginal layer is evaluated by summarizing the conditions of the entire network. For computing the marginal distribution function of the hidden layer, the following equation is used:</p>
          
            <disp-formula>
              <label>(17)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>P</mi>
                <mi>h</mi>
                <mi>i</mi>
                <mi>d</mi>
                <mi>θ</mi>
                <mo stretchy="false">(</mo>
                <mo>∣</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <mfrac>
                  <mn>1</mn>
                  <mrow>
                    <mi>Z</mi>
                    <mi>θ</mi>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                  </mrow>
                </mfrac>
                <munder>
                  <mo data-mjx-texclass="OP">∑</mo>
                  <mrow data-mjx-texclass="ORD">
                    <mi>v</mi>
                    <mi>i</mi>
                    <mi>s</mi>
                  </mrow>
                </munder>
                <msup>
                  <mi>e</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mo>−</mo>
                    <mo stretchy="false">(</mo>
                    <mo>,</mo>
                    <mo>∣</mo>
                    <mo stretchy="false">)</mo>
                    <mi>E</mi>
                    <mi>v</mi>
                    <mi>i</mi>
                    <mi>s</mi>
                    <mi>h</mi>
                    <mi>i</mi>
                    <mi>d</mi>
                    <mi>θ</mi>
                  </mrow>
                </msup>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">Given the binary nature of its components, a sigmoid activation function is deemed suitable for the proposed RBM architecture. The conditional probability values within the RBM are calculated as follows, given the independence of its visible and hidden layers:</p>
          
            <disp-formula>
              <label>(18)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>P</mi>
                <mi>v</mi>
                <mi>i</mi>
                <mi>s</mi>
                <mi>h</mi>
                <mi>i</mi>
                <mi>d</mi>
                <mi>P</mi>
                <mo stretchy="false">(</mo>
                <mo>∣</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <munder>
                  <mo data-mjx-texclass="OP">∏</mo>
                  <mi>i</mi>
                </munder>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo>∣</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <mi>v</mi>
                  <mi>i</mi>
                  <mi>h</mi>
                  <mi>i</mi>
                  <mi>d</mi>
                  <msub>
                    <mi>s</mi>
                    <mi>j</mi>
                  </msub>
                </mrow>
              </math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(19)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>P</mi>
                <mi>h</mi>
                <mi>i</mi>
                <mi>d</mi>
                <mi>v</mi>
                <mi>i</mi>
                <mi>s</mi>
                <mi>P</mi>
                <mo stretchy="false">(</mo>
                <mo>∣</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <munder>
                  <mo data-mjx-texclass="OP">∏</mo>
                  <mi>k</mi>
                </munder>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo>∣</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <mi>h</mi>
                  <mi>i</mi>
                  <mi>v</mi>
                  <mi>i</mi>
                  <mi>s</mi>
                  <msub>
                    <mi>d</mi>
                    <mi>k</mi>
                  </msub>
                </mrow>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">The probabilities for the RBM structure's visible and hidden layers are then derived using this activation function:</p>
          
            <disp-formula>
              <label>(20)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>sigmoid</mi>
                <mi>y</mi>
                <mo stretchy="false">(</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <mfrac>
                  <mn>1</mn>
                  <mrow data-mjx-texclass="INNER">
                    <mo data-mjx-texclass="OPEN">(</mo>
                    <mo>+</mo>
                    <mo data-mjx-texclass="CLOSE">)</mo>
                    <mn>1</mn>
                    <msup>
                      <mi>e</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mo>−</mo>
                        <mi>y</mi>
                      </mrow>
                    </msup>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">With this activation function as a basis, the probabilities for the RBM structure's visible and hidden layers are as follows:</p>
          
            <disp-formula>
              <label>(21)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>P</mi>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo>=</mo>
                  <mo>∣</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <mi>v</mi>
                  <mi>i</mi>
                  <mi>h</mi>
                  <mi>i</mi>
                  <mi>d</mi>
                  <msub>
                    <mi>s</mi>
                    <mi>j</mi>
                  </msub>
                  <mn>1</mn>
                </mrow>
                <mo>=</mo>
                <mfrac>
                  <mn>1</mn>
                  <mrow>
                    <mn>1</mn>
                    <mo>+</mo>
                    <mo data-mjx-texclass="NONE">⁡</mo>
                    <mi>exp</mi>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo>−</mo>
                      <mo>−</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <msub>
                        <mi>b</mi>
                        <mi>j</mi>
                      </msub>
                      <msub>
                        <mi>w</mi>
                        <mrow data-mjx-texclass="ORD">
                          <mi>j</mi>
                          <mi>k</mi>
                        </mrow>
                      </msub>
                      <msub>
                        <mi>h</mi>
                        <mrow data-mjx-texclass="ORD">
                          <mi>i</mi>
                          <msub>
                            <mi>d</mi>
                            <mi>k</mi>
                          </msub>
                        </mrow>
                      </msub>
                      <munderover>
                        <mo data-mjx-texclass="OP">∑</mo>
                        <mrow data-mjx-texclass="ORD">
                          <mi>k</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>n</mi>
                      </munderover>
                    </mrow>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(22)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>P</mi>
                <mi>h</mi>
                <mi>i</mi>
                <mi>d</mi>
                <mi>v</mi>
                <mi>i</mi>
                <mi>s</mi>
                <mo stretchy="false">(</mo>
                <mo>=</mo>
                <mo>∣</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <mn>1</mn>
                <mfrac>
                  <mn>1</mn>
                  <mrow>
                    <mn>1</mn>
                    <mo>+</mo>
                    <mo data-mjx-texclass="NONE">⁡</mo>
                    <mi>exp</mi>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo>−</mo>
                      <mo>−</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <msub>
                        <mi>c</mi>
                        <mi>k</mi>
                      </msub>
                      <msub>
                        <mi>w</mi>
                        <mrow data-mjx-texclass="ORD">
                          <mi>j</mi>
                          <mi>k</mi>
                        </mrow>
                      </msub>
                      <msub>
                        <mi>s</mi>
                        <mi>j</mi>
                      </msub>
                      <munderover>
                        <mo data-mjx-texclass="OP">∑</mo>
                        <mrow data-mjx-texclass="ORD">
                          <mi>j</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>m</mi>
                      </munderover>
                      <mi>v</mi>
                      <mi>i</mi>
                    </mrow>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">The next phase involves updating the rules for relevant parameters =<italic>W,b,c</italic>. The Gibbs distribution function, foundational to the RBM model, necessitates an efficient learning methodology due to its complexity. Thus, contrast divergence is employed as a rapid learning technique to minimize time consumption. The updated parameter values, based on this learning method, are quantitatively presented in the following equations:</p>
          
            <disp-formula>
              <label>(23)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <msup>
                  <mi>W</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mtext>time </mtext>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                </msup>
                <msup>
                  <mi>W</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mtext>time </mtext>
                  </mrow>
                </msup>
                <mo>=</mo>
                <mo>+</mo>
                <mi>ε</mi>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo>−</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <mi>P</mi>
                  <mi>P</mi>
                  <mrow data-mjx-texclass="INNER">
                    <mo data-mjx-texclass="OPEN">(</mo>
                    <mo>∣</mo>
                    <mo data-mjx-texclass="CLOSE">)</mo>
                    <mi>h</mi>
                    <mi>i</mi>
                    <mi>d</mi>
                    <mi>v</mi>
                    <mi>i</mi>
                    <msup>
                      <mi>s</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mo stretchy="false">(</mo>
                        <mo stretchy="false">)</mo>
                        <mn>0</mn>
                      </mrow>
                    </msup>
                  </mrow>
                  <mrow data-mjx-texclass="INNER">
                    <mo data-mjx-texclass="OPEN">(</mo>
                    <mo>∣</mo>
                    <mo data-mjx-texclass="CLOSE">)</mo>
                    <mi>h</mi>
                    <mi>i</mi>
                    <mi>d</mi>
                    <mi>v</mi>
                    <mi>i</mi>
                    <msup>
                      <mi>s</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mo stretchy="false">(</mo>
                        <mo stretchy="false">)</mo>
                        <mn>1</mn>
                      </mrow>
                    </msup>
                  </mrow>
                  <msup>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">[</mo>
                      <mo data-mjx-texclass="CLOSE">]</mo>
                      <mi>v</mi>
                      <mi>i</mi>
                      <msup>
                        <mi>s</mi>
                        <mrow data-mjx-texclass="ORD">
                          <mo stretchy="false">(</mo>
                          <mo stretchy="false">)</mo>
                          <mn>0</mn>
                        </mrow>
                      </msup>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mtext>time </mtext>
                    </mrow>
                  </msup>
                  <msup>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">[</mo>
                      <mo data-mjx-texclass="CLOSE">]</mo>
                      <mi>v</mi>
                      <mi>i</mi>
                      <msup>
                        <mi>s</mi>
                        <mrow data-mjx-texclass="ORD">
                          <mo stretchy="false">(</mo>
                          <mo stretchy="false">)</mo>
                          <mn>1</mn>
                        </mrow>
                      </msup>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mtext>time </mtext>
                    </mrow>
                  </msup>
                </mrow>
              </math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(24)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <msup>
                  <mi>b</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mtext>time </mtext>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                </msup>
                <msup>
                  <mi>b</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mtext>time </mtext>
                  </mrow>
                </msup>
                <mo>=</mo>
                <mo>+</mo>
                <mi>ε</mi>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo>−</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <mi>v</mi>
                  <mi>i</mi>
                  <mi>v</mi>
                  <mi>i</mi>
                  <msup>
                    <mi>s</mi>
                    <mrow data-mjx-texclass="ORD">
                      <mo stretchy="false">(</mo>
                      <mo stretchy="false">)</mo>
                      <mn>0</mn>
                    </mrow>
                  </msup>
                  <msup>
                    <mi>s</mi>
                    <mrow data-mjx-texclass="ORD">
                      <mo stretchy="false">(</mo>
                      <mo stretchy="false">)</mo>
                      <mn>1</mn>
                    </mrow>
                  </msup>
                </mrow>
              </math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(25)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <msup>
                  <mi>c</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mrow data-mjx-texclass="ORD">
                      <mi data-mjx-auto-op="false">time</mi>
                    </mrow>
                    <mo>+</mo>
                    <mn>1</mn>
                  </mrow>
                </msup>
                <msup>
                  <mi>c</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mrow data-mjx-texclass="ORD">
                      <mi data-mjx-auto-op="false">time</mi>
                    </mrow>
                  </mrow>
                </msup>
                <mo>=</mo>
                <mo>+</mo>
                <mi>ε</mi>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo>−</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <mi>P</mi>
                  <mi>P</mi>
                  <mrow data-mjx-texclass="INNER">
                    <mo data-mjx-texclass="OPEN">(</mo>
                    <mo>∣</mo>
                    <mo data-mjx-texclass="CLOSE">)</mo>
                    <mi>h</mi>
                    <mi>i</mi>
                    <mi>d</mi>
                    <mi>v</mi>
                    <mi>i</mi>
                    <msup>
                      <mi>s</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mo stretchy="false">(</mo>
                        <mo stretchy="false">)</mo>
                        <mn>0</mn>
                      </mrow>
                    </msup>
                  </mrow>
                  <mrow data-mjx-texclass="INNER">
                    <mo data-mjx-texclass="OPEN">(</mo>
                    <mo data-mjx-texclass="CLOSE">)</mo>
                    <mi>v</mi>
                    <mi>i</mi>
                    <msup>
                      <mi>s</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mo stretchy="false">(</mo>
                        <mo stretchy="false">)</mo>
                        <mn>1</mn>
                      </mrow>
                    </msup>
                  </mrow>
                </mrow>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">In these equations, 'time' denotes the magnitude of iterative learning steps. This process continues until the parameter values have been adjusted to yield a feature representation that is more abstract and representable than that produced by the lower layer. The DBN model utilized these equations for efficient feature extraction. The training method of the coot optimization process allows the proposed detection model to bypass specialized capabilities, producing a more sophisticated set of features overall, thereby enhancing the effectiveness of the detection system.</p>
          
            <sec disp-level="level3">
              
                <title>3.4.1 Training dbn model with coa</title>
              
              <p style="text-align: justify">The optimization of the DBN model is achieved through the application of the COA, an innovative optimization technique derived from the behavioral patterns of coots, as detailed in (<xref ref-type="bibr" rid="ref_9">Houssein et al., 2022</xref>). The COA emulates the synchronized activities of a coot flock on water, encompassing behaviors such as aimless roving, chain walking, position adjustments relative to leaders, and guiding the flock to optimal areas. These behaviors are mathematically modeled to enable their implementation in the optimization process.</p><p style="text-align: justify">Initially, a population of coots is generated randomly, given a problem of D-dimensions. Utilizing Eq. (26), a population (<italic>N</italic>)<italic> </italic>of coots is created within the constraints of upper boundaries (UB):</p>
              
                <disp-formula>
                  <label>(26)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>PosCoot</mi>
                    <mi>i</mi>
                    <mi>random</mi>
                    <mi>D</mi>
                    <mi>U</mi>
                    <mi>B</mi>
                    <mi>L</mi>
                    <mi>B</mi>
                    <mi>L</mi>
                    <mi>B</mi>
                    <mi>i</mi>
                    <mi>N</mi>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo>,</mo>
                    <mo stretchy="false">)</mo>
                    <mo>×</mo>
                    <mo stretchy="false">(</mo>
                    <mo>−</mo>
                    <mo stretchy="false">)</mo>
                    <mo>+</mo>
                    <mo>,</mo>
                    <mo>=</mo>
                    <mo>,</mo>
                    <mo>,</mo>
                    <mo>…</mo>
                    <mo>,</mo>
                    <mn>1</mn>
                    <mn>1</mn>
                    <mn>2</mn>
                  </math>
                </disp-formula>
              
              <p style="text-align: justify">This equation ensures that the initial positions of the coots in the multidimensional space are randomly determined, respecting the predefined upper boundaries. The fitness of this initial population is assessed as per Eq. (27):</p>
              
                <disp-formula>
                  <label>(27)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>F</mi>
                    <mi>i</mi>
                    <mi>PosCoot</mi>
                    <mi>i</mi>
                    <mi>i</mi>
                    <mi>N</mi>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo stretchy="false">)</mo>
                    <mo>,</mo>
                    <mo>=</mo>
                    <mo>,</mo>
                    <mo>,</mo>
                    <mo>…</mo>
                    <mo>,</mo>
                    <mtext> Fitness </mtext>
                    <mn>1</mn>
                    <mn>2</mn>
                  </math>
                </disp-formula>
              
              <p style="text-align: justify">Subsequently, random locations for the coots are generated using Eq. (28). This is followed by recalculating these locations as per Eq. (29):</p>
              
                <disp-formula>
                  <label>(28)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>R</mi>
                    <mi>random</mi>
                    <mi>D</mi>
                    <mi>U</mi>
                    <mi>B</mi>
                    <mi>L</mi>
                    <mi>B</mi>
                    <mi>L</mi>
                    <mi>B</mi>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo>,</mo>
                    <mo stretchy="false">)</mo>
                    <mo>×</mo>
                    <mo stretchy="false">(</mo>
                    <mo>−</mo>
                    <mo stretchy="false">)</mo>
                    <mo>+</mo>
                    <mn>1</mn>
                  </math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(29)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>PosCoot</mi>
                    <mi>i</mi>
                    <mi>PosCoot</mi>
                    <mi>i</mi>
                    <mi>A</mi>
                    <mi>R</mi>
                    <mi>N</mi>
                    <mi>R</mi>
                    <mi>PosCoot</mi>
                    <mi>i</mi>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>+</mo>
                    <mo>×</mo>
                    <mo>×</mo>
                    <mo stretchy="false">(</mo>
                    <mo>−</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo stretchy="false">)</mo>
                    <mn>2</mn>
                  </math>
                </disp-formula>
              
              <p style="text-align: justify">In Eq. (29), RN2 denotes a random number between 0 and 1.</p>
              
                <disp-formula>
                  <label>(30)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>A</mi>
                    <mi>B</mi>
                    <mi>i</mi>
                    <mi>I</mi>
                    <mi>t</mi>
                    <mi>e</mi>
                    <mi>r</mi>
                    <mi>M</mi>
                    <mi>a</mi>
                    <mi>x</mi>
                    <mo>=</mo>
                    <mo>−</mo>
                    <mo>,</mo>
                    <mo>=</mo>
                    <mo>−</mo>
                    <mo>=</mo>
                    <mo>,</mo>
                    <mo>,</mo>
                    <mo>…</mo>
                    <mo>,</mo>
                    <mn>1</mn>
                    <mn>2</mn>
                    <mn>1</mn>
                    <mn>2</mn>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo stretchy="false">(</mo>
                      <mo stretchy="false">)</mo>
                      <mo>×</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mi>T</mi>
                      <mi>i</mi>
                      <mfrac>
                        <mn>1</mn>
                        <mrow>
                          <mi>I</mi>
                          <mi>t</mi>
                          <mi>e</mi>
                          <mi>r</mi>
                          <mi>M</mi>
                          <mi>a</mi>
                          <mi>x</mi>
                        </mrow>
                      </mfrac>
                    </mrow>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo stretchy="false">(</mo>
                      <mo stretchy="false">)</mo>
                      <mo>×</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mi>T</mi>
                      <mi>i</mi>
                      <mfrac>
                        <mn>1</mn>
                        <mrow>
                          <mi>I</mi>
                          <mi>t</mi>
                          <mi>e</mi>
                          <mi>r</mi>
                          <mi>M</mi>
                          <mi>a</mi>
                          <mi>x</mi>
                        </mrow>
                      </mfrac>
                    </mrow>
                  </math>
                </disp-formula>
              
              <p style="text-align: justify">During the current iteration <italic>T</italic>(<italic>i</italic>), the maximum number of iterations <italic>IterMax</italic> is used in Eq. (30). Chain movement, a critical behavior of coots, is simulated as one coot moving towards another, as described in Eq. (31):</p>
              
                <disp-formula>
                  <label>(31)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>PosCoot</mi>
                    <mi>i</mi>
                    <mi>PosCoot</mi>
                    <mi>i</mi>
                    <mi>PosCoot</mi>
                    <mi>i</mi>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo>×</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">(</mo>
                    <mo>−</mo>
                    <mo stretchy="false">)</mo>
                    <mo>+</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo stretchy="false">)</mo>
                    <mn>0.5</mn>
                    <mn>1</mn>
                  </math>
                </disp-formula>
              
              <p>Leader selection among the coots is governed by Eq. (32):</p>
              
                <disp-formula>
                  <label>(32)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msub>
                      <mi>L</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mtext>ind </mtext>
                      </mrow>
                    </msub>
                    <mo>=</mo>
                    <mo>+</mo>
                    <mn>1</mn>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mi>i</mi>
                      <mi>M</mi>
                      <mi>O</mi>
                      <mi>D</mi>
                      <mtext> </mtext>
                      <msub>
                        <mi>N</mi>
                        <mi>L</mi>
                      </msub>
                    </mrow>
                  </math>
                </disp-formula>
              
              <p style="text-align: justify">In Eq. (32), <italic>N<sub>L</sub></italic> represents the parameterized total number of leaders, while Lind signifies the index of a leader. The probability <italic>p</italic> is also defined in this context. The ranking of leaders is then conducted as per the criteria in Eq. (33):</p>
              
                <disp-formula>
                  <label>(33)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>L</mi>
                    <mi>e</mi>
                    <mi>a</mi>
                    <mi>d</mi>
                    <mi>e</mi>
                    <mi>r</mi>
                    <mi>P</mi>
                    <mi>o</mi>
                    <mi>s</mi>
                    <mi>i</mi>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">{</mo>
                      <mo data-mjx-texclass="CLOSE">}</mo>
                      <mtable columnalign="left" columnspacing="1em" rowspacing="4pt">
                        <mtr>
                          <mtd>
                            <mi>B</mi>
                            <mi>R</mi>
                            <mi>cos</mi>
                            <mi>R</mi>
                            <mi>π</mi>
                            <mi>g</mi>
                            <mi>B</mi>
                            <mi>e</mi>
                            <mi>s</mi>
                            <mi>t</mi>
                            <mi>L</mi>
                            <mi>e</mi>
                            <mi>a</mi>
                            <mi>d</mi>
                            <mi>e</mi>
                            <mi>r</mi>
                            <mi>P</mi>
                            <mi>o</mi>
                            <mi>s</mi>
                            <mi>i</mi>
                            <mi>g</mi>
                            <mi>B</mi>
                            <mi>e</mi>
                            <mi>s</mi>
                            <mi>t</mi>
                            <mi>R</mi>
                            <mi>P</mi>
                            <mo>×</mo>
                            <mo>×</mo>
                            <mo data-mjx-texclass="NONE">⁡</mo>
                            <mo stretchy="false">(</mo>
                            <mo stretchy="false">)</mo>
                            <mo>×</mo>
                            <mo stretchy="false">(</mo>
                            <mo>−</mo>
                            <mo stretchy="false">(</mo>
                            <mo stretchy="false">)</mo>
                            <mo>+</mo>
                            <mo>⋯</mo>
                            <mo>&lt;</mo>
                            <mo stretchy="false">)</mo>
                            <mn>3</mn>
                            <mn>2</mn>
                            <mn>4</mn>
                          </mtd>
                        </mtr>
                        <mtr>
                          <mtd>
                            <mi>B</mi>
                            <mi>R</mi>
                            <mi>cos</mi>
                            <mi>R</mi>
                            <mi>π</mi>
                            <mi>g</mi>
                            <mi>B</mi>
                            <mi>e</mi>
                            <mi>s</mi>
                            <mi>t</mi>
                            <mi>L</mi>
                            <mi>e</mi>
                            <mi>a</mi>
                            <mi>d</mi>
                            <mi>e</mi>
                            <mi>r</mi>
                            <mi>P</mi>
                            <mi>o</mi>
                            <mi>s</mi>
                            <mi>i</mi>
                            <mi>g</mi>
                            <mi>B</mi>
                            <mi>e</mi>
                            <mi>s</mi>
                            <mi>t</mi>
                            <mi>R</mi>
                            <mi>P</mi>
                            <mo>×</mo>
                            <mo>×</mo>
                            <mo data-mjx-texclass="NONE">⁡</mo>
                            <mo stretchy="false">(</mo>
                            <mo stretchy="false">)</mo>
                            <mo>×</mo>
                            <mo stretchy="false">(</mo>
                            <mo>−</mo>
                            <mo stretchy="false">(</mo>
                            <mo stretchy="false">)</mo>
                            <mo>+</mo>
                            <mo>⋯</mo>
                            <mo>≥</mo>
                            <mn>3</mn>
                            <mn>2</mn>
                            <mn>4</mn>
                          </mtd>
                        </mtr>
                      </mtable>
                    </mrow>
                  </math>
                </disp-formula>
              
              <p style="text-align: justify">Eq. (33) incorporates random integers <italic>R3</italic> and <italic>R4</italic> within the range [0, 1], the current global best <italic>gBest</italic>, and a constant value of 3.14. The pseudocode for the COA is detailed in Algorithm 1.</p>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Procedure 1. Pseudocode of the COA</p></td></tr><tr><td colspan="1" rowspan="1"><p>1.<span style="font-family: Times New Roman"> </span>Initialize the first population of coots randomly by Eq. (26)</p><p>2.<span style="font-family: Times New Roman"> </span>Initialize the termination criteria,probability p,number of leaders and number of coots</p><p>3.<span style="font-family: Times New Roman"> </span>Ncoot=Number of coots-Number of leaders</p><p>4.<span style="font-family: Times New Roman"> </span>Random selection of leaders from the coots</p><p>5.<span style="font-family: Times New Roman"> </span>Calculate the fitness of coots and leaders</p><p>6.<span style="font-family: Times New Roman"> </span>Find the best coot or leader the global optimum while the end criterion is not satisfied</p><p>7.<span style="font-family: Times New Roman"> </span>Calculate A,B parameters by Eq. (30)</p><p>8. If rand &lt;P</p><p>9.<span style="font-family: Times New Roman"> </span>R,R1,and R3 are random vectors along the dimensions of the problem</p><p>10.<span style="font-family: Times New Roman"> </span>Else</p><p>11.<span style="font-family: Times New Roman"> </span>R,R1,and R3 are random number</p><p>12.<span style="font-family: Times New Roman"> </span>End</p><p>13.<span style="font-family: Times New Roman"> </span>For i=1 to the number of the coot</p><p>14.<span style="font-family: Times New Roman"> </span>Calculate the parameter of K by Eq. (32)</p><p>15.<span style="font-family: Times New Roman"> </span>If rand&gt;0.5</p><p>16.<span style="font-family: Times New Roman"> </span>Update the position of the coot by Eq. (33)</p><p>17.<span style="font-family: Times New Roman"> </span>Else</p><p>18.<span style="font-family: Times New Roman"> </span>If rand &lt;0.5 i~=1</p><p>19.<span style="font-family: Times New Roman"> </span>Update the position of the coot by Eq. (33)</p><p>20.<span style="font-family: Times New Roman"> </span>Else</p><p>21.<span style="font-family: Times New Roman"> </span>Update the position of the coot by Eq. (31)</p><p>22.<span style="font-family: Times New Roman"> </span>End</p><p>23.<span style="font-family: Times New Roman"> </span>End</p><p>24.<span style="font-family: Times New Roman"> </span>Calculate the fitness of coot</p><p>25.<span style="font-family: Times New Roman"> </span>If the fitness of coot</p><p>26.<span style="font-family: Times New Roman"> </span>If the fitness of coot &lt; the fitness of leader (k)</p><p>27.<span style="font-family: Times New Roman"> </span>Temp=Leader(k);leader(k)=coot; coot=Temp;</p><p>28.<span style="font-family: Times New Roman"> </span>End</p><p>29.<span style="font-family: Times New Roman"> </span>For number of Leaders</p><p>30.<span style="font-family: Times New Roman"> </span>Update the position of the leader using the rules given in Eq. (33)</p><p>31.<span style="font-family: Times New Roman"> </span>If the fitness of leader</p><p>32.<span style="font-family: Times New Roman"> </span>Temp=gBest; gBest=leader; leader=Temp;(update global optimum)</p><p>33.<span style="font-family: Times New Roman"> </span>End</p><p>34.<span style="font-family: Times New Roman"> </span>End</p><p>35.<span style="font-family: Times New Roman"> </span>Iter=iter+1;</p><p>36.<span style="font-family: Times New Roman"> </span>End</p><p>37.<span style="font-family: Times New Roman"> </span>Postprocessor results</p></td></tr></tbody></table>
            </sec>
          
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>4. Results and discussion</title>
      <p style="text-align: justify">The experimental studies were conducted on a system equipped with an Intel(R) Core (TM) i5-7200u processor, ranging from 2.50 to 2.7 GHz, complemented by 16 GB RAM and an 8 GB graphics card, utilizing MATLAB 2020b.</p>
      
        <sec disp-level="level2">
          
            <title>4.1. Segmentation analysis</title>
          
          <p style="text-align: justify">In evaluating the segmentation performance, metrics such as accuracy (ACC), specificity (SPE), and sensitivity (SEN), alongside the Jaccard and Dice similarity coefficients (JSC and DSC, respectively), were employed. These metrics are pivotal in assessing the effectiveness of the segmentation method. The true positive (TP) rate was determined by identifying the overlapping area between the segmented region and the ground truth, denoted as <italic>y</italic> and <italic>x</italic>, respectively. This intersection is represented mathematically as <italic>TP = yx</italic>. Conversely, the false positive (FP) rate was defined as <italic>FP = yx</italic>, which represents the segmented region in <italic>x</italic> not corresponding to <italic>y</italic>. The false negative (FN) rate, indicating the proportion of incorrect predictions, was calculated as <italic>FN = yx</italic>. The true negative (TN) set, representing the complement of the segmented and ground truth regions, is described as <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>T</mi>
    <mi>N</mi>
    <mo>=</mo>
    <mo>∩</mo>
    <mrow data-mjx-texclass="ORD">
      <mover>
        <mi>y</mi>
        <mo stretchy="false">¯</mo>
      </mover>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mover>
        <mi>x</mi>
        <mo stretchy="false">¯</mo>
      </mover>
    </mrow>
  </math>
</inline-formula>. The following equations precisely articulate the calculations of the aforementioned metrics:</p><p style="text-align: justify">These metrics were integral in quantitatively assessing the segmentation's accuracy and reliability, thereby providing a comprehensive understanding of the model's performance in skin lesion segmentation.</p>
          
            <disp-formula>
              <label>(34)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>A</mi>
                <mi>C</mi>
                <mi>C</mi>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                    <mi>T</mi>
                    <mi>N</mi>
                    <mo>+</mo>
                  </mrow>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                    <mi>T</mi>
                    <mi>N</mi>
                    <mi>F</mi>
                    <mi>P</mi>
                    <mi>F</mi>
                    <mi>N</mi>
                    <mo>+</mo>
                    <mo>+</mo>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(35)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>D</mi>
                <mi>S</mi>
                <mi>C</mi>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mn>2.</mn>
                    <mi>T</mi>
                    <mi>P</mi>
                  </mrow>
                  <mrow>
                    <mn>2.</mn>
                    <mi>T</mi>
                    <mi>P</mi>
                    <mi>F</mi>
                    <mi>P</mi>
                    <mi>F</mi>
                    <mi>N</mi>
                    <mo>+</mo>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(36)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>J</mi>
                <mi>S</mi>
                <mi>C</mi>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                  </mrow>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                    <mi>F</mi>
                    <mi>P</mi>
                    <mi>F</mi>
                    <mi>N</mi>
                    <mo>+</mo>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(37)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>S</mi>
                <mi>E</mi>
                <mi>N</mi>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                  </mrow>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                    <mi>F</mi>
                    <mi>N</mi>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(38)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>S</mi>
                <mi>P</mi>
                <mi>E</mi>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mi>T</mi>
                    <mi>N</mi>
                  </mrow>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                    <mi>F</mi>
                    <mi>P</mi>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>4.2. Performance analysis of segmentation models on three datasets</title>
          
          <p style="text-align: justify">The segmentation performance of the proposed model was evaluated across three different datasets, with the findings detailed in <xref ref-type="table" rid="table_1">Table 1</xref>, <xref ref-type="table" rid="table_2">Table 2</xref>, and <xref ref-type="table" rid="table_3">Table 3</xref>.</p><p style="text-align: justify"><xref ref-type="table" rid="table_1">Table 1</xref> shows the validation investigation of the proposed model on Dataset 1. In the analysis of Dataset 1, the performance of the proposed model was evaluated using various statistical measures. For Image 1, it was observed that the dice coefficient reached 84.90, the Jaccard index was calculated at 76.5, sensitivity was found to be 82.5, specificity stood at 97.5, and accuracy was measured at 93.40. The analysis of subsequent images in the dataset revealed variations in these metrics, indicating the model's differentiated performance across various images. Specifically, Image 2 demonstrated a dice coefficient of 76.27, Jaccard index of 61.64, sensitivity of 67.15, specificity of 97.24, and accuracy of 90.14. In contrast, Image 3 showed improved outcomes with a dice coefficient of 87.08, Jaccard index of 77.11, sensitivity of 85.40, specificity of 96.69, and accuracy of 94.03. Continuing this trend, Image 4 exhibited a dice coefficient of 87.80, Jaccard index of 78.20, sensitivity of 81.60, specificity of 98.30, and accuracy of 93.60. Image 5 further reflected the model's robustness with a dice coefficient of 88.06, Jaccard index of 82.03, sensitivity of 89.80, specificity of 96.44, and accuracy of 94.70.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>Validation analysis of the proposed model on Dataset 1</caption>
              <abstract/>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Image</p></td><td colspan="1" rowspan="1"><p>Dice</p></td><td colspan="1" rowspan="1"><p>Jaccard</p></td><td colspan="1" rowspan="1"><p>Sensitivity</p></td><td colspan="1" rowspan="1"><p>Specificity</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image 1</p></td><td colspan="1" rowspan="1"><p>84.90</p></td><td colspan="1" rowspan="1"><p>76.5</p></td><td colspan="1" rowspan="1"><p>82.5</p></td><td colspan="1" rowspan="1"><p>97.5</p></td><td colspan="1" rowspan="1"><p>93.40</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image 2</p></td><td colspan="1" rowspan="1"><p>76.27</p></td><td colspan="1" rowspan="1"><p>61.64</p></td><td colspan="1" rowspan="1"><p>67.15</p></td><td colspan="1" rowspan="1"><p>97.24</p></td><td colspan="1" rowspan="1"><p>90.14</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image 3</p></td><td colspan="1" rowspan="1"><p>87.08</p></td><td colspan="1" rowspan="1"><p>77.11</p></td><td colspan="1" rowspan="1"><p>85.40</p></td><td colspan="1" rowspan="1"><p>96.69</p></td><td colspan="1" rowspan="1"><p>94.03</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image 4</p></td><td colspan="1" rowspan="1"><p>87.80</p></td><td colspan="1" rowspan="1"><p>78.20</p></td><td colspan="1" rowspan="1"><p>81.60</p></td><td colspan="1" rowspan="1"><p>98.30</p></td><td colspan="1" rowspan="1"><p>93.60</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image 5</p></td><td colspan="1" rowspan="1"><p>88.06</p></td><td colspan="1" rowspan="1"><p>82.03</p></td><td colspan="1" rowspan="1"><p>89.80</p></td><td colspan="1" rowspan="1"><p>96.44</p></td><td colspan="1" rowspan="1"><p>94.70</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>These results demonstrate the model's varying degrees of accuracy in segmenting and identifying lesions in dermoscopic images across different cases within the dataset. The variation in metrics such as dice, Jaccard, sensitivity, specificity, and accuracy across different images is indicative of the model's adaptability and effectiveness in handling diverse image characteristics.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>Experimental analysis on Dataset 2</caption>
              <abstract/>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Image</p></td><td colspan="1" rowspan="1"><p>Dice</p></td><td colspan="1" rowspan="1"><p>Jaccard</p></td><td colspan="1" rowspan="1"><p>Sensitivity</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>Specificity</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image 1</p></td><td colspan="1" rowspan="1"><p>84.90</p></td><td colspan="1" rowspan="1"><p>76.50</p></td><td colspan="1" rowspan="1"><p>82.50</p></td><td colspan="1" rowspan="1"><p>93.40</p></td><td colspan="1" rowspan="1"><p>97.50</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image 2</p></td><td colspan="1" rowspan="1"><p>84.70</p></td><td colspan="1" rowspan="1"><p>76.20</p></td><td colspan="1" rowspan="1"><p>82.00</p></td><td colspan="1" rowspan="1"><p>93.20</p></td><td colspan="1" rowspan="1"><p>97.80</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image 3</p></td><td colspan="1" rowspan="1"><p>87.08</p></td><td colspan="1" rowspan="1"><p>77.11</p></td><td colspan="1" rowspan="1"><p>85.40</p></td><td colspan="1" rowspan="1"><p>94.03</p></td><td colspan="1" rowspan="1"><p>96.69</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image 4</p></td><td colspan="1" rowspan="1"><p>88.13</p></td><td colspan="1" rowspan="1"><p>79.54</p></td><td colspan="1" rowspan="1"><p>83.63</p></td><td colspan="1" rowspan="1"><p>92.99</p></td><td colspan="1" rowspan="1"><p>94.02</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image 5</p></td><td colspan="1" rowspan="1"><p>84.26</p></td><td colspan="1" rowspan="1"><p>74.81</p></td><td colspan="1" rowspan="1"><p>90.82</p></td><td colspan="1" rowspan="1"><p>93.39</p></td><td colspan="1" rowspan="1"><p>92.68</p></td></tr></tbody></table>
            </table-wrap>
          
          <p style="text-align: justify"><xref ref-type="table" rid="table_2">Table 2</xref> characterises the experimental analysis on Dataset 2. In the experimental analysis conducted on Dataset 2, the performance of the proposed model was systematically evaluated across multiple images. The analysis revealed that for Image 1, the dice score was recorded at 84.90, the Jaccard index was 76.50, sensitivity was 82.50, accuracy stood at 93.40, and specificity was 97.50. These metrics illustrate the model's effectiveness in accurately segmenting and classifying skin lesions. Subsequent images within the dataset displayed similar levels of performance, albeit with some variations. For instance, Image 2 demonstrated a dice score of 84.70 and a Jaccard index of 76.20, while Image 5 showed a dice score of 84.26 and a Jaccard index of 74.81. Notably, sensitivity, accuracy, and specificity also varied across the images, indicating the model's capacity to handle diverse image characteristics. In particular, Image 3 exhibited a dice score of 87.08 and a Jaccard index of 77.11, combined with a sensitivity of 85.40 and an accuracy of 94.03. Image 4, on the other hand, achieved a dice score of 88.13 and a Jaccard index of 79.54, highlighting the model's consistent performance in terms of segmentation accuracy. Overall, the results from Dataset 2 underscore the proposed model's capability in effectively segmenting and classifying dermoscopic images. The consistency observed in key metrics such as dice, Jaccard, sensitivity, specificity, and accuracy across different images further reinforces the model's applicability in clinical diagnostic settings.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>Validation analysis of the proposed model on Dataset 3</caption>
              <abstract/>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Image</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>Dice</p></td><td colspan="1" rowspan="1"><p>Jaccard</p></td><td colspan="1" rowspan="1"><p>Sensitivity</p></td><td colspan="1" rowspan="1"><p>Specificity</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image 1</p></td><td colspan="1" rowspan="1"><p>94.21</p></td><td colspan="1" rowspan="1"><p>87.63</p></td><td colspan="1" rowspan="1"><p>90.17</p></td><td colspan="1" rowspan="1"><p>83.34</p></td><td colspan="1" rowspan="1"><p>87.63</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image 2</p></td><td colspan="1" rowspan="1"><p>92.68</p></td><td colspan="1" rowspan="1"><p>93.91</p></td><td colspan="1" rowspan="1"><p>92.56</p></td><td colspan="1" rowspan="1"><p>88.72</p></td><td colspan="1" rowspan="1"><p>91.26</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image 3</p></td><td colspan="1" rowspan="1"><p>94.83</p></td><td colspan="1" rowspan="1"><p>94.57</p></td><td colspan="1" rowspan="1"><p>94.83</p></td><td colspan="1" rowspan="1"><p>89.33</p></td><td colspan="1" rowspan="1"><p>92.61</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image 4</p></td><td colspan="1" rowspan="1"><p>91.34</p></td><td colspan="1" rowspan="1"><p>92.04</p></td><td colspan="1" rowspan="1"><p>91.29</p></td><td colspan="1" rowspan="1"><p>85.27</p></td><td colspan="1" rowspan="1"><p>88.18</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image 5</p></td><td colspan="1" rowspan="1"><p>92.12</p></td><td colspan="1" rowspan="1"><p>92.44</p></td><td colspan="1" rowspan="1"><p>92.36</p></td><td colspan="1" rowspan="1"><p>86.67</p></td><td colspan="1" rowspan="1"><p>89.24</p></td></tr></tbody></table>
            </table-wrap>
          
          <p style="text-align: justify"><xref ref-type="table" rid="table_3">Table 3</xref> presents a comprehensive validation analysis of the proposed model on Dataset 3. In this analysis, the performance metrics for each image were carefully evaluated. For Image 1, it was observed that the dice coefficient achieved a value of 94.21, and the Jaccard index was 87.63. sensitivity was recorded at 90.17, with a corresponding specificity of 87.63. These metrics collectively indicate the model's high accuracy in lesion detection and segmentation. Subsequent images within this dataset exhibited similar levels of performance, albeit with variations indicative of the model's adaptability to different image characteristics. For example, Image 2 displayed a dice score of 92.68 and a Jaccard index of 93.91, along with sensitivity and specificity scores of 92.56 and 91.26, respectively. This trend continued with Image 3, where the dice score rose to 94.83, and the Jaccard index reached 94.57, further demonstrating the model's effectiveness. Image 4 and Image 5 maintained this pattern of high performance, with dice scores of 91.34 and 92.12, and Jaccard indices of 92.04 and 92.44, respectively. Sensitivity and specificity scores for these images were also within a high range, underscoring the robustness of the proposed model in accurately classifying and segmenting skin lesions in dermoscopic images. Overall, the results from Dataset 3 reinforce the proposed model's proficiency in handling diverse dermoscopic images. The consistency in high scores across key metrics such as accuracy, dice, Jaccard, sensitivity, and specificity across different images further cements the model's potential for clinical application in the accurate diagnosis of skin lesions.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>4.3. Experiment on classification results</title>
          
          <p>To assess the performance of the proposed model in classification, various metrics were evaluated. These included accuracy, sensitivity, specificity, precision, false positive rate (FPR), false negative rate (FNR), negative predictive value (NPV), false discovery rate (FDR), F1-score, and MCC.</p><p>a. Accuracy: Defined as the ratio of correctly predicted observations to the total number of observations (Eq. (39)). This metric is crucial in determining the overall effectiveness of the model.</p>
          
            <disp-formula>
              <label>(39)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <msup>
                  <mi>T</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mtext>accuracy </mtext>
                  </mrow>
                </msup>
                <mo>=</mo>
                <mfrac>
                  <mrow data-mjx-texclass="INNER">
                    <mo data-mjx-texclass="OPEN">(</mo>
                    <mo>+</mo>
                    <mo data-mjx-texclass="CLOSE">)</mo>
                    <msup>
                      <mi>Tr</mi>
                      <mi>p</mi>
                    </msup>
                    <msup>
                      <mi>Tr</mi>
                      <mi>n</mi>
                    </msup>
                  </mrow>
                  <mrow data-mjx-texclass="INNER">
                    <mo data-mjx-texclass="OPEN">(</mo>
                    <mo>+</mo>
                    <mo>+</mo>
                    <mo>+</mo>
                    <mo data-mjx-texclass="CLOSE">)</mo>
                    <msup>
                      <mi>Tr</mi>
                      <mi>p</mi>
                    </msup>
                    <msup>
                      <mi>Tr</mi>
                      <mi>n</mi>
                    </msup>
                    <msup>
                      <mi>a</mi>
                      <mi>p</mi>
                    </msup>
                    <msup>
                      <mi>a</mi>
                      <mi>n</mi>
                    </msup>
                    <mi>F</mi>
                    <mi>F</mi>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">b. Sensitivity: Measured as the number of true positives accurately identified (Eq. (40)). Sensitivity is indicative of the model's ability to correctly detect the presence of a condition.</p>
          
            <disp-formula>
              <label>(40)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>S</mi>
                <mi>e</mi>
                <mo>=</mo>
                <mfrac>
                  <msup>
                    <mi>Tr</mi>
                    <mi>p</mi>
                  </msup>
                  <mrow>
                    <msup>
                      <mi>Tr</mi>
                      <mi>p</mi>
                    </msup>
                    <msup>
                      <mi>a</mi>
                      <mi>n</mi>
                    </msup>
                    <mo>+</mo>
                    <mi>F</mi>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">c. Specificity: Calculated as the number of true negatives that are precisely determined (Eq. (41)). This metric assesses the model's accuracy in identifying the absence of a condition.</p>
          
            <disp-formula>
              <label>(41)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>s</mi>
                <mi>p</mi>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mi>T</mi>
                    <msup>
                      <mi>r</mi>
                      <mi>n</mi>
                    </msup>
                  </mrow>
                  <mrow>
                    <mi>F</mi>
                    <msup>
                      <mi>a</mi>
                      <mi>n</mi>
                    </msup>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">d. Precision: Represents the ratio of accurately predicted positive observations to the total number of positive predictions made (Eq. (42)). Precision is vital for understanding the model's exactness.</p>
          
            <disp-formula>
              <label>(42)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>P</mi>
                <mi>r</mi>
                <mo>=</mo>
                <mfrac>
                  <msup>
                    <mi>Tr</mi>
                    <mi>p</mi>
                  </msup>
                  <mrow>
                    <msup>
                      <mi>Tr</mi>
                      <mi>p</mi>
                    </msup>
                    <msup>
                      <mi>Tr</mi>
                      <mi>p</mi>
                    </msup>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">e. FPR: The ratio of false positive predictions to the total count of negative predictions (Eq. (43)). FPR provides insights into the instances where the model incorrectly predicts a positive outcome.</p>
          
            <disp-formula>
              <label>(43)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>F</mi>
                <mi>P</mi>
                <mi>R</mi>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mi>F</mi>
                    <msup>
                      <mi>a</mi>
                      <mi>P</mi>
                    </msup>
                  </mrow>
                  <mrow>
                    <mi>F</mi>
                    <mi>T</mi>
                    <msup>
                      <mi>a</mi>
                      <mi>P</mi>
                    </msup>
                    <msup>
                      <mi>r</mi>
                      <mi>n</mi>
                    </msup>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">f. FNR: The proportion of positive cases that result in negative test outcomes (Eq. (44)). FNR is crucial for assessing the model's ability to avoid missed diagnoses.</p>
          
            <disp-formula>
              <label>(44)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>F</mi>
                <mi>N</mi>
                <mi>R</mi>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mi>F</mi>
                    <msup>
                      <mi>a</mi>
                      <mi>n</mi>
                    </msup>
                  </mrow>
                  <mrow>
                    <mi>T</mi>
                    <mi>T</mi>
                    <msup>
                      <mi>r</mi>
                      <mi>n</mi>
                    </msup>
                    <msup>
                      <mi>r</mi>
                      <mi>p</mi>
                    </msup>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">g. NPV: The likelihood that subjects with a negative test result genuinely do not have the condition (Eq. (45)). NPV is essential in evaluating the model's reliability in negative predictions.</p>
          
            <disp-formula>
              <label>(45)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>N</mi>
                <mi>P</mi>
                <mi>V</mi>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mi>F</mi>
                    <msup>
                      <mi>a</mi>
                      <mi>n</mi>
                    </msup>
                  </mrow>
                  <mrow>
                    <mi>F</mi>
                    <mi>T</mi>
                    <msup>
                      <mi>a</mi>
                      <mi>n</mi>
                    </msup>
                    <msup>
                      <mi>r</mi>
                      <mi>n</mi>
                    </msup>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">h. FDR: Calculated as the number of false positives in all rejected hypotheses (Eq. (46)). FDR is critical for understanding the rate of error in the model's predictions.</p>
          
            <disp-formula>
              <label>(46)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>F</mi>
                <mi>D</mi>
                <mi>R</mi>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mi>F</mi>
                    <msup>
                      <mi>a</mi>
                      <mi>p</mi>
                    </msup>
                  </mrow>
                  <mrow>
                    <mi>F</mi>
                    <mi>T</mi>
                    <msup>
                      <mi>a</mi>
                      <mi>p</mi>
                    </msup>
                    <msup>
                      <mi>r</mi>
                      <mi>p</mi>
                    </msup>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          <p>i. F1-score: Defined as the harmonic mean between precision and recall (Eq. (47)). The F1-score is a crucial statistical measure for evaluating the model's performance balance.</p>
          
            <disp-formula>
              <label>(47)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>F</mi>
                <mi>s</mi>
                <mi>c</mi>
                <mi>o</mi>
                <mi>r</mi>
                <mi>e</mi>
                <mn>1</mn>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mi data-mjx-auto-op="false">Se</mi>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mi data-mjx-auto-op="false">Pr</mi>
                    </mrow>
                    <mo>.</mo>
                  </mrow>
                  <mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mi data-mjx-auto-op="false">Pr</mi>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mi data-mjx-auto-op="false">Se</mi>
                    </mrow>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          <p style="text-align: justify">j. MCC: A correlation coefficient computed using four values (Eq. (48)). MCC is a comprehensive measure that considers all four quadrants of the confusion matrix, providing a holistic view of the model's performance.</p>
          
            <disp-formula>
              <label>(48)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>M</mi>
                <mi>C</mi>
                <mi>C</mi>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mi>T</mi>
                    <mi>T</mi>
                    <mi>F</mi>
                    <mi>F</mi>
                    <msup>
                      <mi>r</mi>
                      <mi>p</mi>
                    </msup>
                    <msup>
                      <mi>r</mi>
                      <mi>n</mi>
                    </msup>
                    <msup>
                      <mi>a</mi>
                      <mi>p</mi>
                    </msup>
                    <msup>
                      <mi>a</mi>
                      <mi>n</mi>
                    </msup>
                    <mo>×</mo>
                    <mo>−</mo>
                    <mo>×</mo>
                  </mrow>
                  <msqrt>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo>+</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mi>T</mi>
                      <mi>F</mi>
                      <msup>
                        <mi>r</mi>
                        <mi>p</mi>
                      </msup>
                      <msup>
                        <mi>a</mi>
                        <mi>p</mi>
                      </msup>
                    </mrow>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo>+</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mi>T</mi>
                      <mi>F</mi>
                      <msup>
                        <mi>r</mi>
                        <mi>p</mi>
                      </msup>
                      <msup>
                        <mi>a</mi>
                        <mi>n</mi>
                      </msup>
                    </mrow>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo>+</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mi>T</mi>
                      <mi>F</mi>
                      <msup>
                        <mi>r</mi>
                        <mi>n</mi>
                      </msup>
                      <msup>
                        <mi>a</mi>
                        <mi>p</mi>
                      </msup>
                    </mrow>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo>+</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mi>T</mi>
                      <mi>F</mi>
                      <msup>
                        <mi>r</mi>
                        <mi>p</mi>
                      </msup>
                      <msup>
                        <mi>a</mi>
                        <mi>n</mi>
                      </msup>
                    </mrow>
                  </msqrt>
                </mfrac>
              </math>
            </disp-formula>
          
          <p>Each of these metrics plays an integral role in providing a detailed and nuanced understanding of the model's capabilities, strengths, and areas for improvement in the context of medical image analysis.</p><p style="text-align: justify">The performance of existing models such as MD2N (<xref ref-type="bibr" rid="ref_12">Kumar et al., 2023</xref>) and DCNN (<xref ref-type="bibr" rid="ref_17">Singh et al., 2023</xref>; <xref ref-type="bibr" rid="ref_16">Sethanan et al., 2023</xref>), alongside the proposed model, was scrutinized. The results are presented in <xref ref-type="table" rid="table_4">Table 4</xref>, <xref ref-type="table" rid="table_5">Table 5</xref> and <xref ref-type="table" rid="table_6">Table 6</xref>.</p>
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>Classifier analysis on Dataset 1</caption>
              <abstract/>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Measures</p></td><td colspan="1" rowspan="1"><p>MD2N</p></td><td colspan="1" rowspan="1"><p>DCNN</p></td><td colspan="1" rowspan="1"><p>DBN</p></td><td colspan="1" rowspan="1"><p>AE</p></td><td colspan="1" rowspan="1"><p>DBN-COA</p></td></tr><tr><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>0.92969</p></td><td colspan="1" rowspan="1"><p>0.91927</p></td><td colspan="1" rowspan="1"><p>0.95573</p></td><td colspan="1" rowspan="1"><p>0.94271</p></td><td colspan="1" rowspan="1"><p>0.965</p></td></tr><tr><td colspan="1" rowspan="1"><p>Sensitivity</p></td><td colspan="1" rowspan="1"><p>0.89063</p></td><td colspan="1" rowspan="1"><p>0.88021</p></td><td colspan="1" rowspan="1"><p>0.94792</p></td><td colspan="1" rowspan="1"><p>0.91667</p></td><td colspan="1" rowspan="1"><p>0.942</p></td></tr><tr><td colspan="1" rowspan="1"><p>Specificity</p></td><td colspan="1" rowspan="1"><p>0.96875</p></td><td colspan="1" rowspan="1"><p>0.95833</p></td><td colspan="1" rowspan="1"><p>0.96354</p></td><td colspan="1" rowspan="1"><p>0.96875</p></td><td colspan="1" rowspan="1"><p>0.988</p></td></tr><tr><td colspan="1" rowspan="1"><p>Precision</p></td><td colspan="1" rowspan="1"><p>0.9661</p></td><td colspan="1" rowspan="1"><p>0.9548</p></td><td colspan="1" rowspan="1"><p>0.96296</p></td><td colspan="1" rowspan="1"><p>0.96703</p></td><td colspan="1" rowspan="1"><p>0.988</p></td></tr><tr><td colspan="1" rowspan="1"><p>FPR</p></td><td colspan="1" rowspan="1"><p>0.03125</p></td><td colspan="1" rowspan="1"><p>0.041667</p></td><td colspan="1" rowspan="1"><p>0.036458</p></td><td colspan="1" rowspan="1"><p>0.03125</p></td><td colspan="1" rowspan="1"><p>0.0155</p></td></tr><tr><td colspan="1" rowspan="1"><p>FNR</p></td><td colspan="1" rowspan="1"><p>0.10938</p></td><td colspan="1" rowspan="1"><p>0.11979</p></td><td colspan="1" rowspan="1"><p>0.052083</p></td><td colspan="1" rowspan="1"><p>0.083333</p></td><td colspan="1" rowspan="1"><p>0.0523</p></td></tr><tr><td colspan="1" rowspan="1"><p>NPV</p></td><td colspan="1" rowspan="1"><p>0.96875</p></td><td colspan="1" rowspan="1"><p>0.95833</p></td><td colspan="1" rowspan="1"><p>0.96354</p></td><td colspan="1" rowspan="1"><p>0.96875</p></td><td colspan="1" rowspan="1"><p>0.988</p></td></tr><tr><td colspan="1" rowspan="1"><p>FDR</p></td><td colspan="1" rowspan="1"><p>0.033898</p></td><td colspan="1" rowspan="1"><p>0.045198</p></td><td colspan="1" rowspan="1"><p>0.037037</p></td><td colspan="1" rowspan="1"><p>0.032967</p></td><td colspan="1" rowspan="1"><p>0.0116</p></td></tr><tr><td colspan="1" rowspan="1"><p>F1-score</p></td><td colspan="1" rowspan="1"><p>0.92683</p></td><td colspan="1" rowspan="1"><p>0.91599</p></td><td colspan="1" rowspan="1"><p>0.95538</p></td><td colspan="1" rowspan="1"><p>0.94118</p></td><td colspan="1" rowspan="1"><p>0.9652</p></td></tr><tr><td colspan="1" rowspan="1"><p>MCC</p></td><td colspan="1" rowspan="1"><p>0.86201</p></td><td colspan="1" rowspan="1"><p>0.84111</p></td><td colspan="1" rowspan="1"><p>0.91157</p></td><td colspan="1" rowspan="1"><p>0.88662</p></td><td colspan="1" rowspan="1"><p>0.9321</p></td></tr></tbody></table>
            </table-wrap>
          
          <p style="text-align: justify"><xref ref-type="table" rid="table_4">Table 4</xref> in the study provides an insightful classifier analysis for Dataset 1, evaluating various models including MD2N, DCNN, DBN, AE, and DBN-COA. The evaluation encompassed several key metrics. In the analysis of Accuracy, it was observed that the MD2N model achieved a score of 0.92969, while the DCNN model recorded 0.91927, and the DBN model reached 0.95573. The AE model demonstrated an accuracy rate of 0.94271, with the DBN-COA model surpassing all with a score of 0.96615. When assessing sensitivity, the MD2N model was found to have a rate of 0.89063, and the DCNN model's rate stood at 0.88021. The DBN model achieved 0.94792, paralleled by the AE model at 0.91667, and the DBN-COA model also reached 0.94792. Specificity calculations revealed that the MD2N model achieved 0.96875, the DCNN model attained 0.95833, and the DBN model scored 0.96354. The AE model's specificity was noted as 0.96875, with the DBN-COA model achieving the highest at 0.98438. In precision, the MD2N model scored 0.9661, and the DCNN model attained 0.9548. The DBN model reached 0.96296, closely followed by the AE model at 0.96703, and the DBN-COA model achieved 0.98378. The FPR analysis showed the MD2N model with 0.03125, the DCNN model with 0.041667, and the DBN model with 0.036458. The AE model recorded an FPR of 0.03125, while the DBN-COA model had a lower rate of 0.015625. In the FNR metric, the MD2N model recorded 0.10938, the DCNN model 0.11979, and the DBN model 0.052083. The AE model scored 0.083333, closely followed by the DBN-COA model at 0.052083. For the NPV, the MD2N model attained 0.96875, the DCNN model reached 0.95833, and the DBN model scored 0.96354. The AE model had an NPV of 0.96875, with the DBN-COA model achieving 0.98438. In the FDR, the MD2N model scored 0.033898, the DCNN model 0.045198, and the DBN model 0.037037. The AE model recorded an FDR of 0.032967, and the DBN-COA model had a rate of 0.016216. The F1-score calculations showed the MD2N model with 0.92683, the DCNN model with 0.91599, and the DBN model with 0.95538. The AE model achieved 0.94118, and the DBN-COA model excelled with 0.96552. Finally, in the MCC, the MD2N model achieved 0.86201, the DCNN model 0.84111, and the DBN model 0.91157. The AE model recorded 0.88662, with the DBN-COA model reaching the highest at 0.93291. These results from Dataset 1 demonstrate the varying degrees of efficacy of the different models, with the DBN-COA model generally showing superior performance across most metrics.</p>
          
            <table-wrap id="table_5">
              <label>Table 5</label>
              <caption>Analysis of different models on Dataset 2</caption>
              <abstract/>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Measures</p></td><td colspan="1" rowspan="1"><p>MD2N</p></td><td colspan="1" rowspan="1"><p>DCNN</p></td><td colspan="1" rowspan="1"><p>DBN</p></td><td colspan="1" rowspan="1"><p>AE</p></td><td colspan="1" rowspan="1"><p>DBN-COA</p></td></tr><tr><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>0.95517</p></td><td colspan="1" rowspan="1"><p>0.94828</p></td><td colspan="1" rowspan="1"><p>0.95172</p></td><td colspan="1" rowspan="1"><p>0.95862</p></td><td colspan="1" rowspan="1"><p>0.9786</p></td></tr><tr><td colspan="1" rowspan="1"><p>Sensitivity</p></td><td colspan="1" rowspan="1"><p>0.94483</p></td><td colspan="1" rowspan="1"><p>0.92414</p></td><td colspan="1" rowspan="1"><p>0.92414</p></td><td colspan="1" rowspan="1"><p>0.93103</p></td><td colspan="1" rowspan="1"><p>0.9741</p></td></tr><tr><td colspan="1" rowspan="1"><p>Specificity</p></td><td colspan="1" rowspan="1"><p>0.96552</p></td><td colspan="1" rowspan="1"><p>0.97241</p></td><td colspan="1" rowspan="1"><p>0.97931</p></td><td colspan="1" rowspan="1"><p>0.98621</p></td><td colspan="1" rowspan="1"><p>0.9731</p></td></tr><tr><td colspan="1" rowspan="1"><p>Precision</p></td><td colspan="1" rowspan="1"><p>0.96479</p></td><td colspan="1" rowspan="1"><p>0.97101</p></td><td colspan="1" rowspan="1"><p>0.9781</p></td><td colspan="1" rowspan="1"><p>0.9854</p></td><td colspan="1" rowspan="1"><p>0.97917</p></td></tr><tr><td colspan="1" rowspan="1"><p>FPR</p></td><td colspan="1" rowspan="1"><p>0.034483</p></td><td colspan="1" rowspan="1"><p>0.027586</p></td><td colspan="1" rowspan="1"><p>0.02069</p></td><td colspan="1" rowspan="1"><p>0.013793</p></td><td colspan="1" rowspan="1"><p>0.0269</p></td></tr><tr><td colspan="1" rowspan="1"><p>FNR</p></td><td colspan="1" rowspan="1"><p>0.055172</p></td><td colspan="1" rowspan="1"><p>0.075862</p></td><td colspan="1" rowspan="1"><p>0.075862</p></td><td colspan="1" rowspan="1"><p>0.068966</p></td><td colspan="1" rowspan="1"><p>0.0286</p></td></tr><tr><td colspan="1" rowspan="1"><p>NPV</p></td><td colspan="1" rowspan="1"><p>0.96552</p></td><td colspan="1" rowspan="1"><p>0.97241</p></td><td colspan="1" rowspan="1"><p>0.97931</p></td><td colspan="1" rowspan="1"><p>0.98621</p></td><td colspan="1" rowspan="1"><p>0.9731</p></td></tr><tr><td colspan="1" rowspan="1"><p>FDR</p></td><td colspan="1" rowspan="1"><p>0.035211</p></td><td colspan="1" rowspan="1"><p>0.028986</p></td><td colspan="1" rowspan="1"><p>0.021898</p></td><td colspan="1" rowspan="1"><p>0.014599</p></td><td colspan="1" rowspan="1"><p>0.0233</p></td></tr><tr><td colspan="1" rowspan="1"><p>F1-score</p></td><td colspan="1" rowspan="1"><p>0.9547</p></td><td colspan="1" rowspan="1"><p>0.947</p></td><td colspan="1" rowspan="1"><p>0.95035</p></td><td colspan="1" rowspan="1"><p>0.95745</p></td><td colspan="1" rowspan="1"><p>0.9778</p></td></tr><tr><td colspan="1" rowspan="1"><p>MCC</p></td><td colspan="1" rowspan="1"><p>0.91054</p></td><td colspan="1" rowspan="1"><p>0.8976</p></td><td colspan="1" rowspan="1"><p>0.90483</p></td><td colspan="1" rowspan="1"><p>0.91864</p></td><td colspan="1" rowspan="1"><p>0.9575</p></td></tr></tbody></table>
            </table-wrap>
          
          <p style="text-align: justify"><xref ref-type="table" rid="table_5">Table 5</xref> in the study delineates the analysis of various models' performance on Dataset 2, employing a range of metrics to evaluate their effectiveness. The accuracy metric revealed that the MD2N model achieved a score of 0.95517, while the DCNN model recorded 0.94828, and the DBN model reached 0.95172. The AE model demonstrated an accuracy rate of 0.95862, with the DBN-COA model surpassing all with a score of 0.97586. In terms of sensitivity, the MD2N model was found to have a rate of 0.94483, and the DCNN model's rate stood at 0.92414. The DBN model achieved 0.92414, paralleled by the AE model at 0.93103, and the DBN-COA model also reached 0.97241. Specificity calculations revealed that the MD2N model achieved 0.96552, the DCNN model attained 0.97241, and the DBN model scored 0.97931. The AE model's specificity was noted as 0.98621, with the DBN-COA model achieving the highest at 0.97931. In precision, the MD2N model scored 0.96479, and the DCNN model attained 0.97101. The DBN model reached 0.9781, closely followed by the AE model at 0.9854, and the DBN-COA model achieved 0.97917. The FPR analysis showed the MD2N model with 0.034483, the DCNN model with 0.027586, and the DBN model with 0.02069. The AE model recorded an FPR of 0.013793, while the DBN-COA model had a lower rate of 0.02069. In the FNR metric, the MD2N model recorded 0.055172, the DCNN model 0.075862, and the DBN model 0.075862. The AE model scored 0.068966, closely followed by the DBN-COA model at 0.027586. For the NPV, the MD2N model attained 0.96552, the DCNN model reached 0.97241, and the DBN model scored 0.97931. The AE model had an NPV of 0.98621, with the DBN-COA model achieving 0.97931. In the FDR, the MD2N model scored 0.035211, the DCNN model 0.028986, and the DBN model 0.021898. The AE model recorded an FDR of 0.014599, and the DBN-COA model had a rate of 0.020833. The F1-score calculations showed the MD2N model with 0.9547, the DCNN model with 0.947, and the DBN model with 0.95035. The AE model achieved 0.95745, and the DBN-COA model excelled with 0.97578. Finally, in the MCC, the MD2N model achieved 0.91054, the DCNN model 0.8976, and the DBN model 0.90483. The AE model recorded 0.91864, with the DBN-COA model reaching the highest at 0.95175. These results from Dataset 2 demonstrate the varying degrees of efficacy of the different models, with the DBN-COA model generally showing superior performance across most metrics.</p><p style="text-align: justify"><xref ref-type="table" rid="table_6">Table 6</xref> presents a comparative analysis of various models on Dataset 3, employing a multitude of metrics to ascertain their performance efficacy. The accuracy metric revealed that the MD2N model achieved a score of 0.91467, the DCNN model recorded 0.89069, and the DBN model reached 0.91733. The AE model demonstrated an accuracy rate of 0.91467, with the DBN-COA model surpassing all with a score of 0.9466. In terms of FDR, the MD2N model recorded a rate of 0.13559, the DCNN model 0.1791, and the DBN model 0.14583. The AE model had an FDR of 0.15625, while the DBN-COA model achieved a lower rate of 0.0769. The F1-score calculations showed the MD2N model with 0.76119, the DCNN model with 0.77465, and the DBN model with 0.66667. The AE model achieved 0.77698, and the DBN-COA model excelled with 0.85714. Sensitivity assessments indicated that the MD2N model achieved 0.68, the DCNN model 0.73333, and the AE model 0.54667. The DBN model scored 0.72, and the DBN-COA model attained the highest at 0.8. Specificity analysis showed the MD2N model with 0.97333, the DCNN model with 0.96, and the AE model with 0.97678. The DBN model reached 0.966, and the DBN-COA model achieved 0.983. In precision, the MD2N model scored 0.86441, and the DCNN model attained 0.8209. The DBN model reached 0.85425, closely followed by the AE model at 0.843, and the DBN-COA model achieved 0.923. The FPR for the MD2N model was 0.026667, the DCNN model 0.04, and the AE model 0.023333. The DBN model recorded an FPR of 0.0333, while the DBN-COA model had a lower rate of 0.0166. In the FNR metric, the MD2N model recorded 0.32, the DCNN model 0.26667, and the AE model 0.45333. The DBN model scored 0.28, closely followed by the DBN-COA model at 0.2. For the NPV, the MD2N model attained 0.97333, the DCNN model reached 0.96, and the AE model scored 0.97667. The DBN model had an NPV of 0.96667, with the DBN-COA model achieving 0.983. Finally, in the MCC, the MD2N model achieved 0.71772, the DCNN model 0.72397, and the AE model 0.62658. The DBN model recorded 0.73007, with the DBN-COA model reaching the highest at 0.82775. These results from Dataset 3 demonstrate the varying degrees of efficacy of the different models, with the DBN-COA model generally showing superior performance across most metrics.</p>
          
            <table-wrap id="table_6">
              <label>Table 6</label>
              <caption>Comparative analysis of various models on Dataset 3</caption>
              <abstract/>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Measures</p></td><td colspan="1" rowspan="1"><p>MD2N</p></td><td colspan="1" rowspan="1"><p>DCNN</p></td><td colspan="1" rowspan="1"><p>DBN</p></td><td colspan="1" rowspan="1"><p>AE</p></td><td colspan="1" rowspan="1"><p>DBN-COA</p></td></tr><tr><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>0.91467</p></td><td colspan="1" rowspan="1"><p>0.91467</p></td><td colspan="1" rowspan="1"><p>0.89069</p></td><td colspan="1" rowspan="1"><p>0.91733</p></td><td colspan="1" rowspan="1"><p>0.9466</p></td></tr><tr><td colspan="1" rowspan="1"><p>FDR</p></td><td colspan="1" rowspan="1"><p>0.13559</p></td><td colspan="1" rowspan="1"><p>0.1791</p></td><td colspan="1" rowspan="1"><p>0.14583</p></td><td colspan="1" rowspan="1"><p>0.15625</p></td><td colspan="1" rowspan="1"><p>0.0769</p></td></tr><tr><td colspan="1" rowspan="1"><p>F1-score</p></td><td colspan="1" rowspan="1"><p>0.76119</p></td><td colspan="1" rowspan="1"><p>0.77465</p></td><td colspan="1" rowspan="1"><p>0.66667</p></td><td colspan="1" rowspan="1"><p>0.77698</p></td><td colspan="1" rowspan="1"><p>0.85714</p></td></tr><tr><td colspan="1" rowspan="1"><p>Sensitivity</p></td><td colspan="1" rowspan="1"><p>0.68</p></td><td colspan="1" rowspan="1"><p>0.73333</p></td><td colspan="1" rowspan="1"><p>0.54667</p></td><td colspan="1" rowspan="1"><p>0.72</p></td><td colspan="1" rowspan="1"><p>0.8</p></td></tr><tr><td colspan="1" rowspan="1"><p>Specificity</p></td><td colspan="1" rowspan="1"><p>0.97333</p></td><td colspan="1" rowspan="1"><p>0.96</p></td><td colspan="1" rowspan="1"><p>0.97678</p></td><td colspan="1" rowspan="1"><p>0.966</p></td><td colspan="1" rowspan="1"><p>0.983</p></td></tr><tr><td colspan="1" rowspan="1"><p>Precision</p></td><td colspan="1" rowspan="1"><p>0.86441</p></td><td colspan="1" rowspan="1"><p>0.8209</p></td><td colspan="1" rowspan="1"><p>0.85425</p></td><td colspan="1" rowspan="1"><p>0.843</p></td><td colspan="1" rowspan="1"><p>0.923</p></td></tr><tr><td colspan="1" rowspan="1"><p>FPR</p></td><td colspan="1" rowspan="1"><p>0.026667</p></td><td colspan="1" rowspan="1"><p>0.04</p></td><td colspan="1" rowspan="1"><p>0.023333</p></td><td colspan="1" rowspan="1"><p>0.0333</p></td><td colspan="1" rowspan="1"><p>0.0166</p></td></tr><tr><td colspan="1" rowspan="1"><p>FNR</p></td><td colspan="1" rowspan="1"><p>0.32</p></td><td colspan="1" rowspan="1"><p>0.26667</p></td><td colspan="1" rowspan="1"><p>0.45333</p></td><td colspan="1" rowspan="1"><p>0.28</p></td><td colspan="1" rowspan="1"><p>0.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>NPV</p></td><td colspan="1" rowspan="1"><p>0.97333</p></td><td colspan="1" rowspan="1"><p>0.96</p></td><td colspan="1" rowspan="1"><p>0.97667</p></td><td colspan="1" rowspan="1"><p>0.96667</p></td><td colspan="1" rowspan="1"><p>0.983</p></td></tr><tr><td colspan="1" rowspan="1"><p>MCC</p></td><td colspan="1" rowspan="1"><p>0.71772</p></td><td colspan="1" rowspan="1"><p>0.72397</p></td><td colspan="1" rowspan="1"><p>0.62658</p></td><td colspan="1" rowspan="1"><p>0.73007</p></td><td colspan="1" rowspan="1"><p>0.82775</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>5. Conclusion</title>
      <p style="text-align: justify">This research has successfully developed a lesion detection model based on dermoscopy images, integrating a pre-trained lightweight network for feature extraction. A novel augmentation approach, SAMAug, was introduced, enhancing medical image segmentation by leveraging the SAM to augment the inputs for established medical image segmentation models. The efficacy of this innovative strategy was empirically validated across three distinct segmentation tasks. In the classification phase, DBNs were employed, with the COA optimizing the selection of DBN weights. The proposed model's performance was rigorously tested and validated using three datasets, evaluated against a comprehensive set of metrics.</p><p style="text-align: justify">Looking ahead, the exploration of a more advanced and sophisticated augmentation function warrants consideration. Future research will focus on identifying and analyzing the variables and factors influencing the performance of the proposed method. Additionally, the transformer architecture within this context will be scrutinized, with necessary modifications to the proposed method being contemplated. Another critical avenue for future work lies in the collection and curation of larger and more diverse datasets of dermoscopy images. The development of a substantial dataset is imperative for enhancing the robustness and generalizability of the model. Collaborative efforts in creating annotated datasets will prove immensely beneficial for the research community, facilitating advancements in the field of medical image analysis.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p></p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p style="text-align: justify">The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p style="text-align: justify"><span style="color: windowtext">The authors declare no conflict of interest.</span></p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>100154</page-range>
          <issue/>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>D.</given-names>
              <surname>Adla</surname>
            </name>
            <name>
              <given-names>G. V. R.</given-names>
              <surname>Reddy</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Nayak</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Karuna</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.health.2023.100154.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A full-resolution convolutional network with a dynamic graph cut algorithm for skin cancer classification and detection</article-title>
          <source>Healthc. Anal.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>262</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>F.</given-names>
              <surname>Alenezi</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Armghan</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Polat</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/diagnostics13020262.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A novel multi-task learning network based on melanoma segmentation and classification with skin lesion images</article-title>
          <source>Diagnostics</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>100034</page-range>
          <issue>4</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>K.</given-names>
              <surname>Ali</surname>
            </name>
            <name>
              <given-names>Z. A.</given-names>
              <surname>Shaikh</surname>
            </name>
            <name>
              <given-names>A. A.</given-names>
              <surname>Khan</surname>
            </name>
            <name>
              <given-names>A. A.</given-names>
              <surname>Laghari</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neuri.2021.100034.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Multiclass skin cancer classification using EfficientNets – a first step towards preventing skin cancer</article-title>
          <source>Neurosci. Inf.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>100036</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M. S.</given-names>
              <surname>Ali</surname>
            </name>
            <name>
              <given-names>M. S.</given-names>
              <surname>Miah</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Haque</surname>
            </name>
            <name>
              <given-names>M. M.</given-names>
              <surname>Rahman</surname>
            </name>
            <name>
              <given-names>M. K.</given-names>
              <surname>Islam</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.mlwa.2021.100036.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>An enhanced technique of skin cancer classification using deep convolutional neural network with transfer learning models</article-title>
          <source>Mach. Learn. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>213</volume>
          <page-range>119230</page-range>
          <issue/>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>V.</given-names>
              <surname>Anand</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Gupta</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Koundal</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Singh</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2022.119230.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Fusion of U-Net and CNN model for segmentation and classification of skin lesion from dermoscopy images</article-title>
          <source>Expert Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>1-5</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J.</given-names>
              <surname>Daghrir</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Tlig</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Bouchouicha</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Sayadi</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/atsip49331.2020.9231544.</pub-id>
          <article-title>Melanoma skin cancer detection using deep learning and classical machine learning techniques: A hybrid approach</article-title>
          <source>In 2020 5th International Conference on Advanced Technologies for Signal and Image Processing (ATSIP), Sousse, Tunisia</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>18</volume>
          <page-range>5479</page-range>
          <issue>10</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Dildar</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Akram</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Irfan</surname>
            </name>
            <name>
              <given-names>H. U.</given-names>
              <surname>Khan</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Ramzan</surname>
            </name>
            <name>
              <given-names>A. R.</given-names>
              <surname>Mahmood</surname>
            </name>
            <name>
              <given-names>S. A.</given-names>
              <surname>Alsaiari</surname>
            </name>
            <name>
              <given-names>A. H. M.</given-names>
              <surname>Saeed</surname>
            </name>
            <name>
              <given-names>M. O.</given-names>
              <surname>Alraddadi</surname>
            </name>
            <name>
              <given-names>M. H.</given-names>
              <surname>Mahnashi</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/ijerph18105479.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Skin cancer detection: a review using deep learning techniques</article-title>
          <source>Int. J. Environ. Res. Publ. Health</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>82</volume>
          <page-range>6829-6847</page-range>
          <issue>5</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Y.</given-names>
              <surname>Hong</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Wei</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Cong</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Xu</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Zhang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-022-13606-4.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Weakly supervised semantic segmentation for skin cancer via CNN superpixel region response</article-title>
          <source>Multimedia Tool Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>46</volume>
          <page-range>103848</page-range>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>E. H.</given-names>
              <surname>Houssein</surname>
            </name>
            <name>
              <given-names>F. A.</given-names>
              <surname>Hashim</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Ferahtia</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Rezk</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.est.2021.103848.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Battery parameter identification strategy based on modified coot optimization algorithm</article-title>
          <source>J. Energy Storage</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>28</volume>
          <page-range>299-308</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>P. K.</given-names>
              <surname>Jakkulla</surname>
            </name>
            <name>
              <given-names>K. M.</given-names>
              <surname>Ganesh</surname>
            </name>
            <name>
              <given-names>P. K.</given-names>
              <surname>Jayapal</surname>
            </name>
            <name>
              <given-names>S. J.</given-names>
              <surname>Malla</surname>
            </name>
            <name>
              <given-names>S. B.</given-names>
              <surname>Chandanapalli</surname>
            </name>
            <name>
              <given-names>E.</given-names>
              <surname>Sandhya</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/isi.280205.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Selection of features using adaptive tunicate swarm algorithm with optimized deep learning model for thyroid disease classification</article-title>
          <source>Ing. Syst. Inf.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>1319-1329</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Kumar</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Alshehri</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>AlGhamdi</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Sharma</surname>
            </name>
            <name>
              <given-names>V.</given-names>
              <surname>Deep</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11036-020-01550-2.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A DE-ANN inspired skin cancer detection approach using fuzzy c-means clustering</article-title>
          <source>Mobile Netw Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>82</volume>
          <page-range>35995-36018</page-range>
          <issue/>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M. D.</given-names>
              <surname>Kumar</surname>
            </name>
            <name>
              <given-names>G. V.</given-names>
              <surname>Sivanarayana</surname>
            </name>
            <name>
              <given-names>D. N. V. S. L. S.</given-names>
              <surname>Indira</surname>
            </name>
            <name>
              <given-names>M. P.</given-names>
              <surname>Raj</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-023-14605-9.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Skin cancer segmentation with the aid of multi-class dilated D-net (MD2N) framework</article-title>
          <source>Multimed Tools Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>28</volume>
          <page-range>433-441</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>H.</given-names>
              <surname>Macherla</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Kotapati</surname>
            </name>
            <name>
              <given-names>M. T.</given-names>
              <surname>Sunitha</surname>
            </name>
            <name>
              <given-names>K. R.</given-names>
              <surname>Chittipireddy</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Attuluri</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Vatambeti</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/isi.280219.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Deep learning framework-based chaotic hunger games search optimization algorithm for prediction of air quality index</article-title>
          <source>Ing. Syst. Inf.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>116</volume>
          <page-range>103545</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A. G.</given-names>
              <surname>Pacheco</surname>
            </name>
            <name>
              <given-names>R. A.</given-names>
              <surname>Krohling</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compbiomed.2019.103545.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>The impact of patient clinical information on automated skin cancer detection</article-title>
          <source>Comput. Biol. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <page-range>781-793</page-range>
          <issue>7</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>N.</given-names>
              <surname>Razmjooy</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Ashourian</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Karimifard</surname>
            </name>
            <name>
              <given-names>V. V.</given-names>
              <surname>Estrela</surname>
            </name>
            <name>
              <given-names>Hermes J.</given-names>
              <surname>Loschi</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>do Nascimento</surname>
            </name>
            <name>
              <given-names>Reinaldo P.</given-names>
              <surname>França</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Vishnevski</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.2174/1573405616666200129095242.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Computer-aided Diagnosis of Skin Cancer: A Review</article-title>
          <source>Curr. Med. Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>234</volume>
          <page-range>121047</page-range>
          <issue/>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>K.</given-names>
              <surname>Sethanan</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Pitakaso</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Srichok</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Khonjun</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Thannipat</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Wanram</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Boonmee</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Gonwirat</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Enkvetchakul</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Kaewta</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Nanthasamroeng</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2023.121047.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Double AMIS-ensemble deep learning for skin cancer classification</article-title>
          <source>Expert Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>8927</page-range>
          <issue>15</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S. K.</given-names>
              <surname>Singh</surname>
            </name>
            <name>
              <given-names>V.</given-names>
              <surname>Abolghasemi</surname>
            </name>
            <name>
              <given-names>M. H.</given-names>
              <surname>Anisi</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app13158927.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Fuzzy logic with deep learning for detection of skin cancer</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range>1-22</page-range>
          <issue/>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>G.</given-names>
              <surname>Subhashini</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Chandrasekar</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/13682199.2023.2241794.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Hybrid deep learning technique for optimal segmentation and classification of multi-class skin cancer</article-title>
          <source>Imaging Sci. J.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>2022</volume>
          <page-range>1709842</page-range>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>P.</given-names>
              <surname>Thapar</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Rakhra</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Cazzato</surname>
            </name>
            <name>
              <given-names>M. S.</given-names>
              <surname>Hossain</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2022/1709842.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A novel hybrid deep learning approach for skin lesion segmentation and classification</article-title>
          <source>J. Healthcare Eng</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>68</volume>
          <page-range>101915</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Simon M.</given-names>
              <surname>Thomas</surname>
            </name>
            <name>
              <given-names>James G.</given-names>
              <surname>Lefevre</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Baxter</surname>
            </name>
            <name>
              <given-names>Nicholas A.</given-names>
              <surname>Hamilton</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.media.2020.101915.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Interpretable deep learning systems for multi-class segmentation and classification of non-melanoma skin cancer</article-title>
          <source>Med. Image Anal.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>144</volume>
          <page-range>110714</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Toğaçar</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Cömert</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Ergen</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.chaos.2021.110714.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Intelligent skin cancer detection applying autoencoder, MobileNetV2 and spiking neural networks</article-title>
          <source>Chaos, Solitons Fractals</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range>arXiv:2006.10726</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>D.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>E.</given-names>
              <surname>Shelhamer</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Olshausen</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Darrell</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2006.10726.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Tent: Fully test-time adaptation by entropy minimization</article-title>
          <source>arXiv preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>99633-99647</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>L.</given-names>
              <surname>Wei</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Ding</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Hu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/access.2020.2997710.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Automatic skin cancer detection in dermoscopy images based on ensemble lightweight deep learning network</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>860-871</page-range>
          <issue>1</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Z.</given-names>
              <surname>Xu</surname>
            </name>
            <name>
              <given-names>F. R.</given-names>
              <surname>Sheykhahmad</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Ghadimi</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Razmjooy</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1515/med-2020-0131.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Computer-aided diagnosis of skin cancer based on soft computing techniques</article-title>
          <source>Open Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>102</volume>
          <page-range>101756</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>N.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>Y. X.</given-names>
              <surname>Cai</surname>
            </name>
            <name>
              <given-names>Y. Y.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>Y. T.</given-names>
              <surname>Tian</surname>
            </name>
            <name>
              <given-names>X. L.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Badami</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.artmed.2019.101756.</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Skin cancer diagnosis based on optimized convolutional neural network</article-title>
          <source>Artif. Intell. Med.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>