<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-GGtR7ZfM3a46uxTVcNVt9Ikm4PkO4d9L</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml010203</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Gait Based Person Identification Using Deep Learning Model of Generative Adversarial Network</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2611-4925</contrib-id>
          <name>
            <surname>Vatambeti</surname>
            <given-names>Ramesh</given-names>
          </name>
          <email>ramesh.v@vitap.ac.in</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2445-4747</contrib-id>
          <name>
            <surname>Damera</surname>
            <given-names>Vijay Kumar</given-names>
          </name>
          <email>vijay.kumar@klh.edu.in</email>
          <xref ref-type="aff" rid="aff_2">2</xref>
        </contrib>
        <aff id="aff_1">School of Computer Science and Engineering, VIT-AP University, 522237 Vijayawada, India</aff>
        <aff id="aff_2">Department of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, 500090 Hyderabad, India</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>30</day>
        <month>12</month>
        <year>2022</year>
      </pub-date>
      <volume>1</volume>
      <issue>2</issue>
      <fpage>90</fpage>
      <lpage>100</lpage>
      <page-range>90-100</page-range>
      <history>
        <date date-type="received">
          <day>10</day>
          <month>09</month>
          <year>2022</year>
        </date>
        <date date-type="accepted">
          <day>14</day>
          <month>11</month>
          <year>2022</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2022 by the author(s)</copyright-statement>
        <copyright-year>2022</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>The proliferation of digital age security tools is often attributed to the rise of visual surveillance. Since an individual's gait is highly indicative of their identity, it is becoming an increasingly popular biometric modality for use in autonomous visual surveillance and monitoring. There are various steps used in gait recognition frameworks such as segmentation, feature extraction, feature learning and similarity measurement. These steps are mutually independent with each part fixed, which results in a suboptimal performance in a challenging condition. It can be done independently of the users' involvement. Low-resolution video and straightforward instrumentation can verify an individual's identity, making impersonation a rarity. Using the benefits of the Generative Adversarial Network (GAN), this investigation tackles the problem of unevenly distributed unlabeled data with infrequently performed tasks. To estimate the data circulation in various circumstances using constrained observed gait data, a multimodal generator is applied here. When it comes to sharing knowledge, the variety provided by the data generated by a multimodal generator is hard to beat. The capability to distinguish gait activities with varying patterns due to environmental dynamics is enhanced by this multimodal generator. This system is more stable than other gait-based recognition methods because it can process data that is not equally dispersed throughout a different environment. The system's reliability is enhanced by the multimodal generator's capacity to produce a wide variety of outputs. The testing results show that this algorithm is superior to other gait-based recognition methods because it can adapt to changing environments.</p></abstract>
      <kwd-group>
        <kwd>Generative Adversarial Network</kwd>
        <kwd>Gait based recognition</kwd>
        <kwd>Walking patterns</kwd>
        <kwd>Visual surveillance</kwd>
        <kwd>Low resolution videos</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="2"/>
        <fig-count count="2"/>
        <table-count count="4"/>
        <ref-count count="34"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>To identify individuals based on how they walk is the goal of gait recognition, a biometrics application [<xref ref-type="bibr" rid="ref_1">1</xref>]. This paper defines gait recognition as the challenge of determining which individuals can be identified in a set of gait sequences collected by visual cameras. Gait biometrics have the distinct benefit of being practical for long-distance human identification [<xref ref-type="bibr" rid="ref_2">2</xref>]. In other words, low-resolution gait analysis is feasible. Human mobility is described by gait, which encapsulates both its spatial statics and its temporal dynamics. Unlike other biostatistics, gait biometrics can be used to identify people from a distance. Another benefit of using gait recognition is that it does not rely as largely on the subjects' active cooperation as other biometrics do. Gait recognition has proven to be superior in emergency settings like the recent COVID-19 pandemic [<xref ref-type="bibr" rid="ref_3">3</xref>], when surveillance systems failed. There are growing hopes that it will soon be able to guarantee the safety of the general population.</p><p>Although studies on gait recognition only began a short time ago [<xref ref-type="bibr" rid="ref_4">4</xref>], they have been actively promoting and expanding the area ever since. We may roughly categorize the history of gait recognition study into three phases. Beginning in the early 1990s [<xref ref-type="bibr" rid="ref_5">5</xref>], the first phase set out to test whether or not remote human identification was possible. Current gait analysis methods have shown reasonable performance; however these methods have only been tested on benchmarks with as few as ten participants [<xref ref-type="bibr" rid="ref_6">6</xref>]. Subsequently, DARPA's Human ID at a Distance (HumanID) [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>] programme promoted not only methodologies but also datasets in its second stage. Several template-based methods [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>], [<xref ref-type="bibr" rid="ref_12">12</xref>] stand out as the most common automatic recognition methods nowadays. On the other hand, a different class of methods represented pedestrians' structural and kinetic properties using coarse human-model techniques [<xref ref-type="bibr" rid="ref_13">13</xref>], [<xref ref-type="bibr" rid="ref_14">14</xref>], [<xref ref-type="bibr" rid="ref_15">15</xref>]. Over a hundred people and various critical aspects, including view variants and appearance-changing, were considered in the early stages of dataset building the encouraging consequences of the test, it appears that gait recognition is doable and could be useful in the future [<xref ref-type="bibr" rid="ref_16">16</xref>].</p><p>At that point, gait recognition studies entered the era of deep learning. Unlike traditional systems relying on hand-crafted features, such as many template-based methods, deep era recognition methods may capture complicated motion features immediately from sequential inputs [<xref ref-type="bibr" rid="ref_17">17</xref>]. Thanks to the advent of deep learning methods, gait detection has shown impressive results on a variety of benchmarks. Gait recognition in increasingly complex settings, such as acknowledgement in a dataset with over ten thousand participants and robust gratitude in the wild, is also a focus of deep era research [<xref ref-type="bibr" rid="ref_18">18</xref>].</p><p>The major contributions of deep learning algorithms have led to considerable increases in recognition performance, and the results on the most common benchmarks demonstrate the viability of gait credit as a potent instrument for public safety [<xref ref-type="bibr" rid="ref_19">19</xref>], [<xref ref-type="bibr" rid="ref_20">20</xref>]. The total recognition accuracy on CASIA-B is greater than 93% even in the most challenging appearance-changing environment within the era of deep gait recognition. More than 10,000 participants were delivered to deep gait models as part of the study's recognition on a bigger participant, with the models achieving 97.5% rank-1 correctness on the OUMVLP dataset. To address this problem in a natural environment, GREW and Gait3D set out to investigate large-scale gait identification in the wild, with Gait3D additionally providing 3D explanation to investigate model-based applications. Surprisingly, the HID2022 competition achieved even higher than 95.9% on the setting, with people occasionally pausing to take a closer look.</p><p>In this study, we present a gait-based recognition system that, among other things, functions more reliably and consistently in a variety of circumstances. A generator, a translating generator, and a classifier make up the proposed model. Labeled data from the training atmosphere setting are all that is needed to train the system. Some events, like falling down, may not generate any data since they are not routine. In order to solve these issues, the boosting generator creates a significant amount of synthetic, unlabeled data to swell the available datasets in the target domain. The generator of the translation performs a "domain transfer," moving control from the original domain to the new one. Information from the original domain, when translated, displays properties not dissimilar to those in the target domain. In a variety of conditions, this ensures the system's continued reliability. The experimental results show that the system performs well in a dynamic environment and outperforms the systems that were compared to it. Section 2 provides a summary of literature, while Section 3 details the methods being projected. In Section 4, the validation investigation is presented, followed by a summary in Section 5.</p>
    </sec>
    <sec sec-type="">
      <title>2. Related works</title>
      
        <sec>
          
            <title>2.1. Deep learning-based gait analysis</title>
          
          <p>Many different approaches and gait models have been investigated for their potential in extracting useful features from the recorded skeletons for pathological gait analysis. To classify data, traditional approaches require manually selecting features of important clinical relevance. So, to aid in the diagnosis of various disorders, several gait models with predetermined properties are presented. They have inherent limitations due to the fact that the preselected features often fail to adequately capture a gait description and may overlook important details in the abstract data. As DNNs have progressed.</p><p>Through the use of a convolutional neural network, Sarin et al. [<xref ref-type="bibr" rid="ref_21">21</xref>] offer a touchless multimodal person recognition model that combines gait and speech modalities. For each modality, a unique pipeline was built using convolutional neural networks. This article also investigates many fusion approaches for integrating the two pipelines, illustrating the impact that each approach has on a number of different measures. Weighted regular and product fusion algorithms perform the best for the experimental data.</p><p>The purpose of the work of Merlin Linda et al. [<xref ref-type="bibr" rid="ref_22">22</xref>] is to present a smart recognition scheme for viewpoint differences in gait and speech. This paper presents a capsule network (CNN-CapsNet) model for recognising gait and voice changes and details the system's performance. Due to translational invariances in subsampling and speech fluctuations, the gait characteristic relative spatial hierarchies in the image entities are the main focus of the suggested intelligent system. The suggested work known as CNN-CapsNet is able to automatically train spatial information and by adapting to equal variances regardless of the viewpoint used. The proposed research would use a CNN-CapsNet model to settle disagreements about cofactors and gait recognition. To detect walking marks in surveillance videos, multimodal fusion techniques utilising hardware sensor strategies are preferable to the suggested CNN-CapsNet, which has several limitations. It can also serve as a prerequisite instrument for studying, recognising, detecting, and confirming malware procedures. Recently, (GCNN) has been used in gait investigation research [<xref ref-type="bibr" rid="ref_23">23</xref>] since it is a more advanced form. By zeroing in on the relationships between highly linked nodes, graph convolution is able to accurately extract spatial information from human skeletons.</p><p>Modeling and forecasting time-series signals is a specialty of the recurrent neural network (RNN), which works particularly well with signals of variable lengths. Gait data is one area where this method has been put to use. For the purpose of online gait phase estimation using input joint angle and position data, Kidziński et al. [<xref ref-type="bibr" rid="ref_24">24</xref>] opted for a long short-term memory (LSTM) system.</p><p>A gait anomaly acknowledgement is proposed by Sadeghzadehyazdi et al. [<xref ref-type="bibr" rid="ref_25">25</xref>], which makes use of Kinect skeleton data to capture spatiotemporal patterns. The suggested model represents the interdependence of various body joints during locomotion by taking the skeleton as a whole into account. The proposed model takes into account a multi-class framework, as opposed to the standard two-class or single-class approaches used in skeleton-based systems. A multi-class approach like this can be easily modified for use in settings other than motion capture labs, allowing for more frequent and cheaper gait evaluations. In order to detect nine distinct gaits, The publicly accessible Walking gait dataset is used to train and test the suggested deep learning model, which has a regular accuracy of 90.57%. Using transfer learning, the validation process examines the model's performance on two more publicly available datasets. The model achieved an average accuracy of 83.64% on one dataset with three classes and 90.83% on another with six classes of normal/pathological gait patterns. This work demonstrates the promise of marker less modalities like Kinect for the development of more efficient and cost-effective health infrastructures for ageing in place.</p><p>Using LSTM and the inertial sensors built into modern cell phones, Zou et al. [<xref ref-type="bibr" rid="ref_26">26</xref>] accomplished recognition in the wild. Due to its folded design, the RNN is more complicated and time-consuming to train than the CNN. Even while promising results have been shown in recent research on further improvement or the combination of RNN and CNN [<xref ref-type="bibr" rid="ref_26">26</xref>], their work is not suited for generalisation across other subjects.</p><p>Generative methods. The initial application of GAN was to normalise variations in gait photographs [<xref ref-type="bibr" rid="ref_27">27</xref>]. To further enhance generative approaches, Yu et al. [<xref ref-type="bibr" rid="ref_28">28</xref>] developed a multi-loss strategy, with the goal of cumulative while decreasing the intra-class variation. In order to learn view-specific features, He et al. [<xref ref-type="bibr" rid="ref_29">29</xref>] created a Multi-task Generative Adversarial Network (MGAN). To extract view-invariant features, (DiGGAN) [<xref ref-type="bibr" rid="ref_30">30</xref>] framework was presented. To separate gait features from appearance features, Zhang et al. [<xref ref-type="bibr" rid="ref_31">31</xref>] designed a system.</p>
        </sec>
      
      
        <sec>
          
            <title>2.2. Problem statement with contribution</title>
          
          <p>Training the system requires labelled data from the training environment (referred to as the original environment or source domain) and unlabelled data from users in the new environment (referred to as the test environment or target domain). Asking users to collect large amounts of data of all activities in the new ecosystem is not user-friendly, so only small amounts of data Users can perform actions randomly. Some non-regular activities such as falling were not reported. To solve the problems, a boosting generator is used to generate large amounts of pseudo-unlabelled data to increase the amount of data in the target domain. We add a small loss in the incentive generator loss function, which is useful in situations where data collected from users is not uniformly distributed among different functions. A translation generator translates a domain from a source domain to a target domain.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Proposed system</title>
      
        <sec>
          
            <title>3.1. System overview</title>
          
          <p>As can be seen in <xref ref-type="fig" rid="fig_1">Figure 1</xref>, there are a total of three components to the system: two generators and a categorization model. An electric booster is the first component. It is trained to provide artificial gait data that closely mimics actual gait data from the target field during the boosting generator training segment. The boosting generator is used to create additional artificial data that lacks labels because there is a limit to the amount of unlabelled data that can be obtained from the target domain before training the system. The automatic translation system comes next. The original domain's labelled gait data are all translated into the new domain. Target domain activity labels can be applied to the translated outputs because the variety of actions does not change during translation. The translation generator might help fill in any gaps if any actions taking place in the target domain are overlooked while collecting data. The distribution of gait data in the goal domain is more closely approximated. At last, a classification model is developed using both the real gait data two areas and the fake gait data from the two producers.</p>
          
            <sec>
              
                <title>3.1.1 Boosting generator</title>
              
              <p>Let's pretend we have two collections of real data, one branded (<italic>X</italic><sub><italic>l</italic></sub>, <italic>Y</italic>) and one unlabeled (<italic>X</italic><sub><italic>u</italic></sub>). The "<italic>X</italic><sub><italic>l</italic></sub>, <italic>Y</italic>" data originates from the "source domain," or the original training environment, whereas the "<italic>X</italic><sub><italic>u</italic></sub>" data comes from the "target domain," or the new environment. The ground-truth label for the activity is <italic>y</italic>, and <italic>X</italic><sub><italic>l</italic></sub> is the gait sample from <italic>X</italic><sub><italic>l</italic></sub>. The <italic>x</italic><sub><italic>u</italic></sub> notation designates the <italic>X</italic><sub><italic>u</italic></sub> gait sample. It is possible to train the boosting generator <italic>G</italic> bo to produce unlabeled false gait data <italic>x</italic><sub><italic>fu</italic></sub> that is identical to <italic>x</italic><sub><italic>u</italic></sub>. A discriminator <italic>D</italic> is utilised to tell the difference between real and created gait samples, and the loss of these distinctions is then propagated back to the boosting generator during training. The generator has been fine-tuned to produce the fewest possible deviations between generated and real samples.</p>
              
                <fig id="fig_1">
                  <label>Figure 1</label>
                  <caption>
                    <title>The scheme chart of the projected method</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/0/img_5d8rsUCDG2v1eI37.png"/>
                </fig>
              
              <p>Due to the fact that some tasks are not routinely carried out by users, the distribution of the unlabeled data composed from the target field may not be consistent [<xref ref-type="bibr" rid="ref_32">32</xref>]. For the sake of the system's ability to thoroughly examine the aspects of gait data for all activities, we recommend that the generated data be spread more consistently. To this end, we craft the marginal loss <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mi>m</mi>
      </mrow>
    </msub>
  </math>
</inline-formula>, several activities, for training the increasing generator. The Shannon entropy of a sample drawn from a specific distribution is the expected value of the information term that sample. If there is no discernible pattern in the distribution of classes for a given element, then its classification is unknown with certainty. In other words, we can coerce the model into making the gait examples generated in batches more evenly distributed by increasing their entropy. This can be done by increasing the resulting gait data's Shannon entropy, which is denoted in Eq. (1).</p>
              
                <disp-formula>
                  <label>(1)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msub>
                      <mrow data-mjx-texclass="ORD">
                        <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi>
                      </mrow>
                      <mrow data-mjx-texclass="ORD">
                        <mi>m</mi>
                      </mrow>
                    </msub>
                    <mo>=</mo>
                    <mi>H</mi>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">[</mo>
                      <mo data-mjx-texclass="CLOSE">]</mo>
                      <mfrac>
                        <mn>1</mn>
                        <mi>M</mi>
                      </mfrac>
                      <munderover>
                        <mo data-mjx-texclass="OP">∑</mo>
                        <mrow data-mjx-texclass="ORD">
                          <mi>i</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>M</mi>
                      </munderover>
                      <mi>p</mi>
                      <mrow data-mjx-texclass="INNER">
                        <mo data-mjx-texclass="OPEN">(</mo>
                        <mo>∣</mo>
                        <mo data-mjx-texclass="CLOSE">)</mo>
                        <mi>y</mi>
                        <mi>G</mi>
                        <mrow data-mjx-texclass="INNER">
                          <mo data-mjx-texclass="OPEN">(</mo>
                          <mo>,</mo>
                          <mo data-mjx-texclass="CLOSE">)</mo>
                          <msup>
                            <mi>x</mi>
                            <mi>i</mi>
                          </msup>
                          <mi>D</mi>
                        </mrow>
                      </mrow>
                    </mrow>
                  </math>
                </disp-formula>
              
              <p>where, <italic>H</italic> stands for the Shannon entropy, <italic>p</italic> for the class distribution, <italic>M</italic> for the number of generated data, <italic>I</italic> for the index of generated data, and y for the output class. As a result, it is possible to maximise the marginal loss and find a more unvarying deliver <italic>y</italic> of output data. Entropy loss from trying to minimise the gap among the actual and simulated data is used to determine the boosting generator loss <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <msub>
          <mrow data-mjx-texclass="ORD">
            <mi>G</mi>
          </mrow>
          <mrow data-mjx-texclass="ORD">
            <mi>b</mi>
            <mi>o</mi>
          </mrow>
        </msub>
      </mrow>
    </msub>
  </math>
</inline-formula>.</p>
              
                <disp-formula>
                  <label>(2)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msub>
                      <mrow data-mjx-texclass="ORD">
                        <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi>
                      </mrow>
                      <mrow data-mjx-texclass="ORD">
                        <msub>
                          <mrow data-mjx-texclass="ORD">
                            <mi>G</mi>
                          </mrow>
                          <mrow data-mjx-texclass="ORD">
                            <mi>b</mi>
                            <mi>o</mi>
                          </mrow>
                        </msub>
                      </mrow>
                    </msub>
                    <mo>=</mo>
                    <msubsup>
                      <mrow data-mjx-texclass="ORD"/>
                      <mrow data-mjx-texclass="ORD">
                        <msub>
                          <mi>G</mi>
                          <mrow data-mjx-texclass="ORD">
                            <mi>b</mi>
                            <mn>0</mn>
                          </mrow>
                        </msub>
                      </mrow>
                      <mrow data-mjx-texclass="ORD">
                        <munder>
                          <mo data-mjx-texclass="OP" movablelimits="true">min</mo>
                          <mi>b</mi>
                        </munder>
                      </mrow>
                    </msubsup>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">[</mo>
                      <mo>−</mo>
                      <mo>+</mo>
                      <mo data-mjx-texclass="CLOSE">]</mo>
                      <mi>λ</mi>
                      <msub>
                        <mi>L</mi>
                        <mi>m</mi>
                      </msub>
                      <msub>
                        <mi>ε</mi>
                        <mrow data-mjx-texclass="ORD">
                          <mi>z</mi>
                          <mo>∼</mo>
                          <msub>
                            <mi>p</mi>
                            <mi>z</mi>
                          </msub>
                        </mrow>
                      </msub>
                      <mrow data-mjx-texclass="INNER">
                        <mo data-mjx-texclass="OPEN">[</mo>
                        <mo data-mjx-texclass="NONE">⁡</mo>
                        <mo data-mjx-texclass="CLOSE">]</mo>
                        <mi>log</mi>
                        <mrow data-mjx-texclass="INNER">
                          <mo data-mjx-texclass="OPEN">(</mo>
                          <mo>−</mo>
                          <mo data-mjx-texclass="CLOSE">)</mo>
                          <mn>1</mn>
                          <mi>D</mi>
                          <mrow data-mjx-texclass="INNER">
                            <mo data-mjx-texclass="OPEN">(</mo>
                            <mo stretchy="false">(</mo>
                            <mo stretchy="false">)</mo>
                            <mo data-mjx-texclass="CLOSE">)</mo>
                            <msub>
                              <mi>G</mi>
                              <mrow data-mjx-texclass="ORD">
                                <mi>b</mi>
                                <mn>0</mn>
                              </mrow>
                            </msub>
                            <mi>x</mi>
                          </mrow>
                        </mrow>
                      </mrow>
                    </mrow>
                  </math>
                </disp-formula>
              
              <p>where, the marginal loss is represented by <italic>λ</italic> and the entropy loss by <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>ε</mi>
  </math>
</inline-formula>, respectively. In order to fabricate information in the desired domain, we can employ the generator.</p>
            </sec>
          
          
            <sec>
              
                <title>3.1.2 Translation generator</title>
              
              <p>The translator is split into a decoder (G). It's what gets used when you need to get gait data out of one domain encoder (E) and into another. The gait data in the source domain and the target domain are randomly sampled and then translated into each other to train the translation generator. The discriminator separates translated outputs from data from the target domain. To reduce the distributional divergence, adversarial training can be applied to the translation generator.</p><p>By utilising style transfer approaches, a new domain transferring procedure matrix is provided, which gives the translation generator a multimodal structure. In order to introduce variety into the transfer procedure, the style matrix is simulated at random. Styles in transferred images can vary depending on the style matrix used. Our translation generator mimics environmental dynamics that could impact gait data by generating an interference matrix at random. The gait data of one activity can be translated from the source area into the target domain with alternative meddling matrices using the translation generator. This allows us to mimic the gait data of this activity while taking into account the various environmental dynamics present in the intended domain. Since this interference matrix introduces randomness, gait data can be transformed in several ways across domains. The model benefits from the diversity of translated gait data by increasing its understanding of gait data in a variety of contexts and environmental dynamics. Two gait samples, <italic>x</italic><sub><italic>1</italic></sub> and <italic>x</italic><sub><italic>2</italic></sub>, one from the source domain and the other from the target domain, are taken into consideration. The dynamic interference matrix between the gait and the environment is designated as <italic>sj</italic>.</p><p>Prior to its translation into the target domain, <italic>x</italic><sub><italic>1</italic></sub> is encoded with encoder <italic>E</italic>. (<italic>c</italic>, <italic>s</italic><sub><italic>1</italic></sub>) = <italic>E</italic>(<italic>x</italic><sub><italic>1</italic></sub>). The second interference matrix, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>s</mi>
      <mn>2</mn>
    </msub>
  </math>
</inline-formula>, is generated by the generator at random. The translated information, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>x</mi>
      <mn>1</mn>
    </msub>
    <msub>
      <mi>G</mi>
      <mn>1</mn>
    </msub>
    <mo>=</mo>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">(</mo>
      <mo>,</mo>
      <mo data-mjx-texclass="CLOSE">)</mo>
      <mi>c</mi>
      <msub>
        <mi>s</mi>
        <mn>1</mn>
      </msub>
    </mrow>
  </math>
</inline-formula> and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>x</mi>
      <mn>2</mn>
    </msub>
    <msub>
      <mi>G</mi>
      <mn>2</mn>
    </msub>
    <mo>=</mo>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">(</mo>
      <mo>,</mo>
      <mo data-mjx-texclass="CLOSE">)</mo>
      <mi>c</mi>
      <msub>
        <mi>s</mi>
        <mn>2</mn>
      </msub>
    </mrow>
  </math>
</inline-formula>, is obtained by recombining the original data with <italic>c</italic>, which was taken from <italic>x</italic><sub><italic>1</italic></sub>. Then, we'll utilise a discriminator <italic>D</italic> to tell the created sample apart from data in the target field. We reduce the translation loss <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi>
      </mrow>
      <mi>t</mi>
    </msub>
  </math>
</inline-formula> in the following way to get optimal generator performance:</p>
              
                <disp-formula>
                  <label>(3)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msub>
                      <mrow data-mjx-texclass="ORD">
                        <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi>
                      </mrow>
                      <mrow data-mjx-texclass="ORD">
                        <mi>t</mi>
                      </mrow>
                    </msub>
                    <msub>
                      <mi>ε</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mi>c</mi>
                        <mo>,</mo>
                        <msub>
                          <mi>s</mi>
                          <mn>2</mn>
                        </msub>
                      </mrow>
                    </msub>
                    <mo>=</mo>
                    <munder>
                      <mo data-mjx-texclass="OP" movablelimits="true">min</mo>
                      <mrow data-mjx-texclass="ORD">
                        <msub>
                          <mi>G</mi>
                          <mn>2</mn>
                        </msub>
                      </mrow>
                    </munder>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">[</mo>
                      <mo data-mjx-texclass="NONE">⁡</mo>
                      <mo data-mjx-texclass="CLOSE">]</mo>
                      <mi>log</mi>
                      <mrow data-mjx-texclass="INNER">
                        <mo data-mjx-texclass="OPEN">(</mo>
                        <mo>−</mo>
                        <mo data-mjx-texclass="CLOSE">)</mo>
                        <mn>1</mn>
                        <mi>D</mi>
                        <mrow data-mjx-texclass="INNER">
                          <mo data-mjx-texclass="OPEN">(</mo>
                          <mo data-mjx-texclass="CLOSE">)</mo>
                          <msub>
                            <mi>G</mi>
                            <mn>2</mn>
                          </msub>
                          <mrow data-mjx-texclass="INNER">
                            <mo data-mjx-texclass="OPEN">(</mo>
                            <mo>,</mo>
                            <mo data-mjx-texclass="CLOSE">)</mo>
                            <mi>c</mi>
                            <msub>
                              <mi>s</mi>
                              <mn>2</mn>
                            </msub>
                          </mrow>
                        </mrow>
                      </mrow>
                    </mrow>
                  </math>
                </disp-formula>
              
              <p>Dissimilar <italic>s</italic><sub><italic>2</italic></sub> allow the translation generator to provide unique translated productions with the same <italic>c</italic>. Therefore, we can generate a wide variety of false data in another domain using data from the first domain, each of which corresponds to a unique form of environmental interference. A better estimate of the data distribution in the target domain may be obtained, and the system's adaptability to different types of environmental dynamics can be improved. Our approach employs a translation generator with a multimodal structure, rather than a deterministic one. Coding generated gait data with decoder G requires the use of a simulated interference matrix with a random coefficient distribution. This adds variety to the process of translation. While the gait data is same in structure, it may have various characteristics depending on the interference matrix used in translation. More data, with more varied features, can be created with the aid of the multimodal structure and used to train the system. As a result, the system will be more stable when subjected to varying environmental dynamics.</p><p>The technology is able to do a translation in reverse, which aids in driving the convergence. To encode the expression <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>x</mi>
      <mrow data-mjx-texclass="ORD">
        <mn>1</mn>
        <mn>2</mn>
        <mo stretchy="false">→</mo>
      </mrow>
    </msub>
    <msub>
      <mi>E</mi>
      <mn>2</mn>
    </msub>
    <mo>,</mo>
    <mo>,</mo>
    <mo>=</mo>
    <msup>
      <mi>c</mi>
      <mo>∗</mo>
    </msup>
    <msubsup>
      <mi>s</mi>
      <mn>2</mn>
      <mo>∗</mo>
    </msubsup>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">(</mo>
      <mo data-mjx-texclass="CLOSE">)</mo>
      <msub>
        <mi>x</mi>
        <mrow data-mjx-texclass="ORD">
          <mn>1</mn>
          <mn>2</mn>
          <mo stretchy="false">→</mo>
        </mrow>
      </msub>
    </mrow>
  </math>
</inline-formula>, where <italic>c*</italic> and s <italic>2*</italic> are the afresh interference matrix, respectively, we make use of an encoder. The previously extracted <italic>s</italic><sub><italic>1</italic></sub> is recombined with <italic>c*</italic>. Similarity among the combined data and the original <italic>x</italic><sub><italic>1</italic></sub> is what the reconstruction loss measures. This reconstruction loss aids in the unification of content code c across domains. The <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi>
      </mrow>
      <mi>R</mi>
    </msub>
  </math>
</inline-formula> loss is the reconstruction loss.</p>
              
                <disp-formula>
                  <label>(4)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msub>
                      <mrow data-mjx-texclass="ORD">
                        <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi>
                      </mrow>
                      <mrow data-mjx-texclass="ORD">
                        <mi>R</mi>
                      </mrow>
                    </msub>
                    <msub>
                      <mi>ε</mi>
                      <mrow data-mjx-texclass="ORD">
                        <msub>
                          <mi>x</mi>
                          <mn>1</mn>
                        </msub>
                      </mrow>
                    </msub>
                    <mo>=</mo>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">[</mo>
                      <mo data-mjx-texclass="CLOSE">]</mo>
                      <msub>
                        <mrow data-mjx-texclass="INNER">
                          <mo data-mjx-texclass="OPEN" symmetric="true">‖</mo>
                          <mo>−</mo>
                          <mo data-mjx-texclass="CLOSE" symmetric="true">‖</mo>
                          <msub>
                            <mi>G</mi>
                            <mn>1</mn>
                          </msub>
                          <msub>
                            <mi>x</mi>
                            <mn>1</mn>
                          </msub>
                          <mrow data-mjx-texclass="INNER">
                            <mo data-mjx-texclass="OPEN">(</mo>
                            <mo>,</mo>
                            <mo data-mjx-texclass="CLOSE">)</mo>
                            <msup>
                              <mi>c</mi>
                              <mo>∗</mo>
                            </msup>
                            <msubsup>
                              <mi>s</mi>
                              <mn>1</mn>
                              <mo>∗</mo>
                            </msubsup>
                          </mrow>
                        </mrow>
                        <mn>1</mn>
                      </msub>
                    </mrow>
                  </math>
                </disp-formula>
              
              <p>The overall translation generator loss is calculated by adding the reconstruction loss.</p>
              
                <disp-formula>
                  <label>(5)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msub>
                      <mrow data-mjx-texclass="ORD">
                        <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi>
                      </mrow>
                      <mrow data-mjx-texclass="ORD">
                        <msub>
                          <mi>G</mi>
                          <mrow data-mjx-texclass="ORD">
                            <mi>t</mi>
                            <mi>r</mi>
                          </mrow>
                        </msub>
                      </mrow>
                    </msub>
                    <msub>
                      <mrow data-mjx-texclass="ORD">
                        <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi>
                      </mrow>
                      <mi>t</mi>
                    </msub>
                    <msub>
                      <mrow data-mjx-texclass="ORD">
                        <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi>
                      </mrow>
                      <mi>R</mi>
                    </msub>
                    <mo>=</mo>
                    <mo>+</mo>
                    <mi>β</mi>
                  </math>
                </disp-formula>
              
              <p>where, is the renovation loss coefficient. After the conversion generator has been trained, we convert the remaining source-domain data to the target-domain format. The labels <italic>y</italic> from the original domain can be inherited by the translated data set <italic>x</italic><sub><italic>fl</italic></sub>, yielding the inherited label <italic>y</italic><sub><italic>0</italic></sub>. Furthermore, we can construct a wide variety of <italic>x</italic><sub><italic>fl</italic></sub> by employing a variety of <italic>s</italic><sub><italic>2</italic></sub>. Our classifier can be trained using all of these labelled synthetic data.</p>
            </sec>
          
          
            <sec>
              
                <title>3.1.3 Classification model</title>
              
              <p>A CNN is used to extract, and a fully connected layer is used as the classifier in this model. In order to solve classification challenges, CNN is commonly employed. As its feature extractors can efficiently learn pertinent characteristics for high dimensional data, it lowers the necessity for expert knowledge. To classify data, we use a model with three convolutional layers C(nk nk; n fm), The classifier is built with these last three layers, which have full connectivity between them. The abbreviated notation for the model's structure is: The formula goes as follows: <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>C</mi>
    <mi>P</mi>
    <mi>C</mi>
    <mi>P</mi>
    <mi>C</mi>
    <mi>P</mi>
    <mi>F</mi>
    <mi>F</mi>
    <mi>F</mi>
    <mo stretchy="false">(</mo>
    <mo>×</mo>
    <mo>;</mo>
    <mo stretchy="false">)</mo>
    <mo stretchy="false">↓</mo>
    <mo stretchy="false">→</mo>
    <mo stretchy="false">(</mo>
    <mo>×</mo>
    <mo>;</mo>
    <mo stretchy="false">)</mo>
    <mo stretchy="false">→</mo>
    <mo stretchy="false">→</mo>
    <mo stretchy="false">(</mo>
    <mo>×</mo>
    <mo>;</mo>
    <mo stretchy="false">)</mo>
    <mo stretchy="false">→</mo>
    <mo stretchy="false">→</mo>
    <mo stretchy="false">→</mo>
    <mo stretchy="false">→</mo>
    <mn>5</mn>
    <mn>5</mn>
    <mn>32</mn>
    <mn>5</mn>
    <mn>5</mn>
    <mn>128</mn>
    <mn>5</mn>
    <mn>5</mn>
    <mn>128</mn>
  </math>
</inline-formula>. Use leaky rectified linear units (leaky ReLus) in this case. The classifier must divide the genuine data (<italic>x</italic><sub><italic>l</italic></sub> and <italic>x</italic><sub><italic>u</italic></sub>) and the boosting generator's fake data <italic>x</italic><sub><italic>fl</italic></sub> into <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mo stretchy="false">(</mo>
    <mo>+</mo>
    <mo stretchy="false">)</mo>
    <mo>+</mo>
    <mi>n</mi>
    <mi>k</mi>
    <mn>1</mn>
    <mn>1</mn>
  </math>
</inline-formula> categories, where k is the sum of activities we wish to categorise. There are k classes in total; the first k are the genuine data classes (1,...,k), the next k are the fake data classes last class is the unlabeled fake data from the generator. Generally speaking, here is how the objective function <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi>
      </mrow>
      <mi>o</mi>
    </msub>
  </math>
</inline-formula> looks:</p>
              
                <disp-formula>
                  <label>(6)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtable columnalign="" columnspacing="1em" rowspacing="4pt">
                      <mtr>
                        <mtd>
                          <mrow data-mjx-texclass="ORD">
                            <msub>
                              <mrow data-mjx-texclass="ORD">
                                <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">L</mi>
                              </mrow>
                              <mi>o</mi>
                            </msub>
                            <msub>
                              <mi>ε</mi>
                              <mrow data-mjx-texclass="ORD">
                                <msub>
                                  <mi>x</mi>
                                  <mn>1</mn>
                                </msub>
                                <mi>y</mi>
                              </mrow>
                            </msub>
                            <msub>
                              <mi>ω</mi>
                              <mn>1</mn>
                            </msub>
                            <msub>
                              <mi>ε</mi>
                              <mrow data-mjx-texclass="ORD">
                                <msub>
                                  <mi>x</mi>
                                  <mi>u</mi>
                                </msub>
                              </mrow>
                            </msub>
                            <mo>=</mo>
                            <mo>−</mo>
                            <mo data-mjx-texclass="NONE">⁡</mo>
                            <mo>−</mo>
                            <mo data-mjx-texclass="NONE">⁡</mo>
                            <mi>log</mi>
                            <mi>log</mi>
                            <mrow data-mjx-texclass="INNER">
                              <mo data-mjx-texclass="OPEN">[</mo>
                              <mo data-mjx-texclass="CLOSE">]</mo>
                              <mi>p</mi>
                              <mrow data-mjx-texclass="INNER">
                                <mo data-mjx-texclass="OPEN">(</mo>
                                <mo>∣</mo>
                                <mo>,</mo>
                                <mo>&lt;</mo>
                                <mo>+</mo>
                                <mo data-mjx-texclass="CLOSE">)</mo>
                                <mi>y</mi>
                                <mi>y</mi>
                                <mi>k</mi>
                                <msub>
                                  <mi>x</mi>
                                  <mi>l</mi>
                                </msub>
                                <mn>1</mn>
                              </mrow>
                            </mrow>
                            <mrow data-mjx-texclass="INNER">
                              <mo data-mjx-texclass="OPEN">[</mo>
                              <mo data-mjx-texclass="CLOSE">]</mo>
                              <mi>p</mi>
                              <mrow data-mjx-texclass="INNER">
                                <mo data-mjx-texclass="OPEN">(</mo>
                                <mo>&lt;</mo>
                                <mo>+</mo>
                                <mo>∣</mo>
                                <mo data-mjx-texclass="CLOSE">)</mo>
                                <mi>y</mi>
                                <mi>k</mi>
                                <mn>1</mn>
                                <msub>
                                  <mi>x</mi>
                                  <mi>u</mi>
                                </msub>
                              </mrow>
                            </mrow>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd>
                          <mrow data-mjx-texclass="ORD">
                            <mo>−</mo>
                            <mo data-mjx-texclass="NONE">⁡</mo>
                            <mo>−</mo>
                            <mo data-mjx-texclass="NONE">⁡</mo>
                            <munderover>
                              <mo data-mjx-texclass="OP">∑</mo>
                              <mrow data-mjx-texclass="ORD">
                                <mi>a</mi>
                                <mo>=</mo>
                                <mn>1</mn>
                              </mrow>
                              <mi>n</mi>
                            </munderover>
                            <msub>
                              <mi>ω</mi>
                              <mrow data-mjx-texclass="ORD">
                                <mn>2</mn>
                                <mo>,</mo>
                                <mi>n</mi>
                              </mrow>
                            </msub>
                            <msub>
                              <mi>ε</mi>
                              <mrow data-mjx-texclass="ORD">
                                <msub>
                                  <mi>x</mi>
                                  <mrow data-mjx-texclass="ORD">
                                    <mi>f</mi>
                                    <mi>l</mi>
                                    <mo>,</mo>
                                    <msup>
                                      <mi>y</mi>
                                      <mrow data-mjx-texclass="ORD">
                                        <mi data-mjx-alternate="1">′</mi>
                                      </mrow>
                                    </msup>
                                  </mrow>
                                </msub>
                              </mrow>
                            </msub>
                            <msub>
                              <mi>ω</mi>
                              <mrow data-mjx-texclass="ORD">
                                <mn>3</mn>
                                <mo>,</mo>
                                <mi>n</mi>
                              </mrow>
                            </msub>
                            <msub>
                              <mi>ε</mi>
                              <mrow data-mjx-texclass="ORD">
                                <msub>
                                  <mi>x</mi>
                                  <mrow data-mjx-texclass="ORD">
                                    <mi>f</mi>
                                    <mi>u</mi>
                                  </mrow>
                                </msub>
                              </mrow>
                            </msub>
                            <mi>log</mi>
                            <mi>log</mi>
                            <mrow data-mjx-texclass="INNER">
                              <mo data-mjx-texclass="OPEN">[</mo>
                              <mo data-mjx-texclass="CLOSE">]</mo>
                              <mi>p</mi>
                              <mrow data-mjx-texclass="INNER">
                                <mo data-mjx-texclass="OPEN">(</mo>
                                <mo>∣</mo>
                                <mo>,</mo>
                                <mo>&lt;</mo>
                                <mo>&lt;</mo>
                                <mo stretchy="false">(</mo>
                                <mo>+</mo>
                                <mo stretchy="false">)</mo>
                                <mo>+</mo>
                                <mo data-mjx-texclass="CLOSE">)</mo>
                                <msup>
                                  <mi>y</mi>
                                  <mrow data-mjx-texclass="ORD">
                                    <mi data-mjx-alternate="1">′</mi>
                                  </mrow>
                                </msup>
                                <msub>
                                  <mi>x</mi>
                                  <mrow data-mjx-texclass="ORD">
                                    <mi>f</mi>
                                    <mi>l</mi>
                                  </mrow>
                                </msub>
                                <mi>a</mi>
                                <mi>k</mi>
                                <mi>y</mi>
                                <mi>a</mi>
                                <mi>k</mi>
                                <mn>1</mn>
                                <mn>1</mn>
                              </mrow>
                            </mrow>
                            <mrow data-mjx-texclass="ORD">
                              <mo stretchy="false">[</mo>
                              <mo>∣</mo>
                              <mo>&gt;</mo>
                              <mo>∣</mo>
                              <mo stretchy="false">]</mo>
                              <mi>p</mi>
                              <mi>y</mi>
                              <mi>n</mi>
                              <mi>k</mi>
                              <mi>u</mi>
                              <msub>
                                <mi>x</mi>
                                <mi>f</mi>
                              </msub>
                            </mrow>
                          </mrow>
                        </mtd>
                      </mtr>
                    </mtable>
                  </math>
                </disp-formula>
              
              <p>where, <italic>y</italic> and <italic>y'</italic> are labels from the original domain and the translated domain, correspondingly, and an is the index of data sets produced by the translation generator. Each phrase in L c has a weight, meant by <italic>w</italic><sub><italic>1</italic></sub>, <italic>w</italic><sub><italic>2</italic></sub>, and <italic>w</italic><sub><italic>3</italic></sub>, that is determined by validation. In the first term, we look at the first k classes to see if <italic>x</italic><sub><italic>l</italic></sub> &amp;amp;nbsp;is in the right one. The second step is to make sure that xu falls into one of the valid data types (1, 2, ..., <italic>k</italic>). Crucial information is found in the third term. Due to the fact that the translation generator produced n unique outputs, it is necessary to properly categorise the translated data inside each group. As a result of the numerous possible dynamic changes, the system is provided with additional info of conceivable CSI data for one class in various environmental circumstances. A more diverse and well-balanced set of data is used to train the classifier, rather than only one set of deterministic translated outputs. This safeguards against inadequate training owing to a lack of data and boosts the system's ability to deal with a constantly shifting, unpredictable environment.</p>
            </sec>
          
          
            <sec>
              
                <title>3.1.4 System training</title>
              
              <p>A brief overview of the model-training process is provided here. First, as illustrated in Algorithm 1, train the boosting generator using the unlabeled statistics collected from the target environment. As a next step, we employ the trained to fabricate unlabeled data with a more consistent distribution. The translation generator learns to convert data from the source domain to the target domain by combining data from both domains. Once the translation generator has been trained, any data in the source domain can be automatically converted into the desired format in the target field. These translated products may retain the labels assigned to them in the original domain. The classification system is then trained using <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>x</mi>
      <mi>u</mi>
    </msub>
    <msub>
      <mi>x</mi>
      <mrow data-mjx-texclass="ORD">
        <mi>f</mi>
        <mi>u</mi>
      </mrow>
    </msub>
    <mo>,</mo>
    <mo>,</mo>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">(</mo>
      <mo data-mjx-texclass="CLOSE">)</mo>
      <msub>
        <mi>x</mi>
        <mi>l</mi>
      </msub>
      <mi>y</mi>
    </mrow>
  </math>
</inline-formula> and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">(</mo>
      <mo>,</mo>
      <mo data-mjx-texclass="CLOSE">)</mo>
      <msub>
        <mi>x</mi>
        <mrow data-mjx-texclass="ORD">
          <mi>f</mi>
          <mi>l</mi>
        </mrow>
      </msub>
      <msup>
        <mi>y</mi>
        <mrow data-mjx-texclass="ORD">
          <mi data-mjx-alternate="1">′</mi>
        </mrow>
      </msup>
    </mrow>
  </math>
</inline-formula>.</p><p>Algorithm 1. Training Phase of proposed model</p><p><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/0/img_23Rxo0X7hgeoT44g.png" /></p>
            </sec>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Results and discussion</title>
      
        <sec>
          
            <title>4.1. Datasets</title>
          
          <p>There are 124 people in CASIA-B [<xref ref-type="bibr" rid="ref_33">33</xref>]. Each subject has 110 arrangements total across 11 views (0°, 18°, ..., 162°, 180°), with 10 sequences corresponding to each of the 11 views. Six of the ten sequences (nm01-nm06) were captured in natural light, two were captured inside coats, and two were captured using bags. NM01-NM04 are used as a reference set, while NM05-NM06, BG01-BG02, and CL01-CL02 serve as probe sets during testing. The dataset satisfies the training for multi-view gait picture sequence peer group by providing samples under varying view, garment, and carrying circumstances.</p><p>This is a big view-variation gait dataset called OUMVLP [<xref ref-type="bibr" rid="ref_34">34</xref>]. More than 10,000 people are represented in this dataset. Fourteen angles (0, 15, 30, 45, 60, 75, 90, 180, 195, 210, 215, 240, 255, and 270) are used to record each subject's sequences. Each perspective has two sequences, just like OULP. With the help of our proposed gait sequence creation approach, we may increase the sum of arrangements for each subject in the dataset. <xref ref-type="fig" rid="fig_2">Figure 2</xref> shows a few examples.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>A few walks taken from the CASIA-B and OUMVLP datasets. Gait samples from CASIA-B may be seen in the first row, and those from OUMVLP in the second</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/0/img_W0tZLsscQnpD4SFM.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.2. Evaluation metrics</title>
          
          <p>Experimentation is conducted on a PC with an Intel(R) Core (TM) i7-7700K CPU, 8 GB of RAM, and an NVIDIA GeForce 1050-ti graphics processing unit. The data is then averaged among the 11 perspectives, with duplicates removed. The accuracy of the 18° probe, for instance, is determined by averaging the results of 10 gallery views other than the 18° gallery. Not only does gait data include information in the walking direction, but it also provides data perpendicular to the walking direction. The parallel view angle (90 degrees) and the vertical view angles (zero degrees and one hundred eighty degrees) can cause some of the gait information to be lost. The results of these experiments are shown in <xref ref-type="table" rid="table_1">Table 1</xref>, <xref ref-type="table" rid="table_2">Table 2</xref> and <xref ref-type="table" rid="table_3">Table 3</xref> for the conditions of everyday walking, bag carrying, and various outfits, respectively.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Results of gait recognition using the proposed and additional benchmarking techniques on the CASIA-B dataset for normal (NM) walking</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Method</p></td><td colspan="1" rowspan="1"><p>0</p></td><td colspan="1" rowspan="1"><p>18</p></td><td colspan="1" rowspan="1"><p>36</p></td><td colspan="1" rowspan="1"><p>54</p></td><td colspan="1" rowspan="1"><p>72</p></td><td colspan="1" rowspan="1"><p>90</p></td><td colspan="1" rowspan="1"><p>108</p></td><td colspan="1" rowspan="1"><p>126</p></td><td colspan="1" rowspan="1"><p>144</p></td><td colspan="1" rowspan="1"><p>162</p></td><td colspan="1" rowspan="1"><p>180</p></td><td colspan="1" rowspan="1"><p>meanSTD</p></td></tr><tr><td colspan="1" rowspan="1"><p>CNN</p></td><td colspan="1" rowspan="1"><p>93.1</p></td><td colspan="1" rowspan="1"><p>92.6</p></td><td colspan="1" rowspan="1"><p>90.8</p></td><td colspan="1" rowspan="1"><p>92.4</p></td><td colspan="1" rowspan="1"><p>87.6</p></td><td colspan="1" rowspan="1"><p>95.1</p></td><td colspan="1" rowspan="1"><p>94.1</p></td><td colspan="1" rowspan="1"><p>94.2</p></td><td colspan="1" rowspan="1"><p>92.4</p></td><td colspan="1" rowspan="1"><p>90.2</p></td><td colspan="1" rowspan="1"><p>90.3</p></td><td colspan="1" rowspan="1"><p>92.32.4</p></td></tr><tr><td colspan="1" rowspan="1"><p>MGAN</p></td><td colspan="1" rowspan="1"><p>74</p></td><td colspan="1" rowspan="1"><p>84.7</p></td><td colspan="1" rowspan="1"><p>89.9</p></td><td colspan="1" rowspan="1"><p>86.3</p></td><td colspan="1" rowspan="1"><p>79.4</p></td><td colspan="1" rowspan="1"><p>77.1</p></td><td colspan="1" rowspan="1"><p>83.5</p></td><td colspan="1" rowspan="1"><p>86.2</p></td><td colspan="1" rowspan="1"><p>80.2</p></td><td colspan="1" rowspan="1"><p>81.2</p></td><td colspan="1" rowspan="1"><p>69.4</p></td><td colspan="1" rowspan="1"><p>81.56.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>DiGGAN</p></td><td colspan="1" rowspan="1"><p>89</p></td><td colspan="1" rowspan="1"><p>96.4</p></td><td colspan="1" rowspan="1"><p>99.5</p></td><td colspan="1" rowspan="1"><p>96.7</p></td><td colspan="1" rowspan="1"><p>91.4</p></td><td colspan="1" rowspan="1"><p>88.6</p></td><td colspan="1" rowspan="1"><p>94.5</p></td><td colspan="1" rowspan="1"><p>97.8</p></td><td colspan="1" rowspan="1"><p>94.6</p></td><td colspan="1" rowspan="1"><p>93.02</p></td><td colspan="1" rowspan="1"><p>84.2</p></td><td colspan="1" rowspan="1"><p>93.54.9</p></td></tr><tr><td colspan="1" rowspan="1"><p>Proposed</p></td><td colspan="1" rowspan="1"><p>91</p></td><td colspan="1" rowspan="1"><p>98.3</p></td><td colspan="1" rowspan="1"><p>99.4</p></td><td colspan="1" rowspan="1"><p>98.1</p></td><td colspan="1" rowspan="1"><p>93.2</p></td><td colspan="1" rowspan="1"><p>91.9</p></td><td colspan="1" rowspan="1"><p>95.3</p></td><td colspan="1" rowspan="1"><p>98.7</p></td><td colspan="1" rowspan="1"><p>95.6</p></td><td colspan="1" rowspan="1"><p>95.2</p></td><td colspan="1" rowspan="1"><p>92.1</p></td><td colspan="1" rowspan="1"><p>95.33.8</p></td></tr></tbody></table>
            </table-wrap>
          
          <p><xref ref-type="table" rid="table_1">Table 1</xref> shows that the proposed technique still performs well. The average improvement for the 90 degree, 0 degree, and 180 degree viewing angles over the current algorithms was 1.8%, 2.4%, and 2.7%, respectively. Meanwhile, after 11 views, it got better by 0.8%. Small sample training (ST) using only 24 items across three different walking circumstances has been upgraded with the proposed strategy (NM, BG, CL). The suggested model outperforms the state-of-the-art by 2.8%, 4.4%, and 0.7%, respectively, when compared to CNN, MGAN, and DiGGAN. The reason for this is that our model obtains more silhouettes than based on gait templates in a single batch since it treats the input as a set. Therefore, more time and space data are available to this model. After that, an improved training model is attained by using metric learning to acquire more discriminative gait features than the DiGGAN.</p><p>Recognizing someone's gait when their appearance has changed is difficult. In the BG sequence on CASIA-B, the approach performs admirably. As can be shown in <xref ref-type="table" rid="table_2">Table 2</xref>, this strategy has proven to be highly effective in BG subsequence. It outperforms state-of-the-art algorithms, topping the competition across 11 different perspectives with an average accuracy that is 4.4%, 1.3%, and 1.5% higher than that of competing models. All the major gait variants are accounted for, further demonstrating the high invariance of gait features. The cross-view criteria are also considered by certain existing methods. This model is also the most accurate when the training set is large and data interference is high. Importantly, this is a large performance gap between BG and NM in all techniques except for the proposed method, which further proves that our gait features have robust invariance to all chief gait alterations.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Results of gait recognition using various carried bags (BG) from the CASIA-B dataset for the benchmarking techniques that have been suggested</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Method</p></td><td colspan="1" rowspan="1"><p>0</p></td><td colspan="1" rowspan="1"><p>18</p></td><td colspan="1" rowspan="1"><p>36</p></td><td colspan="1" rowspan="1"><p>54</p></td><td colspan="1" rowspan="1"><p>72</p></td><td colspan="1" rowspan="1"><p>90</p></td><td colspan="1" rowspan="1"><p>108</p></td><td colspan="1" rowspan="1"><p>126</p></td><td colspan="1" rowspan="1"><p>144</p></td><td colspan="1" rowspan="1"><p>162</p></td><td colspan="1" rowspan="1"><p>180</p></td><td colspan="1" rowspan="1"><p>meanSTD</p></td></tr><tr><td colspan="1" rowspan="1"><p>CNN</p></td><td colspan="1" rowspan="1"><p>48.5</p></td><td colspan="1" rowspan="1"><p>58.5</p></td><td colspan="1" rowspan="1"><p>59.7</p></td><td colspan="1" rowspan="1"><p>58</p></td><td colspan="1" rowspan="1"><p>53.7</p></td><td colspan="1" rowspan="1"><p>49.8</p></td><td colspan="1" rowspan="1"><p>54</p></td><td colspan="1" rowspan="1"><p>61.3</p></td><td colspan="1" rowspan="1"><p>59.5</p></td><td colspan="1" rowspan="1"><p>55.9</p></td><td colspan="1" rowspan="1"><p>43.1</p></td><td colspan="1" rowspan="1"><p>54.7 ± 5.6</p></td></tr><tr><td colspan="1" rowspan="1"><p>MGAN</p></td><td colspan="1" rowspan="1"><p>64.7</p></td><td colspan="1" rowspan="1"><p>73.8</p></td><td colspan="1" rowspan="1"><p>80.1</p></td><td colspan="1" rowspan="1"><p>75.6</p></td><td colspan="1" rowspan="1"><p>67.9</p></td><td colspan="1" rowspan="1"><p>64.9</p></td><td colspan="1" rowspan="1"><p>71.0</p></td><td colspan="1" rowspan="1"><p>77.8</p></td><td colspan="1" rowspan="1"><p>79.4</p></td><td colspan="1" rowspan="1"><p>77.0</p></td><td colspan="1" rowspan="1"><p>66.6</p></td><td colspan="1" rowspan="1"><p>72.6 ± 5.9</p></td></tr><tr><td colspan="1" rowspan="1"><p>DiGGAN</p></td><td colspan="1" rowspan="1"><p>81.6</p></td><td colspan="1" rowspan="1"><p>91.7</p></td><td colspan="1" rowspan="1"><p>91.6</p></td><td colspan="1" rowspan="1"><p>89.1</p></td><td colspan="1" rowspan="1"><p>82.1</p></td><td colspan="1" rowspan="1"><p>80.00</p></td><td colspan="1" rowspan="1"><p>82.9</p></td><td colspan="1" rowspan="1"><p>90.8</p></td><td colspan="1" rowspan="1"><p>92.7</p></td><td colspan="1" rowspan="1"><p>91.6</p></td><td colspan="1" rowspan="1"><p>77.9</p></td><td colspan="1" rowspan="1"><p>86.5 ± 5.6</p></td></tr><tr><td colspan="1" rowspan="1"><p>Proposed</p></td><td colspan="1" rowspan="1"><p>86.0</p></td><td colspan="1" rowspan="1"><p>93.3</p></td><td colspan="1" rowspan="1"><p>95.1</p></td><td colspan="1" rowspan="1"><p>92.1</p></td><td colspan="1" rowspan="1"><p>88.0</p></td><td colspan="1" rowspan="1"><p>82.3</p></td><td colspan="1" rowspan="1"><p>87.0</p></td><td colspan="1" rowspan="1"><p>94.2</p></td><td colspan="1" rowspan="1"><p>95.9</p></td><td colspan="1" rowspan="1"><p>90.7</p></td><td colspan="1" rowspan="1"><p>82.4</p></td><td colspan="1" rowspan="1"><p>89.7 ± 4.9</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>TableResults of gait identification using the proposed and additional benchmarking techniques on the CASIA-B dataset with various garment types (CL)</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Method</p></td><td colspan="1" rowspan="1"><p>0</p></td><td colspan="1" rowspan="1"><p>18</p></td><td colspan="1" rowspan="1"><p>36</p></td><td colspan="1" rowspan="1"><p>54</p></td><td colspan="1" rowspan="1"><p>72</p></td><td colspan="1" rowspan="1"><p>90</p></td><td colspan="1" rowspan="1"><p>108</p></td><td colspan="1" rowspan="1"><p>126</p></td><td colspan="1" rowspan="1"><p>144</p></td><td colspan="1" rowspan="1"><p>162</p></td><td colspan="1" rowspan="1"><p>180</p></td><td colspan="1" rowspan="1"><p>meanSTD</p></td></tr><tr><td colspan="1" rowspan="1"><p>CNN</p></td><td colspan="1" rowspan="1"><p>37.7</p></td><td colspan="1" rowspan="1"><p>57.2</p></td><td colspan="1" rowspan="1"><p>66.6</p></td><td colspan="1" rowspan="1"><p>61.1</p></td><td colspan="1" rowspan="1"><p>55.2</p></td><td colspan="1" rowspan="1"><p>54.6</p></td><td colspan="1" rowspan="1"><p>55.2</p></td><td colspan="1" rowspan="1"><p>59.1</p></td><td colspan="1" rowspan="1"><p>58.9</p></td><td colspan="1" rowspan="1"><p>48.8</p></td><td colspan="1" rowspan="1"><p>39.4</p></td><td colspan="1" rowspan="1"><p>54.0 ± 8.9</p></td></tr><tr><td colspan="1" rowspan="1"><p>MGAN</p></td><td colspan="1" rowspan="1"><p>55.0</p></td><td colspan="1" rowspan="1"><p>62.4</p></td><td colspan="1" rowspan="1"><p>66.0</p></td><td colspan="1" rowspan="1"><p>61.1</p></td><td colspan="1" rowspan="1"><p>55.9</p></td><td colspan="1" rowspan="1"><p>54.4</p></td><td colspan="1" rowspan="1"><p>59.4</p></td><td colspan="1" rowspan="1"><p>60.8</p></td><td colspan="1" rowspan="1"><p>62.8</p></td><td colspan="1" rowspan="1"><p>52.7</p></td><td colspan="1" rowspan="1"><p>44.6</p></td><td colspan="1" rowspan="1"><p>57.7 ± 6.0</p></td></tr><tr><td colspan="1" rowspan="1"><p>DiGGAN</p></td><td colspan="1" rowspan="1"><p>57.2</p></td><td colspan="1" rowspan="1"><p>76.1</p></td><td colspan="1" rowspan="1"><p>80.9</p></td><td colspan="1" rowspan="1"><p>77.2</p></td><td colspan="1" rowspan="1"><p>72.3</p></td><td colspan="1" rowspan="1"><p>70.2</p></td><td colspan="1" rowspan="1"><p>73.0</p></td><td colspan="1" rowspan="1"><p>72.6</p></td><td colspan="1" rowspan="1"><p>75.0</p></td><td colspan="1" rowspan="1"><p>69.6</p></td><td colspan="1" rowspan="1"><p>50.9</p></td><td colspan="1" rowspan="1"><p>70.4 ± 8.8</p></td></tr><tr><td colspan="1" rowspan="1"><p>Proposed</p></td><td colspan="1" rowspan="1"><p>65.8</p></td><td colspan="1" rowspan="1"><p>80.7</p></td><td colspan="1" rowspan="1"><p>82.5</p></td><td colspan="1" rowspan="1"><p>81.1</p></td><td colspan="1" rowspan="1"><p>72.7</p></td><td colspan="1" rowspan="1"><p>71.5</p></td><td colspan="1" rowspan="1"><p>74.3</p></td><td colspan="1" rowspan="1"><p>74.6</p></td><td colspan="1" rowspan="1"><p>78.7</p></td><td colspan="1" rowspan="1"><p>75.8</p></td><td colspan="1" rowspan="1"><p>64.4</p></td><td colspan="1" rowspan="1"><p>74.7 ± 5.9</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>Results further demonstrate the great value added by recurrently learnt partial gait illustration baseline methods employing labelled representations and unlabelled representations. This model's superiority is most obvious under the most difficult alterations in outward appearance, such as walking while dressed differently (<xref ref-type="table" rid="table_2">Table 2</xref>) or toting around heavy goods (<xref ref-type="table" rid="table_3">Table 3</xref>). As a result of the GAN module's ability to acquire more stable representations in the face of external perturbations, the model is more resistant to changes in appearance. Tabulated in <xref ref-type="table" rid="table_4">Table 4</xref> below are the results for another data set in terms of these angular dimensions:</p>
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Results of gait identification using the proposed and alternative benchmarking techniques on the OU-MVLP dataset</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Method</p></td><td colspan="1" rowspan="1"><p>0</p></td><td colspan="1" rowspan="1"><p>30</p></td><td colspan="1" rowspan="1"><p>60</p></td><td colspan="1" rowspan="1"><p>90</p></td><td colspan="1" rowspan="1"><p>meanSTD</p></td></tr><tr><td colspan="1" rowspan="1"><p>CNN</p></td><td colspan="1" rowspan="1"><p>77.7</p></td><td colspan="1" rowspan="1"><p>86.9</p></td><td colspan="1" rowspan="1"><p>85.3</p></td><td colspan="1" rowspan="1"><p>83.5</p></td><td colspan="1" rowspan="1"><p>83.4 ± 4.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>MGAN</p></td><td colspan="1" rowspan="1"><p>68.9</p></td><td colspan="1" rowspan="1"><p>82.3</p></td><td colspan="1" rowspan="1"><p>82.1</p></td><td colspan="1" rowspan="1"><p>81.7</p></td><td colspan="1" rowspan="1"><p>78.8 ± 6.6</p></td></tr><tr><td colspan="1" rowspan="1"><p>DiGGAN</p></td><td colspan="1" rowspan="1"><p>74.7</p></td><td colspan="1" rowspan="1"><p>84.4</p></td><td colspan="1" rowspan="1"><p>83.7</p></td><td colspan="1" rowspan="1"><p>82.2</p></td><td colspan="1" rowspan="1"><p>81.3 ± 4.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>Proposed</p></td><td colspan="1" rowspan="1"><p>78.5</p></td><td colspan="1" rowspan="1"><p>87.5</p></td><td colspan="1" rowspan="1"><p>85.8</p></td><td colspan="1" rowspan="1"><p>85.4</p></td><td colspan="1" rowspan="1"><p>84.3 ± 4.0</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>Gait data recorded at 0 degrees, 90 degrees, and 180 degrees show that the proposed model performs similarly to the aforementioned benchmarking approaches. This is because information about the gait, such as the walker's stride, is obscured at zero and 180 degrees, when the camera lens is perpendicular to the walker's line of motion. As an added complication, when the camera lens is at a right angle to the direction in which the subject is walking, it may miss important details about the gait, such as the subject's body swing, which would otherwise be apparent. But the results demonstrate that the proposed method outperforms the benchmarking methods in majority of these challenging scenarios, suggesting that it is able to learn more discriminative features for extreme viewing angle changes. Gait data acquired at intermediate angles, such as 36 degrees, 54 degrees, 126 degrees, and 144 degrees, where postures are evident, also show outstanding results when using this method. An additional intriguing finding is that the suggested model is robust in the face of varying subject and training sample sizes, as well as imaging configurations. The CASIA-B and OU-MVLP datasets contain 259,013 and 13,680 gait sequences from 125 and 10,307 individuals, respectively, but were acquired using different methods. Despite this, the suggested model routinely outperforms the benchmarking methods on both datasets. At the end, it's important to note that the suggested approach, MGAN. The extracted gait representations can then be more easily classified, retrieved, and transmitted.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>This work explores the issue of performance degradation under environment dynamics in gait-based recognition systems trained on data from a small number of static environments. To address this issue, the authors of this study proposed a model that makes use of multimodal GAN and the marginal loss generator to make the scheme more resistant to the impact of a wide variety of unforeseen dynamic changes in its surrounding environment. This new approach is created to learn robust view-invariant characteristics of a person's stride. The approach has been rigorously evaluated across four distinct test protocols on the massive CASIA-B and OU-MVLP gait datasets. The obtained recognition results demonstrated the model's superiority over other state-of-the-art techniques. For normal walk, the proposed model achieved nearly 91.9%, the existing DiGGAN model achieved 88.6%. These same models achieved 82.3% and 80% for carried bags. There are still issues with gait identification that need to be fixed, such as gait occlusion. Therefore, moving forward, research will centre on addressing these issues and expanding gait recognition's use cases to include areas like Person Re-Identification and Behavior Acknowledgement.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>Y.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Jiang</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Sun</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Xu</surname>
            </name>
          </person-group>
          <article-title>3D gait recognition based on a CNN-LSTM network with the fusion of SkeGEI and DA features</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance, Taipei, Taiwan</conf-name>
          <conf-loc>25 November</conf-loc>
          <conf-date>2019</conf-date>
          <year>2019</year>
          <page-range>1-8</page-range>
          <fpage>1</fpage>
          <lpage>8</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/AVSS.2019.8909881.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>L.</given-names>
              <surname>Yao</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Kusakunniran</surname>
            </name>
            <name>
              <given-names>Q.</given-names>
              <surname>Wu</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Tang</surname>
            </name>
          </person-group>
          <article-title>Robust CNN-based Gait Verification and Identification Using Skeleton Gait Energy Image</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2018 digital image computing: Techniques and applications, Canberra, ACT, Australia</conf-name>
          <conf-loc>17 January</conf-loc>
          <conf-date>2019</conf-date>
          <year>2019</year>
          <page-range>1-7</page-range>
          <fpage>1</fpage>
          <lpage>7</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/DICTA.2018.8615802.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>J. S.</given-names>
              <surname>Manoharan</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Jayaseelan</surname>
            </name>
          </person-group>
          <article-title>Single Image Dehazing Using Deep Belief Neural Networks to Reduce Computational Complexity</article-title>
          <publisher-name>Springer</publisher-name>
          <conf-name>International Conference on Computational Vision and Bio Inspired Computing, Coimbatore, India</conf-name>
          <conf-loc>28 September</conf-loc>
          <conf-date>2020</conf-date>
          <year>2020</year>
          <page-range>1471-1478</page-range>
          <fpage>1471</fpage>
          <lpage>1478</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/978-3-030-41862-5_151.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Singhal</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>A review of coronavirus disease-2019 (COVID-19)</article-title>
          <source>Indian. J. Pediatr.</source>
          <year>2020</year>
          <volume>87</volume>
          <issue>4</issue>
          <page-range>281-286</page-range>
          <fpage>281</fpage>
          <lpage>286</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/s12098-020-03263-6</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kumar</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>COVID Tweet Analysis using NLP</article-title>
          <source>Int. J. Health Sci.</source>
          <year>2022</year>
          <volume>14</volume>
          <issue>2</issue>
          <page-range>4764-4771</page-range>
          <fpage>4764</fpage>
          <lpage>4771</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.9756/INT-JECSE/V14I2.530</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>G. V.</given-names>
              <surname>Veres</surname>
            </name>
            <name>
              <given-names>M. S.</given-names>
              <surname>Nixon</surname>
            </name>
            <name>
              <given-names>J. N.</given-names>
              <surname>Carter</surname>
            </name>
          </person-group>
          <article-title>Model-based Approaches for Predicting Gait Changes Over Time</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2005 International Conference on Intelligent Sensors, Sensor Networks and Information Processing, Melbourne, VIC, Australia</conf-name>
          <conf-loc>21 February</conf-loc>
          <conf-date>2006</conf-date>
          <year>2006</year>
          <page-range>325-330</page-range>
          <fpage>325</fpage>
          <lpage>330</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ISSNIP.2005.1595600.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Gadaleta</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Cisotto</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Rossi</surname>
            </name>
            <name>
              <given-names>R. Z. U.</given-names>
              <surname>Rehman</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Rochester</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Del Din</surname>
            </name>
          </person-group>
          <article-title>Deep Learning Techniques for Improving Digital Gait Segmentation</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society, Berlin, Germany</conf-name>
          <conf-loc>7 October</conf-loc>
          <conf-date>2019</conf-date>
          <year>2019</year>
          <page-range>1834-1837</page-range>
          <fpage>1834</fpage>
          <lpage>1837</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/EMBC.2019.8856685.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Manoharan</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Study of variants of Extreme Learning Machine brands and its performance measure on classification algorithm</article-title>
          <source>J. Soft Com. Paradigm</source>
          <year>2021</year>
          <volume>3</volume>
          <issue>2</issue>
          <page-range>83-95</page-range>
          <fpage>83</fpage>
          <lpage>95</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.36548/jscp.2021.2.003</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Eswaran</surname>
              <given-names/>
            </name>
            <name>
              <surname>Rani</surname>
              <given-names/>
            </name>
            <name>
              <surname>Daniel</surname>
              <given-names/>
            </name>
            <name>
              <surname>Ramakrishnan</surname>
              <given-names/>
            </name>
            <name>
              <surname>Selvakumar</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>An enhanced network intrusion detection system for malicious crawler detection and security event correlations in ubiquitous banking infrastructure</article-title>
          <source>Int. J. Per. Compu. Commu.</source>
          <year>2021</year>
          <volume>18</volume>
          <issue>1</issue>
          <page-range>59-78</page-range>
          <fpage>59</fpage>
          <lpage>78</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1108/ijpcc-04-2021-0102</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Maheswari</surname>
              <given-names/>
            </name>
            <name>
              <surname>Aluvalu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Kantipudi</surname>
              <given-names/>
            </name>
            <name>
              <surname>Chennam</surname>
              <given-names/>
            </name>
            <name>
              <surname>Kotecha</surname>
              <given-names/>
            </name>
            <name>
              <surname>Saini</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Driver Drowsiness Prediction Based on Multiple Aspects Using Image Processing Techniques</article-title>
          <source>IEEE Access</source>
          <year>2022</year>
          <volume>10</volume>
          <page-range>54980-54990</page-range>
          <fpage>54980</fpage>
          <lpage>54990</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ACCESS.2022.3176451</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Pu</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Yuan</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Wang</surname>
            </name>
          </person-group>
          <article-title>Chronogait Image: A Novel Temporal Template for Gait Recognition</article-title>
          <publisher-name>Springer</publisher-name>
          <conf-name>European Conference on Computer Vision, Berlin</conf-name>
          <conf-loc>Heidelberg</conf-loc>
          <conf-date>ECCV 2010</conf-date>
          <year>2010</year>
          <page-range>257-270</page-range>
          <fpage>257</fpage>
          <lpage>270</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/978-3-642-15549-9_19.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bobick</surname>
              <given-names/>
            </name>
            <name>
              <surname>Davis</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>The recognition of human movement using temporal templates</article-title>
          <source>IEEE T. Pattern. Anal.</source>
          <year>2001</year>
          <volume>23</volume>
          <issue>3</issue>
          <page-range>257-267</page-range>
          <fpage>257</fpage>
          <lpage>267</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/34.910878</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hawas</surname>
              <given-names/>
            </name>
            <name>
              <surname>El-Khobby</surname>
              <given-names/>
            </name>
            <name>
              <surname>Abd-Elnaby</surname>
              <given-names/>
            </name>
            <name>
              <surname>El-Samie</surname>
              <given-names/>
            </name>
            <name>
              <surname>Fathi</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Gait identification by convolutional neural networks and optical flow</article-title>
          <source>Multimed. Tools. Appl.</source>
          <year>2019</year>
          <volume>78</volume>
          <issue>8</issue>
          <page-range>25873-25888</page-range>
          <fpage>25873</fpage>
          <lpage>25888</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/s11042-019-7638-9</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Daniel</surname>
              <given-names/>
            </name>
            <name>
              <surname>Preethi</surname>
              <given-names/>
            </name>
            <name>
              <surname>Jakka</surname>
              <given-names/>
            </name>
            <name>
              <surname>Eswaran</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Collaborative Intrusion Detection System in Cognitive Smart City Network (CSC-Net)</article-title>
          <source>Int. J. Know. Sys. Sci.</source>
          <year>2021</year>
          <volume>12</volume>
          <issue>1</issue>
          <page-range>60-73</page-range>
          <fpage>60</fpage>
          <lpage>73</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.4018/ijkss.2021010105</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Nageswari</surname>
              <given-names/>
            </name>
            <name>
              <surname>Kumar</surname>
              <given-names/>
            </name>
            <name>
              <surname>Raveena</surname>
              <given-names/>
            </name>
            <name>
              <surname>Sharma</surname>
              <given-names/>
            </name>
            <name>
              <surname>Devi</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>An identification and classification of thyroid diseases using deep learning methodology</article-title>
          <source>Rev. Geintec-Gestao Ino. E Tec.</source>
          <year>2021</year>
          <volume>11</volume>
          <issue>2</issue>
          <page-range>2004-2015</page-range>
          <fpage>2004</fpage>
          <lpage>2015</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.47059/revistageintec.v11i2.1820</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Delgado-Escano</surname>
              <given-names/>
            </name>
            <name>
              <surname>Castro</surname>
              <given-names/>
            </name>
            <name>
              <surname>Cózar</surname>
              <given-names/>
            </name>
            <name>
              <surname>Marín-Jiménez</surname>
              <given-names/>
            </name>
            <name>
              <surname>Guil</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>An end-to-end multi-task and fusion CNN for inertial-based gait recognition</article-title>
          <source>IEEE Access</source>
          <year>2018</year>
          <volume>7</volume>
          <page-range>1897-1908</page-range>
          <fpage>1897</fpage>
          <lpage>1908</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ACCESS.2018.288689</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Uma Maheswari</surname>
              <given-names/>
            </name>
            <name>
              <surname>Aluvalu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Chennam</surname>
              <given-names/>
            </name>
          </person-group>
          <source>Application of Machine Learning Algorithms for Facial Expression Analysis, Machine Learning for Sustainable Development</source>
          <publisher-name>Walter de Gruyter GmbH &amp; Co KG</publisher-name>
          <year>2021</year>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>Y.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Xun</surname>
            </name>
            <name>
              <given-names>Q.</given-names>
              <surname>Yan</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>Q. W.</given-names>
              <surname>Gao</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Xia</surname>
            </name>
          </person-group>
          <article-title>A convolutional neural network for gait recognition based on plantar pressure images</article-title>
          <publisher-name>Springer</publisher-name>
          <conf-name>Chinese Conference on Biometric Recognition, Shenzhen, China</conf-name>
          <conf-loc>20 October</conf-loc>
          <conf-date>2017</conf-date>
          <year>2017</year>
          <page-range>466-473</page-range>
          <fpage>466</fpage>
          <lpage>473</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/978-3-319-69923-3_50.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names/>
            </name>
            <name>
              <surname>Makihara</surname>
              <given-names/>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Yagi</surname>
              <given-names/>
            </name>
            <name>
              <surname>Ren</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Joint intensity transformer network for gait recognition robust against clothing and carrying status</article-title>
          <source>IEEE T. Inf. Forensic Sec.</source>
          <year>2019</year>
          <volume>14</volume>
          <issue>2</issue>
          <page-range>3102-3115</page-range>
          <fpage>3102</fpage>
          <lpage>3115</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/TIFS.2019.2912577</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lv</surname>
              <given-names/>
            </name>
            <name>
              <surname>Xing</surname>
              <given-names/>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Guan</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Class energy image analysis for video sensor-based gait recognition: A review</article-title>
          <source>Sensors</source>
          <year>2015</year>
          <volume>15</volume>
          <issue>1</issue>
          <page-range>932-964</page-range>
          <fpage>932</fpage>
          <lpage>964</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.3390/s150100932</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sarin</surname>
              <given-names/>
            </name>
            <name>
              <surname>Mittal</surname>
              <given-names/>
            </name>
            <name>
              <surname>Chugh</surname>
              <given-names/>
            </name>
            <name>
              <surname>Srivastava</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Cnn-based multimodal touchless biometric recognition system using gait and speech</article-title>
          <source>J. Int. Fuzzy Syste.</source>
          <year>2022</year>
          <volume>42</volume>
          <issue>2</issue>
          <page-range>981-990</page-range>
          <fpage>981</fpage>
          <lpage>990</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.3233/JIFS-189765</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Merlin Linda</surname>
              <given-names/>
            </name>
            <name>
              <surname>Sree Rathna Lakshmi</surname>
              <given-names/>
            </name>
            <name>
              <surname>Murugan</surname>
              <given-names/>
            </name>
            <name>
              <surname>Mahapatra</surname>
              <given-names/>
            </name>
            <name>
              <surname>Muthukumaran</surname>
              <given-names/>
            </name>
            <name>
              <surname>Sivaram</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Intelligent recognition system for viewpoint variations on gait and speech using CNN-CapsNet</article-title>
          <source>Int. J. Int. Comp. Cybern.</source>
          <year>2022</year>
          <volume>15</volume>
          <issue>3</issue>
          <page-range>363-382</page-range>
          <fpage>363</fpage>
          <lpage>382</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1108/IJICC-08-2021-0178</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Mei</surname>
              <given-names/>
            </name>
            <name>
              <surname>Martens</surname>
              <given-names/>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names/>
            </name>
            <name>
              <surname>Lewis</surname>
              <given-names/>
            </name>
            <name>
              <surname>Feng</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Vision-based freezing of gait detection with anatomic directed graph representation</article-title>
          <source>IEEE J. Biomed. Health Inform.</source>
          <year>2019</year>
          <volume>24</volume>
          <issue>4</issue>
          <page-range>1215-1225</page-range>
          <fpage>1215</fpage>
          <lpage>1225</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/JBHI.2019.2923209</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kidziński</surname>
              <given-names/>
            </name>
            <name>
              <surname>Delp</surname>
              <given-names/>
            </name>
            <name>
              <surname>Schwartz</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Automatic real-time gait event detection in children using deep neural networks</article-title>
          <source>PLOS ONE</source>
          <year>2019</year>
          <volume>14</volume>
          <issue>1</issue>
          <pub-id pub-id-type="doi">https://doi.org/10.1371/journal.pone.0211466</pub-id>
          <pub-id pub-id-type="publisher-id">e0211466</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sadeghzadehyazdi</surname>
              <given-names/>
            </name>
            <name>
              <surname>Batabyal</surname>
              <given-names/>
            </name>
            <name>
              <surname>Acton</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Modeling spatiotemporal patterns of gait anomaly with a CNN-LSTM deep neural network</article-title>
          <source>Expert. Syst. Appl.</source>
          <year>2021</year>
          <volume>185</volume>
          <page-range>115582-115582</page-range>
          <fpage>115582</fpage>
          <lpage>115582</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1016/j.eswa.2021.115582</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zou</surname>
              <given-names/>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names/>
            </name>
            <name>
              <surname>Li</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Deep learningbased gait recognition using smartphones in the wild</article-title>
          <source>IEEE T. Inf. Foren. Sec.</source>
          <year>2020</year>
          <volume>15</volume>
          <page-range>3197-3212</page-range>
          <fpage>3197</fpage>
          <lpage>3212</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/TIFS.2020.2985628</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Yu</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>E. B. G.</given-names>
              <surname>Reyes</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Poh</surname>
            </name>
          </person-group>
          <article-title>Gaitgan: Invariant gait feature extraction using generative adversarial networks</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, Honolulu, HI, USA</conf-name>
          <conf-loc>24 August</conf-loc>
          <conf-date>2017</conf-date>
          <year>2017</year>
          <page-range>30-37</page-range>
          <fpage>30</fpage>
          <lpage>37</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/CVPRW.2017.80.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names/>
            </name>
            <name>
              <surname>Reyes</surname>
              <given-names/>
            </name>
            <name>
              <surname>Poh</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>GaitGANv2: Invariant gait feature extraction using generative adversarial networks</article-title>
          <source>Pattern. Recogn.</source>
          <year>2019</year>
          <volume>87</volume>
          <page-range>179-189</page-range>
          <fpage>179</fpage>
          <lpage>189</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1016/j.patcog.2018.10.019</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>He</surname>
              <given-names/>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Shan</surname>
              <given-names/>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Multi-task GANs for viewspecific feature learning in gait recognition</article-title>
          <source>IEEE Trans. Inf. Forensics Security</source>
          <year>2018</year>
          <volume>14</volume>
          <issue>1</issue>
          <page-range>102-113</page-range>
          <fpage>102</fpage>
          <lpage>113</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/TIFS.2018.2844819</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Guan</surname>
              <given-names/>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names/>
            </name>
            <name>
              <surname>Long</surname>
              <given-names/>
            </name>
            <name>
              <surname>Lane</surname>
              <given-names/>
            </name>
            <name>
              <surname>Ploetz</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Robust cross-view gait recognition with evidence: A discriminant gait GAN (DiGGAN) approach</article-title>
          <source>arXiv</source>
          <year>2018</year>
          <volume>2018</volume>
          <pub-id pub-id-type="doi">https://doi.org/10.48550/arXiv.1811.10493</pub-id>
          <pub-id pub-id-type="publisher-id">10493</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>Z.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Tran</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Yin</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Atoum</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Wan</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Wang</surname>
            </name>
          </person-group>
          <article-title>Gait recognition via disentangled representation learning</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA</conf-name>
          <conf-loc>09 January</conf-loc>
          <conf-date>2020</conf-date>
          <year>2020</year>
          <page-range>4710-4719</page-range>
          <fpage>4710</fpage>
          <lpage>4719</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/CVPR.2019.00484.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Cui</surname>
              <given-names/>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names/>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Multimodal CSI-based human activity recognition using GANs</article-title>
          <source>IEEE Int. Things J.</source>
          <year>2021</year>
          <volume>8</volume>
          <issue>4</issue>
          <page-range>17345-17355</page-range>
          <fpage>17345</fpage>
          <lpage>17355</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1186/s41074-018-0039-6</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Yu</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Tan</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Tan</surname>
            </name>
          </person-group>
          <article-title>A framework for evaluating the effect of view angle, clothing and carrying condition on gait recognition</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>18th International Conference on Pattern Recognition, Hong Kong, China</conf-name>
          <conf-loc>18 September</conf-loc>
          <conf-date>2006</conf-date>
          <year>2006</year>
          <page-range>441-444</page-range>
          <fpage>441</fpage>
          <lpage>444</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICPR.2006.67.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Takemura</surname>
              <given-names/>
            </name>
            <name>
              <surname>Makihara</surname>
              <given-names/>
            </name>
            <name>
              <surname>Muramatsu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Echigo</surname>
              <given-names/>
            </name>
            <name>
              <surname>Yagi</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Multi-view large population gait dataset and its performance evaluation for cross-view gait recognition</article-title>
          <source>Ipsj Trans. Comput. Vis. Appl.</source>
          <year>2018</year>
          <volume>10</volume>
          <issue>1</issue>
          <page-range>1-14</page-range>
          <fpage>1</fpage>
          <lpage>14</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1186/s41074-018-0039-6</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>