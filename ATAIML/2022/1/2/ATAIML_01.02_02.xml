<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-MivlrMSzkUgTodKMZywi6N5-fp7wQLWd</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml010202</article-id>
      <title-group>
        <article-title>Modelling of Depth Prediction Algorithm for Intra Prediction Complexity Reduction</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Joy</surname>
            <given-names>Helen K.</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7776-3856</contrib-id>
          <email>R19PEC09@ece.reva.edu.in</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="2">2</xref>
          <name>
            <surname>Kounte</surname>
            <given-names>Manjunath R.</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2432-2552</contrib-id>
          <email>kounte@reva.edu.in</email>
        </contrib>
        <aff id="1">School of Electronics and Communication Engineering, REVA University, 560064 Bengaluru, India</aff>
        <aff id="2">Department of Electronics and Computer Engineering, School of ECE, REVA University, 560064 Bengaluru, India</aff>
      </contrib-group>
      <year>2022</year>
      <volume>1</volume>
      <issue>2</issue>
      <fpage>81</fpage>
      <lpage>89</lpage>
      <page-range>81-89</page-range>
      <history>
        <date date-type="received">
          <month>09</month>
          <day>19</day>
          <year>2022</year>
        </date>
        <date date-type="accepted">
          <month>11</month>
          <day>11</day>
          <year>2022</year>
        </date>
        <date date-type="pub">
          <month>12</month>
          <day>30</day>
          <year>2022</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2022 by the authors</copyright-statement>
        <copyright-year>2022</copyright-year>
        <license>. Licensee Acadlore Publishing Services Limited, Hong Kong. This article can be downloaded for free, and reused and quoted with a citation of the original published version, under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 license</ext-link>.</license>
      </permissions>
      <abstract><p>Video compression gained its relevance with the boon of the internet, mobile phones, variable resolution acquisition device etc. The redundant information is explored in initial stages of compression that’s is prediction. Inter prediction that is prediction within the frame generates high computational complexity when working with traditional signal processing procedures. The paper proposes the design of a deep convolutional neural network model to perform inter prediction by crossing out the flaws in the traditional method. It briefs the modeling of network, mathematics behind each stage and evaluation of the proposed model with sample dataset. The video frame’s coding tree unit (CTU) of 64x64 is the input, the model converts and store it as a 16-element vector with the goodness of CNN network. It gives an overview of deep depth decision algorithm. The evaluation process shows that the model performs better for compression with less computational complexity.</p></abstract>
      <kwd-group>
        <kwd>Intra prediction</kwd>
        <kwd>CTU</kwd>
        <kwd>Computational complexity</kwd>
        <kwd>Deep learning</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors">2</count>
        <fig-count>8</fig-count>
        <table-count>0</table-count>
        <ref-count>17</ref-count>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec disp-level="level1" sec-type="intro">
      <title>1. Introduction</title>
      <p>Video compression in its actual form helps the data transfer in fast way maintaining the quality. When it comes to video, quality is an important parameter. The codecs should have a compression procedure that produces better quality output with less computational cos, considering recent video codec like H.264 [<xref ref-type="bibr" rid="ref_1">1</xref>], H.265(HEVC) [<xref ref-type="bibr" rid="ref_2">2</xref>], H.266 [<xref ref-type="bibr" rid="ref_3">3</xref>]. Versatile video coding (VCC) the issue facing is computational complexity, compression artifact, low coding efficiency etc., the total compression procedure when split into blocks the 70% of compression happens in the initial stage i.e., prediction stage. The main predictions in video compression are intra prediction and inter prediction. Intra prediction compares the similarities within the frame and inter prediction corelate the similarities between the frames. The more the similarities the chance of compression is more. As video compression advances, expect more accuracy with lower bitrates. The compression age is actively investigating VVC, advancement of existing HEVC, and much more to focus on developing. When the conveyance of data at a low bit rate became necessary, several compression methods emerged, but image quality was also a concern. As a result, video compression and its variations became prominent in the study field.</p><p>The traditional prediction method compares the similarity within/ between the frames and calculates the RDO [<xref ref-type="bibr" rid="ref_4">4</xref>] (rate distortion cost). Based on the value of rate distortion RD cost of the blocks, the split of the blocks is decided. The frame is split or not is decided by rate distortion algorithm, for that the frame is split to blocks of coding unit (CU). The coding units are arranged according to pixel length like 32*32, 16*16, 8*8, 4*4. After splitting the rate distortion optimization of best CU is calculated and compared. If the parent CU is having less value of RDO than the child CU [<xref ref-type="bibr" rid="ref_5">5</xref>] then the split is not done else the frame will be split. This will be done for the entire frame. The total procedure is complicated with many calculations. The computational complexity [<xref ref-type="bibr" rid="ref_5">5</xref>] is more in this case.</p><p>This motivated to frame a new concept to reduce computational complexities in inter prediction, out of all traditional signal processing scheme, the computational complexity reduction was a hard task to be done. So the focus turned to artificial neural network. The neural network in its traditional form couldn’t be able to satisfy the need as it wanted more layers to complete the procedure. The deep layer helped to satisfy this need. The deep learning and its ability to extract the features, analyze the content and classify helps it strong to be used for this purpose.</p><p>Because of its dense layer, the deep learning computational model was ideal for video coding. Since the 1990s, there has been research on neural network-based compression in both video and picture, but it has not been able to prove itself good by delivering a greater compression efficiency since its network was not very deep. As time passed, computational power increased, and it could now handle and train massive databases with deep or many layers. With this information, an overview of NN/DL based compression algorithms and their scope is a worthwhile topic to investigate. Some study schemes are focusing on the implementation of deep learning in traditional coding approaches, i.e., by incorporating deep learning techniques into the existing video coding procedures.</p><p>The modelling of deep learning algorithm chooses convolutional neural network as it was good in classifying the details. The network is deep and should extract and analyze the features correctly using the selected kernels, compressing the total content and narrowing down helps the total reduction of the computational complexity. Analyzing the features of deep network with CNN helps the modelling of a new network for reducing the computational complexity, time for encoding is a smart way.</p><p>The paper is divided into 3 parts chapter 1 gives an overview of motivation to use CNN network for inter prediction, chapter 2 analyses the proposed design and its features, chapter 3 analyses and evaluate the model with dataset, followed by the results and conclusion. </p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>2. Motivation behind the deep cnn network design</title>
      <p style="text-indent: 10.2pt;">Signal processing was meant to be the master in video compression field. Traditional signal processing techniques uses the neighboring pixel information to do the predictions [<xref ref-type="bibr" rid="ref_6">6</xref>] in video compression. The shift and rate distortion optimization cost comparison between the pixels helps in intra prediction. The intra prediction procedure uses the similarity index between the pixels within the frame and decides split of the CTU is required or not. Here in this traditional technique the calculations are more that is the computational complexity of the total procedure is high. This was noted as an issue in the HEVC video compressions intra prediction. This research gap was a motivation to design a network with less complexity by maintaining the quality. The overall idea is presented with <xref ref-type="fig" rid="fig_1">Figure 1</xref>. The proposed model was able to reduce the computational complexity. It is proved by the difference in time of encoding. </p><p style="text-indent: 10.2pt;">The deep learning came into the picture with its goodness in dealing with the content-based analysis than pixel-based calculations. This feature of deep learning helps in resolving the issue faced in intra-prediction in HEVC. The feature extraction [<xref ref-type="bibr" rid="ref_7">7</xref>] in deep learning make the procedure easy and reduce computation. Considering these features, the deep convolutional neutral network [<xref ref-type="bibr" rid="ref_8">8</xref>] showed its power in the designing of this deep depth decision algorithm for intra prediction. The kernel size of each layer decides which features needs to be extracted from that. Here for the design low to high resolution kernels are used to extract the multiple features [<xref ref-type="bibr" rid="ref_9">9</xref>] in the frame. The kernels used for the design are 5x5,3x3,4x4,8x8,16x16 [<xref ref-type="bibr" rid="ref_10">10</xref>] etc. The low sized kernel helps in the extraction of local data while the high sized kernel derives the global information’s from the given frame. The design needs a mix of local and global information so a mixture of various size id preferred in the design.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>Flow of designing inter prediction with deep depth decision algorithm</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/0/img_v-WNkMx4KiuZCklJ.png"/>
        </fig>
      
      <p style="text-indent: 10.2pt;">The convolutional neural network [<xref ref-type="bibr" rid="ref_8">8</xref>] extract features with kernel and compress it by Maxpool or averaging the information and works in both forward and backward propagation [<xref ref-type="bibr" rid="ref_11">11</xref>]. The activation function is used to excite the neurons. The activation function helps to boost the Maxpooled values to fire the neurons. Here the activation function decided for the design of deep depth decision algorithm is ReLu (rectified linear unit) that maintain the value in the range 0 and infinity as shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>. This function helps to the values to remain in positive coordinate thus by helping the firing of neurons. The last stage is flattening the information and crunching it to vector of lower size that holds the depth information of the total frame. This helps in the reduction of computational complexity as the network can predict the depth when input is given as video frame.</p><p style="text-indent: 10.2pt;">The path of development started from a con in the intra prediction that is the computational complexity because of the calculation of RDO cost and its comparison between parent CTU and CU. This leads to the concept of content-based analysis than pixel-based analysis with traditional signal processing. The content-based analysis for comparison makes the procedure simple and calculations less tedious. As the procedure needs an output of correlation between the contents within the frame. Deep learning techniques will be a good resolving this issue [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_12">12</xref>]. The deep layers help to extract the features and recover the perfect match for intra prediction. The CNN as it is good for comparison it showed its power to match for this. The depth of each CTU is calculated and kept as a 16-length vector by this algorithm thus by reducing the complexity.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>Activation function rectified linear unit</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/0/img_W1YFjdZx7EZoRBZo.png"/>
        </fig>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>3. Modeling of sample deep cnn for inter prediction</title>
      <p>Wherever TimesFor modeling the network <xref ref-type="fig" rid="fig_3">Figure 3</xref> shows the flow of development in the design. The input to the system is a video frame with specific features. Its send through multiple convolutional layers to extract the feature by using various kernels. The information is then Maxpooled and send to fully connected network for flattening the information and for softmax. The output derived is a 16 length vector of information with depth details of 64x64 CTU.</p><p>The network that extracts the feature should be flattened by using SoftMax. The data in the model are flattened and SoftMax to a 16-length vector. The whole input is crunched to a 16-length vector by reducing the complexity.</p><p>To model the network initially input is chosen with defined format that goes to filter if the input is i filter is represented by f, the convolution layer output z.</p>
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>Designing steps for CNN based deep depth decision algorithm</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/0/img_XXBGuvxnV5XbC0oI.png"/>
        </fig>
      
      
        <disp-formula>
          <label>(1)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>z</mi>
            <mo>=</mo>
            <mo>∗</mo>
            <mrow data-mjx-texclass="ORD">
              <mi mathvariant="normal">i</mi>
            </mrow>
            <mrow data-mjx-texclass="ORD">
              <mi mathvariant="normal">f</mi>
            </mrow>
          </math>
        </disp-formula>
      
      <p>where, * = convolution operation, the size of the output depends on input if input is of size (n x n) and if the size is (m x n) then the output is a size of (n – m + 1, n – m +1).</p><p>The pooling layer averages the output of convolution layer, the pooling layer output can be represented as.</p>
      
        <disp-formula>
          <label>(2)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>p</mi>
            <mo>=</mo>
            <mfrac>
              <mn>1</mn>
              <mi>k</mi>
            </mfrac>
            <munderover>
              <mo data-mjx-texclass="OP">∑</mo>
              <mrow data-mjx-texclass="ORD">
                <mi>a</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>k</mi>
            </munderover>
            <msub>
              <mi>z</mi>
              <mi>a</mi>
            </msub>
          </math>
        </disp-formula>
      
      <p><span style="color: black;">The fully connected network flattened the information.</span></p>
      
        <disp-formula>
          <label>(3)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">[</mo>
              <mo data-mjx-texclass="CLOSE">]</mo>
              <mtable columnalign="center center center center center center" columnspacing="1em" rowspacing="4pt">
                <mtr>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>11</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>12</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>13</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>1</mn>
                        <mi>x</mi>
                      </mrow>
                    </msub>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>21</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>22</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>23</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>2</mn>
                        <mi>x</mi>
                      </mrow>
                    </msub>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd/>
                  <mtd/>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd/>
                  <mtd/>
                  <mtd/>
                </mtr>
                <mtr>
                  <mtd/>
                  <mtd/>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd/>
                  <mtd/>
                  <mtd/>
                </mtr>
                <mtr>
                  <mtd/>
                  <mtd/>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd/>
                  <mtd/>
                  <mtd/>
                </mtr>
                <mtr>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mi>x</mi>
                        <mn>1</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mi>x</mi>
                        <mn>2</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mi>x</mi>
                        <mn>3</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mi>x</mi>
                        <mi>y</mi>
                      </mrow>
                    </msub>
                  </mtd>
                </mtr>
              </mtable>
            </mrow>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">[</mo>
              <mo data-mjx-texclass="CLOSE">]</mo>
              <mtable columnspacing="1em" rowspacing="4pt">
                <mtr>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>11</mn>
                      </mrow>
                    </msub>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>12</mn>
                      </mrow>
                    </msub>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mi>x</mi>
                        <mi>y</mi>
                      </mrow>
                    </msub>
                  </mtd>
                </mtr>
              </mtable>
            </mrow>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">[</mo>
              <mo data-mjx-texclass="CLOSE">]</mo>
              <mtable columnspacing="1em" rowspacing="4pt">
                <mtr>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mn>1</mn>
                    </msub>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mn>2</mn>
                    </msub>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mi>z</mi>
                    </msub>
                  </mtd>
                </mtr>
              </mtable>
            </mrow>
            <mo stretchy="false">→</mo>
            <mo>=</mo>
          </math>
        </disp-formula>
      
      
        <disp-formula>
          <label>(4)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>y</mi>
            <mi>p</mi>
            <mi>b</mi>
            <mo>=</mo>
            <mo>⋅</mo>
            <mo>+</mo>
            <msup>
              <mi>ω</mi>
              <mi>T</mi>
            </msup>
          </math>
        </disp-formula>
      
      <p>where, <italic>y</italic>= output, <italic>w</italic>=weight (randomly initiated), <italic>b</italic>= bias.</p>
      
        <disp-formula>
          <label>(5)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">[</mo>
              <mo data-mjx-texclass="CLOSE">]</mo>
              <mtable columnalign="center center center center center center" columnspacing="1em" rowspacing="4pt">
                <mtr>
                  <mtd>
                    <msub>
                      <mi>w</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>11</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>w</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>12</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>w</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>13</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>w</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>1</mn>
                        <mi>x</mi>
                      </mrow>
                    </msub>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <msub>
                      <mi>w</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>21</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>w</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>22</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>w</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>23</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>w</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>2</mn>
                        <mi>x</mi>
                      </mrow>
                    </msub>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd/>
                  <mtd/>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd/>
                  <mtd/>
                  <mtd/>
                </mtr>
                <mtr>
                  <mtd/>
                  <mtd/>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd/>
                  <mtd/>
                  <mtd/>
                </mtr>
                <mtr>
                  <mtd/>
                  <mtd/>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd/>
                  <mtd/>
                  <mtd/>
                </mtr>
                <mtr>
                  <mtd>
                    <msub>
                      <mi>w</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mi>n</mi>
                        <mn>1</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>w</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mi>n</mi>
                        <mn>2</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>w</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mi>n</mi>
                        <mn>3</mn>
                      </mrow>
                    </msub>
                  </mtd>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                  <mtd>
                    <msub>
                      <mi>w</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mi>n</mi>
                        <mi>z</mi>
                      </mrow>
                    </msub>
                  </mtd>
                </mtr>
              </mtable>
            </mrow>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">[</mo>
              <mo data-mjx-texclass="CLOSE">]</mo>
              <mtable columnspacing="1em" rowspacing="4pt">
                <mtr>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mn>1</mn>
                    </msub>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mn>2</mn>
                    </msub>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <msub>
                      <mi>p</mi>
                      <mi>z</mi>
                    </msub>
                  </mtd>
                </mtr>
              </mtable>
            </mrow>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">[</mo>
              <mo data-mjx-texclass="CLOSE">]</mo>
              <mtable columnspacing="1em" rowspacing="4pt">
                <mtr>
                  <mtd>
                    <msub>
                      <mi>y</mi>
                      <mn>1</mn>
                    </msub>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <msub>
                      <mi>y</mi>
                      <mn>2</mn>
                    </msub>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mo>⋅</mo>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <msub>
                      <mi>y</mi>
                      <mi>n</mi>
                    </msub>
                  </mtd>
                </mtr>
              </mtable>
            </mrow>
            <mo>⋅</mo>
            <mo>=</mo>
          </math>
        </disp-formula>
      
      <p style="text-indent: 10.2pt;"><span style="color: black;"><italic>n</italic></span><span style="color: black;"> is the required flattening required. </span></p><p style="text-indent: 10.2pt;"><span style="color: black;">Activation function used is ReLu:</span></p>
      
        <disp-formula>
          <label>(6)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>R</mi>
            <mi>z</mi>
            <mo stretchy="false">(</mo>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">{</mo>
              <mo data-mjx-texclass="CLOSE" fence="true" stretchy="true" symmetric="true"/>
              <mtable columnalign="center center" columnspacing="1em" rowspacing="4pt">
                <mtr>
                  <mtd>
                    <mi>z</mi>
                    <mo>,</mo>
                  </mtd>
                  <mtd>
                    <mi>z</mi>
                    <mo>&gt;</mo>
                    <mn>0</mn>
                  </mtd>
                </mtr>
                <mtr>
                  <mtd>
                    <mn>0</mn>
                    <mo>,</mo>
                  </mtd>
                  <mtd>
                    <mi>z</mi>
                    <mo>≤</mo>
                    <mn>0</mn>
                  </mtd>
                </mtr>
              </mtable>
            </mrow>
          </math>
        </disp-formula>
      
      <p style="text-indent: 10pt;"><span style="color: black;">The error function can be calculated by:</span></p>
      
        <disp-formula>
          <label>(7)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>E</mi>
            <mo>=</mo>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">|</mo>
              <mo>−</mo>
              <mo data-mjx-texclass="CLOSE">|</mo>
              <mi>y</mi>
              <msup>
                <mi>y</mi>
                <mrow data-mjx-texclass="ORD">
                  <mo>∧</mo>
                </mrow>
              </msup>
            </mrow>
          </math>
        </disp-formula>
      
      <p>In Eq. (7) the <italic>y</italic> represents the actual output and <italic>y</italic>^ is predicted output. The difference between the actual and predicted input is calculated and with the modules operator analyses the network designed. If error value is high it represents that the variations are more and perdition efficiency is less, whereas if error value is less that shows the similarity index between input and perdition is more and that shows the high efficiency of the designed network.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>4. Design of deep cnn inter prediction network</title>
      <p style="text-indent: 10pt;">The model designed for inter-prediction focus on computational complexity reduction [<xref ref-type="bibr" rid="ref_13">13</xref>]. The model’s input is video frame and the output is its representation as 16 length vectors. The convolution [<xref ref-type="bibr" rid="ref_14">14</xref>] procedure converts it to a minimum of 16 length vector by using various filter n 4 layers by extracting global and local information. <xref ref-type="fig" rid="fig_4">Figure 4</xref> represent the total design and layers in deep depth decision algorithm.</p><p style="text-indent: 10.2pt;">Initially after preprocessing the input to YUV the frame is cropped to 64x64, its send to a convolution layer with filter size 5x5 to extract the global content and is maxpooled to 4x4 range resulting in a 16x16 image patch. Meanwhile a cropped 32x32 part is convoluted with same filter of 5x5 and pooled with 2x2 to get same 16x16 patch. Both are concatenated and send to various convolution layer of different filters. The activation function used is ReLu and the stride as the length of the filter to reduce computation. The output is sent to fully connected network that flatten the input. The flattened input with weight and bias is compressed to lower sizes. SoftMax is done in last stage to generate the 16-length vector. </p>
      
        <fig id="fig_4">
          <label>Figure 4</label>
          <caption>Deep depth decision algorithm with convolutional neural network a sample model [<xref ref-type="bibr" rid="ref_12">12</xref>]</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/0/img_L0uauWGuwPHDrL82.png"/>
        </fig>
      
      <p>The model designed for inter-prediction focus on computational complexity reduction. The model’s input is video frame [<xref ref-type="bibr" rid="ref_15">15</xref>] and the output is its representation as 16 length vectors. The convolution procedure converts it to a minimum of 16 length vector by using various filter n 4 layers by extracting global and local information. <xref ref-type="fig" rid="fig_4">Figure 4</xref> represent the total design and layers in deep depth decision algorithm.</p><p>Initially after preprocessing the input to YUV [<xref ref-type="bibr" rid="ref_16">16</xref>] the frame is cropped to 64x64, its send to a convolution layer with filter size 5x5 to extract the global content and is maxpooled to 4x4 range resulting in a 16x16 image patch. Meanwhile a cropped 32x32 [<xref ref-type="bibr" rid="ref_17">17</xref>] part is convoluted with same filter of 5x5 and pooled with 2x2 to get same 16x16 patch. Both are concatenated and send to various convolution layer of different filters. The activation function used is ReLu and the stride as the length of the filter to reduce computation. The output is sent to fully connected network that flatten the input. The flattened input with weight and bias is compressed to lower sizes. SoftMax is done in last stage to generate the 16-length vector.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>5. Evaluation of deep depth decision algorithm</title>
      <p>The proposed model is evaluated on a sample data set of various video frames. The dataset is rich with various resolution video frames for the test, train and validation. It consists of a video frame and its labels. The frame is split to 64x64, 32x32 etc and its corresponding label s saved in another folder. For each image patch, a python list of 16 element length is saved. Using this input and output folder, the model designed is trained, tested and validated.</p><p>The data set used for this purpose has wide range of video frames split into image frames the size of the dataset is 110,000 images for testing and 40,000for validation of various resolution. The images are split into test train and validate to check the performance of the algorithm. Each image has a label associated with it which hold the frame number, CTU number, video number etc. </p><p>The image file is further split into 64x64,32x32 size patches for analysis. The label representing it is a python list. Image and label are interlinked. The resolution is of wide range from 4K to lower resolution for evaluation. The images and video frames in the dataset are of YUV with luminance and chrominance data. This data set support only YUV file videos for analysis. The test to validation split ratio is maintained properly and overlapping of members in the data set are avoided to get genuine output. The dataset can be extended for further analysis too. Adding the information to the dataset can enhance the performance of the algorithm.</p><p>The loss observed for the testing for cross entropy loss and can be represented as:</p>
      
        <disp-formula>
          <label>(8)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>L</mi>
            <mo>=</mo>
            <mfrac>
              <mn>1</mn>
              <mi>R</mi>
            </mfrac>
            <munderover>
              <mo data-mjx-texclass="OP">∑</mo>
              <mrow data-mjx-texclass="ORD">
                <mi>γ</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>R</mi>
            </munderover>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">[</mo>
              <mo data-mjx-texclass="NONE">⁡</mo>
              <mo>+</mo>
              <mo data-mjx-texclass="NONE">⁡</mo>
              <mo data-mjx-texclass="CLOSE">]</mo>
              <msup>
                <mi>y</mi>
                <mi>r</mi>
              </msup>
              <msup>
                <mi>y</mi>
                <mrow data-mjx-texclass="ORD">
                  <mo>∧</mo>
                  <mi>r</mi>
                </mrow>
              </msup>
              <mi>log</mi>
              <mi>log</mi>
              <mrow data-mjx-texclass="INNER">
                <mo data-mjx-texclass="OPEN">(</mo>
                <mo>−</mo>
                <mo data-mjx-texclass="CLOSE">)</mo>
                <mn>1</mn>
                <msup>
                  <mi>y</mi>
                  <mi>r</mi>
                </msup>
              </mrow>
              <mrow data-mjx-texclass="INNER">
                <mo data-mjx-texclass="OPEN">(</mo>
                <mo>−</mo>
                <mo data-mjx-texclass="CLOSE">)</mo>
                <mn>1</mn>
                <msup>
                  <mi>y</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mo>∧</mo>
                    <mi>r</mi>
                  </mrow>
                </msup>
              </mrow>
            </mrow>
          </math>
        </disp-formula>
      
      <p>The model designed is tested for various input in the data set and the output observed can be noted as follows.</p><p>The training loss of the deep depth decision algorithm is calculated as 3.1049. The prediction accuracy of each label in the algorithm is 66.12%. The evaluation of the designed model can be done by using pipelining the proposed algorithm to the original HEVC system and evaluate the performance of both. The performance evaluation is given in <xref ref-type="fig" rid="fig_5">Figure 5</xref>. Subgraph (a) of <xref ref-type="fig" rid="fig_5">Figure 5</xref> represents various video frame samples compressed with original HEVC and subgraph (b) of <xref ref-type="fig" rid="fig_5">Figure 5</xref> represents the results of compression by pipelining deep depth decision algorithm in it. The <xref ref-type="fig" rid="fig_6">Figure 6</xref> represent the comparison chart showing the ‘time of encoding” and “Bitrate” of samples with original method and our proposed method. The comparison chart showing the ‘YUV-PSNR” and “Y-PSNR” of samples with original method and our proposed method is plotted in <xref ref-type="fig" rid="fig_7">Figure 7</xref>. <xref ref-type="fig" rid="fig_8">Figure 8</xref> shows the comparison chart showing the ‘time of encoding of HEVC and CNN” and “Bitrate of HEVC and CNN” of various samples. The results clearly shows that the encoding time have drastically reduced in proposed method. This proves that proposed method helps in reducing the computational complexity of the intra predation in HEVC.</p>
      
        <fig id="fig_5">
          <label>Figure 5</label>
          <caption>Output window showing the encoding time, Y-PSNR, U-PSNR, V-PSNR, YUV-PSNR of a sample video with 20 frame: (a) using HEVC; (b)by deep depth decision algorithm.</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/0/img_13VK1uOipI1vmPL8.png"/>
        </fig>
      
      <p>The results clearly show the reduction in encoding time thus proves the complexity reduction in computation of intra perdition.</p>
      
        <fig id="fig_6">
          <label>Figure 6</label>
          <caption>The comparison chart showing the ‘time of encoding” and “Bitrate” of samples with original method and our proposed method</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/0/img_lX6DIqVALhGyED57.png"/>
        </fig>
      
      
        <fig id="fig_7">
          <label>Figure 7</label>
          <caption>The comparison chart showing the ‘YUV-PSNR” and “Y-PSNR” of samples with original method and our proposed method</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/0/img_rTO83ZhKdPpCjdS0.png"/>
        </fig>
      
      
        <fig id="fig_8">
          <label>Figure 8</label>
          <caption>The comparison chart showing the “time of encoding of HEVC and CNN” and “Bitrate of HEVC and CNN” of samples</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/0/img_GH2x3myJm7OIkQgQ.png"/>
        </fig>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>6. Conclusion</title>
      <p>The need for compression in a smarted way leads to the plan of developing a model based on convolutional neural network to reduce the computational complexity in intra-prediction. The complexity of the calculation in HEVC prediction is closely connected to encoding time. A deep depth decision approach is proposed here to evaluate the depth of CTU with a deep CNN network and store it as a 16-element vector by deleting superfluous elements instead of closing it as a 16*16 matrix. This saves not only the bit but also the computation time. The proposed approach is included into the original HEVC encoder (through HM software) to test its performance in a real-world setting. The time for encoding, bit rate, Y-PSNR, and YVU PSNR with and without the deep depth decision technique in the HEVC encoder are all evaluated. The proposed model, its motivation, design and evaluation id incorporated in this paper. The model is designed with the support of deep convolutional neural network with the input as 64X64 coding tree unit and output as 16-length vector. The model chooses each element in the design carefully to extract the essence of the coding tree unit to compress and reconstruct with high efficiency. The kernels, depth, pooling, stride is selected rightly to extract the features. The fully connected network compresses the information. The model possesses a loss of 3.1049 on test model. The accuracy of the prediction while using the dataset on a sample test model is 66.12%. This model can be used to pipeline with the existing HEVC encoder to check its compatibility and efficiency with the existing model.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      <p>The authors are greatly indebted to the anonymous reviewers whose thought-provoking and encouraging comments have motivated them to modify significantly and update the paper. They also like to express their gratitude to REVA University for extending research facilities to carry out this research.</p>
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p></p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>H.</given-names>
              <surname>Kalva</surname>
            </name>
          </person-group>
          <article-title>Standards: The H.264 video coding standard</article-title>
          <source>IEEE Multimed.</source>
          <year>2006</year>
          <volume>13</volume>
          <issue>4</issue>
          <page-range>86-90</page-range>
          <fpage>86</fpage>
          <lpage>90</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/MMUL.2006.93</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>H. K.</given-names>
              <surname>Joy</surname>
            </name>
            <name>
              <given-names>M. R.</given-names>
              <surname>Kounte</surname>
            </name>
          </person-group>
          <article-title>A comprehensive review of traditional video processing</article-title>
          <source>Adv. Sci. Technol. Eng. Syst.</source>
          <year>2020</year>
          <volume>5</volume>
          <issue>6</issue>
          <page-range>274-279</page-range>
          <fpage>274</fpage>
          <lpage>279</lpage>
          <pub-id pub-id-type="doi">http://dx.doi.org/10.25046/aj050633</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>B.</given-names>
              <surname>Bross</surname>
            </name>
            <name>
              <given-names>Y. K.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Ye</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>G. J.</given-names>
              <surname>Sullivan</surname>
            </name>
            <name>
              <given-names>J. R.</given-names>
              <surname>Ohm</surname>
            </name>
          </person-group>
          <article-title>Overview of the Versatile Video Coding (VVC) standard and its applications</article-title>
          <source>IEEE Trans. Circuits Syst. Video Technol.</source>
          <year>2021</year>
          <volume>31</volume>
          <issue>0</issue>
          <page-range>3736-3764</page-range>
          <fpage>3736</fpage>
          <lpage>3764</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/TCSVT.2021.3101953</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>A.</given-names>
              <surname>Dhanalakshmi</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Nagarajan</surname>
            </name>
          </person-group>
          <article-title>Combined spatial temporal based In-loop filter for scalable extension of HEVC</article-title>
          <source>ICT Express</source>
          <year>2020</year>
          <volume>6</volume>
          <issue>4</issue>
          <page-range>306-311</page-range>
          <fpage>306</fpage>
          <lpage>311</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1016/j.icte.2020.04.006</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Bouaafia</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Khemiri</surname>
            </name>
            <name>
              <given-names>F. E.</given-names>
              <surname>Sayadi</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Atri</surname>
            </name>
          </person-group>
          <article-title>Fast CU partition-based machine learning approach for reducing HEVC complexity</article-title>
          <source>J. Real-Time Image Process.</source>
          <year>2020</year>
          <volume>17</volume>
          <issue>1</issue>
          <page-range>185-196</page-range>
          <fpage>185</fpage>
          <lpage>196</lpage>
          <pub-id pub-id-type="doi">http://dx.doi.org/10.1007/s11554-019-00936-0</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>L.</given-names>
              <surname>Zhao</surname>
            </name>
            <name>
              <given-names>S. Q.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>X. F.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>S. S.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>S. W.</given-names>
              <surname>Ma</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Gao</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Enhanced Ctu-level inter prediction with deep frame rate up-conversion for high efficiency video coding</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2018 25th IEEE International Conference on Image Processing</conf-name>
          <conf-acronym>ICIP</conf-acronym>
          <conf-loc>Athens, Greece</conf-loc>
          <conf-date>October 7-10, 2018</conf-date>
          <year>2018</year>
          <volume/>
          <issue/>
          <page-range>206-210</page-range>
          <fpage>206</fpage>
          <lpage>210</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICIP.2018.8451465.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>O.</given-names>
              <surname>Alharbi</surname>
            </name>
          </person-group>
          <article-title>A deep learning approach combining CNN and Bi-LSTM with SVM classifier for Arabic sentiment analysis</article-title>
          <source>Int. J. Adv. Comput. Sci. Appl.</source>
          <year>2021</year>
          <volume>12</volume>
          <issue>6</issue>
          <page-range>165-172</page-range>
          <fpage>165</fpage>
          <lpage>172</lpage>
          <pub-id pub-id-type="doi">http://dx.doi.org/10.14569/IJACSA.2021.0120618</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>H. K.</given-names>
              <surname>Joy</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Deep learning approach in intra -prediction of high efficiency video coding</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2020 International Conference on Smart Technologies in Computing, Electrical and Electronics</conf-name>
          <conf-acronym>ICSTCEE</conf-acronym>
          <conf-loc>Bengaluru, India, December 8</conf-loc>
          <conf-date>2020</conf-date>
          <year>2020</year>
          <volume/>
          <issue/>
          <page-range>134-138</page-range>
          <fpage>134</fpage>
          <lpage>138</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICSTCEE49637.2020.9277189.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>D.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Wu</surname>
            </name>
          </person-group>
          <article-title>Deep learning-based technology in responses to the joint call for proposals on video compression with capability beyond HEVC</article-title>
          <source>IEEE Trans. Circuits Syst. Video Technol.</source>
          <year>2020</year>
          <volume>30</volume>
          <issue>5</issue>
          <page-range>1267-1280</page-range>
          <fpage>1267</fpage>
          <lpage>1280</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/TCSVT.2019.2945057</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>X.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Gong</surname>
            </name>
          </person-group>
          <article-title>Run-time deep learning enhanced fast coding unit decision for high efficiency video coding</article-title>
          <source>J. Circuits Syst. Comput.</source>
          <year>2020</year>
          <volume>29</volume>
          <issue>3</issue>
          <page-range>1-19</page-range>
          <fpage>1</fpage>
          <lpage>19</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1142/S0218126620500462</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Zuo</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Gu</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Zhao</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Zhang</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Learning convolutional networks for content-weighted image compression</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Lake City, UT, USA</conf-name>
          <conf-acronym/>
          <conf-loc>December 16</conf-loc>
          <conf-date>2018</conf-date>
          <year>2018</year>
          <volume/>
          <issue/>
          <page-range>3214-3223</page-range>
          <fpage>3214</fpage>
          <lpage>3223</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/CVPR.2018.00339.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>H. K.</given-names>
              <surname>Joy</surname>
            </name>
            <name>
              <given-names>M. R.</given-names>
              <surname>Kounte</surname>
            </name>
            <name>
              <given-names>B. K.</given-names>
              <surname>Sujatha</surname>
            </name>
          </person-group>
          <article-title>Design and Implementation of deep depth decision algorithm for complexity reduction in High Efficiency Video Coding (HEVC)</article-title>
          <source>Int. J. Adv. Comput. Science Appl.</source>
          <year>2022</year>
          <volume>12</volume>
          <issue>1</issue>
          <page-range>553-560</page-range>
          <fpage>553</fpage>
          <lpage>560</lpage>
          <pub-id pub-id-type="doi">http://dx.doi.org/10.14569/IJACSA.2022.0130168</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>M. U. K.</given-names>
              <surname>Khan</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Shafique</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Henkel</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>An adaptive complexity reduction scheme with fast prediction unit decision for HEVC intra encoding</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2013 IEEE International Conference on Image Processing</conf-name>
          <conf-acronym>ICIP</conf-acronym>
          <conf-loc>Melbourne, VIC, Australia</conf-loc>
          <conf-date>September 15-18, 2013</conf-date>
          <year>2013</year>
          <volume/>
          <issue/>
          <page-range>1578-1582</page-range>
          <fpage>1578</fpage>
          <lpage>1582</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICIP.2013.6738325.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>Z.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Yu</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Wang</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>CNN oriented fast HEVC intra CU mode decision</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2016 IEEE International Symposium on Circuits and Systems</conf-name>
          <conf-acronym>ISCAS</conf-acronym>
          <conf-loc>Montreal, QC, Canada</conf-loc>
          <conf-date>May 22-25, 2016</conf-date>
          <year>2016</year>
          <volume/>
          <issue/>
          <page-range>2270-2273</page-range>
          <fpage>2270</fpage>
          <lpage>2273</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ISCAS.2016.7539036.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>Z.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Yu</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Gao</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Ji</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Wang</surname>
            </name>
          </person-group>
          <article-title>CU partition mode decision for HEVC hardwired Intra encoder using convolution neural network</article-title>
          <source>IEEE T. Image Process.</source>
          <year>2016</year>
          <volume>25</volume>
          <issue>1</issue>
          <page-range>5088-5103</page-range>
          <fpage>5088</fpage>
          <lpage>5103</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/tip.2016.2601264</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>Y.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Kwong</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Yuan</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Pan</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Xu</surname>
            </name>
          </person-group>
          <article-title>Machine learning-based coding unit depth decisions for flexible complexity allocation in high efficiency video coding</article-title>
          <source>IEEE TIP</source>
          <year>2015</year>
          <volume>24</volume>
          <issue>7</issue>
          <page-range>2225-2238</page-range>
          <fpage>2225</fpage>
          <lpage>2238</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/tip.2015.2417498</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Bouaafia</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Khemiri</surname>
            </name>
            <name>
              <given-names>F. E.</given-names>
              <surname>Sayadi</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Atri</surname>
            </name>
          </person-group>
          <article-title>Fast CU partition-based machine learning approach for reducing HEVC complexity</article-title>
          <source>J. Real-Time Image Process.</source>
          <year>2020</year>
          <volume>17</volume>
          <issue>1</issue>
          <page-range>185-196</page-range>
          <fpage>185</fpage>
          <lpage>196</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/s11554-019-00936-0</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>