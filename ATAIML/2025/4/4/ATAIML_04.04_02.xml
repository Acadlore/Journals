<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-EB0GLB6d5QMcqPDsSTKkmZGsfswO7vVG</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml040402</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>LMS-YOLO: A StarNet-Enhanced Lightweight Framework for Robust Marine Object Detection in Complex Water Surface Environments</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1,2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-2817-9684</contrib-id>
          <name>
            <surname>Sun</surname>
            <given-names>Yuhan</given-names>
          </name>
          <email>23226023@stu.sdjtu.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1,2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7807-5892</contrib-id>
          <name>
            <surname>Liu</surname>
            <given-names>Xin</given-names>
          </name>
          <email>axinzaixian@163.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1,2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-5688-8794</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Qingfa</given-names>
          </name>
          <email>23226028@stu.sdjtu.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1,2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0008-3737-0482</contrib-id>
          <name>
            <surname>Shao</surname>
            <given-names>Mingzhi</given-names>
          </name>
          <email>23226021@stu.sdjtu.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1,2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-4559-6842</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Tengwen</given-names>
          </name>
          <email>23226029@stu.sdjtu.edu.cn</email>
        </contrib>
        <aff id="aff_1">College of Shipbuilding and Port Engineering, Shandong Jiaotong University, 264210 Weihai, China</aff>
        <aff id="aff_2">Weihai Institute of Marine Information Science and Technology, 264200 Weihai, China</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>16</day>
        <month>10</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>4</issue>
      <fpage>247</fpage>
      <lpage>262</lpage>
      <page-range>247-262</page-range>
      <history>
        <date date-type="received">
          <day>21</day>
          <month>08</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>11</day>
          <month>10</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Accurate and efficient detection of small-scale targets on dynamic water surfaces remains a critical challenge in the deployment of unmanned surface vehicles (USVs) for maritime applications. Complex background interference—such as wave motion, sunlight reflections, and low contrast—often leads to missed or false detections, particularly when using conventional convolutional neural networks. To address these issues, this study introduces LMS-YOLO, a lightweight detection framework built upon the YOLOv8n architecture and optimized for real-time marine object recognition. The proposed network integrates three key components: (1) a C2f-SBS module incorporating StarNet-based Star Blocks, which streamlines multi-scale feature extraction while reducing parameter overhead; (2) a Shared Convolutional Lightweight Detection Head (SCLD), designed to enhance detection precision across scales using a unified convolutional strategy; and (3) a Mixed Local Channel Attention (MLCA) module, which reinforces context-aware representation under complex maritime conditions. Evaluated on the WSODD and FloW-Img datasets, LMS-YOLO achieves a 5.5% improvement in precision and a 2.3% gain in mAP@0.5 compared to YOLOv8n, while reducing parameter count and computational cost by 37.18% and 34.57%, respectively. The model operates at 128 FPS on standard hardware, demonstrating its practical viability for embedded deployment in marine perception systems. These results highlight the potential of LMS-YOLO as a deployable solution for high-speed, high-accuracy marine object detection in real-world environments.</p></abstract>
      <kwd-group>
        <kwd>Marine object detection</kwd>
        <kwd>YOLOv8</kwd>
        <kwd>StarNet</kwd>
        <kwd>Lightweight neural network</kwd>
        <kwd>Shared convolution</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="5"/>
        <fig-count count="12"/>
        <table-count count="5"/>
        <ref-count count="20"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>In recent years, the rapid integration of USVs into a wide range of maritime applications—including environmental monitoring, emergency rescue, oceanographic surveys, and waterborne logistics—has significantly raised the demand for accurate perception capabilities in complex aquatic environments. Among the core perception tasks, marine object detection plays a decisive role, directly impacting the autonomous navigation, situational awareness, and operational safety of USVs. However, detecting small-scale floating objects such as buoys, debris, and watercraft under dynamic maritime conditions remains a persistent challenge. Factors such as cluttered backgrounds, variable lighting, low contrast, and the inherently small size of many targets contribute to high false-positive and false-negative rates. These difficulties have led to a growing research focus on the development of deep learning–based object detection techniques tailored for marine scenarios.</p><p>Contemporary object detection methods based on deep convolutional neural networks are typically categorized into two paradigms: two-stage and one-stage detectors. Two-stage frameworks, such as Faster R-CNN, first generate region proposals and then perform classification and regression tasks [<xref ref-type="bibr" rid="ref_1">1</xref>]. While offering high detection accuracy, these models are often unsuitable for real-time deployment due to their heavy computational burden. In contrast, one-stage detectors perform classification and localization in a unified manner, typically using predefined anchor boxes, and are known for their speed and simplicity. The YOLO family has emerged as a representative of one-stage models, achieving impressive results across a variety of vision tasks [<xref ref-type="bibr" rid="ref_2">2</xref>].</p><p>From YOLOv3 [<xref ref-type="bibr" rid="ref_3">3</xref>] to the more recent YOLOv4 [<xref ref-type="bibr" rid="ref_4">4</xref>], YOLOv5 [<xref ref-type="bibr" rid="ref_5">5</xref>], YOLOv6 [<xref ref-type="bibr" rid="ref_6">6</xref>], and YOLOv7 [<xref ref-type="bibr" rid="ref_7">7</xref>], successive versions have brought notable improvements in detection accuracy, robustness, and computational efficiency. Nevertheless, existing models continue to face challenges when applied to small object detection in maritime environments, where maintaining a balance between accuracy and model complexity is crucial. For example, Huang et al. [<xref ref-type="bibr" rid="ref_8">8</xref>] proposed an improved YOLOv4-based ship detection algorithm that replaces the original SPP structure with an RFB_s module to expand multi-scale receptive fields and introduces the CBAM attention mechanism to enhance feature representation. The method significantly improves detection accuracy for small ships and reduces background interference, though it still shows performance degradation in complex maritime scenes. Zhang et al. [<xref ref-type="bibr" rid="ref_9">9</xref>] developed SE-NMS-YOLOv5 by integrating SE attention to strengthen channel features, optimizing the NMS process to alleviate missed detections under occlusion,This approach improves detection precision and recall in multi-target maritime environments but remains limited by environmental variations such as reflection and light fluctuation. Jiang et al. [<xref ref-type="bibr" rid="ref_10">10</xref>] developed YOLOv7-Ship by incorporating CA-M and ODConv modules, achieving improvements in both accuracy and real-time performance at the cost of increased architectural complexity. Li et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] proposed YOLO-WSD, which employed a C2F-E module and WIoU loss function to enhance feature representation but still faced challenges in multi-scale feature fusion. More recently, Wang and Zhao [<xref ref-type="bibr" rid="ref_12">12</xref>] proposed YOLOv8-MSS, incorporating an additional detection head and SENetV2 module to improve performance on small targets, though issues related to redundant computations and latency persisted.</p><p>Despite these advancements, several core limitations remain unresolved: insufficient lightweight design, reduced robustness in dynamic environments, and inadequate multi-scale feature fusion. These issues are particularly pressing in embedded deployment scenarios—such as those involving USVs and UAVs—where computational resources are constrained. Deploying large-scale detection networks under such conditions can cause processing delays, hinder real-time decision-making, and even reduce the operational lifespan of the onboard hardware due to sustained high-load computation. Furthermore, environmental challenges—such as continuous wave motion, strong backlighting, and surface reflections—compound the difficulty of detecting small floating targets reliably.</p><p>To address these persistent limitations, this paper presents LMS-YOLO, a lightweight and high-performance marine object detection framework built upon the YOLOv8n backbone. The proposed architecture introduces three core innovations that collectively enhance detection accuracy while preserving computational efficiency:</p><p>(1) C2f-SBS Module: A modified backbone structure that integrates Star Blocks from StarNet [<xref ref-type="bibr" rid="ref_13">13</xref>], reducing parameter count and memory footprint while reinforcing multi-scale feature extraction through a streamlined convolutional design.</p><p>(2) SCLD Head: A novel detection head architecture that shares convolutional parameters across multiple scales, significantly reducing redundancy without compromising localization or classification performance.</p><p>(3) MLCA Module: A lightweight attention mechanism [<xref ref-type="bibr" rid="ref_14">14</xref>] that synergistically combines local and global spatial-channel features to improve detection robustness under challenging maritime conditions, such as surface reflections, wave distortion, and fog.</p><p>By incorporating these modules, LMS-YOLO achieves an improved trade-off between accuracy and model compactness, making it suitable for real-time deployment in embedded marine perception systems. The effectiveness of the proposed framework is validated through extensive experiments on public benchmarks and real-world datasets.</p>
    </sec>
    <sec sec-type="">
      <title>2. Yolov8 algorithm</title>
      <p>The YOLOv8 framework is structured into four key components: input processing, backbone network, neck, and output head, as illustrated in <xref ref-type="fig" rid="fig_1">Figure 1</xref>. At the input stage, the model applies a combination of preprocessing techniques including Mosaic data augmentation, adaptive image resizing, and grayscale padding, aiming to improve generalization and maintain spatial consistency across variable input dimensions.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>The structure of YOLOv8, highlighting its anchor-free head and improved C2f-based backbone</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_hVLFjZXyZ-92z961.png"/>
        </fig>
      
      <p>The backbone is responsible for hierarchical feature extraction and is constructed using a series of convolutional layers, C2f blocks, and a Spatial Pyramid Pooling—Fast (SPPF) module. These elements collectively enhance the network’s ability to capture both local and global contextual information. The neck adopts a Path Aggregation Network (PANet) structure to facilitate the fusion of multi-scale feature maps via up-sampling and down-sampling operations. Feature representations from different levels are integrated to better capture objects of varying sizes.</p><p>In the output head, YOLOv8 employs a decoupled architecture that processes classification and regression tasks independently. This separation allows the network to optimize the two tasks more effectively. During training, the model assigns positive and negative samples based on Intersection-over-Union (IoU) thresholds, and calculates the overall loss using a composite formulation: binary cross-entropy (BCE) for classification and a combination of Distribution Focal Loss (DFL) and Complete Intersection-over-Union (CIoU) for bounding box regression [<xref ref-type="bibr" rid="ref_15">15</xref>], [<xref ref-type="bibr" rid="ref_16">16</xref>].</p><p>Compared with its predecessor YOLOv5, YOLOv8 incorporates several important architectural changes aimed at improving both efficiency and accuracy. In the backbone, YOLOv8 replaces the C3 modules found in YOLOv5 with C2f blocks, which reduce the number of intermediate channels and promote denser gradient propagation. This leads to a more compact network with enhanced convergence characteristics. In the neck, the up-sampling mechanism is redesigned by eliminating convolutional operations prior to up-sampling; instead, a down-sampling operation is first applied, followed by feature aggregation through the updated C2f modules.</p><p>Perhaps most notably, the prediction head in YOLOv8 shifts away from the traditional anchor-based detection paradigm. The model adopts an anchor-free approach, directly predicting the center coordinates of objects along with their relative width and height. This simplification eliminates the need for predefined anchor boxes, streamlines the matching strategy during training, and contributes to improved inference speed and accuracy [<xref ref-type="bibr" rid="ref_17">17</xref>].</p><p>Overall, these enhancements enable YOLOv8 to achieve a favorable trade-off between detection precision and computational efficiency, making it a strong candidate for real-time object detection in both general-purpose and resource-constrained applications.</p>
    </sec>
    <sec sec-type="">
      <title>3. Lms-yolo</title>
      <p>This work introduces LMS-YOLO, an enhanced object detection framework built upon the YOLOv8n baseline, specifically optimized for detecting marine targets with improved accuracy and computational efficiency, as illustrated in <xref ref-type="fig" rid="fig_2">Figure 2</xref>. To reduce model complexity while enhancing feature expressiveness, the standard C2f modules in both the backbone and neck are replaced with a newly designed C2f-SBS module. This substitution simplifies the overall architecture, reduces the number of parameters and floating-point operations, and strengthens the model’s capacity for multi-scale feature representation.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Schematic of LMS-YOLO integrating C2f-SBS, MLCA, and SCLD modules</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_V2VLht9B5L9rwJRm.png"/>
        </fig>
      
      <p>To further improve the network’s ability to capture informative features in complex maritime environments, a MLCA module is embedded within the backbone. This lightweight attention mechanism enhances the extraction of context-aware features by integrating both local and global spatial-channel relationships.</p><p>In addition, the original detection head is replaced by a SCLD Head. This component allows feature maps from multiple detection scales to be processed using shared convolutional layers, effectively reducing parameter redundancy and computational load while preserving scale-specific representational quality.</p>
      
        <sec>
          
            <title>3.1. C2f-sbs</title>
          
          <p>In the YOLOv8 architecture, the C2f module employs multiple bottleneck layers to stabilize training by alleviating vanishing and exploding gradient problems, thereby supporting deeper network construction and improved convergence. Despite these advantages, the repeated stacking of bottlenecks imposes substantial computational overhead and lacks mechanisms for selectively enhancing multi-scale features—factors that hinder both efficiency and adaptability in complex detection tasks.</p><p>To overcome these limitations, this study introduces Star Blocks from the StarNet architecture to replace the dual-branch structure of the original C2f design, as illustrated in <xref ref-type="fig" rid="fig_3">Figure 3</xref>. Developed by Microsoft in 2024, StarNet is a lightweight neural framework that utilizes a star-shaped computation strategy to effectively capture high-dimensional and nonlinear feature interactions. Importantly, it achieves this without increasing computational complexity, making it well suited for integration into lightweight detection networks.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Structure of Star Blocks used in the C2f-SBS module for lightweight feature mapping</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img__Z4zSTndgF6yXbA2.png"/>
            </fig>
          
          <p>StarNet employs a hierarchical design with depthwise separable convolutions and batch normalization (BN) to optimize feature extraction. Specifically, Eq. (1) illustrates that the input feature $x<inline-formula>
  <mml:math id="mbtbeznyg7">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>F<inline-formula>
  <mml:math id="mu18ki73xh">
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>B</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mn>2</mml:mn>
  </mml:math>
</inline-formula>f_C<inline-formula>
  <mml:math id="miat2stvq4">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>U</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>y<inline-formula>
  <mml:math id="m5on2fg4gh">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>x$ via a residual connection, ensuring continuity and stability in information propagation.</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="mghaxtnjl1">
                <mml:mi>x</mml:mi>
                <mml:mi>F</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:msub>
                  <mml:mi>f</mml:mi>
                  <mml:mrow>
                    <mml:mi>D</mml:mi>
                    <mml:mi>W</mml:mi>
                    <mml:mi>C</mml:mi>
                  </mml:mrow>
                </mml:msub>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="mai1700p7c">
                <mml:mi>y</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:msub>
                  <mml:mi>f</mml:mi>
                  <mml:mi>C</mml:mi>
                </mml:msub>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>∗</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mi>ReLU</mml:mi>
                  <mml:mi>x</mml:mi>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msub>
                      <mml:mi>f</mml:mi>
                      <mml:mi>C</mml:mi>
                    </mml:msub>
                    <mml:mi>x</mml:mi>
                  </mml:mrow>
                  <mml:msub>
                    <mml:mi>f</mml:mi>
                    <mml:mi>C</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>In Eq. (1), $F<inline-formula>
  <mml:math id="mtalx50bb8">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:msup>
      <mml:mi>e</mml:mi>
      <mml:mo>′</mml:mo>
    </mml:msup>
  </mml:math>
</inline-formula>x<inline-formula>
  <mml:math id="m0nf6cy689">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>f_DWC<inline-formula>
  <mml:math id="mtuw0sox91">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>2</mml:mn>
  </mml:math>
</inline-formula>y<inline-formula>
  <mml:math id="mc29fh6aok">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>x<inline-formula>
  <mml:math id="mxwvraax37">
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula>f_C$ denotes the convolution plus BN operation, ReLU is the rectified linear unit activation function, and * represents element-wise multiplication, where corresponding elements of two tensors with identical shapes are multiplied.</p><p>We propose the C2f-SBS module, derived from StarNet, as illustrated in <xref ref-type="fig" rid="fig_4">Figure 4</xref>. Unlike traditional bottleneck designs that incur redundant computation due to dimension expansion and reduction, C2f-SBS replaces bottlenecks with Star Blocks, leveraging star-shaped operations to efficiently capture high-dimensional and nonlinear features in low-dimensional spaces while substantially reducing parameters. Importantly, this reduction in parameters does not compromise the extraction of marine object features; instead, it enhances the expressive power of the network.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>C2f-SBS module design used in LMS-YOLO for efficient multi-scale feature representation</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_ffDObESrGwldkQkh.png"/>
            </fig>
          
          <p>The detailed structure of the C2f-SBS module is illustrated in <xref ref-type="fig" rid="fig_4">Figure 4</xref>. First, the input data passes through a convolutional layer (Conv) to extract initial features, followed by a Split operation that divides the output features into two branches. In the main path, features are processed through multiple Star Blocks modules, performing layer-by-layer feature extraction and refinement. Simultaneously, the features in the secondary branch are concatenated (Concat) with the main path output at the end, enabling effective multi-scale feature fusion and enhancing the model’s feature representation capability. Finally, the concatenated features are passed through a convolutional layer to generate the output feature map. Compared with the original neck convolutional structure, the C2f-SBS module significantly improves feature extraction efficiency while reducing computational complexity, demonstrating superior performance advantages.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Mlca</title>
          
          <p>In complex and dynamic maritime environments, conventional object detection networks continue to encounter considerable challenges. Factors such as background clutter, wave-induced distortions, and lighting variations frequently contribute to missed detections or false positives—issues that are particularly pronounced when detecting small-scale targets.</p><p>To mitigate these effects, a lightweight MLCA module is integrated into the final stage of the backbone network. This module is designed to refine feature representation by simultaneously capturing channel-wise dependencies and spatial contextual information, while balancing both local and global feature interactions within the receptive field.</p><p>By reinforcing informative features and suppressing irrelevant background noise, the MLCA module significantly enhances the network’s robustness in visually complex scenarios, leading to improved detection accuracy and reduced false detection rates—especially for small marine objects. The detailed structure of the MLCA module is illustrated in <xref ref-type="fig" rid="fig_5">Figure 5</xref>.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Architecture of the MLCA module for enhancing spatial and channel attention</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_MvHOQcGkNECPLkeG.png"/>
            </fig>
          
          <p>The structure of the MLCA network is shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>. The core of the MLCA module lies in fusing local and global features while integrating both channel and spatial information, thereby enhancing the model’s attention to informative features. First, Eq. (3) extracts channel descriptors <inline-formula>
  <mml:math id="mzy6g0cdn3">
    <mml:msub>
      <mml:mi>f</mml:mi>
      <mml:mi>c</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> from the feature map <inline-formula>
  <mml:math id="mjh0st34fi">
    <mml:mi>F</mml:mi>
    <mml:mo>∈</mml:mo>
    <mml:msup>
      <mml:mi>R</mml:mi>
      <mml:mrow>
        <mml:mi>C</mml:mi>
        <mml:mi>H</mml:mi>
        <mml:mi>W</mml:mi>
        <mml:mo>×</mml:mo>
        <mml:mo>×</mml:mo>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> using Global Average Pooling (GAP), where $C<inline-formula>
  <mml:math id="mu865xs0du">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>H<inline-formula>
  <mml:math id="m5g0otlm2z">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>W<inline-formula>
  <mml:math id="myo40el4a1">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mn>4</mml:mn>
  </mml:math>
</inline-formula>M_c$, which are mapped to the [0,1] range via the Sigmoid function to emphasize important channels and suppress irrelevant ones.</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="mqrlf77ux8">
                <mml:msub>
                  <mml:mi>f</mml:mi>
                  <mml:mi>c</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>F</mml:mi>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mn>1</mml:mn>
                  <mml:mrow>
                    <mml:mi>H</mml:mi>
                    <mml:mi>W</mml:mi>
                    <mml:mo>×</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
                <mml:munderover>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>H</mml:mi>
                </mml:munderover>
                <mml:munderover>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>j</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>W</mml:mi>
                </mml:munderover>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="mgd84c2f67">
                <mml:msub>
                  <mml:mi>M</mml:mi>
                  <mml:mi>c</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mi>Sigmoid</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>⋅</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>W</mml:mi>
                    <mml:mn>1</mml:mn>
                  </mml:msub>
                  <mml:mi>ReLU</mml:mi>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msub>
                      <mml:mi>W</mml:mi>
                      <mml:mn>0</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>f</mml:mi>
                      <mml:mi>c</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>Here, SigmoidandReLUdenote activation functions, <inline-formula>
  <mml:math id="majni906wc">
    <mml:msub>
      <mml:mi>W</mml:mi>
      <mml:mn>0</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mj7gi2e9rd">
    <mml:msub>
      <mml:mi>W</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> are trainable weight matrices.</p><p>The spatial attention mechanism processes the feature map via convolution (typically with a kernel) to generate a spatial attention map <inline-formula>
  <mml:math id="mbsr8lbxfx">
    <mml:msub>
      <mml:mi>M</mml:mi>
      <mml:mi>S</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>, as shown in Eq. (5).</p>
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="m4pg6onaiy">
                <mml:msub>
                  <mml:mi>M</mml:mi>
                  <mml:mi>s</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mi>Sigmoid</mml:mi>
                <mml:mi>Conv</mml:mi>
                <mml:mi>F</mml:mi>
              </mml:math>
            </disp-formula>
          
          <p>Finally, Eq. (6) combines the channel attention <inline-formula>
  <mml:math id="mt97on06pt">
    <mml:msub>
      <mml:mi>M</mml:mi>
      <mml:mi>c</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> and spatial attention <inline-formula>
  <mml:math id="mqmmmppltp">
    <mml:msub>
      <mml:mi>M</mml:mi>
      <mml:mi>s</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> with the original feature map $F<inline-formula>
  <mml:math id="mrvai9vdj0">
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
  </mml:math>
</inline-formula>F^{\prime}$.</p>
          
            <disp-formula>
              <label>(6)</label>
              <mml:math id="mej0jxfb7i">
                <mml:msup>
                  <mml:mi>F</mml:mi>
                  <mml:mrow>
                    <mml:mi>′</mml:mi>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mo>×</mml:mo>
                <mml:mo>×</mml:mo>
                <mml:mi>F</mml:mi>
                <mml:msub>
                  <mml:mi>M</mml:mi>
                  <mml:mi>c</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>M</mml:mi>
                  <mml:mi>s</mml:mi>
                </mml:msub>
              </mml:math>
            </disp-formula>
          
          <p>By reinforcing informative features and suppressing noisy ones, the MLCA module enables the model to more accurately capture small target features in complex backgrounds, reducing both missed detections and false positives.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Scld head</title>
          
          <p>To improve detection precision while maintaining a lightweight structure, YOLOv8 adopts a decoupled head, in which classification and bounding box regression are processed independently. Although this design enhances detection accuracy, it also introduces significant parameter redundancy, thereby limiting its applicability in computationally constrained environments.</p><p>To address this limitation, we propose the SCLD Head, which utilizes a unified convolutional structure shared across multiple detection scales. This approach significantly reduces the number of parameters and computation without sacrificing representational capacity. The architectural design of the SCLD module is illustrated in <xref ref-type="fig" rid="fig_6">Figure 6</xref>.</p>
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Architecture of the SCLD head</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_8WMUpos99mNtJBHK.png"/>
            </fig>
          
          <p>The SCLD head receives feature maps from three resolution stages—P3, P4, and P5—in the YOLOv8n network. To mitigate the potential drawback of shared kernels (i.e., reduced expressiveness at different scales), a scale-adaptive preprocessing step is introduced prior to the shared layers. Specifically, each scale undergoes a 1 × 1 convolution followed by Group Normalization (GN) [<xref ref-type="bibr" rid="ref_18">18</xref>], a process we refer to as differentiated channel compression. This ensures that each scale-specific feature map is appropriately normalized and dimensionally aligned before entering the shared convolution layers, thereby preserving essential scale-dependent characteristics.</p><p>Within the shared convolutional block, two 3 × 3 convolutional layers are applied sequentially. The first performs standard convolution to extract generic spatial features (e.g., edges and corners), while the second employs a dynamically adjustable dilation rate that adapts based on the input scale. This dual-convolution strategy enables multi-receptive field fusion, balancing the advantages of parameter sharing with the necessity for scale-specific spatial resolution.</p><p>Following feature extraction, the network branches into two heads: one for classification (Conv_Cls) and one for regression (Conv_Reg). The regression output passes through a learnable Scale Layer, which adjusts each feature map element-wise to stabilize bounding box predictions. Meanwhile, the classification head incorporates a channel attention mechanism, allowing adaptive weight allocation based on category-specific relevance, thereby improving performance across varying object types and scales.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Experimental results and analysis</title>
      
        <sec>
          
            <title>4.1. Dataset</title>
          
          <p>The Water Surface Object Detection Dataset (WSODD), introduced by Zhou et al. [<xref ref-type="bibr" rid="ref_19">19</xref>], serves as a comprehensive benchmark featuring high-quality annotations and wide scene coverage. It includes imagery collected from five distinct water bodies—Yangtze River, Xuanwu Lake, Nanhaizi Lake, Yellow Sea, and Bohai Sea—with annotations spanning 14 surface object categories, as illustrated in <xref ref-type="fig" rid="fig_7">Figure 7</xref>.</p>
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>Category distribution of the WSODD dataset: (a) object types; (b) instance counts per class</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_EIMeCk-VjPstAX7f.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_rvTZHb5WLY7P5rMi.png"/>
            </fig>
          
          <p>In total, WSODD comprises 7,467 images and 21,911 labeled instances, of which approximately 53% correspond to small objects. This high proportion of small-scale targets makes the dataset particularly suitable for evaluating lightweight detection models. The category distribution, visualized in <xref ref-type="fig" rid="fig_7">Figure 7</xref>, reveals a notable class imbalance: categories such as “boat” and “ship” are well represented, while others, including “grass” and “animal”, contain relatively few samples.</p>
        </sec>
      
      
        <sec>
          
            <title>4.2. Experimental setup and evaluation metrics</title>
          
          <p>To ensure experimental fairness and result reproducibility, all model training and evaluation were performed under a consistent hardware and software environment. Specifically, the experiments were conducted on a Windows 11 system equipped with an NVIDIA GeForce RTX 4060 Laptop GPU (8,188 MiB memory) and an AMD Ryzen 7 7840H CPU featuring 16 cores at 3.8 GHz with integrated Radeon 780M Graphics. The software environment consisted of Python 3.9.18 and the PyTorch 2.2.1 + cu118 deep learning framework. The YOLOv8n model was adopted as the baseline, with its training hyperparameters detailed in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Training configuration of the baseline YOLOv8n model</title>
              </caption>
              <table><tr><th >Training Parameters</th><th >Values</th></tr><tr><td >Learning Rate</td><td >0.01</td></tr><tr><td >Image Size</td><td >640*640</td></tr><tr><td >Momentum</td><td >0.937</td></tr><tr><td >Weight Decay</td><td >0.0005</td></tr><tr><td >Optimizer</td><td >SGD</td></tr><tr><td >Epochs</td><td >200</td></tr><tr><td >Batch Size</td><td >8</td></tr></table>
            </table-wrap>
          
          <p>To comprehensively assess the performance of the proposed model in marine object detection tasks—particularly in USV scenarios—multiple evaluation metrics were employed. These include Precision (P), Recall (R), and mAP, which collectively reflect detection accuracy across various object categories. In addition, model complexity and inference efficiency were quantified using key indicators such as the number of parameters, GFLOPs (giga floating-point operations per second), model size, and frames per second (FPS) during inference.</p><p>The formal definitions of the primary evaluation metrics are provided in Eqs. (7)–(9). Specifically:</p>
          
            <disp-formula>
              <label>(7)</label>
              <mml:math id="m6jzth11kg">
                <mml:mi>P</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>c</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>s</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:msub>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:msub>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mi>P</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>T</mml:mi>
                      <mml:mi>P</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(8)</label>
              <mml:math id="m26zzxj2a0">
                <mml:mi>R</mml:mi>
                <mml:mi>c</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>l</mml:mi>
                <mml:mi>l</mml:mi>
                <mml:mi>S</mml:mi>
                <mml:mi>c</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:msub>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:msub>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>T</mml:mi>
                      <mml:mi>P</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mi>N</mml:mi>
                    </mml:msub>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>Here, <inline-formula>
  <mml:math id="msdidjw9sy">
    <mml:msub>
      <mml:mi>T</mml:mi>
      <mml:mi>P</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> denotes the number of correctly detected marine targets, <inline-formula>
  <mml:math id="m519oqu7t9">
    <mml:msub>
      <mml:mi>F</mml:mi>
      <mml:mi>P</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> denotes the number of falsely detected non-marine targets, and <inline-formula>
  <mml:math id="ma6lksn2zp">
    <mml:msub>
      <mml:mrow>
        <mml:mi>F</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mrow>
          <mml:mi>N</mml:mi>
        </mml:mrow>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> denotes the number of missed marine targets.</p>
          
            <disp-formula>
              <label>(9)</label>
              <mml:math id="masdbbl03h">
                <mml:mi>m</mml:mi>
                <mml:mi>A</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mi>A</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mn>1</mml:mn>
                  <mml:mi>n</mml:mi>
                </mml:mfrac>
                <mml:munderover>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mo>=</mml:mo>
                  </mml:mrow>
                </mml:munderover>
                <mml:msub>
                  <mml:mi>P</mml:mi>
                  <mml:mi>i</mml:mi>
                </mml:msub>
              </mml:math>
            </disp-formula>
          
          <p>Moreover, $n<inline-formula>
  <mml:math id="m842rdunyf">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>i<inline-formula>
  <mml:math id="mbk1eytcjw">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>AP$ stands for the average precision of each category.</p>
        </sec>
      
      
        <sec>
          
            <title>4.3. Ablation study</title>
          
          <p>To evaluate the individual contributions of the C2f-SBS module, MLCA module, and SCLD Head, we conducted a series of incremental ablation experiments based on the YOLOv8n baseline. The results are summarized in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Ablation results</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Experiment</p></th><th colspan="1" rowspan="1"><p>C2f-SBS</p></th><th colspan="1" rowspan="1"><p>MLCA</p></th><th colspan="1" rowspan="1"><p>SCLD</p></th><th colspan="1" rowspan="1"><p>P (%)</p></th><th colspan="1" rowspan="1"><p>R (%)</p></th><th colspan="1" rowspan="1"><p>mAP@0.5 (%)</p></th><th colspan="1" rowspan="1"><p>mAP@0.5:0.95 (%)</p></th><th colspan="1" rowspan="1"><p>Parameters (M)</p></th><th colspan="1" rowspan="1"><p>GFLOPs</p></th><th colspan="1" rowspan="1"><p>Size (MB)</p></th><th colspan="1" rowspan="1"><p>FPS</p></th></tr><tr><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>80%</p></td><td colspan="1" rowspan="1"><p>70.9%</p></td><td colspan="1" rowspan="1"><p>76%</p></td><td colspan="1" rowspan="1"><p>44%</p></td><td colspan="1" rowspan="1"><p>3.0</p></td><td colspan="1" rowspan="1"><p>8.1</p></td><td colspan="1" rowspan="1"><p>6.0</p></td><td colspan="1" rowspan="1"><p>220.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p><mml:math id="md8vr43o4g">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>79.3%</p></td><td colspan="1" rowspan="1"><p>69%</p></td><td colspan="1" rowspan="1"><p>74%</p></td><td colspan="1" rowspan="1"><p>44%</p></td><td colspan="1" rowspan="1"><p>2.5</p></td><td colspan="1" rowspan="1"><p>6.9</p></td><td colspan="1" rowspan="1"><p>5.1</p></td><td colspan="1" rowspan="1"><p>213.6</p></td></tr><tr><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p><mml:math id="mxv5mh0q8m">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>83%</p></td><td colspan="1" rowspan="1"><p>69.6%</p></td><td colspan="1" rowspan="1"><p>77.1%</p></td><td colspan="1" rowspan="1"><p>44.5%</p></td><td colspan="1" rowspan="1"><p>3.0</p></td><td colspan="1" rowspan="1"><p>8.1</p></td><td colspan="1" rowspan="1"><p>6.0</p></td><td colspan="1" rowspan="1"><p>217.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p><mml:math id="mjcvw36rns">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p>84.1%</p></td><td colspan="1" rowspan="1"><p>70.9%</p></td><td colspan="1" rowspan="1"><p>76.9%</p></td><td colspan="1" rowspan="1"><p>45.2%</p></td><td colspan="1" rowspan="1"><p>2.3</p></td><td colspan="1" rowspan="1"><p>6.5</p></td><td colspan="1" rowspan="1"><p>4.7</p></td><td colspan="1" rowspan="1"><p>220.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>5</p></td><td colspan="1" rowspan="1"><p><mml:math id="moi69o0l5d">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="mv6qucen8l">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p><mml:math id="mvms3awcv4">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1"><p>85.5%</p></td><td colspan="1" rowspan="1"><p>69.6%</p></td><td colspan="1" rowspan="1"><p>78.3%</p></td><td colspan="1" rowspan="1"><p>45.7%</p></td><td colspan="1" rowspan="1"><p>1.9</p></td><td colspan="1" rowspan="1"><p>5.3</p></td><td colspan="1" rowspan="1"><p>3.8</p></td><td colspan="1" rowspan="1"><p>208.4</p></td></tr></tbody></table>
            </table-wrap>
          
          <p><inline-formula>
  <mml:math id="m79xz3o5zr">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Experiment 1 establishes the baseline performance using unmodified YOLOv8n.</p><p><inline-formula>
  <mml:math id="mwrijz78qg">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Experiments 2 to 4 assess the isolated impact of integrating the C2f-SBS module, the MLCA module, and the SCLD head, respectively.</p><p><inline-formula>
  <mml:math id="mtnfzja5e9">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Experiment 5 reports the final performance of the full LMS-YOLO model, incorporating all proposed modules.</p><p>In Experiment 2, replacing the original C2f structure with the C2f-SBS module leads to a marginal decrease in Precision (P), FPS, and mAP@0.5, yet significantly reduces model size and computational demands—parameters decrease by 15.73%, GFLOPs by 14.81%, and model size drops from 6.0 MB to 5.1 MB. These results suggest that the C2f-SBS module maintains competitive detection performance while improving the model's deployability, especially for embedded or resource-constrained environments.</p><p>Experiment 3 shows that integrating the MLCA module into the backbone yields noticeable performance gains: Precision improves by 3%, while mAP@0.5 and mAP@0.5:0.95 increase by 1.1% and 0.5%, respectively. Importantly, this improvement is achieved with negligible increases in parameters and computation. The performance gain can be attributed to MLCA’s ability to effectively capture local and global contextual dependencies across spatial and channel dimensions, thus enhancing feature representation. This module proves particularly beneficial in cluttered maritime environments.</p><p>In Experiment 4, replacing the original head with the proposed SCLD design results in a 4.1% improvement in Precision and a 1.2% increase in mAP@0.5:0.95, while simultaneously reducing parameter count by 21.44%, GFLOPs by 19.75%, and model size from 6.0 MB to 4.7 MB. The FPS remains stable at around 220, demonstrating that the shared-convolution strategy effectively reduces complexity without compromising inference speed or accuracy.</p><p>Experiment 5 evaluates the complete LMS-YOLO model. Compared with the YOLOv8n baseline, LMS-YOLO achieves a 5.5% improvement in Precision, and gains of 2.3% in mAP@0.5 and 1.7% in mAP@0.5:0.95. Moreover, the model achieves a 37.18% reduction in parameters and 34.57% in GFLOPs, with the model size compressed to 3.8 MB. Although the FPS drops slightly (5.4% decrease), the inference speed remains well above real-time requirements for marine object detection. These results highlight LMS-YOLO’s ability to effectively balance detection accuracy and computational efficiency, validating its suitability for real-world deployment on low-power platforms.</p>
        </sec>
      
      
        <sec>
          
            <title>4.4. Comparative experiments and visualization</title>
          
          <p>To further evaluate the detection performance of LMS-YOLO across different categories of marine targets, we conducted comparative experiments using the same hardware configuration and dataset described in Section 4.2. The proposed model was benchmarked against several state-of-the-art detection algorithms, including lightweight variants such as YOLOv5n, YOLOv9t, and YOLOv10n, as well as heavier models like YOLOv7 and YOLOv9c. The experimental results are summarized in <xref ref-type="table" rid="table_3">Table 3</xref>, with the top two values in each evaluation metric highlighted in bold.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Performance comparison between LMS-YOLO and mainstream detection models</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p>P (%)</p></th><th colspan="1" rowspan="1"><p>mAP@0.5:0.95 (%)</p></th><th colspan="1" rowspan="1"><p>Parameters (M)</p></th><th colspan="1" rowspan="1"><p>GFLOPs</p></th><th colspan="1" rowspan="1"><p>Fps</p></th></tr><tr><td colspan="1" rowspan="1"><p>YOLOv5n</p></td><td colspan="1" rowspan="1"><p>81.3%</p></td><td colspan="1" rowspan="1"><p>43%</p></td><td colspan="1" rowspan="1"><p>1.7</p></td><td colspan="1" rowspan="1"><p>4.2</p></td><td colspan="1" rowspan="1"><p>78</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv7</p></td><td colspan="1" rowspan="1"><p>85.1</p></td><td colspan="1" rowspan="1"><p>45.5%</p></td><td colspan="1" rowspan="1"><p>37.2</p></td><td colspan="1" rowspan="1"><p>105.2</p></td><td colspan="1" rowspan="1"><p>43</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv8n</p></td><td colspan="1" rowspan="1"><p>80%</p></td><td colspan="1" rowspan="1"><p>44%</p></td><td colspan="1" rowspan="1"><p>3.0</p></td><td colspan="1" rowspan="1"><p>8.1</p></td><td colspan="1" rowspan="1"><p>145</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv9t</p></td><td colspan="1" rowspan="1"><p>80.4%</p></td><td colspan="1" rowspan="1"><p>41.5%</p></td><td colspan="1" rowspan="1"><p>2.6</p></td><td colspan="1" rowspan="1"><p>10.7</p></td><td colspan="1" rowspan="1"><p>56</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv9c</p></td><td colspan="1" rowspan="1"><p>86.9%</p></td><td colspan="1" rowspan="1"><p>46.5%</p></td><td colspan="1" rowspan="1"><p>60.5</p></td><td colspan="1" rowspan="1"><p>264</p></td><td colspan="1" rowspan="1"><p>37</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv10n</p></td><td colspan="1" rowspan="1"><p>75%</p></td><td colspan="1" rowspan="1"><p>39.6%</p></td><td colspan="1" rowspan="1"><p>2.7</p></td><td colspan="1" rowspan="1"><p>8.3</p></td><td colspan="1" rowspan="1"><p>111</p></td></tr><tr><td colspan="1" rowspan="1"><p>LMS-YOLO</p></td><td colspan="1" rowspan="1"><p>85.5%</p></td><td colspan="1" rowspan="1"><p>45.7%</p></td><td colspan="1" rowspan="1"><p>1.9</p></td><td colspan="1" rowspan="1"><p>5.3</p></td><td colspan="1" rowspan="1"><p>128</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>While YOLOv9c marginally outperforms LMS-YOLO in terms of Precision and mAP@0.5:0.95, it comes with a significantly larger number of parameters and higher computational cost. Among all lightweight models, LMS-YOLO achieves the best overall performance. Compared with YOLOv5n, LMS-YOLO shows improvements of 4.2% in Precision, 2.7% in mAP@0.5:0.95, and an additional 50 FPS, albeit with a slight increase in model complexity. Against YOLOv9t and YOLOv10n, LMS-YOLO consistently surpasses them across all evaluation metrics, demonstrating superior performance and better balance between accuracy and efficiency.</p><p>These findings confirm that LMS-YOLO not only enhances detection accuracy in marine environments but also maintains an effective trade-off between model compactness and real-time processing capability, making it suitable for deployment in practical maritime applications.</p><p>To provide an intuitive understanding of LMS-YOLO’s detection robustness under visually complex maritime conditions, we further conducted visualization experiments using two representative background scenarios:</p><p>(a) subtle surface ripples combined with sunlight reflection interference</p><p>(b) strong water surface reflection interference</p><p>The models selected for comparison include YOLOv5n, YOLOv8n, YOLOv9t, and YOLOv10n, with the results visualized in <xref ref-type="fig" rid="fig_8">Figure 8</xref>.</p>
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>Qualitative comparison of detection results under complex maritime conditions: (a) sunlight reflection; (b) surface reflection</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_Ik94-KuC1_-OATfn.jpeg"/>
            </fig>
          
          <p>In scenario (a), LMS-YOLO successfully detected all surface targets, while YOLOv9t and YOLOv10n failed to identify distant “rubbish” objects. In scenario (b), LMS-YOLO again demonstrated superior robustness, detecting all “ball” targets without omission. By contrast, YOLOv5n, YOLOv8n, and YOLOv9t each missed one “ball” target, and YOLOv10n failed to detect two such targets.</p><p>These visual results underscore LMS-YOLO’s ability to maintain stable and accurate detection performance even under challenging environmental conditions such as specular reflections and dynamic surface disturbances. Its effectiveness in detecting small-scale marine objects within noisy, real-world scenarios highlights its strong generalization ability and practical value.</p>
        </sec>
      
      
        <sec>
          
            <title>4.5. Hardware deployment experiments</title>
          
          <p>To evaluate the deployment feasibility of the proposed LMS-YOLO model across various computing environments, we conducted inference performance tests on three different hardware platforms:</p><p><inline-formula>
  <mml:math id="m2joe4tsf9">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> an AMD Ryzen 7 7840H CPU (16 cores),</p><p><inline-formula>
  <mml:math id="mde0mwjg11">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> an NVIDIA GTX 1060 GPU, and</p><p><inline-formula>
  <mml:math id="mkkgmq8lrt">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> an NVIDIA RTX 4060 GPU.</p><p>The evaluation focused on several key indicators: frames per second, model size, parameter count, and computational cost measured in GFLOPs. The goal was to assess the model’s adaptability to resource-constrained edge environments, particularly in the context of real-time marine object detection. The results are summarized in <xref ref-type="table" rid="table_4">Table 4</xref>.</p>
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Inference performance of LMS-YOLO on different hardware platforms</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Hardware Environment</p></th><th colspan="1" rowspan="1"><p>Models</p></th><th colspan="1" rowspan="1"><p>FPS</p></th><th colspan="1" rowspan="1"><p>Size (MB)</p></th><th colspan="1" rowspan="1"><p>Parameters (M)</p></th><th colspan="1" rowspan="1"><p>GFLOPs</p></th></tr><tr><td colspan="1" rowspan="2"><p>AMD Ryzen 7 7840H</p></td><td colspan="1" rowspan="1"><p>YOLOv8n</p></td><td colspan="1" rowspan="1"><p>25.1</p></td><td colspan="1" rowspan="1"><p>6.0</p></td><td colspan="1" rowspan="1"><p>3.0</p></td><td colspan="1" rowspan="1"><p>8.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>LMS-YOLO</p></td><td colspan="1" rowspan="1"><p>22.2</p></td><td colspan="1" rowspan="1"><p>3.8</p></td><td colspan="1" rowspan="1"><p>1.9</p></td><td colspan="1" rowspan="1"><p>5.3</p></td></tr><tr><td colspan="1" rowspan="2"><p>NVIDIA GTX 1060</p></td><td colspan="1" rowspan="1"><p>YOLOv8n</p></td><td colspan="1" rowspan="1"><p>57.4</p></td><td colspan="1" rowspan="1"><p>6.0</p></td><td colspan="1" rowspan="1"><p>3.0</p></td><td colspan="1" rowspan="1"><p>8.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>LMS-YOLO</p></td><td colspan="1" rowspan="1"><p>52.2</p></td><td colspan="1" rowspan="1"><p>3.8</p></td><td colspan="1" rowspan="1"><p>1.9</p></td><td colspan="1" rowspan="1"><p>5.3</p></td></tr><tr><td colspan="1" rowspan="2"><p>NVIDIA RTX 4060</p></td><td colspan="1" rowspan="1"><p>YOLOv8n</p></td><td colspan="1" rowspan="1"><p>220.3</p></td><td colspan="1" rowspan="1"><p>6.0</p></td><td colspan="1" rowspan="1"><p>3.0</p></td><td colspan="1" rowspan="1"><p>8.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>LMS-YOLO</p></td><td colspan="1" rowspan="1"><p>208.4</p></td><td colspan="1" rowspan="1"><p>3.8</p></td><td colspan="1" rowspan="1"><p>1.9</p></td><td colspan="1" rowspan="1"><p>5.3</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>Although LMS-YOLO exhibits a modest reduction in FPS compared to non-lightweight models, it achieves substantial improvements in deployment efficiency. Specifically:</p><p><inline-formula>
  <mml:math id="m5xbc6tamw">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> The smaller model size reduces both storage and transmission demands;</p><p><inline-formula>
  <mml:math id="mna3ypnpd5">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> The lower parameter count minimizes memory consumption;</p><p><inline-formula>
  <mml:math id="mnfyu30cha">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> The reduced GFLOPs effectively ease the computational load during inference.</p><p>These characteristics collectively enhance LMS-YOLO’s compatibility with edge devices, such as embedded systems or mobile marine platforms, where real-time processing, power efficiency, and storage constraints are critical factors. Overall, the experimental results validate LMS-YOLO’s deployment flexibility and practical viability across diverse hardware configurations.</p>
        </sec>
      
      
        <sec>
          
            <title>4.6. Visualization analysis</title>
          
          <p>To provide an intuitive assessment of LMS-YOLO’s detection performance, we conducted extensive qualitative experiments under a range of challenging environmental conditions, including variations in scene context, illumination levels, and weather phenomena. A comparative analysis with the baseline YOLOv8n model is shown in <xref ref-type="fig" rid="fig_9">Figure 9</xref>, illustrating LMS-YOLO’s consistent ability to accurately detect water surface objects across diverse testing scenarios.</p>
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>
                <title>Visual comparison of detection results between YOLOv8n and LMS-YOLO across diverse environmental settings</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_j3EYkcpmyF0n1joO.jpeg"/>
            </fig>
          
          <p>LMS-YOLO successfully detects targets that are missed by the baseline model in a variety of conditions—for instance, identifying “boat” in sunny environments, “ship” and “platform” under fog, and “ball” under both high-intensity illumination and marine wave interference. It also demonstrates strong performance across different water body types, such as rivers and lakes. Moreover, LMS-YOLO significantly reduces false positives: under nighttime conditions, the baseline model erroneously classifies background features as “ball”, whereas LMS-YOLO correctly suppresses such misclassifications.</p><p>These findings confirm LMS-YOLO’s robustness in complex environments—including oceans, lakes, rivers, low-light scenes, high glare, and foggy conditions—highlighting its suitability for practical deployment and real-world maritime applications.</p><p>Despite these strengths, certain extreme environments still challenge LMS-YOLO’s detection capability. For instance, under intense glare or very low light, several “ball” targets were missed. These limitations are mainly attributed to:</p><p>(1) Reduced visual contrast caused by wave motion and high-intensity lighting, which impairs target-background separability;</p><p>(2) Low signal-to-noise ratio in nighttime settings, particularly affecting small object detection.</p><p>To overcome these issues, future enhancements may focus on integrating illumination-adaptive modules, improving low-light feature extraction, and enhancing robustness to reflection artifacts, thereby enabling more reliable performance under extreme lighting conditions.</p><p>To complement the qualitative analysis, we further compared LMS-YOLO and YOLOv8n using a confusion matrix, as shown in <xref ref-type="fig" rid="fig_10">Figure 10</xref>. In this matrix, true labels are plotted on the horizontal axis and predicted labels on the vertical axis. The diagonal elements represent true positives (TP), while values in the lower-left triangle correspond to false negatives (FN)—missed or misclassified targets—and the upper-right triangle denotes false positives (FP), where background or incorrect objects were incorrectly identified as targets.</p>
          
            <fig id="fig_10">
              <label>Figure 10</label>
              <caption>
                <title>Confusion matrix comparing YOLOv8n and LMS-YOLO predictions across all object categories</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_rFcO9tj2NmY_Q15K.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_PPlfEbiX436TGaV0.png"/>
            </fig>
          
          <p>A comparison of the diagonal values reveals that LMS-YOLO achieves higher classification accuracy in most categories. For example, detection accuracy for the “platform” class improves from 0.65 to 0.78. Additionally, LMS-YOLO reduces both FN and FP values across most categories, indicating enhanced reliability, improved recall, and lower false detection rates.</p>
        </sec>
      
      
        <sec>
          
            <title>4.7. Generalization experiments</title>
          
          <p>To comprehensively evaluate the generalization capability of the proposed LMS-YOLO algorithm, we conducted experiments on the FloW-Img dataset [<xref ref-type="bibr" rid="ref_20">20</xref>]. This dataset comprises high-resolution imagery specifically curated for detecting floating debris in real inland water environments, captured from unmanned surface vessels under varied perspectives. It includes 2,000 images with 5,271 finely annotated targets, the majority of which are small objects (less than 32 × 32 pixels)—making it particularly relevant to the objectives of this study.</p><p>As shown in <xref ref-type="table" rid="table_5">Table 5</xref> and <xref ref-type="fig" rid="fig_11">Figure 11</xref>, LMS-YOLO demonstrates strong generalization performance on FloW-Img when compared to the baseline YOLOv8n model. LMS-YOLO achieves a Precision (P) of 0.883 and Recall (R) of 0.833, while YOLOv8n records 0.861 and 0.849, respectively. This reflects a 2.2% improvement in precision, suggesting more effective reduction of false positives, albeit with a slight decrease in recall.</p>
          <p>In terms of mAP metrics, LMS-YOLO obtains 0.879 at mAP@0.5 and 0.498 at mAP@0.5:0.95, whereas YOLOv8n achieves 0.891 and 0.481, respectively. Although LMS-YOLO performs slightly lower at the relaxed IoU threshold (0.5), it clearly outperforms the baseline at the stricter threshold (0.5:0.95), which is more sensitive to boundary localization quality—a critical factor in small object detection.</p>
          
            <table-wrap id="table_5">
              <label>Table 5</label>
              <caption>
                <title>Performance comparison between LMS-YOLO and mainstream detection models</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p>P (%)</p></th><th colspan="1" rowspan="1"><p>R (%)</p></th><th colspan="1" rowspan="1"><p>mAP@0.5 (%)</p></th><th colspan="1" rowspan="1"><p>mAP@0.5:0.95 (%)</p></th><th colspan="1" rowspan="1"><p>Parameters (M)</p></th><th colspan="1" rowspan="1"><p>GFLOPs</p></th></tr><tr><td colspan="1" rowspan="1"><p>YOLOv8n</p></td><td colspan="1" rowspan="1"><p>0.883</p></td><td colspan="1" rowspan="1"><p>0.833</p></td><td colspan="1" rowspan="1"><p>0.879</p></td><td colspan="1" rowspan="1"><p>0.498</p></td><td colspan="1" rowspan="1"><p>1.9</p></td><td colspan="1" rowspan="1"><p>5.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>LMS-YOLO</p></td><td colspan="1" rowspan="1"><p>0.861</p></td><td colspan="1" rowspan="1"><p>0.849</p></td><td colspan="1" rowspan="1"><p>0.891</p></td><td colspan="1" rowspan="1"><p>0.481</p></td><td colspan="1" rowspan="1"><p>3.0</p></td><td colspan="1" rowspan="1"><p>8.1</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <fig id="fig_11">
              <label>Figure 11</label>
              <caption>
                <title>Comparison of key metrics before and after model enhancement on FloW-Img</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_XQ-8zcihmsd8rCLi.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_ihInW1xAo-xD2d9H.png"/>
            </fig>
          
          <p>The improvements in generalization can be attributed to three key architectural enhancements:</p><p><inline-formula>
  <mml:math id="mq21gb0fn8">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> The C2f-SBS module, which improves multi-scale feature representation and fusion;</p><p><inline-formula>
  <mml:math id="magr5kut0p">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> The MLCA module, which combines local and global contextual cues, thereby enhancing the network’s robustness to background interference;</p><p><inline-formula>
  <mml:math id="mtekgrl7al">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> The SCLD head, which enables efficient scale-aware detection with reduced model complexity.</p><p>These components collectively enhance detection accuracy while maintaining a lightweight architecture, making LMS-YOLO well suited for practical deployment.</p><p>As illustrated in <xref ref-type="fig" rid="fig_12">Figure 12</xref>, LMS-YOLO successfully detects all floating debris targets in real-world images from FloW-Img, confirming its generalization ability and robustness across unseen environments.</p>
          
            <fig id="fig_12">
              <label>Figure 12</label>
              <caption>
                <title>Visual detection results of LMS-YOLO on floating debris in real inland waters</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_pGfHJmHgXO3Wu_co.png"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>This study presents LMS-YOLO, a lightweight object detection algorithm tailored for complex water surface environments, addressing the limitations of low detection accuracy and high computational complexity found in existing methods. The model introduces several architectural enhancements:</p><p><inline-formula>
  <mml:math id="m5o6qltm78">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> a C2f-SBS module incorporating Star Blocks to improve multi-scale feature fusion,</p><p><inline-formula>
  <mml:math id="mwmq4qrt3o">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> a shared convolutional detection head to minimize parameter count and computational load, and</p><p><inline-formula>
  <mml:math id="m9hmq9w2yr">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> an MLCA module to enhance contextual feature utilization within challenging visual scenarios.</p><p>Comprehensive experiments demonstrate that LMS-YOLO achieves a 5.5% improvement in precision, a 2.3% gain in mAP@0.5, and a 1.7% increase in mAP@0.5:0.95, while simultaneously reducing the parameter count by 37.18% and computational cost by 34.57% compared to the YOLOv8n baseline. The algorithm also exhibits strong generalization capability on unseen datasets and maintains stable performance across diverse environmental conditions, including reflections, fog, and varying lighting.</p><p>Overall, LMS-YOLO effectively balances detection performance and model efficiency, making it well suited for deployment in real-time applications, particularly on resource-constrained platforms such as USVs. Future work will explore the integration of illumination-adaptive modules and multi-modal fusion strategies to further enhance robustness under extreme visual disturbances.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>Conceptualization, Y.S. and Q.Z.; methodology, Y.S. and Q.Z.; software, Q.Z. and T.Z.; validation, Y.S. and Q.Z.; formal analysis, T.Z. and M.S.; investigation, Y.S., T.Z. and M.S.; resources, X.L.; data curation, Y.S., Q.Z., T.Z. and M.S.; writing—original draft preparation, Y.S.; writing—review and editing, Y.S. and Q.Z.; visualization, Y.S.; supervision, X.L.; project administration, X.L.; funding acquisition, X.L. All authors have read and agreed to the published version of the manuscript.</p>
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>300</volume>
          <page-range>112204</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Karim</surname>
              <given-names>M. J.</given-names>
            </name>
            <name>
              <surname>Nahiduzzaman</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Ahsan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Haider</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.knosys.2024.112204</pub-id>
          <article-title>Development of an early detection and automatic targeting system for cotton weeds using an improved lightweight YOLOv8 architecture on an edge device</article-title>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="conf-paper">
          <page-range>779-788</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Redmon</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Divvala</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Girshick</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Farhadi</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CVPR.2016.91</pub-id>
          <article-title>You only look once: Unified, real-time object detection</article-title>
          <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Redmon</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Farhadi</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1804.02767</pub-id>
          <article-title>YOLOv3: An incremental improvement</article-title>
          <source>arXiv preprint, arXiv:1804.02767</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bochkovskiy</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>C. Y.</given-names>
            </name>
            <name>
              <surname>Liao</surname>
              <given-names>H. Y. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2004.10934</pub-id>
          <article-title>YOLOv4: Optimal speed and accuracy of object detection</article-title>
          <source>arXiv preprint, arXiv:2004.10934</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="webpage">
          <article-title>v6.0-YOLOv5n ‘Nano’ models, Roboflow integration, TensorFlow export, OpenCV DNN support</article-title>
          <source>, https://doi.org/10.5281/zenodo.5563715</source>
          <year>2021</year>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>C. Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>L. L.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>H. L.</given-names>
            </name>
            <name>
              <surname>Weng</surname>
              <given-names>K. H.</given-names>
            </name>
            <name>
              <surname>Geng</surname>
              <given-names>Y. F.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Ke</surname>
              <given-names>Z. D.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Q. Y.</given-names>
            </name>
            <name>
              <surname>Cheng</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Nie</surname>
              <given-names>W. Q.</given-names>
            </name>
            <name>
              <surname>et al.</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2209.02976</pub-id>
          <article-title>YOLOv6: A single-stage object detection framework for industrial applications</article-title>
          <source>arXiv preprint, arXiv:2209.02976</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="conf-paper">
          <page-range>7464-7475</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>C. Y.</given-names>
            </name>
            <name>
              <surname>Bochkovskiy</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Liao</surname>
              <given-names>H. Y. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CVPR52733.2023.00721</pub-id>
          <article-title>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</article-title>
          <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>1302</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Huang</surname>
              <given-names>Z. X.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>X. N.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>F. L.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>T. J.</given-names>
            </name>
            <name>
              <surname>Pei</surname>
              <given-names>J. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app13031302</pub-id>
          <article-title>An improved method for ship target detection based on YOLOv4</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>X. P.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Z. Y.</given-names>
            </name>
            <name>
              <surname>Qu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Qiu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Zhai</surname>
              <given-names>Z. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/jmse10111773</pub-id>
          <article-title>Recognition algorithm of marine ship based on improved YOLOv5 deep learning</article-title>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>190</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jiang</surname>
              <given-names>Z. K.</given-names>
            </name>
            <name>
              <surname>Su</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>Y. X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/jmse12010190</pub-id>
          <article-title>YOLOv7-ship: A lightweight algorithm for ship object detection in complex marine environments</article-title>
          <source>J. Mar. Sci. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>3089</page-range>
          <issue>15</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>C. L.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Y. T.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>S. K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/electronics13153089</pub-id>
          <article-title>Lightweight water surface object detection network for unmanned surface vehicles</article-title>
          <source>Electronics</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>24</volume>
          <page-range>5059</page-range>
          <issue>15</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s24155059</pub-id>
          <article-title>Improved YOLOv8 algorithm for water surface object detection</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>24</volume>
          <page-range>4410</page-range>
          <issue>13</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>B. B.</given-names>
            </name>
            <name>
              <surname>Ding</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>B. J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L. Q.</given-names>
            </name>
            <name>
              <surname>Ning</surname>
              <given-names>S. P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s24134410</pub-id>
          <article-title>A method for real-time recognition of safflower filaments in unstructured environments using the YOLO-SaFi model</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>60</volume>
          <page-range>1-19</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cheng</surname>
              <given-names>Q. M.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>H. Y.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Y. Z.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>H. Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TGRS.2022.3166979</pub-id>
          <article-title>NWPU-captions dataset and MLCA-net for remote sensing image captioning</article-title>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="conf-paper">
          <page-range>3490-3499</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Feng</surname>
              <given-names>C. J.</given-names>
            </name>
            <name>
              <surname>Zhong</surname>
              <given-names>Y. J.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Scott</surname>
              <given-names>M. R.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>W. L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICCV48922.2021.00349</pub-id>
          <article-title>Tood: Task-aligned one-stage object detection</article-title>
          <source>2021 IEEE/CVF International Conference on Computer Vision (ICCV)</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="conf-paper">
          <volume>33</volume>
          <page-range>21002-21012</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>W. H.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>L. J.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>X. L.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>J. H.</given-names>
            </name>
          </person-group>
          <article-title>Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</article-title>
          <source>Advances in Neural Information Processing Systems</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>97</volume>
          <page-range>110817</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Du</surname>
              <given-names>L. Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.jobe.2024.110817</pub-id>
          <article-title>Bi-YOLO: A novel object detection network and dataset for components of China heritage buildings</article-title>
          <source>J. Build. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>583</volume>
          <page-range>127488</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yi</surname>
              <given-names>D. W.</given-names>
            </name>
            <name>
              <surname>Ahmedov</surname>
              <given-names>H. B.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>S. Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y. R.</given-names>
            </name>
            <name>
              <surname>Flinn</surname>
              <given-names>S. J.</given-names>
            </name>
            <name>
              <surname>Fernandes</surname>
              <given-names>P. G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neucom.2024.127488</pub-id>
          <article-title>Coordinate-aware Mask R-CNN with Group Normalization: An underwater marine animal instance segmentation framework</article-title>
          <source>Neurocomputing</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>723336</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhou</surname>
              <given-names>Z. G.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>J. E.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>J. B.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>K. Y.</given-names>
            </name>
            <name>
              <surname>Duan</surname>
              <given-names>J. W.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>C. L. P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fnbot.2021.723336</pub-id>
          <article-title>An image-based benchmark dataset and a novel object detector for water surface object detection</article-title>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="conf-paper">
          <page-range>10953-10962</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cheng</surname>
              <given-names>Y. W.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>J. N.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>M. X.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Pang</surname>
              <given-names>C. S.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Sankaran</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Onabola</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Y. M.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>D. B.</given-names>
            </name>
            <name>
              <surname>Bengio</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICCV48922.2021.01079</pub-id>
          <article-title>Flow: A dataset and benchmark for floating waste detection in inland waters</article-title>
          <source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>