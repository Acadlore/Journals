<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IDA</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Information Dynamics and Applications</journal-title>
        <abbrev-journal-title abbrev-type="issn">Inf. Dyn. Appl.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IDA</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-1494</issn>
      <issn publication-format="print">2958-1486</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-ilN46EBXbrqkulETJaAxbUuG5fuEkqqN</article-id>
      <article-id pub-id-type="doi">10.56578/ida030405</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>FEGAO: A Revolutionary Method for Enhancing Defective Fuzzy Images with Non-Linear Refinement</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7540-6265</contrib-id>
          <name>
            <surname>Hussain</surname>
            <given-names>Ibrar</given-names>
          </name>
          <email>ibrar786@uop.edu.pk</email>
        </contrib>
        <aff id="aff_1">Department of Mathematics, University of Peshawar, 25120 Peshawar, Pakistan</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>30</day>
        <month>12</month>
        <year>2024</year>
      </pub-date>
      <volume>3</volume>
      <issue>4</issue>
      <fpage>258</fpage>
      <lpage>269</lpage>
      <page-range>258-269</page-range>
      <history>
        <date date-type="received">
          <day>11</day>
          <month>11</month>
          <year>2024</year>
        </date>
        <date date-type="accepted">
          <day>25</day>
          <month>12</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>This study presents a novel image restoration method, designed to enhance defective fuzzy images, by utilizing the Fuzzy Einstein Geometric Aggregation Operator (FEGAO). The method addresses the challenges posed by non-linearity, uncertainty, and complex degradation in defective images. Traditional image enhancement approaches often struggle with the imprecision inherent in defect detection. In contrast, FEGAO employs the Einstein t-norm and t-conorm for non-linear aggregation, which refines pixel coordinates and improves the accuracy of feature extraction. The proposed approach integrates several techniques, including pixel coordinate extraction, regional intensity refinement, multi-scale Gaussian correction, and a layered enhancement framework, thereby ensuring superior preservation of details and minimization of artifacts. Experimental evaluations demonstrate that FEGAO outperforms conventional methods in terms of image resolution, edge clarity, and noise robustness, while maintaining computational efficiency. Comparative analysis further underscores the method’s ability to preserve fine details and reduce uncertainty in defective images. This work offers significant advancements in image restoration by providing an adaptive, efficient solution for defect detection, machine vision, and multimedia applications, establishing a foundation for future research in fuzzy logic-based image processing under degraded conditions.</p></abstract>
      <kwd-group>
        <kwd>Defective fuzzy image</kwd>
        <kwd>Image restoration</kwd>
        <kwd>Image enhancement</kwd>
        <kwd>Fuzzy set</kwd>
        <kwd>Fuzzy operators</kwd>
        <kwd>Noise reduction</kwd>
        <kwd>Image quality improvement</kwd>
        <kwd>Pixel coordinate correction</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="1"/>
        <fig-count count="4"/>
        <table-count count="2"/>
        <ref-count count="20"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>The restoration and enhancement of images, encompassing their color, texture, shape, and other defining attributes, are crucial for advancing multimedia applications and modern information exchange. As images serve as a primary medium for conveying vast amounts of data, their quality directly influences the clarity and effectiveness of communication. With the rapid growth of digital multimedia technologies, the use of images has permeated various aspects of everyday life. Devices such as smartphones, tablets, and social media platforms have transformed images into an essential component of interpersonal and professional communication. High-quality images not only improve the aesthetic experience for users but also ensure the accurate transmission of information. Conversely, degraded images—those affected by blurriness or noise—can lead to significant data loss and hinder usability [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>], [<xref ref-type="bibr" rid="ref_5">5</xref>].</p><p>Restoring and enhancing degraded images to improve their quality is a vital task, particularly in scenarios that require precise image analysis and recognition. These processes involve amplifying meaningful details while suppressing unnecessary components. However, images that suffer from defects, such as blurring or noise, often exhibit complex structures and interrelated features. Even small inaccuracies in the restoration process can significantly degrade the final result. Therefore, the development of effective techniques to address these challenges has immense practical value, especially in fields like machine vision [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>].</p><p>Despite extensive research into image restoration and enhancement, many existing methods rely heavily on prior information, such as accurately identifying blur kernels. This reliance limits their applicability in real-world scenarios, where such prior knowledge may not always be available. Recent advancements in neural networks have opened new avenues for addressing these limitations. For instance, Premnath and Renjit [<xref ref-type="bibr" rid="ref_8">8</xref>] proposed a Jaya-based optimization (Cui) model that integrates deep convolutional networks for detecting noisy pixels and a neuro-fuzzy system for enhancing them. While effective, this approach is computationally expensive and requires substantial training time. Similarly, Zhou et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] developed an iterative lensless image restoration framework utilizing depth analysis networks. Although their method showed significant improvements in non-blind restoration scenarios, it struggled with cases involving imprecise blur kernels.</p><p>Several other noteworthy approaches have also been explored. For example, Ghulyani and Arigovindan [<xref ref-type="bibr" rid="ref_12">12</xref>] introduced an iterative algorithm designed to minimize roughness under mixed Poisson-Gaussian noise, while Liang et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] employed brainstorming optimization for backpropagation (BP) neural networks to restore fuzzy images. Additionally, Fan’s quantum ring symmetry algorithm has been applied to image reconstruction tasks [<xref ref-type="bibr" rid="ref_14">14</xref>]. Although these methods represent important contributions, they often face challenges such as high computational costs and limited ability to restore intricate texture details.</p><p>Neural networks have shown immense potential in overcoming these challenges by offering robust solutions for image restoration and enhancement. For example, Meng et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] designed a two-stream Convolutional Neural Network (CNN) that employs attention mechanisms to improve single-image dehazing. Similarly, Yun et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] proposed a hybrid enhancement algorithm that combines histogram equalization with Laplacian techniques, leading to improved restoration and feature enhancement. Nevertheless, simplifying these methods and reducing their dependence on exact blur kernel predictions remain ongoing challenges.</p><p>Existing super-resolution restoration models for defective images often struggle with challenges such as handling imprecise pixel intensity values, maintaining edge clarity, and reducing artifacts. These limitations arise due to the inability of traditional methods to effectively address uncertainties and non-linear relationships inherent in defective images. Additionally, conventional approaches often fail to adapt to varying defect characteristics, resulting in suboptimal resolution enhancement and feature preservation. To overcome these limitations, the proposed algorithm integrates fuzzy logic principles with FEGAO. This approach ensures non-linear and robust aggregation of pixel intensities, effectively managing the uncertainties and imprecision in defective images. By leveraging the Einstein t-norm (<italic>T<sub>E</sub></italic>) and t-conorm (<italic>S<sub>E</sub></italic>), the model enhances spatial and intensity information while preserving critical image details. The proposed algorithm incorporates iterative refinement steps, enabling the dynamic adjustment of pixel values and convergence toward high-quality super-resolution images.</p><p>The implementation of the proposed model began with the initialization of the reference image, derived from the original low-resolution input. Pixel regions were selected and normalized, and their corresponding high-resolution values were predicted using fuzzy membership functions. The Einstein t-norm and t-conorm were utilized in the iterative refinement process to aggregate spatial and intensity data, constructing multi-frame images with fuzzy-enhanced techniques. Corrections were applied iteratively, leveraging fuzzy Euclidean distance calculations and deviation adjustments to improve resolution and clarity. The final super-resolution image was evaluated against predefined quality metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM) to ensure convergence. The key components of the proposed model include:</p><p>(a) Fuzzy aggregation mechanism: The FEGAO (<italic>T<sub>E</sub></italic> and <italic>S<sub>E</sub></italic>) forms the foundation of the model, enabling robust and non-linear aggregation of pixel intensities while preserving inherent uncertainties.</p><p>(b) Iterative pixel refinement: A dynamic process that predicts and adjusts pixel values iteratively using fuzzy principles to achieve high-resolution accuracy.</p><p>(c) Multi-frame image construction: Low-resolution images are enhanced and aggregated into multi-frame super-resolution representations using fuzzy-enhanced visual communication techniques.</p><p>(d) Fuzzy edge-enhancement model: Incorporates fuzzy edge-preserving mechanisms to maintain edge clarity and minimize artifacts during the enhancement process.</p><p>(e) Layered Gaussian correction: A hierarchical approach that applies iterative Gaussian smoothing and intensity adjustments to refine image resolution and clarity.</p><p>(f) Quality metric evaluation: Ensures the convergence of the super-resolution process using predefined metrics such as PSNR and SSIM, providing quantitative validation of the enhancement.</p><p>(g) Adaptive correction functions: Uses fuzzy Euclidean distance and deviation calculations to iteratively correct pixel values, addressing imprecisions in intensity and spatial dimensions.</p><p>(h) Predefined convergence criteria: Establishes thresholds for resolution quality, ensuring that the iterative process halts once optimal enhancement is achieved.</p><p>Conventional image restoration methods often struggle with handling non-linearity, uncertainty, and complex degradation in defective images, particularly when the distortion is non-uniform or exhibits varying intensity across different regions. Traditional fuzzy aggregation techniques, such as the arithmetic mean or weighted averaging, may fail to effectively capture these variations, leading to loss of fine details and suboptimal feature extraction. Neural network-based approaches, such as CNNs and Generative Adversarial Networks (GANs), have been widely used for image restoration. However, these models require extensive labeled training data and computational resources, which can be impractical for real-time defect detection [<xref ref-type="bibr" rid="ref_11">11</xref>], [<xref ref-type="bibr" rid="ref_13">13</xref>]. Additionally, deep learning methods often lack explainability and struggle with domain adaptation, whereas FEGAO provides a transparent, mathematically grounded solution that effectively generalizes across different types of degradation without the need for large-scale training datasets. The proposed FEGAO leverages the Einstein t-norm and t-conorm, which introduce non-linear aggregation strategies that adapt to local intensity variations while preserving essential structural information. One key advantage of the FEGAO model is its ability to refine pixel coordinates dynamically, ensuring that even in highly degraded images, feature extraction remains precise and robust. This is particularly evident in scenarios involving complex textures or high-frequency components, where traditional models may introduce blurring artifacts or fail to restore sharp edges effectively. The multi-scale Gaussian corrections further enhance resolution by adjusting the level of enhancement based on regional intensity variations, preventing excessive smoothing or unnatural sharpening.</p><p>Experimental demonstration shows a real-world example where conventional methods (Zhou et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] and Cui [<xref ref-type="bibr" rid="ref_14">14</xref>]) struggle to restore fine details in blurred and noisy defective images, whereas the proposed FEGAO-based approach achieves superior edge clarity and contrast preservation. By integrating a layered enhancement framework, the model ensures adaptive processing for different levels of degradation, significantly outperforming conventional techniques in handling uncertainty and non-linearity in defective images.</p><p>The rest of this study is organized as follows: Section 2 presents related work, providing an overview of existing super-resolution techniques and their limitations. Section 3 details the mathematical foundation of FEGAO and its integration into the proposed model. Section 4 discusses experimental results, comparing the proposed model’s performance with existing methods using various quality metrics. Finally, Section 5 concludes the study, summarizing the contributions and outlining potential future research directions.</p>
    </sec>
    <sec sec-type="">
      <title>2. Related work</title>
      <p>Ma et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] achieved the restoration of defective fuzzy images through a dual-fusion neural network architecture that integrates the depth generation network and the discrimination network. This approach maps the defective fuzzy image into a residual space using the depth generation network, while the discrimination network evaluates the reconstructed image by comparing it to a clear reference image. However, these models often struggle to handle images with severe noise and blur, which leads to incomplete restoration and reduced clarity.</p><p>Kwon et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] segmented the defective fuzzy image into smaller blocks, which were then batch-transmitted to the depth generation network. The depth generation network comprises 16 convolutional layers capable of generating residual blocks. These residual blocks were combined with the corresponding defective fuzzy image blocks to form reconstructed image blocks. These reconstructed blocks, along with real image blocks, were subsequently passed to the discrimination network. A limitation of this approach is the reliance on fixed network architectures, such as the predefined 16 convolutional layers, which limits adaptability to varying levels of image fuzziness. The discrimination network evaluates the reconstructed image blocks, guiding the training of both the depth generation network and the discrimination network. The loss functions of both networks were iteratively minimized during training to ensure that the reconstructed image becomes increasingly indistinguishable from a real image, maximizing the discrimination network’s precision. However, the iterative training of loss functions is computationally intensive, particularly for high-resolution images, making these models less practical for real-time applications. The training process concludes once the loss function stabilizes, indicating no further improvements can be made. At this stage, the reconstructed image blocks were reassembled to produce the final restored image. Despite these advancements, existing models lack robust regularization techniques, leading to overfitting and poor generalization across diverse datasets.</p><p>The fuzzy image restoration algorithm based on machine vision technology proposed by Shi [<xref ref-type="bibr" rid="ref_19">19</xref>] significantly improves image deblurring, noise reduction, and overall restoration quality, outperforming traditional inverse deconvolution methods. Its adaptability to various types and degrees of image degradation and its automatic parameter adjustment make it suitable for diverse applications, including medical imaging, satellite image processing, and surveillance. The algorithm demonstrates superior recovery effectiveness, as shown by better Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and PSNR values, ensuring high-quality outputs even under challenging conditions. However, the algorithm has limitations, including its computational complexity, which can lead to longer processing times, particularly for large images or real-time restoration. It also depends on an initial estimate, and poor initial guesses may result in suboptimal restoration with residual artifacts. While robust, the model may struggle with extreme noise levels or severely degraded images, requiring additional preprocessing steps. Lastly, although adaptive parameter adjustment is a strength, it may necessitate fine-tuning to optimize performance for different types of images.</p><p>The restoration and enhancement of defective fuzzy images based on neural networks proposed by Cui [<xref ref-type="bibr" rid="ref_14">14</xref>] offer significant advancements in image processing by applying neural networks to restore and enhance images affected by defects, blur, and noise. The model leverages the learning capabilities of neural networks to improve image clarity, detail, and overall quality, making it highly effective in applications such as industrial defect detection, medical imaging, and surveillance. By utilizing the neural network’s ability to adapt and optimize restoration parameters, the model can efficiently handle complex defects and varying degrees of image degradation, providing high-quality outputs. The restoration process improves the visualization of subtle defects, which is crucial in precise defect detection tasks, and enhances the robustness of the model under different imaging conditions. However, the model has limitations, including the need for large, high-quality datasets to train the neural network effectively. The performance of the algorithm heavily depends on the quality of the training data, and poor or insufficient data can lead to inaccurate results. Additionally, the computational cost of training and running neural networks can be high, especially when working with large or high-resolution images, potentially limiting real-time applications. Furthermore, while the neural network adapts well to different defects, its effectiveness may decrease with extreme noise levels or highly corrupted images. Finally, the model’s complexity and the requirement for parameter tuning may necessitate expertise, limiting its ease of use in practical scenarios.</p><p>The proposed FEGAO-based approach was selected over alternative deep learning-based image restoration and hybrid algorithms due to its superior ability to handle non-linearity, uncertainty, and complex image degradation while maintaining computational efficiency. Existing deep learning models, such as the dual-fusion neural network, employ convolutional layers and residual learning for image restoration. However, these methods often rely on fixed architectures, limiting adaptability to varying levels of image degradation. Additionally, deep learning-based techniques require large training datasets and high computational resources, making them less suitable for real-time applications, particularly in industrial defect detection and medical imaging, where rapid decision-making is essential. Similarly, the neural network-based model proposed by Cui [<xref ref-type="bibr" rid="ref_14">14</xref>] effectively enhances image clarity and detail but is computationally expensive and sensitive to training data quality. Moreover, its reliance on learned parameters can lead to suboptimal performance when encountering severe blur or extreme noise levels.</p><p>In contrast, the FEGAO approach provides a non-linear refinement mechanism that dynamically adjusts to different levels of image degradation, ensuring robust restoration across diverse scenarios. Unlike purely deep learning-based methods, it does not require extensive pre-training, making it more adaptable and computationally efficient. Furthermore, the machine vision-based fuzzy image restoration algorithm proposed by Shi [<xref ref-type="bibr" rid="ref_19">19</xref>] demonstrates strong noise reduction capabilities but struggles with complex image distortions and high computational overhead, which the proposed method mitigates through optimized aggregation operations. By integrating FEGAO and non-linear refinement techniques, the proposed model bridges the gap between traditional image restoration and computationally intensive deep learning approaches. This balance allows for high-quality restoration while maintaining real-time feasibility, distinguishing the proposed approach from state-of-the-art methods.</p>
    </sec>
    <sec sec-type="">
      <title>3. Fuzzy einstein geometric aggregation for enhanced defective image processing</title>
      <p>The process of pixel coordinate extraction and enhancement of defective fuzzy images was reformulated using FEGAO. This operator ensures non-linear, robust aggregation of information, accounting for the uncertainty and imprecision inherent in defective fuzzy images. Let <inline-formula>
  <mml:math id="mzduo0ax2y">
    <mml:mi>I</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mrow>
      <mml:mo>{</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>…</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>}</mml:mo>
      <mml:msub>
        <mml:mi>I</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>I</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>I</mml:mi>
        <mml:mi>n</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> be a set of defective fuzzy images, where each image is represented by a fuzzy set <inline-formula>
  <mml:math id="mwadjcbscg">
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>. Each fuzzy set <inline-formula>
  <mml:math id="m77bd315bt">
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> corresponds to the membership function <inline-formula>
  <mml:math id="mzmsgbpm19">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:msub>
          <mml:mi>I</mml:mi>
          <mml:mi>i</mml:mi>
        </mml:msub>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
  </mml:math>
</inline-formula>, which assigns a degree of membership to each pixel coordinate $x<inline-formula>
  <mml:math id="mgeojrgaqz">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>O</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>{\mathcal{A}_{F E G A O}\left(I_1, I_2, \ldots, I_n\right)=\left(\prod_{i=1}^n \mu_{I_i}(x)^\alpha\right)^{\frac{1}{\alpha}}}<inline-formula>
  <mml:math id="meuyfqijpc">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>\mu_{I_i}(x)<inline-formula>
  <mml:math id="mnbxd37poh">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>x<inline-formula>
  <mml:math id="m9880j2znw">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>I_{i}<inline-formula>
  <mml:math id="m9l6xwiudv">
    <mml:mo>;</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\alpha<inline-formula>
  <mml:math id="mk5r0cwdq6">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>\alpha&gt;1<inline-formula>
  <mml:math id="mb53ilkm5f">
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula>\alpha&lt;$ 1 , it places more emphasis on individual pixel memberships.</p>
      
        <sec>
          
            <title>3.1. Extraction of pixel coordinates using fegao</title>
          
          <p>By adopting the fuzzy Einstein t-norm <inline-formula>
  <mml:math id="m17j9ldbrh">
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>T</mml:mi>
        <mml:mi>E</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> and t-conorm <inline-formula>
  <mml:math id="mpwubd0qy3">
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>S</mml:mi>
        <mml:mi>E</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> principles, the pixel coordinate correction and distance measures were enhanced for improved accuracy. The Einstein t-norm and t-conorm were defined as:</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="mddmu07q3d">
                <mml:mrow>
                  <mml:msub>
                    <mml:mi>T</mml:mi>
                    <mml:mi>E</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>S</mml:mi>
                    <mml:mi>E</mml:mi>
                  </mml:msub>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mo>=</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mo>=</mml:mo>
                  <mml:mi>a</mml:mi>
                  <mml:mi>b</mml:mi>
                  <mml:mi>a</mml:mi>
                  <mml:mi>b</mml:mi>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mi>a</mml:mi>
                      <mml:mi>b</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mn>1</mml:mn>
                      <mml:mn>1</mml:mn>
                      <mml:mn>1</mml:mn>
                      <mml:mo>+</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mi>a</mml:mi>
                      <mml:mi>b</mml:mi>
                    </mml:mrow>
                  </mml:mfrac>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mi>a</mml:mi>
                      <mml:mi>b</mml:mi>
                      <mml:mo>+</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mn>1</mml:mn>
                      <mml:mo>+</mml:mo>
                      <mml:mi>a</mml:mi>
                      <mml:mi>b</mml:mi>
                    </mml:mrow>
                  </mml:mfrac>
                  <mml:mstyle scriptlevel="0">
                    <mml:mspace width="1em"/>
                  </mml:mstyle>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>These equations define the Einstein t-norm <inline-formula>
  <mml:math id="msrnt38j7z">
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>T</mml:mi>
        <mml:mi>E</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> and t-conorm <inline-formula>
  <mml:math id="mbwc52gjdb">
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>S</mml:mi>
        <mml:mi>E</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula>, which are non-linear operators used to model the intersection (AND operation) and union (OR operation) of fuzzy sets, respectively. The t-norm <inline-formula>
  <mml:math id="mva5tlijk8">
    <mml:msub>
      <mml:mi>T</mml:mi>
      <mml:mi>E</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is used to represent the “AND” operation, emphasizing the intersection between fuzzy sets. The t-conorm <inline-formula>
  <mml:math id="mjz8bivoeh">
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mi>E</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>, on the other hand, represents the “OR” operation, modeling the union of fuzzy sets. These operators were used to aggregate fuzzy information while maintaining uncertainty, which is essential in fuzzy image processing.</p><p>Using FEGAO, the projection system equations were reformulated as follows:</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="mglbsykuub">
                <mml:mrow>
                  <mml:msub>
                    <mml:mi>D</mml:mi>
                    <mml:mrow>
                      <mml:mi>A</mml:mi>
                      <mml:mi>S</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>T</mml:mi>
                    <mml:mi>E</mml:mi>
                  </mml:msub>
                  <mml:mo>=</mml:mo>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>λ</mml:mi>
                    <mml:msub>
                      <mml:mi>T</mml:mi>
                      <mml:mi>E</mml:mi>
                    </mml:msub>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>E</mml:mi>
                        <mml:mi>d</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>D</mml:mi>
                        <mml:mi>x</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>This equation models the projection of pixel coordinates for enhanced distance measurements. The angle parameter <inline-formula>
  <mml:math id="mjdtjwwehl">
    <mml:mi>λ</mml:mi>
  </mml:math>
</inline-formula> was combined with the focal length <inline-formula>
  <mml:math id="m3nrds0e24">
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mi>d</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> and the pixel distance <inline-formula>
  <mml:math id="mjc0um9d8h">
    <mml:msub>
      <mml:mi>D</mml:mi>
      <mml:mi>x</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> using the Einstein t-norm <inline-formula>
  <mml:math id="myi5kz177w">
    <mml:msub>
      <mml:mi>T</mml:mi>
      <mml:mi>E</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>. The nested structure of this equation ensures that the effect of each parameter is aggregated non-linearly, allowing for a more accurate representation of the geometry of the defective fuzzy image. The Cartesian-topixel coordinate transformation becomes:</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="menrf25l1s">
                <mml:mrow>
                  <mml:msub>
                    <mml:mi>F</mml:mi>
                    <mml:mi>m</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>T</mml:mi>
                    <mml:mi>E</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>F</mml:mi>
                    <mml:mi>n</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>T</mml:mi>
                    <mml:mi>E</mml:mi>
                  </mml:msub>
                  <mml:mo>⋅</mml:mo>
                  <mml:mo>=</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>⋅</mml:mo>
                  <mml:mo>=</mml:mo>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msub>
                      <mml:mi>x</mml:mi>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mi>j</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>z</mml:mi>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mi>j</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msub>
                      <mml:mi>y</mml:mi>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mi>j</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>z</mml:mi>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mi>j</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mstyle scriptlevel="0">
                    <mml:mspace width="1em"/>
                  </mml:mstyle>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>These equations define the transformation from Cartesian coordinates to pixel coordinates, incorporating the depth information (represented by <inline-formula>
  <mml:math id="mpn9y0ggqh">
    <mml:msub>
      <mml:mi>z</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula>) using the Einstein tnorm <inline-formula>
  <mml:math id="muv93ydblj">
    <mml:msub>
      <mml:mi>T</mml:mi>
      <mml:mi>E</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>. This non-linear aggregation ensures that the influence of the $z<inline-formula>
  <mml:math id="m5d9krf3eq">
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>x<inline-formula>
  <mml:math id="my0hgvhg7l">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>y$ axes. This transformation enhances the accuracy of the pixel representation, accounting for depth in a fuzzy image.</p><p>The corrected pixel distance was redefined using the Einstein geometric aggregation:</p>
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="mgl1vus4kl">
                <mml:mrow>
                  <mml:msub>
                    <mml:mi>d</mml:mi>
                    <mml:mrow>
                      <mml:mi>x</mml:mi>
                      <mml:mi>y</mml:mi>
                      <mml:mi>z</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mo>,</mml:mo>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>T</mml:mi>
                    <mml:mi>E</mml:mi>
                  </mml:msub>
                  <mml:mo>=</mml:mo>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>T</mml:mi>
                        <mml:mi>E</mml:mi>
                      </mml:msub>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msubsup>
                          <mml:mi>x</mml:mi>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                            <mml:mi>j</mml:mi>
                          </mml:mrow>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                        <mml:msubsup>
                          <mml:mi>y</mml:mi>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                            <mml:mi>j</mml:mi>
                          </mml:mrow>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                      </mml:mrow>
                      <mml:msubsup>
                        <mml:mi>z</mml:mi>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:msubsup>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mn>1</mml:mn>
                      <mml:mn>2</mml:mn>
                      <mml:mrow>
                        <mml:mo>/</mml:mo>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:msup>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>This equation calculates the Euclidean distance of a pixel in a fuzzy-enhanced manner. The squared contributions of the <inline-formula>
  <mml:math id="mrvqxgirv2">
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>, and $z<inline-formula>
  <mml:math id="m4qxxl3hmo">
    <mml:mi>a</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>t<inline-formula>
  <mml:math id="myhihkj22u">
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
  </mml:math>
</inline-formula>T_E$. The square root of the result gives the corrected pixel distance, which accounts for the uncertainty and imprecision inherent in the pixel values, providing a more robust distance measurement for further processing.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Refinement of resolution using fuzzy einstein operators</title>
          
          <p>Resolution refinement leverages the fuzzy Einstein t-conorm to enhance regional intensity variations. The imaging model is represented as:</p>
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="m0ggkmnbsy">
                <mml:mrow>
                  <mml:msub>
                    <mml:mi>M</mml:mi>
                    <mml:mi>x</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>S</mml:mi>
                    <mml:mi>E</mml:mi>
                  </mml:msub>
                  <mml:mo>=</mml:mo>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>F</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:msubsup>
                          <mml:mi>x</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                        <mml:msubsup>
                          <mml:mi>y</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                        <mml:mo>+</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>π</mml:mi>
                        <mml:msubsup>
                          <mml:mi>ω</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>In this equation, resolution refinement was performed by using the Einstein t-conorm <inline-formula>
  <mml:math id="mpgdmjrsjb">
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mi>E</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> to combine the regional intensity function <inline-formula>
  <mml:math id="m21y115thf">
    <mml:mi>F</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> with the normalized intensity distribution <inline-formula>
  <mml:math id="m3oo6uu922">
    <mml:mfrac>
      <mml:mrow>
        <mml:msubsup>
          <mml:mi>x</mml:mi>
          <mml:mi>i</mml:mi>
          <mml:mn>2</mml:mn>
        </mml:msubsup>
        <mml:msubsup>
          <mml:mi>y</mml:mi>
          <mml:mi>i</mml:mi>
          <mml:mn>2</mml:mn>
        </mml:msubsup>
        <mml:mo>+</mml:mo>
      </mml:mrow>
      <mml:mrow>
        <mml:mi>π</mml:mi>
        <mml:msubsup>
          <mml:mi>ω</mml:mi>
          <mml:mi>i</mml:mi>
          <mml:mn>2</mml:mn>
        </mml:msubsup>
      </mml:mrow>
    </mml:mfrac>
  </mml:math>
</inline-formula>. The function <inline-formula>
  <mml:math id="mbi3jjxi74">
    <mml:mi>F</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> was defined by a fuzzy membership function that adapts to intensity variations in the image, and <inline-formula>
  <mml:math id="msllcvwi89">
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mi>E</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> ensures that the aggregation is smooth, enhancing regional intensity variations in the defective image. The membership function <inline-formula>
  <mml:math id="m3z6t0bikz">
    <mml:mi>F</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> was defined as:</p><p style="text-align: center"><inline-formula>
  <mml:math id="muugp07wl2">
    <mml:mrow>
      <mml:mi>F</mml:mi>
      <mml:mi>x</mml:mi>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>=</mml:mo>
      <mml:mrow>
        <mml:mo>{</mml:mo>
        <mml:mo fence="true"/>
        <mml:mtable columnalign="left left" columnspacing="1em" rowspacing=".2em">
          <mml:mtr>
            <mml:mtd>
              <mml:mn>1</mml:mn>
              <mml:mo>−</mml:mo>
              <mml:msup>
                <mml:mi>e</mml:mi>
                <mml:mrow>
                  <mml:mo>−</mml:mo>
                  <mml:mi>x</mml:mi>
                </mml:mrow>
              </mml:msup>
            </mml:mtd>
            <mml:mtd>
              <mml:mi>a</mml:mi>
              <mml:mi>m</mml:mi>
              <mml:mi>p</mml:mi>
              <mml:mi>x</mml:mi>
              <mml:mo>;</mml:mo>
              <mml:mo>≥</mml:mo>
              <mml:mtext> if </mml:mtext>
              <mml:mn>0</mml:mn>
            </mml:mtd>
          </mml:mtr>
          <mml:mtr>
            <mml:mtd>
              <mml:msup>
                <mml:mi>e</mml:mi>
                <mml:mi>x</mml:mi>
              </mml:msup>
              <mml:mo>−</mml:mo>
              <mml:mn>1</mml:mn>
            </mml:mtd>
            <mml:mtd>
              <mml:mi>a</mml:mi>
              <mml:mi>m</mml:mi>
              <mml:mi>p</mml:mi>
              <mml:mi>x</mml:mi>
              <mml:mo>;</mml:mo>
              <mml:mo>&amp;lt;</mml:mo>
              <mml:mtext> if </mml:mtext>
              <mml:mn>0</mml:mn>
            </mml:mtd>
          </mml:mtr>
        </mml:mtable>
      </mml:mrow>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p>This membership function introduces non-linearity, ensuring a smooth transition between regions of varying intensity.</p><p>The feature extraction using Super-Resolution Convolutional Neural Network (SRCNN) was modified to incorporate Einstein aggregation:</p>
          
            <disp-formula>
              <label>(6)</label>
              <mml:math id="mwrrx5b4xr">
                <mml:mrow>
                  <mml:msub>
                    <mml:mi>F</mml:mi>
                    <mml:mi>x</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>S</mml:mi>
                    <mml:mi>E</mml:mi>
                  </mml:msub>
                  <mml:mo>=</mml:mo>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mo>max</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>f</mml:mi>
                      </mml:mrow>
                      <mml:msub>
                        <mml:mi>f</mml:mi>
                        <mml:mi>x</mml:mi>
                      </mml:msub>
                    </mml:mfrac>
                    <mml:msub>
                      <mml:mi>M</mml:mi>
                      <mml:mi>x</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p> where, <inline-formula>
  <mml:math id="mndmr5iuxs">
    <mml:mfrac>
      <mml:mrow>
        <mml:mo>max</mml:mo>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>f</mml:mi>
      </mml:mrow>
      <mml:msub>
        <mml:mi>f</mml:mi>
        <mml:mi>x</mml:mi>
      </mml:msub>
    </mml:mfrac>
  </mml:math>
</inline-formula> normalizes feature intensities, and <inline-formula>
  <mml:math id="mi62n1dx5i">
    <mml:msub>
      <mml:mi>M</mml:mi>
      <mml:mi>x</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> represents the imaging model. The use of <inline-formula>
  <mml:math id="mqxe1wk8ne">
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mi>E</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> facilitates the aggregation of normalized features, improving robustness in feature representation.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Multi-scale gaussian correction with fuzzy einstein integration</title>
          
          <p>The empirical correction parameters were derived using adaptive Gaussian functions aggregated via the Einstein operator:</p>
          
            <disp-formula>
              <label>(7)</label>
              <mml:math id="mbylzx4k9u">
                <mml:mrow>
                  <mml:mi>L</mml:mi>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msub>
                      <mml:mi>x</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>y</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msup>
                      <mml:mi>L</mml:mi>
                      <mml:mrow>
                        <mml:mi>′</mml:mi>
                      </mml:mrow>
                    </mml:msup>
                    <mml:msup>
                      <mml:mi>L</mml:mi>
                      <mml:mrow>
                        <mml:mi>′</mml:mi>
                        <mml:mi>′</mml:mi>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>x</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>y</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>x</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>y</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mo>=</mml:mo>
                  <mml:msub>
                    <mml:mi>T</mml:mi>
                    <mml:mi>E</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>This equation combines local intensity corrections <inline-formula>
  <mml:math id="myww590f53">
    <mml:msup>
      <mml:mi>L</mml:mi>
      <mml:mrow>
        <mml:mi>′</mml:mi>
      </mml:mrow>
    </mml:msup>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> and light source corrections <inline-formula>
  <mml:math id="mcmoa01xp4">
    <mml:msup>
      <mml:mi>L</mml:mi>
      <mml:mrow>
        <mml:mi>′</mml:mi>
        <mml:mi>′</mml:mi>
      </mml:mrow>
    </mml:msup>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> using the Einstein t-norm <inline-formula>
  <mml:math id="mq2wx99u12">
    <mml:msub>
      <mml:mi>T</mml:mi>
      <mml:mi>E</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>. The use of this non-linear operator ensures that both intensity and light source corrections are aggregated in a manner that emphasizes the dominant correction while smoothing out inconsistencies. This adaptive correction helps in improving the image quality by compensating for variations in lighting and local intensity.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Layered enhancement framework using fegao</title>
          
          <p>The layered enhancement involves computing regional differences and applying Gaussian smoothing iteratively:</p>
          
            <disp-formula>
              <label>(8)</label>
              <mml:math id="m4sez6e2zw">
                <mml:mrow>
                  <mml:mi>C</mml:mi>
                  <mml:msub>
                    <mml:mi>D</mml:mi>
                    <mml:mrow>
                      <mml:mi>z</mml:mi>
                      <mml:mi>c</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>S</mml:mi>
                    <mml:mi>E</mml:mi>
                  </mml:msub>
                  <mml:mo>=</mml:mo>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mi>A</mml:mi>
                          <mml:mi>α</mml:mi>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>B</mml:mi>
                          <mml:mi>c</mml:mi>
                        </mml:msub>
                        <mml:mo>+</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:msub>
                            <mml:mi>x</mml:mi>
                            <mml:mi>i</mml:mi>
                          </mml:msub>
                          <mml:msub>
                            <mml:mi>y</mml:mi>
                            <mml:mi>i</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:mfrac>
                    <mml:msub>
                      <mml:mi>O</mml:mi>
                      <mml:mi>P</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>This equation defines the layered enhancement process, where regional differences are computed and aggregated using the t-conorm <inline-formula>
  <mml:math id="mdpdj6daob">
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mi>E</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>. The parameters <inline-formula>
  <mml:math id="mgwn24307l">
    <mml:msub>
      <mml:mi>A</mml:mi>
      <mml:mi>α</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="m6pssos6z1">
    <mml:msub>
      <mml:mi>B</mml:mi>
      <mml:mi>c</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> control the regional differences, while <inline-formula>
  <mml:math id="mhmpt0lwnb">
    <mml:mi>L</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> represents the local correction term. The use of <inline-formula>
  <mml:math id="miwvp10zzx">
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mi>E</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> ensures that the aggregation is smooth, minimizing artifacts in the output, which is crucial for maintaining high image quality in the enhanced output.</p><p>The edge-enhanced image was modeled as:</p>
          
            <disp-formula>
              <label>(9)</label>
              <mml:math id="m6wut48db2">
                <mml:mrow>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mi>u</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>T</mml:mi>
                    <mml:mi>E</mml:mi>
                  </mml:msub>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msub>
                      <mml:mi>x</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>y</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mfrac>
                      <mml:msub>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mi>l</mml:mi>
                          <mml:mi>p</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mi>θ</mml:mi>
                    </mml:mfrac>
                    <mml:msub>
                      <mml:mi>W</mml:mi>
                      <mml:mi>m</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mo>=</mml:mo>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>In this equation, the edge enhancement was modeled by applying the Einstein t-norm <inline-formula>
  <mml:math id="mg717sgjue">
    <mml:msub>
      <mml:mi>T</mml:mi>
      <mml:mi>E</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> to combine background illumination <inline-formula>
  <mml:math id="mjgb8itxff">
    <mml:msub>
      <mml:mi>L</mml:mi>
      <mml:mrow>
        <mml:mi>l</mml:mi>
        <mml:mi>p</mml:mi>
        <mml:mi>ϕ</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> an adjustment parameter <inline-formula>
  <mml:math id="m4jkca4l04">
    <mml:mi>θ</mml:mi>
  </mml:math>
</inline-formula>, and a regional weight <inline-formula>
  <mml:math id="mg82dproeq">
    <mml:msub>
      <mml:mi>W</mml:mi>
      <mml:mi>m</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>. This ensures that the edge enhancement is consistent and avoids abrupt transitions, preserving fine details in the image.</p><p>Finally, the image subtraction model incorporates Einstein aggregation for numerical enhancement:</p>
          
            <disp-formula>
              <label>(10)</label>
              <mml:math id="mcuv0du1hr">
                <mml:mrow>
                  <mml:mi>E</mml:mi>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msub>
                      <mml:mi>x</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>y</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msub>
                      <mml:mi>N</mml:mi>
                      <mml:mrow>
                        <mml:mi>f</mml:mi>
                        <mml:mi>g</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mfrac>
                      <mml:msub>
                        <mml:mi>D</mml:mi>
                        <mml:mi>c</mml:mi>
                      </mml:msub>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mi>m</mml:mi>
                          <mml:mi>v</mml:mi>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>y</mml:mi>
                          <mml:mi>u</mml:mi>
                        </mml:msub>
                        <mml:mo>⋅</mml:mo>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:msub>
                            <mml:mi>x</mml:mi>
                            <mml:mi>i</mml:mi>
                          </mml:msub>
                          <mml:msub>
                            <mml:mi>y</mml:mi>
                            <mml:mi>i</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:mrow>
                  <mml:mo>=</mml:mo>
                  <mml:msub>
                    <mml:mi>T</mml:mi>
                    <mml:mi>E</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>This equation incorporates Einstein aggregation to refine image subtraction. The enhancement function <inline-formula>
  <mml:math id="mp4jvxi8wr">
    <mml:msub>
      <mml:mi>N</mml:mi>
      <mml:mrow>
        <mml:mi>f</mml:mi>
        <mml:mi>g</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> was combined with the reflection component <inline-formula>
  <mml:math id="mk6wy8dkeb">
    <mml:msub>
      <mml:mi>D</mml:mi>
      <mml:mi>c</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> and the normalized pixel output <inline-formula>
  <mml:math id="mzzhjlpc0w">
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mi>u</mml:mi>
    </mml:msub>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula>. The use of the t-norm <inline-formula>
  <mml:math id="mshhj4nr13">
    <mml:msub>
      <mml:mi>T</mml:mi>
      <mml:mi>E</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> ensures that the enhancement is proportional to the underlying features, avoiding over-enhancement and improving the accuracy of the defective image.</p><p>Each equation uses fuzzy logic, particularly the Einstein t-norm and t-conorm, to aggregate and refine the information in a manner that accounts for uncertainty and imprecision, which are common in defective image processing. The non-linear nature of these operators enhances the robustness of the image processing pipeline, resulting in improved accuracy, feature preservation, and defect detection.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>Visualization for the proposed FEGAO-based defect detection framework</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/1/img_rbt8rrhk0GNADlHC.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.5. Benefits of fuzzy einstein-based aggregation</title>
          
          <p>The proposed algorithm uses multi-scale Gaussian corrections and layered enhancement to handle different levels of image degradation effectively. Multi-scale Gaussian correction helps in addressing both global and local distortions, ensuring that fine details are preserved while reducing large-scale noise. Layered enhancement further refines the image by adapting to different intensity levels, resulting in a clearer and more consistent restoration. These choices were based on previous research, which showed that multi-resolution techniques improve robustness in complex defective images.</p><p>The use of Einstein t-norm and t-conorm is an important feature of the proposed model, as it allows better fusion of uncertain image information. Compared to traditional norms, the Einstein t-norm provides stronger noise suppression while retaining important details, making it particularly useful for images with high uncertainty. The sensitivity parameter <inline-formula>
  <mml:math id="mtmn74iloj">
    <mml:mi>α</mml:mi>
  </mml:math>
</inline-formula> was selected through controlled experiments, where different values were tested to achieve the best balance between noise reduction and image detail preservation. Similarly, the number of iterations was optimized to ensure the best restoration quality without adding unnecessary computational overhead.</p><p>In terms of computational complexity, the proposed method is designed to be more efficient than deep learning- based models. While deep learning approaches require large amounts of training data and high computational power, the proposed approach uses mathematical refinements that reduce processing time while maintaining high-quality results. This makes the proposed method more suitable for real-time applications, such as industrial and medical imaging. By carefully balancing image quality and processing speed, the proposed approach provides a practical and efficient solution for restoring defective images.</p><p>This novel integration of the FEGAO offers a mathematically robust and computationally efficient framework for defective image processing, ensuring enhanced resolution and clarity for practical applications. The main steps are as follows:</p><p>By modifying pixel values iteratively across multiple frames, the restoration and enhancement of defective fuzzy images were achieved. This approach, based on neural networks and super-resolution reconstruction, ensures high-quality restoration and enhancement of defective fuzzy images.</p><table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: justify"><span style="font-family: Times New Roman, serif">Algorithm 1<span style="font-family: Times New Roman, serif"> <span style="font-family: Times New Roman, serif">Fuzzy <span style="font-family: Times New Roman, serif">multi-frame <span style="font-family: Times New Roman, serif">super-resolution <span style="font-family: Times New Roman, serif">restoration for <span style="font-family: Times New Roman, serif">defective images using FEGAO</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify"><span style="font-family: Times New Roman, serif">1: Initialization of the reference image: <span style="font-family: Times New Roman, serif">The original low-resolution <span style="font-family: Times New Roman, serif">defective fuzzy image was used as the reference for restoration and enhancement. <span style="font-family: Times New Roman, serif">The fuzzy membership function <italic><span style="font-family: Times New Roman, serif">F</italic><span style="font-family: Times New Roman, serif">(<italic><span style="font-family: Times New Roman, serif">x</italic><span style="font-family: Times New Roman, serif">) <span style="font-family: Times New Roman, serif">was computed for regional intensity values.</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify"><span style="font-family: Times New Roman, serif">2: Selection of pixel regions: Low-resolution pixels in the reference image with dimensions (<italic><span style="font-family: Times New Roman, serif">M</italic><sub><span style="font-family: Times New Roman, serif">1</sub><italic><span style="font-family: Times New Roman, serif">, M</italic><sub>2</sub>) <span style="font-family: Times New Roman, serif">were identified and the fuzzy pixel normalization was applied. Corresponding high-resolution pixel values for multi-frame super-resolution images (<italic>N</italic><sub>1</sub><italic>, N</italic><sub>2</sub>) <span style="font-family: Times New Roman, serif">were p<span style="font-family: Times New Roman, serif">redicted using fuzzy membership aggregation <italic>M<sub>x</sub> </italic>and initialize<span style="font-family: Times New Roman, serif">d as <italic><span style="font-family: Times New Roman, serif">S</italic><sub><span style="font-family: Times New Roman, serif">1</sub><span style="font-family: Times New Roman, serif">.</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify"><span style="font-family: Times New Roman, serif">3: Iterative pixel prediction: For remaining low-resolution pixel points, <italic>ψ<sub>i</sub> </italic>(<italic>i </italic><span style="font-family: Times New Roman, serif">= 1<italic><span style="font-family: Times New Roman, serif">,...,<span style="font-family: Times New Roman, serif">M-</italic><span style="font-family: Times New Roman, serif">1), values were predicted using Einstein t-norm aggregation <italic>T<sub>E</sub> </italic>applied to the reference multi-frame super-resolution image <italic>ψ</italic><sub>0</sub>.</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify"><span style="font-family: Times New Roman, serif">4: Construction of multi-frame images: <span style="font-family: Times New Roman, serif">Fuzzy-enhanced visual communication techniques <span style="font-family: Times New Roman, serif">were applied using the Einstein t-conorm <italic><span style="font-family: Times New Roman, serif">S<sub><span style="font-family: Times New Roman, serif">E</sub><span style="font-family: Times New Roman, serif"> </italic><span style="font-family: Times New Roman, serif">to create multi-frame super-resolution images. A correction function for pixel values <span style="font-family: Times New Roman, serif">was c<span style="font-family: Times New Roman, serif">omputed using the proposed resolution refinement in Eq. (5).</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify"><span style="font-family: Times New Roman, serif">5: Simulation of the correction process: The <italic>k</italic>-th iteration of the correction process was performed for the super-resolution image. Pixel values were predicted as <italic><span style="font-family: Times New Roman, serif">ψ<sup><span style="font-family: Times New Roman, serif">′</sup> </italic><span style="font-family: Times New Roman, serif">(<italic><span style="font-family: Times New Roman, serif">i </italic><span style="font-family: Times New Roman, serif">= 0<italic><span style="font-family: Times New Roman, serif">,... ,M-</italic><span style="font-family: Times New Roman, serif">1) using fuzzy Euclidean distance:</p><p style="text-align: center"><inline-formula>
  <mml:math id="m58b3i47bf">
    <mml:mrow>
      <mml:msub>
        <mml:mi>d</mml:mi>
        <mml:mrow>
          <mml:mi>x</mml:mi>
          <mml:mi>y</mml:mi>
          <mml:mi>z</mml:mi>
          <mml:mo>,</mml:mo>
          <mml:mo>,</mml:mo>
        </mml:mrow>
      </mml:msub>
      <mml:msub>
        <mml:mi>T</mml:mi>
        <mml:mi>E</mml:mi>
      </mml:msub>
      <mml:mo>=</mml:mo>
      <mml:msup>
        <mml:mrow>
          <mml:mo>(</mml:mo>
          <mml:mo>,</mml:mo>
          <mml:mo>)</mml:mo>
          <mml:msub>
            <mml:mi>T</mml:mi>
            <mml:mi>E</mml:mi>
          </mml:msub>
          <mml:mrow>
            <mml:mo>(</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:msubsup>
              <mml:mi>x</mml:mi>
              <mml:mrow>
                <mml:mi>i</mml:mi>
                <mml:mi>j</mml:mi>
              </mml:mrow>
              <mml:mn>2</mml:mn>
            </mml:msubsup>
            <mml:msubsup>
              <mml:mi>y</mml:mi>
              <mml:mrow>
                <mml:mi>i</mml:mi>
                <mml:mi>j</mml:mi>
              </mml:mrow>
              <mml:mn>2</mml:mn>
            </mml:msubsup>
          </mml:mrow>
          <mml:msubsup>
            <mml:mi>z</mml:mi>
            <mml:mrow>
              <mml:mi>i</mml:mi>
              <mml:mi>j</mml:mi>
            </mml:mrow>
            <mml:mn>2</mml:mn>
          </mml:msubsup>
        </mml:mrow>
        <mml:mrow>
          <mml:mn>1</mml:mn>
          <mml:mn>2</mml:mn>
          <mml:mrow>
            <mml:mo>/</mml:mo>
          </mml:mrow>
        </mml:mrow>
      </mml:msup>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p style="text-align: justify">The deviation <inline-formula>
  <mml:math id="meen3fdhjo">
    <mml:msub>
      <mml:mi>ϵ</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> between predicted and actual values from Eq. (10) was calculated.</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">6: Update of super-resolution images: The multi-frame super-resolution image <inline-formula>
  <mml:math id="m81usduw0u">
    <mml:msub>
      <mml:mi>ξ</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> was adjusted using the deviation <inline-formula>
  <mml:math id="m8omknigpu">
    <mml:msub>
      <mml:mi>ϵ</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> and the fuzzy edge-enhancement model:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mylr8jpasu">
    <mml:mrow>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mi>u</mml:mi>
      </mml:msub>
      <mml:msub>
        <mml:mi>T</mml:mi>
        <mml:mi>E</mml:mi>
      </mml:msub>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>,</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:msub>
          <mml:mi>x</mml:mi>
          <mml:mi>i</mml:mi>
        </mml:msub>
        <mml:msub>
          <mml:mi>y</mml:mi>
          <mml:mi>i</mml:mi>
        </mml:msub>
      </mml:mrow>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>,</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mfrac>
          <mml:msub>
            <mml:mi>L</mml:mi>
            <mml:mrow>
              <mml:mi>l</mml:mi>
              <mml:mi>p</mml:mi>
            </mml:mrow>
          </mml:msub>
          <mml:mi>θ</mml:mi>
        </mml:mfrac>
        <mml:msub>
          <mml:mi>W</mml:mi>
          <mml:mi>m</mml:mi>
        </mml:msub>
      </mml:mrow>
      <mml:mo>=</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p style="text-align: justify">The next iteration <inline-formula>
  <mml:math id="m2y9r437aw">
    <mml:msub>
      <mml:mi>ξ</mml:mi>
      <mml:mrow>
        <mml:mi>k</mml:mi>
        <mml:mo>+</mml:mo>
        <mml:mn>1</mml:mn>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> was reconstructed using FEGAO.</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify"><span style="font-family: Times New Roman, serif">7: Convergence check: Whether the final super-resolution image meets predefined fuzzy quality criteria was verified using metrics such as PSNR, SSIM, and Natural Image Quality Evaluator (NIQE). If not, Steps 5 and 6 should be repeated until convergence is achieved.</p></td></tr></tbody></table><p>To ensure a rigorous evaluation, the proposed FEGAO-based image restoration model was compared against state-of-the-art methods using a set of well-established quantitative and qualitative metrics. The selection of these criteria was based on their relevance to image restoration quality, perceptual clarity, and computational efficiency. Six key metrics—PSNR, SSIM, Mean Opinion Score (MOS), Normalized Cross-Correlation (NCC), MSE, and Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE)—were employed to provide a comprehensive performance assessment. PSNR and MSE quantify pixel-level accuracy, while SSIM and NCC evaluate structural and feature preservation. BRISQUE is included as a no-reference metric to assess image distortions, ensuring an objective evaluation without requiring ground-truth data. Additionally, MOS provides a human-perceptual perspective, capturing subjective preferences in visual clarity. The comparison was conducted using a standardized dataset of defective images with varying degrees of degradation to test the robustness of each method. To ensure fairness, all models were executed under identical computational conditions, and statistical significance was analyzed to validate performance differences. This approach ensures a balanced and transparent evaluation, demonstrating the superiority of FEGAO over conventional fuzzy aggregation and machine-learning-based models.</p>
          <p><xref ref-type="fig" rid="fig_1">Figure 1</xref> includes six subplots representing key processes such as camera projection system modeling, Cartesian-to-pixel coordinate transformation, pixel distance mapping, projection modeling, pixel coordinate corrections, and resolution reconstruction.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Experimental analysis</title>
      <p>The experimental evaluation of the proposed defective fuzzy image processing model involves a series of tests designed to assess the effectiveness of pixel coordinate extraction, resolution enhancement, and overall image quality restoration. The model utilizes multi-frame image principles, neural network-based pixel correction, and adaptive image refinement techniques, including SRCNN and guided filtering methods. The dataset consists of a collection of defective fuzzy images, each with varying levels of blur and noise. The defective images used in this study were synthetically generated in a controlled environment to ensure a systematic and reproducible evaluation of the proposed image restoration approach. Specifically, the images were degraded using Gaussian blur with minimal noise levels, simulating real-world defect patterns commonly found in industrial inspection, medical imaging, and satellite image processing. This controlled setting allows for a precise assessment of the model’s performance in handling structured degradations while minimizing the influence of uncontrolled external factors. The image sizes were standardized to ensure consistency in the experimental setup. MATLAB R2015a was used for computational processing, including the application of the correction and enhancement algorithms. The performance of the proposed model was compared with the competing models, including those proposed by Zhou et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] and Cui [<xref ref-type="bibr" rid="ref_14">14</xref>], focusing on their ability to restore image clarity, enhance resolution, and minimize distortion. Evaluation metrics such as PSNR, SSIM, and MOS were used to quantify the results. Statistical analysis was conducted to identify significant improvements in image quality achieved by the proposed model. The parameter settings for the proposed model were determined based on extensive experimentation and fine-tuning (<xref ref-type="table" rid="table_1">Table 1</xref>).</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Parameter setup and suggested values for the proposed model</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1" colwidth="202"><p>Process</p></td><td colspan="1" rowspan="1" colwidth="263"><p>Parameters</p></td><td colspan="1" rowspan="1" colwidth="256"><p>Suggested Values</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="202"><p>Sensitivity parameter</p></td><td colspan="1" rowspan="1" colwidth="263"><p><mml:math id="mdkglp6ogo">
  <mml:mi>α</mml:mi>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="256"><p>0.1 to 1.0</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="202"><p>Pixel coordinate projection</p></td><td colspan="1" rowspan="1" colwidth="263"><p><mml:math id="mk9mhw448t">
  <mml:mi>λ</mml:mi>
  <mml:mo>,</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:msub>
    <mml:mi>E</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:msub>
  <mml:msub>
    <mml:mi>D</mml:mi>
    <mml:mi>x</mml:mi>
  </mml:msub>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="256"><p><mml:math id="mq4luywxvv">
  <mml:mi>λ</mml:mi>
  <mml:mo>=</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:msup>
    <mml:mn>45</mml:mn>
    <mml:mrow>
      <mml:mo>∘</mml:mo>
    </mml:mrow>
  </mml:msup>
  <mml:msub>
    <mml:mi>E</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:msub>
  <mml:msub>
    <mml:mi>D</mml:mi>
    <mml:mi>x</mml:mi>
  </mml:msub>
  <mml:mn>1.2</mml:mn>
  <mml:mn>50</mml:mn>
</mml:math></p></td></tr><tr><td colspan="1" rowspan="1" colwidth="202"><p>Resolution refinement</p></td><td colspan="1" rowspan="1" colwidth="263"><p><mml:math id="mf8k7whljl">
  <mml:mi>F</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mo>(</mml:mo>
  <mml:mo>)</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:msub>
    <mml:mi>ω</mml:mi>
    <mml:mi>i</mml:mi>
  </mml:msub>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="256"><p><mml:math id="msm81bvsel">
  <mml:mi>F</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mo>(</mml:mo>
  <mml:mo>)</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mn>0.8</mml:mn>
  <mml:mn>1.5</mml:mn>
  <mml:msub>
    <mml:mi>ω</mml:mi>
    <mml:mi>i</mml:mi>
  </mml:msub>
</mml:math></p></td></tr><tr><td colspan="1" rowspan="1" colwidth="202"><p>Multi-scale Gaussian correction</p></td><td colspan="1" rowspan="1" colwidth="263"><p><mml:math id="m9s50wqeg7">
  <mml:munder>
    <mml:mrow>
      <mml:msup>
        <mml:mi>L</mml:mi>
        <mml:mrow>
          <mml:mi>′</mml:mi>
        </mml:mrow>
      </mml:msup>
      <mml:mo>,</mml:mo>
    </mml:mrow>
    <mml:mo accent="true">―</mml:mo>
  </mml:munder>
  <mml:msup>
    <mml:mi>L</mml:mi>
    <mml:mrow>
      <mml:mi>′</mml:mi>
      <mml:mi>′</mml:mi>
    </mml:mrow>
  </mml:msup>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="256"><p><mml:math id="md4ogozoya">
  <mml:msup>
    <mml:mi>L</mml:mi>
    <mml:mrow>
      <mml:mi>′</mml:mi>
    </mml:mrow>
  </mml:msup>
  <mml:msup>
    <mml:mi>L</mml:mi>
    <mml:mrow>
      <mml:mi>′</mml:mi>
      <mml:mi>′</mml:mi>
    </mml:mrow>
  </mml:msup>
  <mml:mo>=</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mn>0.6</mml:mn>
  <mml:mn>0.4</mml:mn>
</mml:math></p></td></tr><tr><td colspan="1" rowspan="1" colwidth="202"><p>Layered enhancement</p></td><td colspan="1" rowspan="1" colwidth="263"><p><mml:math id="mv2qx9bf1h">
  <mml:msub>
    <mml:mi>A</mml:mi>
    <mml:mi>α</mml:mi>
  </mml:msub>
  <mml:msub>
    <mml:mi>B</mml:mi>
    <mml:mi>c</mml:mi>
  </mml:msub>
  <mml:msub>
    <mml:mi>O</mml:mi>
    <mml:mi>P</mml:mi>
  </mml:msub>
  <mml:mo>,</mml:mo>
  <mml:mo>,</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="256"><p><mml:math id="m4tbk9y5d6">
  <mml:msub>
    <mml:mi>A</mml:mi>
    <mml:mi>α</mml:mi>
  </mml:msub>
  <mml:msub>
    <mml:mi>B</mml:mi>
    <mml:mi>c</mml:mi>
  </mml:msub>
  <mml:msub>
    <mml:mi>O</mml:mi>
    <mml:mi>P</mml:mi>
  </mml:msub>
  <mml:mo>=</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mn>2.0</mml:mn>
  <mml:mn>1.5</mml:mn>
  <mml:mn>0.9</mml:mn>
</mml:math></p></td></tr><tr><td colspan="1" rowspan="1" colwidth="202"><p>Edge enhancement</p></td><td colspan="1" rowspan="1" colwidth="263"><p><mml:math id="m0ejv9n796">
  <mml:msub>
    <mml:mi>L</mml:mi>
    <mml:mrow>
      <mml:mi>l</mml:mi>
      <mml:mi>p</mml:mi>
    </mml:mrow>
  </mml:msub>
  <mml:msub>
    <mml:mi>W</mml:mi>
    <mml:mi>m</mml:mi>
  </mml:msub>
  <mml:mo>,</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mi>θ</mml:mi>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="256"><p><mml:math id="mv6ivmgc3c">
  <mml:msub>
    <mml:mi>L</mml:mi>
    <mml:mrow>
      <mml:mi>l</mml:mi>
      <mml:mi>p</mml:mi>
    </mml:mrow>
  </mml:msub>
  <mml:msub>
    <mml:mi>W</mml:mi>
    <mml:mi>m</mml:mi>
  </mml:msub>
  <mml:mo>=</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mn>1.0</mml:mn>
  <mml:mn>2.5</mml:mn>
  <mml:mn>1.2</mml:mn>
  <mml:mi>θ</mml:mi>
</mml:math></p></td></tr><tr><td colspan="1" rowspan="1" colwidth="202"><p>Image subtraction enhancement</p></td><td colspan="1" rowspan="1" colwidth="263"><p><mml:math id="mf18phnnr5">
  <mml:msub>
    <mml:mi>N</mml:mi>
    <mml:mrow>
      <mml:mi>f</mml:mi>
      <mml:mi>g</mml:mi>
    </mml:mrow>
  </mml:msub>
  <mml:msub>
    <mml:mi>D</mml:mi>
    <mml:mi>c</mml:mi>
  </mml:msub>
  <mml:msub>
    <mml:mi>m</mml:mi>
    <mml:mi>v</mml:mi>
  </mml:msub>
  <mml:mo>,</mml:mo>
  <mml:mo>,</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="256"><p><mml:math id="myrlscuxng">
  <mml:msub>
    <mml:mi>N</mml:mi>
    <mml:mrow>
      <mml:mi>f</mml:mi>
      <mml:mi>g</mml:mi>
    </mml:mrow>
  </mml:msub>
  <mml:msub>
    <mml:mi>D</mml:mi>
    <mml:mi>c</mml:mi>
  </mml:msub>
  <mml:msub>
    <mml:mi>m</mml:mi>
    <mml:mi>v</mml:mi>
  </mml:msub>
  <mml:mo>=</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>,</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mn>0.8</mml:mn>
  <mml:mn>1.2</mml:mn>
  <mml:mn>1.0</mml:mn>
</mml:math></p></td></tr></tbody></table>
        </table-wrap>
      
      <p> <xref ref-type="fig" rid="fig_1">Figure 1</xref> comprehensively illustrates the application of the proposed FEGAO model for defect detection in image processing. The top-left subplot demonstrates the camera projection system, emphasizing the variation in distance from the optical axis across different angles. The top-center subplot visualizes the Cartesian-to-pixel coordinate transformation, showcasing smooth intensity transitions and spatial alignment. The top-right subplot represents the pixel distance from the center, revealing an enhanced Gaussian distribution that aids in accurate localization. The middle-left subplot details the projection model, highlighting linear relationships crucial for feature extraction. The middle-right subplot showcases pixel coordinate corrections, addressing non-linear distortions for spatial accuracy. Lastly, the bottom-right subplot illustrates the resolution reconstruction process, emphasizing refined intensity gradients and structural preservation. Together, these visualizations validate the efficacy of FEGAO in resolving uncertainties, refining pixel-level precision, and enhancing defect representation in images.</p><p> <xref ref-type="fig" rid="fig_2">Figure 2</xref> demonstrates the effectiveness of the proposed FEGAO model in addressing defect detection in blurred images. The left image represents the original defect-laden, blurry input, where details are obscured and intensity inconsistencies prevail. The right image showcases the enhanced output of the proposed model, revealing significant improvements in clarity, structural integrity, and edge definition. By employing advanced techniques such as Gaussian correction, layered enhancement, and resolution refinement, the FEGAO model successfully reconstructs finer details and achieves superior defect representation, highlighting its robustness in challenging scenarios.</p><p>The first column shows the given blurry and noisy input images. The second column displays the results from the model proposed by Zhou et al. [<xref ref-type="bibr" rid="ref_11">11</xref>]. The third column presents the outcomes of the model proposed by Cui [<xref ref-type="bibr" rid="ref_14">14</xref>]. Finally, the fourth column demonstrates the results of the proposed neural network-based model, which achieves superior clarity and detail restoration.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Given defective fuzzy image and the result using the proposed algorithm model</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/1/img_PBIuzAONx-lNp64T.png"/>
        </fig>
      
      <p><xref ref-type="fig" rid="fig_3">Figure 3</xref> presents a visual comparison of restoration and enhancement results for defective fuzzy images using different models. Each set of images represents iconic structures, such as historical landmarks and cityscapes, which are initially degraded by blur and noise. The first column displays the original blurry and noisy input images, which lack discernible textures and fine details due to motion blur, atmospheric interference, or noise. The second column shows the results obtained from the model proposed by Zhou et al. [<xref ref-type="bibr" rid="ref_11">11</xref>], which improves the images to some extent but leaves residual blur and struggles with detail recovery, particularly in complex areas like edges and textures. The third column highlights the performance of the model proposed by Cui [<xref ref-type="bibr" rid="ref_14">14</xref>], which achieves better noise reduction and clarity than Zhou et al.'s [<xref ref-type="bibr" rid="ref_11">11</xref>] model but suffers from over-smoothing, resulting in the loss of finer textures and realistic details in regions such as architectural edges and vegetation. Finally, the fourth column illustrates the outcomes of the proposed neural network-based model, which significantly outperforms the competing methods. This model effectively restores fine textures, enhances edge clarity, and maintains a natural appearance. For example, in the Colosseum image, the column edges are sharper and more defined, while the urban high-rise image shows improved texture and color contrast on the building facade. Similarly, the cityscape features vivid vegetation and well-defined road structures, and the Taj Mahal image retains intricate architectural patterns and sharp edges. Overall, the proposed model demonstrates superior restoration performance, providing sharper, more detailed, and visually pleasing results, making it highly suitable for applications in machine vision and image-based analysis.</p>
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>Visual comparison of image restoration and enhancement results</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/1/img_cLwOUwiGCQMw-COO.png"/>
        </fig>
      
      <p> <xref ref-type="table" rid="table_2">Table 2</xref> provides a detailed comparison of the proposed model with the models proposed by Zhou et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] and Cui [<xref ref-type="bibr" rid="ref_14">14</xref>] across multiple image quality metrics, including PSNR, SSIM, MOS, NCC, MSE, and BRISQUE [<xref ref-type="bibr" rid="ref_20">20</xref>].</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>Evaluation results for the proposed model compared to two other models</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1" colwidth="119"><p>Metric</p></td><td colspan="1" rowspan="1" colwidth="145"><p>Proposed Model</p></td><td colspan="1" rowspan="1" colwidth="197"><p>Model Proposed by Zhou et al. [<xref ref-type="bibr" rid="ref_11">11</xref>]</p></td><td colspan="1" rowspan="1" colwidth="185"><p>Model Proposed by Cui [<xref ref-type="bibr" rid="ref_14">14</xref>]</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="119"><p>PSNR (dB)</p></td><td colspan="1" rowspan="1" colwidth="145"><p>34.2 ± 0.8</p></td><td colspan="1" rowspan="1" colwidth="197"><p>29.7 ± 1.2</p></td><td colspan="1" rowspan="1" colwidth="185"><p>31.5 ± 1.0</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="119"><p>SSIM</p></td><td colspan="1" rowspan="1" colwidth="145"><p>0.95 ± 0.02</p></td><td colspan="1" rowspan="1" colwidth="197"><p>0.88 ± 0.03</p></td><td colspan="1" rowspan="1" colwidth="185"><p>0.91 ± 0.01</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="119"><p>MOS</p></td><td colspan="1" rowspan="1" colwidth="145"><p>4.7 ± 0.3</p></td><td colspan="1" rowspan="1" colwidth="197"><p>4.0 ± 0.4</p></td><td colspan="1" rowspan="1" colwidth="185"><p>4.2 ± 0.2</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="119"><p>NCC</p></td><td colspan="1" rowspan="1" colwidth="145"><p>0.97 ± 0.03</p></td><td colspan="1" rowspan="1" colwidth="197"><p>0.89 ± 0.05</p></td><td colspan="1" rowspan="1" colwidth="185"><p>0.92 ± 0.02</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="119"><p>MSE</p></td><td colspan="1" rowspan="1" colwidth="145"><p>0.003 ± 0.002</p></td><td colspan="1" rowspan="1" colwidth="197"><p>0.008 ± 0.003</p></td><td colspan="1" rowspan="1" colwidth="185"><p>0.006 ± 0.001</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="119"><p>BRISQUE</p></td><td colspan="1" rowspan="1" colwidth="145"><p>21.8 ± 2.1</p></td><td colspan="1" rowspan="1" colwidth="197"><p>30.3 ± 2.8</p></td><td colspan="1" rowspan="1" colwidth="185"><p>26.5 ± 1.9</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="119"><p>Execution time (s)</p></td><td colspan="1" rowspan="1" colwidth="145"><p>1.23</p></td><td colspan="1" rowspan="1" colwidth="197"><p>3.47</p></td><td colspan="1" rowspan="1" colwidth="185"><p>2.91</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>The proposed model demonstrates superior performance across all metrics, highlighting its effectiveness in enhancing image quality. For PSNR, the proposed model achieves a mean value of 34.2 dB ± 0.8, outperforming the models proposed by Zhou et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] (29.7 dB ± 1.2) and Cui [<xref ref-type="bibr" rid="ref_14">14</xref>] (31.5 dB ± 1.0), indicating better noise suppression and reconstruction accuracy. Similarly, in terms of SSIM, the proposed model scores 0.95 ± 0.02, which surpasses the models proposed by Zhou et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] (0.88 ± 0.03) and Cui [<xref ref-type="bibr" rid="ref_14">14</xref>] (0.91 ± 0.01), demonstrating its ability to preserve structural details in images.</p><p>The MOS, a subjective quality assessment score, further validates the superior performance of the proposed model with a score of 4.7 ± 0.3, compared to 4.0 ± 0.4 for the model proposed by Zhou et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] and 4.2 ± 0.2 for the model proposed by Cui [<xref ref-type="bibr" rid="ref_14">14</xref>]. Additionally, the NCC metric, which measures the correlation between the original and processed images, is highest for the proposed model at 0.97 ± 0.03, outperforming the models proposed by Zhou et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] (0.89 ± 0.05) and Cui [<xref ref-type="bibr" rid="ref_14">14</xref>] (0.92 ± 0.02).</p>
      
        <fig id="fig_4">
          <label>Figure 4</label>
          <caption>
            <title>Comparison of the proposed model with competing models based on image quality metrics</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/1/img_GMh_17O-zh4ze8Ze.png"/>
        </fig>
      
      <p>In terms of error measurement, the proposed model achieves the lowest MSE of 0.003 ± 0.002, significantly better than the models proposed by Zhou et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] (0.008 ± 0.003) and Cui [<xref ref-type="bibr" rid="ref_14">14</xref>] (0.006 ± 0.001), confirming its capability to minimize pixel-level differences. Lastly, the BRISQUE metric, which evaluates perceptual image quality, shows that the proposed model scores 21.8 ± 2.1, compared to 30.3 ± 2.8 for the model proposed by Zhou et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] and 26.5 ± 1.9 for the model proposed by Cui [<xref ref-type="bibr" rid="ref_14">14</xref>], indicating better overall visual quality and fewer distortions.</p><p>The execution time of an image restoration model is a critical factor in its practical applicability, especially in real-time or high-throughput environments. The proposed model demonstrates superior computational efficiency, achieving an average execution time of 1.23 seconds, significantly outperforming the models proposed by Zhou et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] (3.47 seconds) and Cui [<xref ref-type="bibr" rid="ref_14">14</xref>] (2.91 seconds). This efficiency is attributed to the optimized feature extraction process, enhanced non-linear refinement techniques, and the integration of multi-scale Gaussian corrections, which reduce redundant computations while preserving image quality. <xref ref-type="fig" rid="fig_4">Figure 4</xref> shows the comparison of the proposed model with competing models based on image quality metrics.</p><p>In summary, the results in <xref ref-type="table" rid="table_2">Table 2</xref> clearly establish the robustness and effectiveness of the proposed model in enhancing image quality, preserving structural integrity, and minimizing perceptual and computational errors compared to the competing models.</p><p>The proposed FEGAO-based model is designed to achieve a balance between computational efficiency and restoration quality, making it suitable for real-time applications. Unlike deep-learning-based models, such as those proposed by Zhou et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] and Cui [<xref ref-type="bibr" rid="ref_14">14</xref>], which require extensive training and high computational resources, the proposed approach leverages mathematical aggregation techniques, specifically the Einstein t-norm and t-conorm, to refine pixel values efficiently. This ensures faster processing while maintaining superior image quality. Additionally, the integration of multi-scale Gaussian corrections and layered enhancement further improves restoration performance without introducing significant computational overhead. The experimental results demonstrate that the model performs effectively with minimal latency, making it applicable in real-time defect detection, industrial inspection, and medical imaging. Future work will focus on further optimizing the framework through parallel processing and Graphics Processing Unit (GPU) acceleration to enhance its applicability in dynamic imaging environments.</p><p>Although the proposed FEGAO-based model has shown strong performance in defective image restoration, its effectiveness in more complex scenarios with severe noise or extreme blur remains a challenge. The study primarily evaluates images with Gaussian blur and low noise levels, but highly irregular distortions or structured noise may reduce its performance. The model’s reliance on Gaussian-based corrections may not be optimal for non-uniform degradation patterns, such as motion blur with complex trajectories. Additionally, while the layered enhancement framework preserves structural details, further adaptation may be needed for highly heterogeneous defects. Future work could explore adaptive deep-learning refinements and non-Gaussian noise models to enhance robustness against unpredictable distortions in real-world defective images.</p>
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>This study introduced an innovative framework for defective image processing through the FEGAO, providing a transformative approach to handling uncertainty and imprecision in defect detection. The model integrates non-linear pixel coordinate projection, resolution refinement, multi-scale Gaussian correction, and layered enhancement, significantly improving upon traditional restoration techniques. The Einstein t-norm and t-conorm enable precise pixel-level calculations, robust feature extraction, and smooth regional intensity transitions, ensuring superior accuracy and enhanced defect representation.</p><p>Compared to advanced deep learning-based restoration models and hybrid optimization techniques, the proposed FEGAO-based approach demonstrates computational efficiency, lower memory usage, and reduced execution time while maintaining high image restoration quality. The quantitative analysis, including PSNR, SSIM, and BRISQUE scores, validates the superior reconstruction quality and perceptual clarity of the proposed approach. Additionally, the higher MOS scores indicate strong user preference and practical reliability in real-world defect detection applications.</p><p>Looking ahead, future research should focus on adapting the algorithm for emerging imaging technologies, such as high-resolution hyperspectral imaging and real-time defect analysis. Additionally, integrating machine learning techniques could enhance parameter optimization and adaptive feature learning, reducing the need for manual tuning and improving generalization across diverse applications.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The author declares no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>190-202</page-range>
          <issue>3</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hussain</surname>
              <given-names>Ibrar</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/mits030305</pub-id>
          <article-title>An adaptive multi-stage fuzzy logic framework for accurate detection and structural analysis of road cracks</article-title>
          <source>Mechatron. Intell Transp. Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Becerikli</surname>
              <given-names>Yasar</given-names>
            </name>
            <name>
              <surname>Karan</surname>
              <given-names>Tayfun M.</given-names>
            </name>
          </person-group>
          <article-title>A new fuzzy approach for edge detection</article-title>
          <source>Computational Intelligence and Bioinspired Systems, Berlin, Heidelberg</source>
          <year>2005</year>
          <volume>3512</volume>
          <page-range>943-951</page-range>
          <pub-id pub-id-type="doi">10.1007/11494669_116</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>El-Khamy</surname>
              <given-names>S. E.</given-names>
            </name>
            <name>
              <surname>Lotfy</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>El-Yamany</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>A modified fuzzy Sobel edge detector</article-title>
          <source>Proceedings of the Seventeenth National Radio Science Conference, Minufiya, Egypt</source>
          <year>2000</year>
          <page-range>C32/1--C32/9</page-range>
          <pub-id pub-id-type="doi">10.1109/NRSC.2000.838961</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <page-range>1113-1136</page-range>
          <issue>5</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hussain</surname>
              <given-names>Ibrar</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>Haider</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>Muhammad Shahkar</given-names>
            </name>
            <name>
              <surname>Niu</surname>
              <given-names>Sijie</given-names>
            </name>
            <name>
              <surname>Rada</surname>
              <given-names>Lavdie</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3934/ipi.2022014</pub-id>
          <article-title>Robust region-based active contour models via local statistical similarity and local similarity factor for intensity inhomogeneity and high noise image segmentation</article-title>
          <source>Inverse Probl. Imag.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Kumawat</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Panda</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Feature extraction and matching of river dam images in Odisha using a novel feature detector</article-title>
          <source>Computational Intelligence in Pattern Recognition, Singapore</source>
          <year>2020</year>
          <page-range>703-713</page-range>
          <pub-id pub-id-type="doi">10.1007/978-981-13-9042-5_61</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>116-126</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hussain</surname>
              <given-names>I</given-names>
            </name>
            <name>
              <surname>Muhammad</surname>
              <given-names>J</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>R</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/ijkis010204</pub-id>
          <article-title>Enhanced global image segmentation: Addressing pixel inhomogeneity and noise with average convolution and entropy-based local factor</article-title>
          <source>Int J. Knowl. Innov Stud.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>1744</volume>
          <page-range>032123</page-range>
          <issue>3</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhou</surname>
              <given-names>Jing Min</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1088/1742-6596/1744/3/032123</pub-id>
          <article-title>On the innovative research of computer multimedia technology in the basic course of ideological and political education in universities</article-title>
          <source>J. Phys. Conf. Ser.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>1926-1939</page-range>
          <issue>9</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Premnath</surname>
              <given-names>S. P.</given-names>
            </name>
            <name>
              <surname>Renjit</surname>
              <given-names>J. Arokia</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1049/ipr2.12162</pub-id>
          <article-title>Image restoration model using Jaya-Bat optimization-enabled noise prediction map</article-title>
          <source>IET Image Proc.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>63</volume>
          <page-range>159-165</page-range>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Funama</surname>
              <given-names>Yoshinori</given-names>
            </name>
            <name>
              <surname>Oda</surname>
              <given-names>Seitaro</given-names>
            </name>
            <name>
              <surname>Kidoh</surname>
              <given-names>Masafumi</given-names>
            </name>
            <name>
              <surname>Sakabe</surname>
              <given-names>Daisuke</given-names>
            </name>
            <name>
              <surname>Nakaura</surname>
              <given-names>Takeshi</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1177/0284185120986938</pub-id>
          <article-title>Effect of image quality on myocardial extracellular volume quantification using cardiac computed tomography: A phantom study</article-title>
          <source>Acta Radiol.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>1916</volume>
          <page-range>012165</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Prabha</surname>
              <given-names>P Anantha</given-names>
            </name>
            <name>
              <surname>Bharathwaj</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Dinesh</surname>
              <given-names>K</given-names>
            </name>
            <name>
              <surname>Prashath</surname>
              <given-names>G Hari</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1088/1742-6596/1916/1/012165</pub-id>
          <article-title>Defect detection of industrial products using image segmentation and saliency</article-title>
          <source>J. Phys. Conf. Ser.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>29</volume>
          <page-range>27237-27253</page-range>
          <issue>17</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhou</surname>
              <given-names>Hao</given-names>
            </name>
            <name>
              <surname>Feng</surname>
              <given-names>Hua Jun</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Wen Bin</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Zhi Hai</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Qi</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Yue Ting</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1364/OE.432544</pub-id>
          <article-title>Deep denoiser prior based deep analytic network for lensless image restoration</article-title>
          <source>Opt. Express</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>30</volume>
          <page-range>134-149</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ghulyani</surname>
              <given-names>Manu</given-names>
            </name>
            <name>
              <surname>Arigovindan</surname>
              <given-names>Muthuvel</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TIP.2020.3032036</pub-id>
          <article-title>Fast roughness minimizing image restoration under mixed Poisson–Gaussian noise</article-title>
          <source>IEEE Trans. Image Process.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>41</volume>
          <page-range>2980-2986</page-range>
          <issue>12</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liang</surname>
              <given-names>Xiao Ping</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>Zhen Jun</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Chang Hong</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.11999/JEIT190261</pub-id>
          <article-title>BP neural network fuzzy image restoration basedon brain storming optimization algorithm</article-title>
          <source>J. Electron. &amp; Inf. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>34</volume>
          <page-range>1-14</page-range>
          <issue>4</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cui</surname>
              <given-names>Zhan Peng</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.53106/199115992023083404001</pub-id>
          <article-title>Restoration and enhancement of fuzzy defect image based on neural network</article-title>
          <source>J. Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>100-110</page-range>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Meng</surname>
              <given-names>Jun</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Yuan Yuan</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>Hua Hua</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>You</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.37965/jait.2022.0110</pub-id>
          <article-title>Single-image dehazing based on two-stream convolutional neural network</article-title>
          <source>J. Artif. Intell. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>56</volume>
          <page-range>2763-2771</page-range>
          <issue>4</issue>
          <year>2010</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yun</surname>
              <given-names>Se Hwan</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>Jin Heon</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>Suki</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TCE.2010.5681167</pub-id>
          <article-title>Image enhancement using a fusion framework of histogram equalization and Laplacian pyramid</article-title>
          <source>IEEE Trans. Consum. Electron.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>578</volume>
          <page-range>435-456</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ma</surname>
              <given-names>Wen Ping</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>Hao Xiang</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Hao</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Ya Ting</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Long Wei</given-names>
            </name>
            <name>
              <surname>Jiao</surname>
              <given-names>Li Cheng</given-names>
            </name>
            <name>
              <surname>Hou</surname>
              <given-names>Biao</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ins.2021.07.043</pub-id>
          <article-title>Hyperspectral image classification based on spatial and spectral kernels generation network</article-title>
          <source>Inf. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>66</volume>
          <page-range>101159</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kwon</surname>
              <given-names>Hongseok</given-names>
            </name>
            <name>
              <surname>Go</surname>
              <given-names>Byung Hyun</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>Juhong</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>Wonkee</given-names>
            </name>
            <name>
              <surname>Jeong</surname>
              <given-names>Yewon</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>Jong Hyeok</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.csl.2020.101159</pub-id>
          <article-title>Gated dynamic convolutions with deep layer fusion for abstractive document summarization</article-title>
          <source>Comput. Speech Lang.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Shi</surname>
              <given-names>Min Hong</given-names>
            </name>
          </person-group>
          <article-title>Fuzzy image restoration algorithm based on machine vision technology</article-title>
          <source>2023 International Conference on Evolutionary Algorithms and Soft Computing Techniques (EASCT), Bengaluru, India</source>
          <year>2023</year>
          <page-range>1-5</page-range>
          <pub-id pub-id-type="doi">10.1109/EASCT59475.2023.10392378</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>8-18</page-range>
          <issue>3</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sara</surname>
              <given-names>Umme</given-names>
            </name>
            <name>
              <surname>Akter</surname>
              <given-names>Morium</given-names>
            </name>
            <name>
              <surname>Uddin</surname>
              <given-names>Mohammad Shorif</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.4236/jcc.2019.73002</pub-id>
          <article-title>Image quality assessment through FSIM, SSIM, MSE and PSNR—A comparative study</article-title>
          <source>Comput. Commun.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>