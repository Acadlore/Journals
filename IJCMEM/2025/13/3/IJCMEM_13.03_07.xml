<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IJCMEM</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>International Journal of Computational Methods and Experimental Measurements</journal-title>
        <abbrev-journal-title abbrev-type="issn">Int. J. Comput. Methods Exp. Meas.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IJCMEM</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2046-0554</issn>
      <issn publication-format="print">2046-0546</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-hc98ViWqoJYGsCrP6xE4XYjlQ3HG7wG8</article-id>
      <article-id pub-id-type="doi">10.56578/ijcmem130307</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>DeepHarvestNet: Depth-Visual Fusion Network for Robust Apple Detection in Complex Orchard Environments</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9123-3296</contrib-id>
          <name>
            <surname>Uriti</surname>
            <given-names>Archana</given-names>
          </name>
          <email>auriti2@gitam.in</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6397-5951</contrib-id>
          <name>
            <surname>Pothabathula</surname>
            <given-names>Naga Jyothi</given-names>
          </name>
          <email/>
        </contrib>
        <aff id="aff_1">Department of Computer Science and Engineering, GSCSE, GITAM (Deemed to be University), Visakhapatnam 530045, India</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>14</day>
        <month>10</month>
        <year>2025</year>
      </pub-date>
      <volume>13</volume>
      <issue>3</issue>
      <fpage>554</fpage>
      <lpage>575</lpage>
      <page-range>554-575</page-range>
      <history>
        <date date-type="received">
          <day>13</day>
          <month>07</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>27</day>
          <month>08</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Accurate fruit recognition in natural orchard environments remains a major challenge due to heavy occlusion, illumination variation, and dense clustering. Conventional object detectors, even those incorporating attention mechanisms such as YOLOv7 with attribute attention, often fail to preserve fine spatial details and lose robustness under complex visual conditions. To overcome these limitations, this study proposes DeepHarvestNet, a YOLOv8-based hybrid network that jointly learns depth and visual representations for precise apple detection and localization. The architecture integrates three key modules: (1) Efficient Bidirectional Cross-Attention (EBCA) for handling overlapping fruits and contextual dependencies; (2) Focal Modulation (FM) for enhancing visible apple regions under partial occlusion; and (3) KernelWarehouse Convolution (KWConv) for extracting scale-aware features across varying fruit sizes. In addition, a transformer-based AdaBins depth estimation module enables pixel-wise depth inference, effectively separating foreground fruits from the background to support accurate 3D positioning. Experimental results on a drone-captured orchard dataset demonstrate that DeepHarvestNet achieves a precision of 0.94, recall of 0.95, and F1-score of 0.95—surpassing the enhanced YOLOv7 baseline. The integration of depth cues significantly improves detection reliability and facilitates depth-aware decision-making, underscoring the potential of DeepHarvestNet as a foundation for intelligent and autonomous harvesting systems in precision agriculture.</p></abstract>
      <kwd-group>
        <kwd>Yolov7</kwd>
        <kwd>Attribute attention mechanism</kwd>
        <kwd>Adaptive pooling</kwd>
        <kwd>Yolov8</kwd>
        <kwd>Efficient bidirectional cross- attention</kwd>
        <kwd>Focal modulation</kwd>
        <kwd>KernelWarehouse Convolution</kwd>
        <kwd>Adabins</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="2"/>
        <fig-count count="11"/>
        <table-count count="2"/>
        <ref-count count="44"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>The world’s demand for fresh fruits is increasing by the day, putting tremendous pressure on contemporary farming to boost productivity, efficiency, and minimize dependence on labor. As skilled labor increasingly becomes scarce and the cost of manual harvesting increases, most regions of fruit production are aggressively seeking automated solutions. In a variety of fruits, apples rank among the most economically important horticultural crops globally, being extensively grown in a wide range of climatic belts. Apple picking, relying on traditional manual labor, is labor-intensive and time-consuming as a result of the fragile nature of the fruit, inconsistent orchard design, and selective picking on account of ripeness and quality. Apple harvesting automation has the great potential to revolutionize orchard management by increasing consistency in harvesting, minimizing labor dependence, and facilitating constant tracking of yield and quality of fruit. In this regard, artificial intelligence together with computer vision has been a main enabler for the potential to detect, localize, and count apples automatically from sensor data using robotic and drone platforms published.</p><p>Bargoti et al. [<xref ref-type="bibr" rid="ref_1">1</xref>] developed a deep learning model by using Faster R-CNN to identify the fruits in high-resolution with an incorporation of data augmentation and it demonstrate the deep fruit detection feasibility for yield estimation. However it struggles with the occlusion, poor localization and lacks depth information. Sa et al. [<xref ref-type="bibr" rid="ref_2">2</xref>] developed a fruit detection system which is named as DeepFruits by using Faster R-CNN and it improves detection accuracy in detecting multiple fruit types. But it has limitation of handling heavy occlusion and lightning variations. Lin et al. [<xref ref-type="bibr" rid="ref_3">3</xref>] proposed a U-Net based segmentation model to identify the cucumber flowers and fruits which enables the automated monitoring plant growth. This model is not tested under occlusion, overlapping conditions and does not perform precise fruit localization.</p><p>Even with advancements in automation, precise detection of apples in natural orchard settings is a very challenging task. A few practical issues complicate the process. Occlusion is quite common, as apples are usually occluded behind leaves, branches, or other apples. Overlapping fruits make it difficult to identify instances uniquely, and light changes resulting from sunlight, shadows, and weather conditions further impair detection performance. These challenges are shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>. These issues dramatically impact the faithfulness of automated detection systems, particularly when apples are tightly packed or at multiple distances and orientations within the canopy.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>(a) Illumination variation, (b) Occluded by leaves or branches, (c) Overlapping of apples</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_-iMxR0BRApy0vw9p.jpeg"/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_7CUSDY6obafmkcUh.jpeg"/>
        </fig>
      
      <p>To handle and tackle the agricultural detection problems, deep learning and computer vision usage have been increased in recent years. In which, Convolutional neural networks facilitated YOLO family have been accepted for balancing the real-time execution in fruit detection tasks. Still the performance lacks in handling the occlusion, overlapping, fruit localization and depth of each fruit. This provides a closer look in dealing these limitations with current deep-learning fruit detection models.</p><p>The Initial studies utilized the CNN and Mask-RCNN to perform fruit detection and its quality grading. For example, Chu et al. [<xref ref-type="bibr" rid="ref_4">4</xref>] utilized a Suppression Mask R-CNN framework to detect apples in complex backgrounds. It achieved higher precision than standard Mask-RCNN but still struggles in handling occlusion, accurate localization and requires considerable annotated data. Siricharoen et al. [<xref ref-type="bibr" rid="ref_5">5</xref>] proposed Mask R-CNN framework to localize and grade the maturity of pineapple fruits. It doesnot incorporate depth information that limits the precise localization and may not handle occlusion, overlapping fruits well due to controlled conditions. Janowski et al. [<xref ref-type="bibr" rid="ref_6">6</xref>] compared three methods like YOLO, Viola-Jones and Hough transform in automatic apple fruit detection and its count to estimate apple yield. These methods still challenging in handling occlusion and overlapping fruit. Gongal et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] focused on technologies like Vision-based systems, LiDAR and depth sensors for fruit detection and localization in supporting automated harvesting and yield estimation problems. It gives overview on existing systems struggle with issues like occlusion, lightning conditions and precise localization. The integration of all these is cost effective challenge.</p><p>While these studies having limitations leads to increase the development of YOLO models which gives high speed with more accuracy in handling fruit detection in an orchard. The YOLO family is demonstrating the improvement in each version for fruit detection tasks and is suitable for this domain. For instance, Xue et al. [<xref ref-type="bibr" rid="ref_8">8</xref>] introduced improvised version of YOLOv2 and is termed as Tiny-YOLO-Dense by integrating into Tiny-YOLO architecture. This work improves the feature extraction in detection of immature mango fruit in an orchard environment. It addresses the challenges in fruit detection but not extensively discuss these challenges and lacking in depth information. Tian et al. [<xref ref-type="bibr" rid="ref_9">9</xref>] proposed an enhanced YOLOv3 model termed as YOLOv3-Dense to detect apples at various growth stages. It demonstrates superior performance than original YOLOv3 and Faster R-CNN in handling illumination and complex backgrounds. However the model struggles with detecting small or occluded apples and lacking depth information. Parico and Ahamed [<xref ref-type="bibr" rid="ref_10">10</xref>] proposed YOLOv4 framework by combining YOLOv4, YOLOv4-CSP and YOLOv4-tiny for real-time pear detection and uses DeepSORT for tracking in video. But it has limitations in handling occlusion and varying illumination. Some pears are missed when partially occluded that leads to errors in counting. Wang et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] proposed a system that is based on YOLOv5 to recognize real time apple stem and calyxes for automated apple fruit harvest. The detection sometimes fails to handle partially occluded by stem and overlapped or shadowed. It used only 2D imaging so lacks in depth information. Ma et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] proposed the enhanced YOLOv7-tiny for detecting apple fruit and counting small apples in complex background and weather. This method shows less performance when apples are very similar to background leading to missed detections. Also it remains challenging when more occlusions or overlapping apples and no depth information available. Liu et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] proposed YOLOv8-MSP-PD which is a lightweight detection model that modified the backbone with MobileNetV4, adds a spatial pyramid pooling for multi-scale feature fusion and applied detection on Jinxiu Malus fruit. This model faces challenges in very abrupt light conditions and heavy occlusion. The total works on YOLO family of models gives a clear focus on how they are applied and suitable for fruit detection.</p><p>With this progressive improvements in YOLO models, inspired to research into crop related detection and analysis tasks. Li et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] proposed Lemon-YOLO model that is mainly for lemons detection by modifying the YOLOv3 by a replacement of DarkNet-53 backbone with SE ResNet module. Still it struggles with very small or severely occluded lemons and lacks depth information. Lin et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] developed AG-YOLO model using a NextViT backbone together with Global Context Fusion Module to improve citrus fruit detection. But still struggles in heavily occluded and complex backgrounds and no depth information. Yu et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] proposed an improved YOLOv5 model for detecting citrus fruit and introduced Receptive Field Convolutions with Full 3D weights to improve feature extraction under occlusion, overlap and variable natural lighting. However it struggle with many fruits missed and does not incorporate depth information. Uriti and Pothabathula [<xref ref-type="bibr" rid="ref_17">17</xref>] developed the enhanced YOLOv5 model with an integration of attribute attention and adaptive pooling that aims to reduce losses and improve performance under challenging conditions for detecting apple fruit. But this model stills struggles in handling occlusion and lacks depth information that limits the precise localization. Zhao et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] proposed the improved YOLOv5 by incorporating the Cross Spatial Partial and Spatial Pyramid Pooling module for apple fruit detection. It shows the degraded performance when apples are densely overlapped and poor lighting and lacks depth information. Liu et al. [<xref ref-type="bibr" rid="ref_19">19</xref>] proposed YOLOv5-ACS which enhances the YOLOv5 model on apple fruit detection. The detection performance drops under severe overlap and occlusion and depth information is not addressed.</p><p>Other researchers expanded YOLO to different fruits, including pomegranate detection by Zhao et al. [<xref ref-type="bibr" rid="ref_20">20</xref>], olive fruit detection by Osco-Mamani et al. [<xref ref-type="bibr" rid="ref_21">21</xref>], and apple counting using CNNs by Hani et al. [<xref ref-type="bibr" rid="ref_22">22</xref>]. Fischer et al. [<xref ref-type="bibr" rid="ref_23">23</xref>] further extended fruit detection into tracking using quasi-dense learning. Although these studies improved efficiency, they remained challenged by occlusion, overlapping fruits, scale variation, and illumination changes, often leading to false positives or missed detections. Lightweight models also emerged, such as EfficientDet-Lite2 applied to apples by Agung Enriko et al. [<xref ref-type="bibr" rid="ref_24">24</xref>] and YOLOv7 for fruit health monitoring by Oei et al. [<xref ref-type="bibr" rid="ref_25">25</xref>], which enhanced portability but frequently sacrificed accuracy in complex orchard environments. More recent works, such as RLK-YOLOv8 by He et al. [<xref ref-type="bibr" rid="ref_26">26</xref>] for strawberries and YO-AFD by Wang et al. [<xref ref-type="bibr" rid="ref_27">27</xref>] for apple flower detection, introduced large kernels, multi-stage fusion, and attention mechanisms, achieving notable gains across growth stages. However, even these state-of-the-art YOLOv7 and YOLOv8 models do not explicitly address the combined challenges of partial visibility,overlapping fruits in dense clusters, multiscale variability, and—most critically—the need for three-dimensional depth estimation to support precise fruit localization.</p><p>From this, it is clear that while deep learning and YOLO-based detectors have advanced fruit detection across diverse crops, significant research gaps remain in robust apple detection under orchard conditions, particularly in handling occlusion, overlapping fruits, variable scales, illumination variation, and depth-aware localization. To bridge these gaps, the novelty of this study lies in the design of an improved YOLOv8-based detection framework that leverages context-aware spatial feature learning for partially visible fruits, incorporates overlap-aware strategies to separate densely clustered apples, integrates multiscale feature extraction to improve detection across fruit sizes and growth stages, and embeds depth estimation into the pipeline for accurate three-dimensional localization. This combination of improvements directly addresses the limitations of previous approaches and provides a more reliable and practical solution for automated apple detection in real orchard environments.</p><p>Though YOLOv7 combined with attribute attention and adaptive pooling showed improved detection under mod- erate conditions, it still suffered in detecting small or distant apples and failed under heavy occlusion. Additionally, lacking depth perception limited its effectiveness in robotic harvesting, where accurate spatial localization is essential to avoid collisions and false detections.</p><p>To resolve these issues, DeepHarvestNet enhances the YOLOv8 backbone with four specialized modules. The EBCA module enables strong bidirectional information flow across feature scales, improving the network’s ability to isolate overlapping apples. FM replaces standard attention by dynamically focusing on visible fruit regions, significantly improving performance under occlusion. KWConv learns kernel sizes for maintaining apple detail at multiple scales, particularly those small or partially occluded. AdaBins incorporates depth estimation by learning adaptive bins for pixel-wise accuracy, allowing for robust foreground fruit separation from intricately textured backgrounds. Combined, these enhancements render DeepHarvestNet an end-to-end solution for accurate apple detection and depth-sensitive localization under difficult orchard environments.</p><p>The main objective of this work is summarized as below:</p><p><p>Design a novel model that improves the apple fruit recognition under partial visible and occluded by leaves, branches.</p><p>Enhance the novel model detection performance, incorporate feature learning strategy to handle overlapping fruits. Hence reduce the false positives and missed detections.</p><p>Develop fruit localization model with an integration of multi-scale feature extraction and estimating the depth to filter out the background fruits and better localization.</p></p><p>The main goal of this study is a three stage enhanced process novel model. The first stage is mainly focusing on apple fruit detection under occluded by leaves or branches. Then the second stage improvise the model to handle overlapping of a fruit to reduce false positives and missed detections. The final stage is to focus on mutli-scale features and fruit localization. It provides the depth information of each fruit for handling foreground apples. This overall work is carried out by incorporating four components to handle all these challenges. Each challenge is taken care by individual new component that is added in the backbone and neck of YOLOv8 model.</p><p>The added new components in the proposed model is mainly to address the challenges in apple fruit detection for better accuracy in detection and providing depth information of each apple fruit. Here the EBCA is added in the backbone of model to solve the overlapping apples by providing cross regional interactions to identify features from different regions. This will lead to distinguish overlapping fruits by separate adjacent fruit boundaries. The FM module introduced by Yang et al. [<xref ref-type="bibr" rid="ref_28">28</xref>] is to extract the important features by suppressing irrelevant or occluded area. It improves the context-aware that highlights the useful information and thus provide the robust occlusion handling. Li and Yao [<xref ref-type="bibr" rid="ref_29">29</xref>] proposed KWConv module, that aggregates the kernels of different sizes in cpaturing the multi-scale information. This module helps in precise fruit localization and has ability to share apple features across layers in handling scale variations. The AdaBins which is a depth estimation approach proposed by Bhat et al. [<xref ref-type="bibr" rid="ref_30">30</xref>] that predicts the depth by adaptively partioning the bin widths. This method has Mini-ViT transformer module to produce pixel-wise depth of each fruit. The depth information helps in handling the precise localization of fruit and can differentiate the foreground and background fruits to avoid duplicate counts. Adding all these modules in the proposed DeepHarvestNet model will enhance the detection in handling fruit challenges in an orchard and also focus on providing 3D information in depth handling.</p><p>The introduced DeepHarvestNet workflow architecture, shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>, builds on YOLOv8 with architectural improvements and depth estimation to facilitate secure apple detection. The workflow starts by preprocessing orchard images taken by drones—scaling all frames to a constant resolution and performing augmentations such as brightness adjustments and synthetic occlusions to improve model resilience against real-world lighting and occlusion variations. For extracting features, the backbone incorporates Efficient Bidirectional Cross-Attention and Focal Modulation.</p><p>EBCA facilitates multi-scale spatial feature interaction across bottom-up and top-down routes to assist the model in disambiguating overlapping apples, while FM dynamically highlights visible fruit regions in occlusion environments.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Workflow of DeepHarvestNet</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_pa2zMe28QefgSaQ3.jpeg"/>
        </fig>
      
      <p>KernelWarehouse Convolution in the neck modifies kernel sizes according to object size, retaining small or far-away apple details. The detection head subsequently generates bounding boxes, class predictions, and confidence scores. In parallel, a transformer-based AdaBins module predicts pixel-wise depth by discretizing depth into adaptive bins, supporting accurate 3D localization.  Bochkovsky et al. [<xref ref-type="bibr" rid="ref_31">31</xref>] explore the use of Vision Transformers (ViTs) for detection tasks to demonstrate the capability is more than traditional CNNs. This method helps in improving the accuracy in pixel level depth estimation.</p><p>This modular pipeline solves important detection problems: EBCA disentangles grouped apples, FM enhances occlusion robustness, KWConv enriches scale-sensitive information, and AdaBins provides depth-conscious local- ization—rendering DeepHarvestNet very well-suited for robotic harvesting in complicated orchard environments.</p>
    </sec>
    <sec sec-type="">
      <title>2. Literature review on yolo-based fruit detection and depth estimation</title>
      <p>The detection of fruit in orchards has evolved significantly since deep learning has advanced, particularly with the YOLO family of models. The introduction of YOLOv4 proposed by Bochkovskiy et al. [<xref ref-type="bibr" rid="ref_32">32</xref>] brought notable improvements in real-time object detection by integrating CSPDarknet53 and PANet, allowing efficient feature reuse and enhanced localization in agricultural images. Building on this, YOLOv5 which is presented by Khanam and Hussain [<xref ref-type="bibr" rid="ref_33">33</xref>] optimized training speed and deployment efficiency with its auto-learning bounding box anchor mechanism and model scaling capabilities, making it widely used for fruit recognition. The further development of YOLOv7 by Wang et al. [<xref ref-type="bibr" rid="ref_34">34</xref>] improved inference speed without sacrificing accuracy, enabling high-performance detection in dense and occluded fruit clusters. More recently, Varghese and M [<xref ref-type="bibr" rid="ref_35">35</xref>] introduced YOLOv8 has shown state-of-the-art accuracy in fruit detection tasks due to its decoupled head architecture and streamlined post- processing pipeline. While CNN-based detectors have improved detection accuracy, they often struggle to capture long-range dependencies, especially in cluttered orchard environments. To address this, transformer-based models such as the Pyramid Vision Transformer v2 (PVTv2) have been proposed by Wang et al. [<xref ref-type="bibr" rid="ref_36">36</xref>]. PVTv2 introduces improved spatial reduction attention to enhance multi-scale feature representation, making it suitable for complex object detection tasks. Similarly, Tu et al.  [<xref ref-type="bibr" rid="ref_37">37</xref>] introduced MaxViT model incorporates both local and global attention using multi-axis attention blocks, which enables better occlusion handling and scale adaptation in dense scenes like fruit orchards.</p><p>Incorporating depth information has become crucial for distinguishing overlapping fruits and enhancing local- ization precision. The AdaBins framework introduced adaptive bin widths for monocular depth estimation, which significantly improved depth prediction in cluttered scenes. In parallel, Godard et al. [<xref ref-type="bibr" rid="ref_38">38</xref>] developed Monodepth2 applied self-supervised learning to monocular RGB images, generating high-quality disparity maps that support 3D understanding in open-field agricultural scenes. These methods enable robust perception even under variable illumination and partially visible targets. Model optimization is also essential for real-time agricultural applications where resources are limited. Ding et al. [<xref ref-type="bibr" rid="ref_39">39</xref>] proposed ResRep pruning strategy introduced a method to decouple the learning and pruning phases, enabling lossless channel pruning and making deep models deployable on edge devices without compromising accuracy. Such efficiency gains are crucial when deploying complex architectures like DeepHarvestNet on UAVs or mobile robots in the field.</p><p>Despite these technological advancements, existing systems still face limitations in detecting small, distant, or occluded fruits. Most approaches overfit to certain datasets or are not able to generalize across development stages and settings. This points to the requirement for an end-to-end solution that incorporates spatial attention, depth estimation, and multi-scale feature aggregation to enhance robustness and spatial accuracy. Towards this goal, the proposed DeepHarvestNet architecture merges Efficient Bidirectional Cross-Attention, Focal Modulation, and KernelWarehouse Convolution with AdaBins-based depth estimation to fully tackle issues in fruit detection and 3D localization and the fusion of these methods leads to improved YOLOv8 model proposed by Song et al. [<xref ref-type="bibr" rid="ref_40">40</xref>] for accurate detection and localize the tomato stems. Besides that, Liu et al. [<xref ref-type="bibr" rid="ref_41">41</xref>] introduced ConvNeXt, a lightweight ConvNet architecture that pointed out hybrid architectures fusing convolutional inductive bias and transformer pliability are both improved in terms of representation learning as well as feasibility of deployment on edge hardware. Most recently, Uriti and Pothabathula [<xref ref-type="bibr" rid="ref_42">42</xref>] presented a comprehensive review on evaluating the various object detection methods that is used for fruit detection. They highlighted the progress made by various deep learning models and comparison shown in improvement but still noted the challenges remains such as occlusion, overlapping, variable lighting, fruit localization and depth estimation in an orchard. The YOLOv5 model has been also extended towards stronger multi-scale fruit detection via integrating multi-branch heads with better attention blocks and was shown useful for occluded apples among heavy foliage. Yet, under hard occlusion, even attention- based CNNs do not fare well due to a semantic understanding deficiency. To address this, transformer-based detectors with spatial attention and cross-layer aggregation have been put forth, but bring along increased latency.</p><p>In short, fruit detection, localization, and depth estimation technology in agriculture has progressed considerably through the combination of deep learning models, attention mechanisms, and novel architectures. Researchers have overcome major issues like occlusion, changing illumination conditions, and overlapping objects through the use of YOLO-based detection architectures, multi-scale feature extraction algorithms, and monocular depth estimation techniques such as AdaBins, which together boost fruit detection system accuracy and efficiency. The introduction of attention mechanisms and feature fusion methods has further advanced the models’ ability to focus on certain features, thus improving detection performance in extremely complicated orchard scenarios. Building on top of these advancements, this paper addresses the challenge of enhancing fruit recognition systems by incorporating even more advanced attention modules and state-of-the-art depth estimation approaches, with the aim of improving spatial precision and adaptability for real-time crop management systems. Through the application of depth-aware recognition combined with advanced localization methods, the system proposed here aims to mitigate longstanding problems of object overlap and visual blocking ubiquitous in orchard-based fruit detection applications. In the future, ongoing research will probably focus on further enhancing such frameworks to gain stable real-time operation under varied agricultural environments. This survey of recent methods forms the basis for designing the envisioned DeepHarvestNet architecture, which combines state-of-the-art deep learning approaches, attention-based feature augmentation, and depth estimation to address orchard fruit detection complexities directly. Through the solution of detection, localization, and depth estimation simultaneously, the envisioned framework aims to provide more accurate, scalable, and efficient solutions that shape the future of smart agriculture.</p>
    </sec>
    <sec sec-type="">
      <title>3. Materials and methods</title>
      <p>This section discusses the methodology used to design the proposed DeepHarvestNet framework for accurate apple fruit detection and depth-aware 3D localization in orchard settings. The process is divided into three major phases: (i) Data Acquisition, (ii) Data Augmentation and Preprocessing, and (iii) Model Development, where the DeepHarvestNet architecture utilizes several state-of-the-art modules to enhance detection accuracy, occlusion robustness, and accurate 3D spatial localization.</p>
      
        <sec>
          
            <title>3.1. Data acquisition</title>
          
          <p>The accuracy and dependability of the model are greatly dependent on the diversity and quality of data that is gathered. Joora Drones Pvt. Ltd., being a service organization based in Visakhapatnam district, Andhra Pradesh state, and incubated in an innovation center, conducted the data collection using the DJI Mavic 3 drone, a high-tech drone that is characterized by its lightness, improved flight stability, and high-resolution video capacity. This drone comes equipped with a Hasselblad L2D-20c camera that has a 4/3 CMOS sensor, 24 mm equivalent focal length, and 84° field of view. The aperture varies from f/2.8 to f/11 so that it is flexible in all types of lighting conditions, and the camera can record up to 5.1K video resolution along with 20MP photo capture. The drone was operated at altitudes ranging from 20 to 60 meters over chosen agricultural fields in India, recording canopy-level and full-plot images under ambient light conditions. Videos were recorded at varying angles and weather conditions ranging from clear to partly cloudy skies, with no artificial lighting. The three-axis gimbal of the drone, GPS hover, and vision sensors provided stability and accuracy, which resulted in reliable and distortion-free footage.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Data augmentation</title>
          
          <p>In this study, various data augmentation techniques were applied to the original apple orchard images to improve the robustness of the detection model under diverse real-world conditions. <xref ref-type="fig" rid="fig_3">Figure 3</xref> illustrates the representative augmentations applied to the original dataset, which consists of drone-captured apple orchard images obtained from publicly available sources. These augmentations include brightness variation, rotation, motion blur, mirroring, and Gaussian noise—each designed to simulate challenges typically encountered in aerial agricultural imaging.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Data augmentation process</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_a1uPALQvmewAFVYz.jpeg"/>
            </fig>
          
          <p><p>Brightness Variation: To simulate lighting fluctuations, the RGB images were first converted into HSV color space using the ‘rgb2hsv’ function. The brightness component (V channel) was scaled using different coefficients. For brightness enhancement, the formulas H+S+1.2V and H+S+1.6V were used, while for brightness reduction, H+S+0.6V and H+S+0.8V were applied. The modified images were converted back to RGB using the ‘hsv2rgb’ function. These changes help the model learn to detect apples under both high- and low-illumination conditions.</p><p style="text-align: justify">Image Rotation: The images were rotated by fixed angles of 900, 1800 and 2700to mimic different camera orientations. This rotation helps the model maintain detection accuracy even when the image capture angles vary.</p><p style="text-align: justify">Mirroring: Horizontal image flipping was performed by mirroring pixel values along the vertical axis. This technique increases dataset diversity and allows the model to generalize symmetric apple features effectively.</p><p style="text-align: justify">Motion Blur: To simulate the blurring effect caused by drone movement during aerial data collection, four motion blur kernels were applied with configurations of (6, 30), (6, 135), (7, 45), and (7, 90)—where the first number represents the blur length and the second the angle in degrees. This prepares the model to detect apples even when motion artifacts are present.</p><p style="text-align: justify">Gaussian Noise Addition: Gaussian noise with a variance of 0.02 and zero mean was added to simulate sensor noise caused by environmental factors such as poor lighting or electronic interference. This makes the product more resistant to deterioration in image quality.</p></p><p style="text-align: justify">By these augmentation techniques, the dataset was profoundly enhanced, and the detection model could learn more generalizable features and function consistently under diverse imaging conditions.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Proposed deepharvestnet architecture for enhanced detection framework and depth estimation</title>
          
          <p>The designed DeepHarvestNet presented in <xref ref-type="fig" rid="fig_4">Figure 4</xref> is a shared model that uses deep learning for object detection and depth estimation of spatial locations. The architecture has a tailored backbone, an improved neck structure, and a dual-head module for depth estimation and object detection. The model can be implemented on high-resolution orchard images of size 640×640 to precisely calculate spatial locations and 3D scene understanding in intricate agricultural environments.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Data augmentation process</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_6wNvsvX2eOH1j-yV.jpeg"/>
            </fig>
          
          <p>The DeepHarvestNet architecture proposed here adds an improved object detection and depth prediction frame- work designed for apple fruit localization under orchard scenarios. As shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>, the architecture comprises four main components: (i) input preprocessing and augmentation, (ii) feature extraction backbone, (iii) multi-scale feature fusion neck, and (iv) hybrid detection and depth estimation head combined with 3D position refinement.</p><p>The input pipeline starts by resizing all raw orchard images to a uniform resolution of 640×640 pixels in order to have uniform input dimensions for training and inference. Augmentation techniques such as random brightness normalization and synthetic occlusion generation are used to make the model robust against typical orchard issues such as inconsistent illumination and partial exposure of fruit.</p><p>In the Backbone, hierarchical visual features are learned by sequential convolutional layers (Conv1 to Conv5). For strengthening feature learning, C2f modules are introduced at various points, followed by two essential attention- based innovations: Efficient Bidirectional Cross-Attention and Focal Modulation. EBCA facilitates long-range bidirectional feature interactions such that the model can differentiate between overlapping apples by relating spatially distant yet semantically relevant regions. Simultaneously, FM substitutes traditional self-attention with dynamic feature suppression of occluded areas and highlighting visible parts of apple fruits, thus enhancing detection robustness against severe occlusions.</p><p>The extracted features are passed into the Neck, where multi-scale feature fusion is performed. Here, feature maps are first upsampled and concatenated, followed by the application of KernelWarehouse Convolution blocks. KWConv adaptively selects convolutional kernels based on object scale, ensuring the preservation of fine-grained details for both small and large apple instances across multiple spatial scales. This allows the model to maintain scale-invariant detection performance even under varying fruit sizes and distances.</p><p>The output from the Neck feeds into the Detection Head, which operates at three spatial scales (small, medium, and large) to localize apple fruits of different sizes. Each scale outputs bounding boxes, class labels, and confidence scores. Simultaneously, depth estimation is performed through a dedicated AdaBins Module. The AdaBins depth estimation pipeline consists of a standard encoder-decoder network augmented with a Mini Vision Transformer (mViT) that captures global context information, followed by a hybrid regression mechanism that discretizes depth predictions into adaptive bins. This enables pixel-wise absolute depth estimation directly from monocular RGB input, producing precise distance information for each detected apple.</p><p>Finally, the detection outputs (X, Y coordinates) and the estimated depth (d) are combined in the Relative Position Processing Module (RPPM). This module fuses positional and depth information to generate complete 3D localization data (X, Y, d), facilitating depth-aware robotic harvesting and accurate yield estimation.</p>
          
            <sec>
              
                <title>3.3.1 Efficient bidirectional cross-attention for overlapping object separation</title>
              
              <p>The Efficient Bidirectional Cross-Attention mechanism is designed to enhance the localization accuracy of apple fruits, particularly in scenarios where multiple apples overlap or appear closely clustered. By modeling bidirectional interaction across spatial features from multiple scales, EBCA allows the network to distinguish between individual apple instances even in complex orchard environments. Zhu et al. [<xref ref-type="bibr" rid="ref_43">43</xref>] proposed vision transformer architecture which is termed as BiFormer which is mainly for efficient feature representation by capturing local and global dependencies with the help of bi-level attention mechanism. The architecture is shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>.<br><br></p>
              
                <fig id="fig_5">
                  <label>Figure 5</label>
                  <caption>
                    <title>Efficient bidirectional cross-attention architecture</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_pPJw4W8srjyRfsn-.png"/>
                </fig>
              
              <p>The EBCA module receives multiscale feature maps denoted as, denoted as <inline-formula>
  <mml:math id="mwsso35il8">
    <mml:msub>
      <mml:mi>F</mml:mi>
      <mml:mn>3</mml:mn>
    </mml:msub>
    <mml:mo>∈</mml:mo>
    <mml:msup>
      <mml:mi>R</mml:mi>
      <mml:mrow>
        <mml:msub>
          <mml:mi>H</mml:mi>
          <mml:mn>3</mml:mn>
        </mml:msub>
        <mml:msub>
          <mml:mi>W</mml:mi>
          <mml:mn>3</mml:mn>
        </mml:msub>
        <mml:mi>C</mml:mi>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mnzhwbadoa">
    <mml:msub>
      <mml:mi>F</mml:mi>
      <mml:mn>4</mml:mn>
    </mml:msub>
    <mml:mo>∈</mml:mo>
    <mml:msup>
      <mml:mi>R</mml:mi>
      <mml:mrow>
        <mml:msub>
          <mml:mi>H</mml:mi>
          <mml:mn>4</mml:mn>
        </mml:msub>
        <mml:msub>
          <mml:mi>W</mml:mi>
          <mml:mn>4</mml:mn>
        </mml:msub>
        <mml:mi>C</mml:mi>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula>, where <span style="font-family: Georgia, serif">H<italic><span style="font-family: Georgia, serif">, </italic>W, and C represent height, width and channel depth, respectively. These feature maps are first enhanced with 2D positional encodings, denoted as <italic><span style="font-family: Georgia, serif">PF</italic><sub><span style="font-family: Georgia, serif">3</sub><span style="font-family: Georgia, serif"> and <italic><span style="font-family: Georgia, serif">PF</italic><sub><span style="font-family: Georgia, serif">4</sub><span style="font-family: Georgia, serif"> to incorporate spatial information critical for distinguishing closely positioned apples. The positionally encoded feature maps are formulated as:</p>
              
                <disp-formula>
                  <label>(1)</label>
                  <mml:math id="mqf59ietml">
                    <mml:msubsup>
                      <mml:mi>F</mml:mi>
                      <mml:mn>3</mml:mn>
                      <mml:mn>1</mml:mn>
                    </mml:msubsup>
                    <mml:msubsup>
                      <mml:mi>F</mml:mi>
                      <mml:mn>4</mml:mn>
                      <mml:mn>1</mml:mn>
                    </mml:msubsup>
                    <mml:mo>=</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mn>3</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mn>3</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mn>4</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mn>4</mml:mn>
                    </mml:msub>
                    <mml:mi>P</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mstyle scriptlevel="0">
                      <mml:mspace width="1em"/>
                    </mml:mstyle>
                  </mml:math>
                </disp-formula>
              
              <p><span style="font-family: Times New Roman, serif">To capture the relationships between the enhanced multiscale features, spatial multi-head attention is computed by projecting the feature maps into query(Q), key(K) and value(V) vectors using learned projection matrices. Spatial multi-head attention is computed using <inline-formula>
  <mml:math id="myfdnm5i7a">
    <mml:mrow>
      <mml:mi>Q</mml:mi>
    </mml:mrow>
    <mml:mo>=</mml:mo>
    <mml:msub>
      <mml:mrow>
        <mml:mi>W</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mrow>
          <mml:mi>Q</mml:mi>
        </mml:mrow>
      </mml:mrow>
    </mml:msub>
    <mml:msubsup>
      <mml:mrow>
        <mml:mtext> </mml:mtext>
        <mml:mi>F</mml:mi>
      </mml:mrow>
      <mml:mn>3</mml:mn>
      <mml:mn>1</mml:mn>
    </mml:msubsup>
  </mml:math>
</inline-formula>, <inline-formula>
  <mml:math id="mihsgrjkzt">
    <mml:mrow>
      <mml:mi>K</mml:mi>
    </mml:mrow>
    <mml:mo>=</mml:mo>
    <mml:msub>
      <mml:mrow>
        <mml:mi>W</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mrow>
          <mml:mi>K</mml:mi>
        </mml:mrow>
      </mml:mrow>
    </mml:msub>
    <mml:msubsup>
      <mml:mrow>
        <mml:mtext> </mml:mtext>
        <mml:mi>F</mml:mi>
      </mml:mrow>
      <mml:mn>4</mml:mn>
      <mml:mn>1</mml:mn>
    </mml:msubsup>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="m151ohy3wk">
    <mml:mrow>
      <mml:mi>V</mml:mi>
    </mml:mrow>
    <mml:mo>=</mml:mo>
    <mml:msub>
      <mml:mrow>
        <mml:mi>W</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mrow>
          <mml:mi>V</mml:mi>
        </mml:mrow>
      </mml:mrow>
    </mml:msub>
    <mml:msubsup>
      <mml:mrow>
        <mml:mtext> </mml:mtext>
        <mml:mi>F</mml:mi>
      </mml:mrow>
      <mml:mn>4</mml:mn>
      <mml:mn>1</mml:mn>
    </mml:msubsup>
  </mml:math>
</inline-formula> producing:</p>
              
                <disp-formula>
                  <label>(2)</label>
                  <mml:math id="m8d9piflio">
                    <mml:mi>Attention</mml:mi>
                    <mml:mi>Softmax</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mi>Q</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext> </mml:mtext>
                      <mml:mi>K</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext> </mml:mtext>
                      <mml:mi>V</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>Q</mml:mi>
                          <mml:msup>
                            <mml:mi>K</mml:mi>
                            <mml:mi>T</mml:mi>
                          </mml:msup>
                        </mml:mrow>
                        <mml:msqrt>
                          <mml:msub>
                            <mml:mi>d</mml:mi>
                            <mml:mi>k</mml:mi>
                          </mml:msub>
                        </mml:msqrt>
                      </mml:mfrac>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>V</mml:mi>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where, <italic><span style="font-family: Georgia, serif">d<sub><span style="font-family: Georgia, serif">k</sub> </italic>is the dimension of the key vector.</p><p>To focus attention specifically on apple fruit regions, an Apple Feature Focus mask Ma is applied to emphasize fruitrelevant spatial regions, by computing a <span style="font-family: Georgia, serif">1 <italic><span style="font-family: Meiryo UI, sans-serif">× </italic><span style="font-family: Georgia, serif">1 convolution over the concatenated positionally encoded features <inline-formula>
  <mml:math id="ms23co7kbr">
    <mml:msubsup>
      <mml:mi>F</mml:mi>
      <mml:mn>3</mml:mn>
      <mml:mn>1</mml:mn>
    </mml:msubsup>
    <mml:msubsup>
      <mml:mi>F</mml:mi>
      <mml:mn>4</mml:mn>
      <mml:mn>1</mml:mn>
    </mml:msubsup>
    <mml:mo>⊕</mml:mo>
  </mml:math>
</inline-formula> :<br><br></p>
              
                <disp-formula>
                  <label>(3)</label>
                  <mml:math id="mgpdgcq4va">
                    <mml:msub>
                      <mml:mi>M</mml:mi>
                      <mml:mi>a</mml:mi>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mi>σ</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>Conv</mml:mi>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                          <mml:mn>1</mml:mn>
                          <mml:mo>∗</mml:mo>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>⊕</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msubsup>
                          <mml:mi>F</mml:mi>
                          <mml:mn>3</mml:mn>
                          <mml:mn>1</mml:mn>
                        </mml:msubsup>
                        <mml:msubsup>
                          <mml:mi>F</mml:mi>
                          <mml:mn>4</mml:mn>
                          <mml:mn>1</mml:mn>
                        </mml:msubsup>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where <span style="font-family: Meiryo UI, sans-serif">⊕<italic><span style="font-family: Meiryo UI, sans-serif"> </italic>denotes channel-wise concatenation and σ represents the sigmoid activation function. The final attended feature is obtained by applying element-wise multiplication ⊙ between the attention output and the mask:</p>
              
                <disp-formula>
                  <label>(4)</label>
                  <mml:math id="m1zcs1u6vg">
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mrow>
                        <mml:mtext>attn </mml:mtext>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>M</mml:mi>
                      <mml:mi>a</mml:mi>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mo>⊙</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mtext> Attention </mml:mtext>
                    <mml:mrow>
                      <mml:mi>Q</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext> </mml:mtext>
                      <mml:mi>K</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext> </mml:mtext>
                      <mml:mi>V</mml:mi>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>To incorporate multi-resolution cues and scale-awareness, scale-aware attention with eight attention heads is applied. This enables the model to capturing diverse apple fruit sizes across orchard scenes:</p>
              
                <disp-formula>
                  <label>(5)</label>
                  <mml:math id="mc3lh5rzhi">
                    <mml:mi>M</mml:mi>
                    <mml:mi>S</mml:mi>
                    <mml:mi>A</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msubsup>
                        <mml:mi>F</mml:mi>
                        <mml:mn>3</mml:mn>
                        <mml:mn>1</mml:mn>
                      </mml:msubsup>
                      <mml:msubsup>
                        <mml:mi>F</mml:mi>
                        <mml:mn>4</mml:mn>
                        <mml:mn>1</mml:mn>
                      </mml:msubsup>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>…</mml:mo>
                      <mml:mo>…</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mtext> head </mml:mtext>
                        <mml:mn>1</mml:mn>
                      </mml:msub>
                      <mml:msub>
                        <mml:mtext> head </mml:mtext>
                        <mml:mn>8</mml:mn>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:mtext> Concat </mml:mtext>
                    <mml:msub>
                      <mml:mi>W</mml:mi>
                      <mml:mn>0</mml:mn>
                    </mml:msub>
                  </mml:math>
                </disp-formula>
              
              <p>where each head captures spatial dependencies at different scales, improving detection robustness for both small distant apples and larger foreground instances.</p><p>Following attention refinement, the resulting features are passed through a Cross-Stage Partial (CSP) block to enhance learning stability and preserve feature diversity:</p>
              
                <disp-formula>
                  <label>(6)</label>
                  <mml:math id="mu7o3zm51s">
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mrow>
                        <mml:mi>C</mml:mi>
                        <mml:mi>S</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mi>C</mml:mi>
                    <mml:mi>o</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mi>c</mml:mi>
                    <mml:mi>a</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msubsup>
                        <mml:mi>F</mml:mi>
                        <mml:mrow>
                          <mml:mi>a</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>n</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:mi>C</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>n</mml:mi>
                      <mml:mi>v</mml:mi>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msubsup>
                          <mml:mi>F</mml:mi>
                          <mml:mrow>
                            <mml:mi>a</mml:mi>
                            <mml:mi>t</mml:mi>
                            <mml:mi>t</mml:mi>
                            <mml:mi>n</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mo>(</mml:mo>
                            <mml:mo>)</mml:mo>
                            <mml:mn>2</mml:mn>
                          </mml:mrow>
                        </mml:msubsup>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>Finally, an FPN (Feature Pyramid Network) integration layer aligns the channels and outputs the enhanced multiscale feature maps, which are:</p>
              
                <disp-formula>
                  <label>(7)</label>
                  <mml:math id="mgt89vhvkp">
                    <mml:mtext> Enhanced P3, Enhanced P4 </mml:mtext>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">FPN</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>F</mml:mi>
                        <mml:mrow>
                          <mml:mi>C</mml:mi>
                          <mml:mi>S</mml:mi>
                          <mml:mi>P</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>This full EBCA design allows the model to effectively isolate individual apple fruits even in highly clustered, overlapping orchard scenes, significantly improving object separation accuracy and overall localization precision for apple harvesting applications.</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.2 Focal modulation for occlusion handling and feature enhancement</title>
              
              
                <fig id="fig_6">
                  <label>Figure 6</label>
                  <caption>
                    <title>(a) Focal modulation, (b) Context aggregation</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_MOVRjHxSnNILUDAZ.jpeg"/>
                </fig>
              
              <p><italic>1.<span style="font-family: Times New Roman">  Linear Transformation</italic></p><p>The input feature map <italic><span style="font-family: Georgia, serif">X </italic>is projected into a new representation space using a learnable weight matrix <italic><span style="font-family: Georgia, serif">W </italic>:</p>
              
                <disp-formula>
                  <label>(8)</label>
                  <mml:math id="ma65jllayn">
                    <mml:msup>
                      <mml:mi>X</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msup>
                    <mml:mo>=</mml:mo>
                    <mml:mo>.</mml:mo>
                    <mml:mi>X</mml:mi>
                    <mml:mi>W</mml:mi>
                  </mml:math>
                </disp-formula>
              
              <p>This operation extracts relevant feature vectors that serve as the foundation for subsequent context aggregation and modulation.</p><p><italic>2.<span style="font-family: Times New Roman"> Aggregation Operation</italic></p><p>A global context representation <italic><span style="font-family: Georgia, serif">F<sub><span style="font-family: Georgia, serif">C</sub> </italic>is obtained by aggregating the transformed features through mean pooling:</p>
              
                <disp-formula>
                  <label>(9)</label>
                  <mml:math id="mf7zcnzma9">
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mi>C</mml:mi>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mo>∑</mml:mo>
                    <mml:mfrac>
                      <mml:mn>1</mml:mn>
                      <mml:mi>N</mml:mi>
                    </mml:mfrac>
                    <mml:msup>
                      <mml:mi>X</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msup>
                  </mml:math>
                </disp-formula>
              
              <p>This aggregated context <italic><span style="font-family: Georgia, serif">F<sub><span style="font-family: Georgia, serif">C</sub> </italic>captures essential spatial information across the feature map, providing reference signals for enhancing task-relevant features while minimizing background distractions in apple detection.</p><p><italic>3.<span style="font-family: Times New Roman">  Modulation Operation</italic></p><p>A learned modulator applies element-wise interactions to enhance or suppress certain features. The modulation process is defined as:</p>
              
                <disp-formula>
                  <label>(10)</label>
                  <mml:math id="m8nejlwfi8">
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mi>m</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mi>c</mml:mi>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mi>q</mml:mi>
                    <mml:mi>σ</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>F</mml:mi>
                        <mml:mi>c</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>Here <italic><span style="font-family: Georgia, serif">q </italic>is the modulated query, and <italic><span style="font-family: Georgia, serif">σ </italic>is the activation (e.g., sigmoid), which adaptively emphasizes apple-relevant features while suppressing occluded or irrelevant regions.</p><p><italic>4.<span style="font-family: Times New Roman">  Output Transformation</italic></p><p>The modulated features <italic><span style="font-family: Georgia, serif">F<sub><span style="font-family: Georgia, serif">m</sub> </italic>undergo a final transformation to obtain the output feature map <italic><span style="font-family: Georgia, serif">Y </italic>:</p>
              
                <disp-formula>
                  <label>(11)</label>
                  <mml:math id="m26vrktc8i">
                    <mml:mi>Y</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:msub>
                      <mml:mi>W</mml:mi>
                      <mml:mi>m</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mi>m</mml:mi>
                    </mml:msub>
                  </mml:math>
                </disp-formula>
              
              <p>The resulting feature map <italic><span style="font-family: Georgia, serif">Y </italic>is then passed into subsequent detection heads for refined prediction of apple locations, even under challenging occlusion conditions.</p><p><italic>5.<span style="font-family: Times New Roman">  Hierarchical Contextualization and Gated Aggregation</italic></p><p style="text-align: justify">To further improve contextual understanding, the modulated output is processed through a hierarchical aggrega- tion structure, capturing multi-level semantic features critical for robust apple fruit detection. This structure operates at three semantic levels:</p><p style="text-align: justify">Level <italic><span style="font-family: Georgia, serif">ℓ </italic><span style="font-family: Georgia, serif">= 1: Extracts fine-grained local features of individual apples</p>
              
                <disp-formula>
                  <label>(12)</label>
                  <mml:math id="mdqdmpnmg0">
                    <mml:msub>
                      <mml:mi>f</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>W</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mi>Conv</mml:mi>
                    <mml:mi>x</mml:mi>
                  </mml:math>
                </disp-formula>
              
              <p>Level <italic><span style="font-family: Georgia, serif">ℓ </italic><span style="font-family: Georgia, serif">= 2: Encodes mid-level spatial clusters, including apple bunches and partial occlusions</p>
              
                <disp-formula>
                  <label>(13)</label>
                  <mml:math id="mce1sugtno">
                    <mml:msub>
                      <mml:mi>f</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>W</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mi>Conv</mml:mi>
                    <mml:mi>x</mml:mi>
                  </mml:math>
                </disp-formula>
              
              <p><span style="font-family: Times New Roman, serif">Level <italic><span style="font-family: Georgia, serif">ℓ </italic><span style="font-family: Georgia, serif">= 3<span style="font-family: Times New Roman, serif">: Captures broader context, such as rows of trees and orchard structure</p>
              
                <disp-formula>
                  <label>(14)</label>
                  <mml:math id="mgg80j1leg">
                    <mml:msub>
                      <mml:mi>f</mml:mi>
                      <mml:mn>3</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>W</mml:mi>
                      <mml:mn>3</mml:mn>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mi>Conv</mml:mi>
                    <mml:mi>x</mml:mi>
                  </mml:math>
                </disp-formula>
              
              <p>Each level’s output is gated to emphasize critical patterns:</p>
              
                <disp-formula>
                  <label>(15)</label>
                  <mml:math id="mnmxpcalnp">
                    <mml:msub>
                      <mml:mi>g</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mi>σ</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>⋅</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>W</mml:mi>
                        <mml:mrow>
                          <mml:msub>
                            <mml:mi>g</mml:mi>
                            <mml:mi>i</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>f</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                      <mml:mi>x</mml:mi>
                    </mml:mrow>
                    <mml:mn>1</mml:mn>
                    <mml:mn>2</mml:mn>
                    <mml:mn>3</mml:mn>
                  </mml:math>
                </disp-formula>
              
              <p>The final aggregated output <italic><span style="font-family: Georgia, serif">Z<sub><span style="font-family: Georgia, serif">out</sub><span style="font-family: Georgia, serif"> </italic>combines all levels:</p>
              
                <disp-formula>
                  <label>(16)</label>
                  <mml:math id="mpavpchizw">
                    <mml:msub>
                      <mml:mi>Z</mml:mi>
                      <mml:mrow>
                        <mml:mtext>out </mml:mtext>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>g</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>f</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>g</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>f</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>g</mml:mi>
                      <mml:mn>3</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>f</mml:mi>
                      <mml:mn>3</mml:mn>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>x</mml:mi>
                  </mml:math>
                </disp-formula>
              
              <p>This hierarchical approach enables the model to attend to both fine details (individual apples) and global orchard- level patterns, thereby providing robust detection even in densely populated and visually complex apple orchard environments.</p><p>Chu et al. [<xref ref-type="bibr" rid="ref_44">44</xref>] developed occluder-occludee network which is called as O2RNet to enhance the detection accuracy under severe occlusion conditions. It provides the spatial relationship between occluding and occluded fruits in an orchard that helps in better detection. The Focal Modulation module offers a robust solution for managing heavy occlusion in apple detection tasks. By adaptively regulating feature importance based on context- sensitive modulation and hierarchical aggregation, the model is capable of accurately localizing visible apples while completely suppressing interfering factors. This adaptive approach greatly improves the model’s performance in detecting apples in cluttered orchard scenes, providing both high detection accuracy and better depth estimation.</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.3 Kernel warehouse convolution for scale-adaptive apple localization</title>
              
              <p>To dynamically scale receptive fields and better extract diverse contextual information in complicated orchard images, the proposed KernelWarehouse Convolution module provides an adaptive convolutional scheme to enhance localization accuracy of apple fruits under different size, scale, and density situations illustrated in <xref ref-type="fig" rid="fig_7">Figure 7</xref>. By assembling scale-aware convolution kernels from a learnable warehouse, KWConv enhances the spatial sensitivity and robustness of the detection pipeline.</p>
              
                <fig id="fig_7">
                  <label>Figure 7</label>
                  <caption>
                    <title>Workflow of KernelWarehouse Convolution</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_zSg4BbHSs5LB_Y9s.jpeg"/>
                </fig>
              
              <p><italic>1. Warehouse Sharing Framework</italic></p><p>KWConv is structured around a multi-stage feature extraction pipeline, where each stage t maintains a dedicated kernel repository known as the <italic>t</italic><sup>th </sup>Warehouse, contains a collection of learnable kernel elements <inline-formula>
  <mml:math id="mmype5va86">
    <mml:mrow>
      <mml:mo>{</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>…</mml:mo>
      <mml:mo>…</mml:mo>
      <mml:mo>}</mml:mo>
      <mml:msub>
        <mml:mi>e</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>e</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>e</mml:mi>
        <mml:mi>n</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> shared across multiple convolutional layers within the stage.</p><p><p>Kernel Reusability: Instead of training new convolution kernels for each layer independently, KWConv reuses a shared kernel set, improving consistency of learned features across layers and reducing computational overhead.</p><p>Stage-Wise Interaction: Warehouses are progressively updated across stages <inline-formula>
  <mml:math id="m6xbjnyff3">
    <mml:mo>(</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>→</mml:mo>
    <mml:mo>→</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>+</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mrow>
      <mml:mi>t</mml:mi>
    </mml:mrow>
    <mml:mrow>
      <mml:mi>t</mml:mi>
    </mml:mrow>
    <mml:mrow>
      <mml:mi>t</mml:mi>
    </mml:mrow>
    <mml:mn>1</mml:mn>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula>, allowing bidirectional refinement of kernel representations. This inter-stage flow enhances the adaptability of the model to changing spatial distributions of apple features across the network depth.</p></p><p><italic>2.<span style="font-family: Times New Roman"> Kernel Partition and Dynamic Assembly</italic></p><p style="text-align: justify">KWC onv dynamically assembles convolution kernels that are adapted to spatially varying content. Each kernel <italic><span style="font-family: Georgia, serif">w<sub><span style="font-family: Georgia, serif">j</sub> </italic>is constructed as a weighted sum of base kernel elements from the warehouse:</p>
              
                <disp-formula>
                  <label>(17)</label>
                  <mml:math id="m305m0050e">
                    <mml:msub>
                      <mml:mi>w</mml:mi>
                      <mml:mi>j</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>α</mml:mi>
                      <mml:mrow>
                        <mml:mi>j</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>e</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>…</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:munderover>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>n</mml:mi>
                    </mml:munderover>
                    <mml:mstyle scriptlevel="0">
                      <mml:mspace width="1em"/>
                    </mml:mstyle>
                    <mml:mrow>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext> </mml:mtext>
                      <mml:mi>m</mml:mi>
                    </mml:mrow>
                    <mml:mn>1</mml:mn>
                  </mml:math>
                </disp-formula>
              
              <p>where,</p><p><p><italic><inline-formula>
  <mml:math id="mj5w2yujmq">
    <mml:msub>
      <mml:mi>e</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> </italic>is the <inline-formula>
  <mml:math id="mtmj6udqm4">
    <mml:msup>
      <mml:mi>i</mml:mi>
      <mml:mrow>
        <mml:mtext>th </mml:mtext>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> kernel element in the warehouse.</p><p><inline-formula>
  <mml:math id="mi0cdo967t">
    <mml:msub>
      <mml:mi>α</mml:mi>
      <mml:mrow>
        <mml:mi>j</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> represents the attention weight that determines the contribution of ei to the <inline-formula>
  <mml:math id="mf3lbmnuqb">
    <mml:msup>
      <mml:mi>j</mml:mi>
      <mml:mrow>
        <mml:mtext>th </mml:mtext>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> assembled kernel <inline-formula>
  <mml:math id="my60l63zjk">
    <mml:msub>
      <mml:mi>w</mml:mi>
      <mml:mi>j</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> .</p><p><italic>m</italic> is the total number of adaptive kernels used in the layer.</p></p><p>This mechanism supports content-aware and spatially adaptive convolution, enabling the model to better localize apples across varying sizes and densities.<br><br><italic>3.<span style="font-family: Times New Roman"> Gating and Attention Mechanism</italic></p><p>To compute the attention weights <italic><span style="font-family: Georgia, serif">α<sub><span style="font-family: Georgia, serif">ji</sub></italic>, a global descriptor is extracted from the input feature map <italic><span style="font-family: Georgia, serif">X </italic>using Global Average Pooling (GAP), followed by a two-layer fully connected network with a ReLU activation:</p>
              
                <disp-formula>
                  <label>(18)</label>
                  <mml:math id="mjq0fntmep">
                    <mml:mi>g</mml:mi>
                    <mml:mi>RELU</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mi>F</mml:mi>
                      <mml:msub>
                        <mml:mi>C</mml:mi>
                        <mml:mn>2</mml:mn>
                      </mml:msub>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>F</mml:mi>
                        <mml:mi>G</mml:mi>
                        <mml:mi>A</mml:mi>
                        <mml:mi>P</mml:mi>
                        <mml:mi>X</mml:mi>
                        <mml:msub>
                          <mml:mi>C</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>This global descriptor <italic>g</italic> is fed into a Non-Linear Attention Function (NAF), which produces attention coefficients for every kernel element.</p>
              
                <disp-formula>
                  <label>(19)</label>
                  <mml:math id="mgqs2phr6l">
                    <mml:msub>
                      <mml:mi>α</mml:mi>
                      <mml:mrow>
                        <mml:mi>j</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mi>j</mml:mi>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>N</mml:mi>
                    <mml:mi>A</mml:mi>
                    <mml:mi>g</mml:mi>
                  </mml:math>
                </disp-formula>
              
              <p>These weights guide the selective kernel composition process so that each output kernel is shaped by the most contextually relevant warehouse components.</p><p><italic>4.<span style="font-family: Times New Roman"> Adaptive Convolution Operation</italic></p><p>Adaptively constructed kernels <inline-formula>
  <mml:math id="mi12ag8d6a">
    <mml:mrow>
      <mml:mo>{</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>…</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>}</mml:mo>
      <mml:msub>
        <mml:mi>W</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>W</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>W</mml:mi>
        <mml:mi>m</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> is then used on the input feature map through normal convolution.</p>
              
                <disp-formula>
                  <label>(20)</label>
                  <mml:math id="m5rjfxwz4t">
                    <mml:mi>y</mml:mi>
                    <mml:mi>w</mml:mi>
                    <mml:mi>f</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mo>∗</mml:mo>
                  </mml:math>
                </disp-formula>
              
              <p>where,</p><p><p><inline-formula>
  <mml:math id="myj118r9hh">
    <mml:mi>W</mml:mi>
    <mml:mo>∈</mml:mo>
    <mml:msup>
      <mml:mi>R</mml:mi>
      <mml:mrow>
        <mml:msup>
          <mml:mi>k</mml:mi>
          <mml:mo>∗</mml:mo>
        </mml:msup>
        <mml:msup>
          <mml:mi>k</mml:mi>
          <mml:mo>∗</mml:mo>
        </mml:msup>
        <mml:mi>c</mml:mi>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> is the dynamically generated convolution kernel tensor.</p><p><italic>f</italic> represents the input feature map.</p><p><italic>∗</italic> denotes convolution operation.</p><p><italic>y</italic> refers the output feature map.</p></p><p>The KWConv module presents an efficient approach to multi-scale feature adaptation with the integration of kernel reuse, composition-by-attention, and hierarchical refinement. Its capacity to produce spatially adaptive filters custom-fit to input features renders it well-adapted to intricate apple orchard environments where the sizes, locations, and occlusion rates of fruits differ greatly. By refining the localization process, KWConv significantly boosts detection performance and contributes to more accurate and scale-aware apple fruit recognition.</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.4 Adabins for absolute depth estimation of apple fruits</title>
              
              <p>To incorporate depth awareness into the DeepHarvestNet architecture, we integrate the AdaBins module, which has demonstrated superior performance in monocular depth estimation. This integration is particularly valuable for enhancing spatial precision and improving the detection of foreground apple fruits under challenging orchard conditions involving overlapping canopies, variable fruit distances, and occlusions.</p><p>An Adaptive binning technique is introduced by the AdaBins architecture illustrated in <xref ref-type="fig" rid="fig_8">Figure 8</xref>, allowing for accurate absolute depth estimate from single monocular RGB pictures. The core innovation lies in its ability to learn non-uniform, data-driven depth bin centers, dynamically adjusted based on scene content. This allows the system to effectively handle varying depth distributions commonly observed in orchard environments.</p><p>The AdaBins depth estimation pipeline comprises three key components:</p><p><p>a standard encoder-decoder architecture.</p><p>a Mini Vision Transformer (mViT) module.</p><p>a hybrid depth regression block for pixel-wise depth prediction.</p></p>
              
                <fig id="fig_8">
                  <label>Figure 8</label>
                  <caption>
                    <title>(a) AdaBins architecture, (b) mViT architecture</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_EsZSVb5DMndetlT7.png"/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_4tSTtmraVcrhMAc2.png"/>
                </fig>
              
              <p><italic>1. Input Specification</italic></p><p>The AdaBins module receives two primary inputs:</p><p>• A monocular RGB image captured from the orchard.</p><p>• A user-defined depth range values <inline-formula>
  <mml:math id="mh4b0003gn">
    <mml:mrow>
      <mml:mo>[</mml:mo>
      <mml:mo fence="true"/>
      <mml:msub>
        <mml:mi>u</mml:mi>
        <mml:mrow>
          <mml:mo>min</mml:mo>
        </mml:mrow>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="m9ay7kfb5y">
    <mml:mrow>
      <mml:mo fence="true"/>
      <mml:mo>]</mml:mo>
      <mml:msub>
        <mml:mi>u</mml:mi>
        <mml:mrow>
          <mml:mo>max</mml:mo>
        </mml:mrow>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> which specifies the possible minimum and maximum distances corresponding to the orchard layout.</p><p>This configuration enables the model to regress absolute depth values directly, rather than relying on relative or normalized depth scales.</p><p><italic>2. Encoder-Decoder Architecture</italic></p><p>The RGB input image is initially processed by a convolutional encoder-decoder pipeline:</p><p>• The encoder extracts hierarchical features while progressively reducing spatial resolution.</p><p>• The decoder reconstructs a dense feature map, preserving essential semantic structures necessary for subsequent adaptive binning and depth regression stages.</p><p><italic>3. Adaptive Bin Width Estimation via MiniViT</italic></p><p>To capture contextual and spatial cues:</p><p>• A lightweight Mini Vision Transformer (mViT) processes the decoded feature map, modeling long-range</p><p>dependencies and global scene structure.</p><p>• The mViT output is fed into a bin regressor that predicts soft weight parameters www, which define the adaptive bin centers.</p><p>The formula to calculate the bin centers b which is available in the <xref ref-type="fig" rid="fig_8">Figure 8</xref>a. AdaBins architecture gives the adaptive bin centers b and is computed as:</p>
              
                <disp-formula>
                  <label>(21)</label>
                  <mml:math id="mqzbvbndlb">
                    <mml:mrow>
                      <mml:mi>b</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>w</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>u</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>max</mml:mo>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>u</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>min</mml:mo>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mi>softmax</mml:mi>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>u</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>min</mml:mo>
                      </mml:mrow>
                    </mml:msub>
                  </mml:math>
                </disp-formula>
              
              <p>where,</p><p>• b represents the set of predicted bin centers distributed across the specified depth range.</p><p>• The softmax normalization ensures that the bins are dynamically adjusted in response to the scene’s depth complexity.</p><p>This adaptive binning allows the network to allocate finer resolution to regions where depth variation is high (e.g., clusters of overlapping apples) while using coarser resolution for homogeneous regions (e.g., uniform backgrounds).</p><p><italic>4. Bin-wise Classification and Depth Regression</italic></p><p>For each pixel, AdaBins performs a bin classification, assigning soft σk across K adaptive bins. The final absolute depth d for each pixel is computed through hybrid regression as represented with the depth image in the <xref ref-type="fig" rid="fig_8">Figure 8</xref>a. AdaBins Architecture and is computed as:</p>
              
                <disp-formula>
                  <label>(22)</label>
                  <mml:math id="max1j9c3wl">
                    <mml:mi>d</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mo>∗</mml:mo>
                    <mml:munderover>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mi>k</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>K</mml:mi>
                    </mml:munderover>
                    <mml:msub>
                      <mml:mi>σ</mml:mi>
                      <mml:mi>k</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>b</mml:mi>
                      <mml:mi>k</mml:mi>
                    </mml:msub>
                  </mml:math>
                </disp-formula>
              
              <p>where,</p><p>• <italic>d</italic> is the estimated depth at the pixel,</p><p>•<italic> K</italic> is the total number of bins,</p><p>• <italic>σk</italic> is the soft assignment probability for the<italic> K<sup>th</sup></italic> bin (it represents each pixel belonging to which bin),</p><p>•<italic> bk</italic> is the corresponding bin center determined in Eq. (2).</p><p>This hybrid strategy effectively combines classification confidence with continuous regression to produce precise depth estimates at pixel level.</p><p><italic>5. Output: Absolute Depth Map</italic></p><p>The last output of the AdaBins module is a dense absolute depth map where every pixel stores its real-world distance from the camera. This depth map greatly helps separate foreground apples from background features like leaves, branches, and orchard rows, thus enhancing both localization and counting accuracy.</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.5 Relative position processing module (rppm)</title>
              
              <p>The Relative Position Processing Module is proposed to improve object localization accuracy by improving the spatial depth relations between apple fruits and background structures. It is especially useful for handling viewpoint changes, camera displacements, and depth normalization across frames to ensure robust and stable apple fruit identification under different orchard conditions.</p><p>Workflow of RPPM</p><p>• Input from AdaBins: The absolute depth map <italic>d(x, y)</italic> produced by the AdaBins module is used as the input to the RPPM. Concurrently, the spatial coordinates<italic> (x, y)</italic> corresponding to candidate apple fruit detections from the neck layer of the detection network are given as input to the module.</p><p>• Conversion to Relative position: Since the absolute depth values may vary due to camera orientation, platform movement, and orchard terrain, it is essential to compute depth-relative positioning that accounts for the optical geometry of the imaging setup. To achieve this, the absolute depth <italic>d(x, y) </italic>for each detected apple is adjusted relative to the optical center <italic>(cx, cy)</italic> of the camera:</p><p>Refining depth using<italic> (x, y)</italic> is computed as:</p>
              
                <disp-formula>
                  <label>(23)</label>
                  <mml:math id="m1s6v886ji">
                    <mml:msub>
                      <mml:mi>D</mml:mi>
                      <mml:mrow>
                        <mml:mi>f</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>n</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>l</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>∗</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mi>y</mml:mi>
                    <mml:mi>d</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>y</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mn>1</mml:mn>
                      <mml:mi>α</mml:mi>
                      <mml:mi>β</mml:mi>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                          <mml:mo>−</mml:mo>
                          <mml:msub>
                            <mml:mi>c</mml:mi>
                            <mml:mi>x</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                        <mml:msub>
                          <mml:mi>c</mml:mi>
                          <mml:mi>x</mml:mi>
                        </mml:msub>
                      </mml:mfrac>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>y</mml:mi>
                          <mml:mo>−</mml:mo>
                          <mml:msub>
                            <mml:mi>c</mml:mi>
                            <mml:mi>y</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                        <mml:msub>
                          <mml:mi>c</mml:mi>
                          <mml:mi>y</mml:mi>
                        </mml:msub>
                      </mml:mfrac>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where,</p><p>• D<sub>final</sub><italic>(x, y)</italic> is the refined depth after processing relative position.</p><p>• <italic>d(x, y)</italic> is the absolute depth value from AdaBins at pixel (x, y).</p><p>•<italic> cx</italic> and <italic>cy</italic> are the optical center coordinates of the image.</p><p>•<italic> (x, y)</italic> values represent the center coordinates of the fruit in the image.</p><p>• <italic>cx </italic>and <italic>cy</italic> can be taken by using the image dimensions.</p><p style="text-align: center"><inline-formula>
  <mml:math id="mg3otzm68w">
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>=</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>=</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>c</mml:mi>
        <mml:mi>x</mml:mi>
      </mml:msub>
      <mml:msub>
        <mml:mi>c</mml:mi>
        <mml:mi>y</mml:mi>
      </mml:msub>
      <mml:mfrac>
        <mml:mtext> image width </mml:mtext>
        <mml:mn>2</mml:mn>
      </mml:mfrac>
      <mml:mfrac>
        <mml:mtext> image height </mml:mtext>
        <mml:mn>2</mml:mn>
      </mml:mfrac>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p><p><italic>α</italic> and <italic>β</italic> can be determined by using <inline-formula>
  <mml:math id="mgu6co7y0r">
    <mml:mi>α</mml:mi>
    <mml:mi>β</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mfrac>
      <mml:mi>B</mml:mi>
      <mml:msub>
        <mml:mi>f</mml:mi>
        <mml:mi>x</mml:mi>
      </mml:msub>
    </mml:mfrac>
    <mml:mfrac>
      <mml:mi>B</mml:mi>
      <mml:msub>
        <mml:mi>f</mml:mi>
        <mml:mi>y</mml:mi>
      </mml:msub>
    </mml:mfrac>
  </mml:math>
</inline-formula>.</p><p><italic>f<sub>x </sub></italic>and <italic>f<sub>y</sub> </italic>are the focal lengths along <italic>x </italic>and <italic>y</italic> direction.</p><p><italic>B</italic> is the baseline parameter that represents an offset factor related to camera position.</p></p>
              
                <table-wrap id="table_1">
                  <label>Table 1</label>
                  <caption>
                    <title>Depth thresholds</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="1"><p>Distance (cm)</p></td><td colspan="1" rowspan="1"><p>Max. Depth (m)</p></td></tr><tr><td colspan="1" rowspan="1"><p>20</p></td><td colspan="1" rowspan="1"><p>0.9</p></td></tr><tr><td colspan="1" rowspan="1"><p>40</p></td><td colspan="1" rowspan="1"><p>1.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>60</p></td><td colspan="1" rowspan="1"><p>1.4</p></td></tr><tr><td colspan="1" rowspan="1"><p>80</p></td><td colspan="1" rowspan="1"><p>1.6</p></td></tr></tbody></table>
                </table-wrap>
              
              <p>After obtaining the relative depth values, the next step involves classifying whether the detected apple fruits lie in the foreground or background. This is achieved by applying depth thresholding, where predefined depth limits are used to segment apples based on their distance from the camera. The threshold values, as presented in <xref ref-type="table" rid="table_1">Table 1</xref>, are determined by considering both the drone’s flying altitude and the maximum observable depth range of the camera system within the orchard environment.</p><p>The algorithm to consider the foreground apples only is as below steps:</p><p>Step 1: Check if camera distance exists in the table</p><p>• If yes, use its corresponding max depth.</p><p>• If no, find the two closest distances and interpolate the max depth.</p><p>Step 2: Interpolation formula (if needed)</p>
              
                <disp-formula>
                  <label>(24)</label>
                  <mml:math id="mtepbi34s1">
                    <mml:msub>
                      <mml:mi>D</mml:mi>
                      <mml:mrow>
                        <mml:mo>max</mml:mo>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>D</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>∗</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mtext> camera_distance </mml:mtext>
                        <mml:mo>−</mml:mo>
                        <mml:msub>
                          <mml:mi>d</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mi>d</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>d</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                        <mml:mo>−</mml:mo>
                      </mml:mrow>
                    </mml:mfrac>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>D</mml:mi>
                        <mml:mn>2</mml:mn>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>D</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:msub>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where,</p><p>• <italic>d<sub>1</sub></italic> and <italic>d<sub>2</sub></italic> are the closest camera distances.</p><p>• <italic>D<sub>2</sub></italic><sub> </sub>and <italic>D<sub>1</sub></italic> are the corresponding max depths.</p><p>Step 3: Compare apples depth d with the threshold <italic>D<sub>max</sub></italic></p><p>If <italic>d ≤ D<sub>max</sub></italic> then the Apple is in Foreground otherwise Apple is in background.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.4. Loss function</title>
          
          <p>In the proposed DeepHarvestNet architecture, precise localization is essential due to the complex orchard conditions involving occlusions, dense clustering, and varying scales of apple fruits. To enhance the model’s sensitivity toward these challenging scenarios, we adopt the Focal Intersection-over-Union (FIoU) loss function in the training process. Unlike conventional IoU-based loss functions that assign equal importance across all samples, FIoU introduces a compound formulation that applies a focal modulation combined with a logarithmic transformation of the IoU score. This design emphasizes difficult-to-predict cases by assigning greater penalties to poorly aligned bounding boxes. As a result, the network is encouraged to prioritize the correction of harder instances, such as small-sized apples, partially occluded fruits, and overlapping clusters, which frequently occur in natural orchard environments.</p><p>The FIoU loss function used in defined as:</p>
          
            <disp-formula>
              <label>(25)</label>
              <mml:math id="mezxqlmzx0">
                <mml:mi>F</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>c</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>l</mml:mi>
                <mml:mi>I</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>U</mml:mi>
                <mml:mi>log</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mo>∗</mml:mo>
                <mml:mo>⁡</mml:mo>
                <mml:msup>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mn>1</mml:mn>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mi data-mjx-auto-op="false">Bp</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi data-mjx-auto-op="false">Bg</mml:mi>
                        </mml:mrow>
                        <mml:mo>∩</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mi data-mjx-auto-op="false">Bp</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi data-mjx-auto-op="false">Bg</mml:mi>
                        </mml:mrow>
                        <mml:mo>∪</mml:mo>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:mrow>
                  <mml:mi>γ</mml:mi>
                </mml:msup>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>∩</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">Bp</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">Bg</mml:mi>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>∪</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">Bp</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">Bg</mml:mi>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>where,</p><p>• Bp and Bg denote the predicted and ground truth bounding boxes, respectively.</p><p>• Bp∩Bg represents the intersection area between the predicted and ground truth bounding boxes.</p><p>• Bp∪Bg represents the union area of the predicted and ground truth bounding boxes.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Experimental results and discussion</title>
      
        <sec>
          
            <title>4.1. Experimental platform</title>
          
          <p>The suggested DeepHarvestNet framework for apple detection and depth estimation was developed and evaluated on a workstation running Ubuntu 16.04 LTS. The deployment was done using Python as the programming language and PyTorch deep learning framework, and it was possible to train high-performance models and host them. The hardware system involved an Intel Core i7 processor, 24 GB of RAM, and an NVIDIA GeForce RTX 3090 graphics processing unit with a 384-bit memory interface and a base frequency of 1395 MHz with the ability for parallel computation at a high rate.</p><p style="text-align: justify">To accelerate the training time, the system employed CUDA Toolkit and cuDNN (CUDA Deep Neural Network Library) to boost GPU efficiency for deep learning processes. DeepHarvestNet architecture enhances the baseline YOLOv8 model through the integration of Efficient Bidirectional Cross-Attention, Focal Modulation, KernelWare- house Convolution, and AdaBins to improve feature representation and depth-aware detection. Experiments were performed at an Intersection over Union (IoU) threshold of 0.75 to compare bounding box prediction accuracy.</p>
        </sec>
      
      
        <sec>
          
            <title>4.2. Evaluation metrics</title>
          
          <p>The performance of the recommended DeepHarvestNet model is evaluated quantitatively on three typical metrics: Precision (P), Recall (R), and F1-Score, which in combination offer a thorough estimate, particularly under orchard- specific conditions such as class imbalance, occlusion, and overlapping apples.</p><p>Precision (P): Measures the proportion of correctly predicted apples among all predicted positives, reflecting the model’s ability to minimize false positives:</p>
          
            <disp-formula>
              <label>(26)</label>
              <mml:math id="mqm7h1lpm4">
                <mml:mtext> Precision </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>Recall (R): Assesses the proportion of actual apples correctly detected, indicating the model’s capability to minimize false negatives:</p>
          
            <disp-formula>
              <label>(27)</label>
              <mml:math id="m4qvbk2zly">
                <mml:mtext> Recall </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>F1-Score: Represents the harmonic mean of Precision and Recall, offering a balanced measure of detection performance</p>
          
            <disp-formula>
              <label>(28)</label>
              <mml:math id="mn748rcfr2">
                <mml:mtext> F1-Score </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mo>∗</mml:mo>
                <mml:mn>2</mml:mn>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mtext> Precison </mml:mtext>
                      <mml:mtext> Recall </mml:mtext>
                      <mml:mo>∗</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mtext> Precision </mml:mtext>
                      <mml:mtext> Recall </mml:mtext>
                      <mml:mo>+</mml:mo>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>Here, TP (True Positives) are correctly identified apples, FP (False Positives) are non-apples misclassified as apples, and FN (False Negatives) are apples missed by the model.</p><p style="text-align: justify">These metrics collectively provide a robust evaluation of detection accuracy and robustness in challenging orchard environments.</p>
        </sec>
      
      
        <sec>
          
            <title>4.3. Comparative evaluation</title>
          
          <p>To assess the performance of the proposed DeepHarvestNet architecture, a set of experiments was carried out, with the outcomes consolidated in <xref ref-type="table" rid="table_2">Table 2</xref>. The model’s effectiveness was benchmarked against multiple well- known object detection frameworks, such as Faster R-CNN, SSD, RetinaNet, YOLOv5, YOLOv7, and YOLOv8. Additionally, a comparison was made with an improved variant of YOLOv7 that integrates Attribute Attention Mechanism and Adaptive Pooling.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Key parameters of our model</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>Original Images</p></td><td colspan="2" rowspan="1"></td><td colspan="1" rowspan="1"><p>Illumination Images</p></td><td colspan="2" rowspan="1"></td></tr><tr><td colspan="1" rowspan="1"><p>Detection Methods</p></td><td colspan="1" rowspan="1"><p>Precision</p></td><td colspan="1" rowspan="1"><p>Recall</p></td><td colspan="1" rowspan="1"><p>F1-Score</p></td><td colspan="1" rowspan="1"><p>Precision</p></td><td colspan="1" rowspan="1"><p>Recall</p></td><td colspan="1" rowspan="1"><p>F1-Score</p></td></tr><tr><td colspan="1" rowspan="1"><p>Faster R-CNN</p></td><td colspan="1" rowspan="1"><p>0.75</p></td><td colspan="1" rowspan="1"><p>0.81</p></td><td colspan="1" rowspan="1"><p>0.76</p></td><td colspan="1" rowspan="1"><p>0.69</p></td><td colspan="1" rowspan="1"><p>0.71</p></td><td colspan="1" rowspan="1"><p>0.70</p></td></tr><tr><td colspan="1" rowspan="1"><p>SSD</p></td><td colspan="1" rowspan="1"><p>0.81</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>0.83</p></td><td colspan="1" rowspan="1"><p>0.70</p></td><td colspan="1" rowspan="1"><p>0.77</p></td><td colspan="1" rowspan="1"><p>0.72</p></td></tr><tr><td colspan="1" rowspan="1"><p>RetinaNet</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>0.87</p></td><td colspan="1" rowspan="1"><p>0.80</p></td><td colspan="1" rowspan="1"><p>0.84</p></td><td colspan="1" rowspan="1"><p>0.82</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv5</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>0.93</p></td><td colspan="1" rowspan="1"><p>0.92</p></td><td colspan="1" rowspan="1"><p>0.82</p></td><td colspan="1" rowspan="1"><p>0.92</p></td><td colspan="1" rowspan="1"><p>0.83</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv7</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>0.92</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>0.81</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>0.81</p></td></tr><tr><td colspan="1" rowspan="1"><p>Improved YOLOv7 (Attribute Attention + Adaptive Pooling)</p></td><td colspan="1" rowspan="1"><p>0.93</p></td><td colspan="1" rowspan="1"><p>0.96</p></td><td colspan="1" rowspan="1"><p>0.94</p></td><td colspan="1" rowspan="1"><p>0.84</p></td><td colspan="1" rowspan="1"><p>0.92</p></td><td colspan="1" rowspan="1"><p>0.85</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv8</p></td><td colspan="1" rowspan="1"><p>0.94</p></td><td colspan="1" rowspan="1"><p>0.96</p></td><td colspan="1" rowspan="1"><p>0.95</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>0.92</p></td><td colspan="1" rowspan="1"><p>0.90</p></td></tr><tr><td colspan="1" rowspan="1"><p>DeepHarvestNet</p></td><td colspan="1" rowspan="1"><p>0.96</p></td><td colspan="1" rowspan="1"><p>0.97</p></td><td colspan="1" rowspan="1"><p>0.97</p></td><td colspan="1" rowspan="1"><p>0.94</p></td><td colspan="1" rowspan="1"><p>0.95</p></td><td colspan="1" rowspan="1"><p>0.95</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>The development of YOLO family by improvising each version with an adding new module that contribute the better fruit detection performance. The key changes in the YOLO versions allow to handle occlusion, lighting vari- ation, multi-scale feature extraction, overlapping and depth aware. YOLOv4 showed an advancement by integrating Cross-Stage Partial, Spatial Pyramid Pooling and Mish activation funcitons. By adding these components helps in improving the feature extraction and gradient flow, enabling the model to handle challenging lighting in an orchard. Building on this, the YOLOv5 introduce a lightweight deployment, mosaic data augmentation to enhance detection of fruit in balancing the speed and accuracy.The model can be improvised by incorporating modules like CBAM attention and specific improvements like SPPFCSPC in handling the robust detection under occlusion, small or blur fruits and overlapping fruits. Further the YOLOv7 is enhanced in adoption of Extended Efficient Layer Aggre- gation Network, re-parameterization and planned gradient path allocation. These improvements help in handiling the better fruit detection under occlusion and improved feature reuse. Most recently, YOLOv8 model contributed better localization and handle the occulsion, overlapping by incorporating anchor-free detection head, decoupled classification and adaptive loss functions resulting in precise localization and detection accuracy under complex orchards. With an enhancement to the model, depth information can be possible with 3D localization. Collectively, these continous advancements in YOLO versions i.e. from YOLOv4 to YOLOv8 have increasingly enhanced the speed and accuracy of fruit detection under occlusion, overlapping fruits, variable lighting and small targets and depth information. These YOLO version models are more suitable for automated fruit monitoring in an orchard with bettter performance.</p><p>The comparison was done with uniform conditions for all the models, so there is a fair and unbiased comparison. Detection performance was measured in terms of metrics such as Precision, Recall, and F1-Score.The results clearly show how DeepHarvestNet improves upon introducing its advancements, showcasing its better capability to accurately localize and detect apples, particularly in challenging cases with occlusion, different sizes of fruits, and cluttered backgrounds. The extensive comparison of all the models, including the proposed model, is given in <xref ref-type="table" rid="table_2">Table 2</xref>.</p><p>From the performance comparison in <xref ref-type="table" rid="table_2">Table 2</xref>, the DeepHarvestNet model outperforms all the current detection methods under both original and illumination-changed image conditions. For original images, DeepHarvestNet has the highest Precision, Recall and F1-Score values of 0.96, 0.97 and 0.97 respectively, reflecting its high accuracy in detecting apples under normal conditions. Even in difficult illumination fluctuations, the model sustains strong performance, achieving 0.94 Precision, 0.95 Recall and 0.95 F1-Score, better than the next best model (YOLOv8) in all the metrics. The results demonstrate DeepHarvestNet capability to deal well with occlusion, diverse fruit sizes, and inconsistent lighting, which were weaknesses in earlier methods such as Faster R-CNN, SSD, and even earlier YOLO versions.</p><p>To better visualize and interpret this comparative performance, a bar plot representation of the metrics in <xref ref-type="table" rid="table_2">Table 2</xref> can be used. This visual depicted in <xref ref-type="fig" rid="fig_9">Figure 9</xref> aid helps in clearly identifying performance differences across models for both original and illumination-affected images, providing a quick and intuitive understanding of DeepHarvestNet improvements over the baseline and advanced detection frameworks.</p>
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>
                <title>Comparative bar plot showing Precision, Recall and F1-Score for various object detection models on original orchard images and under illumination variation conditions</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_5wcJsDPC2_WYpdZN.png"/>
            </fig>
          
          
            <fig id="fig_10">
              <label>Figure 10</label>
              <caption>
                <title>（a) Apple (gold delicious), (b) Apple (gold delicious) detected image by DeepHarvestNet</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_JK91BVeBHHGBcqBQ.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_S8vgjMg02C_LqFV3.png"/>
            </fig>
          
          <p>The <xref ref-type="fig" rid="fig_10">Figure 10</xref> shows the output of gold delicious with improved detection accuracy and followed by depth estimation using DeepHarvestNet architecture.</p>
        </sec>
      
      
        <sec>
          
            <title>4.4. Discussion</title>
          
          <p>The DeepHarvestNet framework was developed in direct response to the limitations identified in the Improved YOLOv7 model, which employed YOLOv7 with attribute attention and adaptive pooling. While that approach performed reasonably under ideal conditions, it struggled significantly with occlusion, overlapping fruits, and detection of small or distant apples. Specifically, the attribute attention failed to focus effectively under dense canopy cover, and fixed pooling layers often led to loss of detail for tiny or clustered apples. These shortcomings led to false positives and inconsistent localization in complex orchard scenarios. The current work addresses these issues by introducing targeted architectural improvements in the form of EBCA, FM, KWConv, and AdaBins within the YOLOv8 backbone, each selected to overcome a specific shortcoming from Improved YOLOv7.</p><p>The DeepHarvestNet framework that is submitted demonstrates considerable enhancement in apple detection and depth estimation through four essential components: Efficient Bidirectional Cross-Attention, Focal Modulation, KernelWarehouse Convolution, and AdaBins. Each of them tackles important shortfalls from previous models. EBCA facilitates efficient information interaction among multi-scale features by top-down and bottom-up attention, enabling the model to more accurately separate foreground apples from overlapping background fruits—a problem salient in previous methods. FM substitutes normal self-attention with context-sensitive modulation, selectively highlighting visible fruit areas while minimizing background noise, thereby improving detection even when over 70% of the fruit is occluded. KWConv improves feature fusion by adaptively adjusting the receptive field to the fruit size so that small or far-away apples are preserved and precisely localized.</p><p>Simultaneously, AdaBins enhances the depth estimation module by dynamically segmenting depth ranges into data-driven bins and allowing for fine-grained, pixel-wise depth prediction. Such adaptive binning addresses depth ambiguity in dense orchard scenes and allows for sharp separation of the foreground apples from the background. These modules combined provide a unified solution—enhancing detection accuracy as well as spatial comprehension. The model ably handles issues like fruit grouping, scale variability, and occlusion, thus making DeepHarvestNet a resilient architecture specific to real-world orchard settings, particularly for applications like automatic harvesting and precision crop monitoring.</p><p>To compare the detection performance of DeepHarvestNet, we performed a comparative analysis with classical and state-of-the-art fruit detection models, as shown in <xref ref-type="fig" rid="fig_11">Figure 11</xref>. The proposed DeepHarvestNet had the best detection accuracy of 95.0%, surpassing recent state-of-the-art architectures like O2RNet (94.0%) and YOLO11 with Vision Transformers (94.0%). These models were tailored to solve occlusion and grouped fruit detection, but DeepHarvestNet proved better performance because of the incorporated architectural modules—Efficient Bidirec- tional Cross-Attention (EBCA) for separating overlaps, Focal Modulation for feature extraction resilient to occlusion, and KernelWarehouse Convolution with AdaBins for depth-guided, scale-aware localization.</p><p>In comparison with slightly older but still applicable techniques like YOLOv7 with attention (91.5%) and multi- scale YOLOv5 (89.2%), DeepHarvestNet achieves a dramatic improvement. This performance difference signifies the shortcomings of existing models in dealing with dense fruit structures and spatial ambiguity. Interestingly, traditional models such as Faster R-CNN utilized in Bargoti and Underwood [<xref ref-type="bibr" rid="ref_1">1</xref>] and Sa et al. [<xref ref-type="bibr" rid="ref_2">2</xref>] had much lower accuracies of 86.3% and 84.7%, respectively, due to difficulties in keeping pace with contemporary orchard complexity without attention or depth-aware modules. These outcomes confirm that DeepHarvestNet not only performs state-of-the-art accuracy but also shows stable robustness under actual orchard conditions and therefore is highly appropriate for precision agriculture use cases like robotic harvesting and extensive orchard monitoring.</p>
          
            <fig id="fig_11">
              <label>Figure 11</label>
              <caption>
                <title>Comparative analysis of detection accuracy between DeepHarvestNet and state-of-the-art fruit detection models in orchard settings</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_IGR3dcA0RBprrcYU.png"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>5. Conclusion and future scope</title>
      <p>The current research proposed DeepHarvestNet, a dedicated apple fruit detection and depth estimation model optimized to tackle the vital issues of occlusion, overlap, and scale variance in intricate orchard scenes. By implementing Efficient Bidirectional Cross-Attention, Focal Modulation, and KernelWarehouse Convolution in the YOLOv8 backbone, the developed model gains better spatial localization and resilience, especially in detecting heavily clustered and partially occluded apples. In addition, the AdaBins module integration allows for precise monocular depth estimation by adaptively estimating scene geometry, successfully separating foreground apples from background objects. The experimental results of achieving a Precision of 0.94, Recall of 0.95, and F1-Score of 0.95 show that DeepHarvestNet surpasses baseline and state-of-the-art models consistently under normal and difficult light conditions. The shown effectiveness and robustness of this method point to its high prospects for implementation in precision agriculture tasks, such as autonomous apple harvesting and orchard monitoring systems.</p><p style="text-align: justify">In future research, adding a real-time tracking module like ByteTrack or DeepSORT would allow for ongoing fruit counting and production estimation over time. Moreover, adding the ability to detect multiple classes of fruits and grade ripeness can enhance its use across different orchard conditions.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>2017</volume>
          <page-range>3626–3633</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bargoti</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Underwood</surname>
              <given-names>J. P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICRA.2017.7989417</pub-id>
          <article-title>Deep fruit detection in orchards</article-title>
          <source>IEEE Int. Conf. Robot. Autom.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <page-range>1222</page-range>
          <issue>8</issue>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>I. Sa</surname>
            </name>
            <name>
              <surname>Z. Ge</surname>
            </name>
            <name>
              <surname>F. Dayoub</surname>
            </name>
            <name>
              <surname>B. Upcroft</surname>
            </name>
            <name>
              <surname>T. Perez</surname>
            </name>
            <name>
              <surname>McCool</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s16081222</pub-id>
          <article-title>DeepFruits: A fruit detection system using deep neural networks</article-title>
          <source>Sens</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>919</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>K. Lin</surname>
            </name>
            <name>
              <surname>L. Gong</surname>
            </name>
            <name>
              <surname>Y. Huang</surname>
            </name>
            <name>
              <surname>C. Liu</surname>
            </name>
            <name>
              <surname>J. Pan</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fpls.2019.00155</pub-id>
          <article-title>Deep learning-based segmentation and quantification of cucumber flowers and fruits in a plant factory</article-title>
          <source>Front. Plant Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <page-range>206–211</page-range>
          <issue>147</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>P. Chu</surname>
            </name>
            <name>
              <surname>Z. Li</surname>
            </name>
            <name>
              <surname>K. Lammers</surname>
            </name>
            <name>
              <surname>R. Lu</surname>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.patrec.2021.04.022</pub-id>
          <article-title>Deep learning-based apple detection using a suppression mask R-CNN</article-title>
          <source>Pattern Recognit. Lett.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>100130</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>P. Siricharoen</surname>
            </name>
            <name>
              <surname>W. Yomsatieankul</surname>
            </name>
            <name>
              <surname>and  Bunsri</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.atech.2022.100130</pub-id>
          <article-title>Fruit maturity grading framework for small dataset using single image multi-object sampling and mask R-CNN</article-title>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>8054</page-range>
          <issue>14</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>A. Janowski</surname>
            </name>
            <name>
              <surname>R. Kaz´mierczak</surname>
            </name>
            <name>
              <surname>C. Kowalczyk</surname>
            </name>
            <name>
              <surname>Szulwic</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/su13148054</pub-id>
          <article-title>Detecting apples in the wild: Potential for harvest quantity estimation</article-title>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>116</volume>
          <page-range>8-19</page-range>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gongal</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Amatya</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Karkee</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Lewis</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.11975/j.issn.1002-6819.2018.07.022</pub-id>
          <article-title>Sensors and systems for fruit detection and localization: A review</article-title>
          <source>Comput. Electron. Agric.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>34</volume>
          <page-range>173–179</page-range>
          <issue>7</issue>
          <person-group person-group-type="author">
            <name>
              <surname>Y. Xue</surname>
            </name>
            <name>
              <surname>N. Huang</surname>
            </name>
            <name>
              <surname>S. Tu</surname>
            </name>
            <name>
              <surname>L. Mao</surname>
            </name>
            <name>
              <surname>A. Yang</surname>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>Immature mango detection based on improved YOLOv2</article-title>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>157</volume>
          <page-range>417–426</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Y. Tian</surname>
            </name>
            <name>
              <surname>G. Yang</surname>
            </name>
            <name>
              <surname>Z. Wang</surname>
            </name>
            <name>
              <surname>H. Wang</surname>
            </name>
            <name>
              <surname>E. Li</surname>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compag.2019.01.012</pub-id>
          <article-title>Apple detection during different growth stages in orchards using the improved YOLO-V3 model</article-title>
          <source>Comput. Electron. Agric.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>4803</page-range>
          <issue>14</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Parico</surname>
              <given-names>A. I. B.</given-names>
            </name>
            <name>
              <surname>Ahamed</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s21144803</pub-id>
          <article-title>Real time pear fruit detection and counting using YOLOv4 models and deep SORT</article-title>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>185</volume>
          <page-range>111808</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Zhipeng</given-names>
            </name>
            <name>
              <surname>Jin</surname>
              <given-names>Luoyi</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Shuai</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Huirong</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.postharvbio.2021.111808</pub-id>
          <article-title>Apple stem/calyx real-time recognition using YOLO-v5 algorithm for fruit automatic loading system</article-title>
          <source>Postharvest Biology and Technology</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>1419</page-range>
          <issue>5</issue>
          <year>2203</year>
          <person-group person-group-type="author">
            <name>
              <surname>L. Ma</surname>
            </name>
            <name>
              <surname>L. Zhao</surname>
            </name>
            <name>
              <surname>Z. Wang</surname>
            </name>
            <name>
              <surname>J. Zhang</surname>
            </name>
            <name>
              <surname>and  Chen</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/agronomy13051419</pub-id>
          <article-title>Detection and counting of small target apples under complicated environments by using improved YOLOv7-Tiny</article-title>
          <source>Agronomy</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>1581</page-range>
          <issue>7</issue>
          <year>2205</year>
          <person-group person-group-type="author">
            <name>
              <surname>Y. Liu</surname>
            </name>
            <name>
              <surname>X. Han</surname>
            </name>
            <name>
              <surname>H. Zhang</surname>
            </name>
            <name>
              <surname>S. Liu</surname>
            </name>
            <name>
              <surname>W. Ma</surname>
            </name>
            <name>
              <surname>Y. Yan</surname>
            </name>
            <name>
              <surname>L. Sun</surname>
            </name>
            <name>
              <surname>L. Jing</surname>
            </name>
            <name>
              <surname>Y. Wang</surname>
            </name>
            <name>
              <surname>and J. Wang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/agronomy15071581</pub-id>
          <article-title>YOLOv8-MSP-PD: A lightweight YOLOv8-based detection method for Jinxiu malus fruit in field conditions</article-title>
          <source>Agronomy</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>1998–2009</page-range>
          <issue>9</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>G. Li</surname>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <given-names>J. Ai</given-names>
            </name>
            <name>
              <surname>Z. Yi</surname>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1049/ipr2.12171</pub-id>
          <article-title>Lemon-YOLO: An efficient object detection method for lemons in the natural environment</article-title>
          <source>IET Image Process.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>114</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Y. Lin</surname>
            </name>
            <name>
              <surname>Z. Huang</surname>
            </name>
            <name>
              <surname>Y. Liang</surname>
            </name>
            <name>
              <surname>Y. Liu</surname>
            </name>
            <name>
              <surname>and  Jiang</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/agriculture14010114</pub-id>
          <article-title>AG-YOLO: A rapid citrus fruit detection algorithm with global context fusion</article-title>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>1798</page-range>
          <issue>10</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Y. Yu</surname>
            </name>
            <name>
              <surname>Y. Liu</surname>
            </name>
            <name>
              <surname>Y. Li</surname>
            </name>
            <name>
              <surname>C. Xu</surname>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/agriculture14101798</pub-id>
          <article-title>Object detection algorithm for citrus fruits based on improved YOLOv5 model</article-title>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>3099</volume>
          <page-range>020020</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Uriti</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Pothabathula</surname>
              <given-names>N. J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1063/5.0279360</pub-id>
          <article-title>A deep learning-based object detection approach enhanced with attention mechanism in YOLO</article-title>
          <source>AIP Conf. Proc.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>23</volume>
          <page-range>5425</page-range>
          <issue>12</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s23125425</pub-id>
          <article-title>Research on apple recognition algorithm in complex orchard environment based on deep learning</article-title>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>2304</page-range>
          <issue>12</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Xing</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/f14122304</pub-id>
          <article-title>YOLOv5-ACS: Improved model for apple detection and positioning in apple forests in complex scenes</article-title>
          <source>Forests</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>16848</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>J. Zhao</surname>
            </name>
            <name>
              <surname>C. Du</surname>
            </name>
            <name>
              <surname>Y. Li</surname>
            </name>
            <name>
              <surname>et al.</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-024-67526-4</pub-id>
          <article-title>YOLO-Granada: A lightweight attented YOLO for pomegranate fruit detection</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <page-range>25</page-range>
          <issue>2</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>E. Osco-Mamani</surname>
            </name>
            <name>
              <surname>O. Santana-Carbajal</surname>
            </name>
            <name>
              <surname>I. Chaparro-Cruz</surname>
            </name>
            <name>
              <surname>D. Ochoa</surname>
            </name>
            <name>
              <surname>S. Alcazar-Alay</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/ai6020025</pub-id>
          <article-title>The detection and counting of olive tree fruits using deep learning models in Tacna, Peru</article-title>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>arXiv:2208.11566</volume>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>N. Hani</surname>
            </name>
            <name>
              <surname>P. Roy</surname>
            </name>
            <name>
              <surname>Isler</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2208.11566</pub-id>
          <article-title>Apple counting using convolutional neural networks</article-title>
          <source>arXiv Prepr</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>45</volume>
          <page-range>15380–15393</page-range>
          <issue>12</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>T. Fischer</surname>
            </name>
            <name>
              <surname>X. Zhang</surname>
            </name>
            <name>
              <surname>C. Li</surname>
            </name>
            <name>
              <surname>H. Chen</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TPAMI.2023.3301975</pub-id>
          <article-title>QDTrack: Quasi-dense similarity learning for appearance-only multiple object tracking</article-title>
          <source>IEEE Trans. Pattern Anal. Mach. Intell.,</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>566–569</page-range>
          <issue>11s</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>I. K. Agung Enriko</surname>
            </name>
            <name>
              <surname>E. L. Istikhomah Puspita Sari</surname>
            </name>
            <name>
              <surname>I. A. Yamin</surname>
            </name>
            <name>
              <surname>N. Albar</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/IJISAE.2024.4477</pub-id>
          <article-title>Detection of fresh and root apples using the TensorFlow lite framework with EfficientDet-Lite2</article-title>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>10546</volume>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>T. A. Oei</surname>
            </name>
            <name>
              <surname>M. S. Wijaya</surname>
            </name>
            <name>
              <surname>J. B. Simanjuntak</surname>
            </name>
            <name>
              <surname>E. F. A. Sihotang</surname>
            </name>
            <name>
              <surname>Irwansyah</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Using YOLOv7 to detect the health and quality of fruits</article-title>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <issue>1552553</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>L. He</surname>
            </name>
            <name>
              <surname>D. Wu</surname>
            </name>
            <name>
              <surname>X. Zheng</surname>
            </name>
            <name>
              <surname>F. Xu</surname>
            </name>
            <name>
              <surname>S. Lin</surname>
            </name>
            <name>
              <surname>S. Wang</surname>
            </name>
            <name>
              <surname>F. Ni</surname>
            </name>
            <name>
              <surname>F. Zheng</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">/10.3389/fpls.2025.1552553</pub-id>
          <article-title>RLK-YOLOv8: Multi-stage detection of strawberry fruits throughout the full growth cycle in greenhouses based on large kernel convolutions and improved YOLOv8</article-title>
          <source>Front. Plant Sci</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <issue>1541266</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>D. Wang</surname>
            </name>
            <name>
              <surname>H. Song</surname>
            </name>
            <name>
              <surname>and  Wang</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fpls.2025.1541266</pub-id>
          <article-title>YO-AFD: An improved YOLOv8-based deep learning approach for rapid and accurate apple flower detection</article-title>
          <source>Front. Plant Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <volume>arXiv:2203.11926</volume>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>J. Yang</surname>
            </name>
            <name>
              <surname>C. Li</surname>
            </name>
            <name>
              <surname>X. Dai</surname>
            </name>
            <name>
              <surname>L. Yuan</surname>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Focal modulation networks</article-title>
          <source>arXiv Prepr.</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <volume>arXiv:2406.07879</volume>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2406.07879</pub-id>
          <article-title>KernelWarehouse: Rethinking the design of dynamic convolution</article-title>
          <source>arXiv Prepr.</source>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Bhat</surname>
              <given-names>S. F.</given-names>
            </name>
            <name>
              <surname>Alhashim</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Wonka</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>AdaBins: Depth estimation using adaptive bins</article-title>
          <source>IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>
          <publisher-name>Nashville, TN, USA</publisher-name>
          <year>2021</year>
          <page-range>4008–4017</page-range>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/CVPR46 437.2021.00400</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="conf-paper">
          <page-range>12 159–12168</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bochkovskiy.</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ranftl</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Koltun</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <article-title>Vision transformers for dense prediction</article-title>
          <source>IEEE/CVF International Conference on Computer Vision</source>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <volume>arXiv:2004.10934</volume>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>A. Bochkovskiy</surname>
            </name>
            <name>
              <surname>C. Y. Wang</surname>
            </name>
            <name>
              <surname>Liao</surname>
              <given-names>H. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2004.10934</pub-id>
          <article-title>YOLOv4: Optimal speed and accuracy of object detection</article-title>
          <source>arXiv Prepr.</source>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <volume>arXiv:2407.20892</volume>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Khanam</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Hussain</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>What is YOLOv5: A deep look into the internal features of the popular object detector</article-title>
          <source>arXiv Prepr.</source>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>C. Y.</given-names>
            </name>
            <name>
              <surname>Bochkovskiy</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Liao</surname>
              <given-names>H. Y. M.</given-names>
            </name>
          </person-group>
          <article-title>YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</article-title>
          <source>IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>
          <publisher-name>Vancouver, BC, Canada</publisher-name>
          <year>2023</year>
          <page-range>7464–7475</page-range>
        </element-citation>
      </ref>
      <ref id="ref_35">
        <label>35.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Varghese</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <given-names>S. M.</given-names>
            </name>
          </person-group>
          <article-title>YOLOv8: A novel object detection algorithm with enhanced performance and robustness</article-title>
          <source>International Conference on Advanced Data Engineering and Intelligent Computing Systems</source>
          <publisher-name>Chennai, India,</publisher-name>
          <year>2024</year>
          <page-range>1-6</page-range>
          <pub-id pub-id-type="doi">10.1109/adics58448.2024.10533619</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_36">
        <label>36.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>415–424</page-range>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>et al</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s41095-022-0274-8</pub-id>
          <article-title>PVT v2: Improved baselines with pyramid vision transformer</article-title>
        </element-citation>
      </ref>
      <ref id="ref_37">
        <label>37.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Tu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Talebi</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Milanfar</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Bovik</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>MaxVit: Multi-Axis vision transformer</article-title>
          <source>European Conference on Computer Vision</source>
          <year>2022</year>
          <page-range>459–479.</page-range>
        </element-citation>
      </ref>
      <ref id="ref_38">
        <label>38.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Godard</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Aodha</surname>
              <given-names>O. M.</given-names>
            </name>
            <name>
              <surname>Firman</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Brostow</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Digging into self-supervised monocular depth estimation</article-title>
          <source>IEEE/CVF International Conference on Computer Vision</source>
          <publisher-name>Seoul, Korea (South)</publisher-name>
          <year>2019</year>
          <issue>3827–3837</issue>
        </element-citation>
      </ref>
      <ref id="ref_39">
        <label>39.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Ding</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>et al.</surname>
            </name>
          </person-group>
          <article-title>ResRep: Lossless CNN pruning via decoupling remembering and forgetting</article-title>
          <source>IEEE/CVF International Conference on Computer Vision</source>
          <publisher-name>Montreal, QC, Canada</publisher-name>
          <year>2021</year>
          <page-range>4490–4500</page-range>
        </element-citation>
      </ref>
      <ref id="ref_40">
        <label>40.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>1447855</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>G. Song</surname>
            </name>
            <name>
              <surname>J. Wang</surname>
            </name>
            <name>
              <surname>R. Ma</surname>
            </name>
            <name>
              <surname>Y. Shi</surname>
            </name>
            <name>
              <surname>and  Wang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fpls.2024.1447855</pub-id>
          <article-title>Study on the fusion of improved YOLOv8 and depth camera for bunch tomato stem picking point recognition and localization</article-title>
        </element-citation>
      </ref>
      <ref id="ref_41">
        <label>41.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Mao</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>C.-Y. Wu</surname>
              <given-names>C.Y.</given-names>
            </name>
            <name>
              <surname>Feichtenhofer</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Darrell</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>A ConvNet for the 2020s</article-title>
          <source>IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>
          <publisher-name>New Orleans, LA, USA</publisher-name>
          <year>2022</year>
          <page-range>11966–11 976</page-range>
        </element-citation>
      </ref>
      <ref id="ref_42">
        <label>42.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Uriti</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Pothabathula</surname>
              <given-names>N. J.</given-names>
            </name>
          </person-group>
          <article-title>Evaluating object detection approaches for fruit detection in precision agriculture: A comprehensive review,</article-title>
          <source>International Conference on Intelligent Computing and Sustainable Innovation Technologies</source>
          <publisher-name>Bhubaneswar, India</publisher-name>
          <year>2024</year>
          <page-range>1-6</page-range>
        </element-citation>
      </ref>
      <ref id="ref_43">
        <label>43.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Zhu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Ke</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Lau</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>BiFormer: Vision transformer with Bi-Level routing attention</article-title>
          <source>IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>
          <publisher-name>Vancouver, BC, Canada</publisher-name>
          <year>2023</year>
          <page-range>10323– 10333</page-range>
        </element-citation>
      </ref>
      <ref id="ref_44">
        <label>44.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>100284</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>P. Chu</surname>
            </name>
            <name>
              <surname>Z. Li</surname>
            </name>
            <name>
              <surname>K. Zhang</surname>
            </name>
            <name>
              <surname>D. Chen</surname>
            </name>
            <name>
              <surname>K. Lammers</surname>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.atech.2023.100284</pub-id>
          <article-title>O2RNet: Occluder-occludee relational network for robust apple detection in clustered orchard environments</article-title>
          <source>Smart Agric. Technol</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>