<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-niqa2XZ2QAVGohV_vVo81VcB3fiWzI9o</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml020301</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Advanced Hybrid Segmentation Model Leveraging AlexNet Architecture for Enhanced Liver Cancer Detection</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8055-1499</contrib-id>
          <name>
            <surname>Nagireddy</surname>
            <given-names>Venkata Raja Sekhar Reddy</given-names>
          </name>
          <email>rajasekhar.nv1@gmail.com</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1079-2665</contrib-id>
          <name>
            <surname>Shaik</surname>
            <given-names>Khaja Shareef</given-names>
          </name>
          <email>khaja.sk0822@gmail.com</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <aff id="aff_1">Department of Information Technology, MLR Institute of Technology, 500043 Hyderabad, India</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>21</day>
        <month>08</month>
        <year>2023</year>
      </pub-date>
      <volume>2</volume>
      <issue>3</issue>
      <fpage>116</fpage>
      <lpage>128</lpage>
      <page-range>116-128</page-range>
      <history>
        <date date-type="received">
          <day>01</day>
          <month>07</month>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <day>11</day>
          <month>08</month>
          <year>2023</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2023 by the author(s)</copyright-statement>
        <copyright-year>2023</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Liver cancer, one of the rapidly escalating forms of cancer, remains a principal cause of mortality globally. Its death rates can be attenuated through vigilant monitoring and early detection. This study aims to develop a sophisticated model to assist medical professionals in the classification of liver tumours using biopsy tissue images, thereby facilitating preliminary diagnosis.The study presents a novel, bio-inspired deep learning strategy purposed for augmenting liver cancer detection. The uniqueness of this approach rests in its two-fold contribution: Firstly, an innovative hybrid segmentation technique, integrating the SegNet network, UNet network, and Al-Biruni Earth Radius (BER) procedure, is introduced to extract liver lesions from Computed Tomography (CT) images. The algorithm initially applies the SegNet to isolate the liver from the abdominal image in a CT scan. Since hyperparameters significantly influence segmentation performance, the BER algorithm is hybridized with each network for optimal tuning. The method proposed herein is inspired by the pursuit of a common objective by swarm members. Al-Biruni's methodology for calculating Earth's radius sets the search space, extending beyond local solutions that require exploration. Secondly, a pre-trained AlexNet model is utilized for diagnosis, further enhancing the method's effectiveness. The proposed segmentation and classification algorithms have been compared with contemporary state-of-the-art techniques. The results demonstrated that in terms of specificity, F1-score, accuracy, and computational time, the proposed method outperforms its competitors, indicating its potential in advancing liver cancer detection.</p></abstract>
      <kwd-group>
        <kwd>Liver cancer detection</kwd>
        <kwd>Al-Biruni earth radius optimization</kwd>
        <kwd>U-Net</kwd>
        <kwd>SegNet</kwd>
        <kwd>Computed tomography</kwd>
        <kwd>AlexNet</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="2"/>
        <fig-count count="6"/>
        <table-count count="5"/>
        <ref-count count="24"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>The liver, the largest internal organ in the human body, carries out an array of vital functions owing to its strategic location, vast size, and multifaceted capabilities. Unfortunately, it is frequently the site of one of the deadliest cancers globally, liver cancer [<xref ref-type="bibr" rid="ref_1">1</xref>]. Early detection is paramount for the prevention and management of liver cancer. Alas, the disease often remains asymptomatic in its nascent stages, rendering early detection challenging, despite its propensity for devastating liver cells [<xref ref-type="bibr" rid="ref_2">2</xref>].</p><p>Liver cancer can be broadly classified into two categories - primary and secondary. Primary liver cancers originate within the liver cells, with Hepatocellular Carcinoma (HCC) being the most prevalent type, constituting about 80% of initial liver malignancies [<xref ref-type="bibr" rid="ref_3">3</xref>]. Secondary liver cancers, often referred to as Metastases (MET), originate in other organs and subsequently metastasize to the liver. The treatment and presentation of these two types are markedly different, despite both having cancerous liver cells [<xref ref-type="bibr" rid="ref_4">4</xref>], [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>].</p><p>Liver tumours can be detected through a myriad of diagnostic techniques, including blood tests, biopsies, and imaging scans. Among these, medical image processing stands out as a non-invasive or minimally invasive method [<xref ref-type="bibr" rid="ref_7">7</xref>]. Non-invasive imaging technologies such as MRI, X-ray, ultrasound, and CT scans have been employed in the diagnosis of liver tumours [<xref ref-type="bibr" rid="ref_8">8</xref>]. These methods allow for tracking the progression of cancer throughout the body, ascertaining its initial location and size. CT scans are particularly efficacious for liver cancer detection, producing two-dimensional images of the organ from various angles [<xref ref-type="bibr" rid="ref_9">9</xref>]. However, the manual analysis of the copious data generated by medical image processing is time-consuming. Therefore, the application of automated computer-aided diagnostic (CAD) approaches is essential for real-time, accurate tumour detection [<xref ref-type="bibr" rid="ref_10">10</xref>].</p><p>Traditional analysis of CT scans of the liver has been manual, a process that is expensive, time-consuming, and prone to errors. To rectify these issues and enhance liver cancer detection, several computational techniques have been proposed. However, these systems have been inadequate in identifying liver lesions due to the complexity of the liver and surrounding organs, small tumour sizes, and irregular tumour growth [<xref ref-type="bibr" rid="ref_11">11</xref>]. Hence, a novel approach is necessitated. Studies have shown that Convolutional Neural Networks (CNNs) can significantly enhance radiological diagnosis without the need to train the system to recognize specific radiological traits [<xref ref-type="bibr" rid="ref_12">12</xref>]. A fusion of the expertise of radiologists and the processing power of AI systems could dramatically improve the efficacy and safety of patient care.</p><p>This study proposes a completely bio-inspired approach for liver cancer detection using CT scans, which stands in contrast to contemporary systems based on either feature engineering methods or hybrid procedures. It integrates BER-optimized deep learning models for the analysis of lesions. The key contributions of this paper include:</p><p>• A comprehensive survey introducing current state-of-the-art procedures for diagnosing liver cancer and other malignancies.</p><p>• The proposal of a novel hybrid segmentation method using the Seg-Net network [<xref ref-type="bibr" rid="ref_13">13</xref>], the UNet [<xref ref-type="bibr" rid="ref_14">14</xref>], and BER for CT images. This approach employs the SegNet network to identify liver tissue in an abdominal CT image, and the UNet network to detect liver lesions within that tissue.</p><p>• The tuning of the deep learning network’s hyperparameters, which significantly influence its segmentation performance, using BER optimization components. As a result, the proposed method can yield near-optimal segmentation results when applied to liver lesions, compared to best-in-class procedures.</p><p>The rest of the paper is structured as follows: Section 2 elaborates on the proposed methodology, while Section 4 presents the results of the trials and their discussion. The study concludes in Section 5 with a discussion on future research prospects.</p>
    </sec>
    <sec sec-type="">
      <title>2. Related works</title>
      <p>A substantial amount of research has been conducted in the field of liver cancer detection using advanced computational techniques. This section provides a review of the significant contributions made by various researchers.</p><p>Zhang et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] developed an innovative FSVM+ based method that employs a feature SVM+ framework for the transfer learning problem. With a specific focus on increasing the gap between classes, the FSVM+ transformation matrix was utilized to minimize the encompassing data ball's dimension. They cleverly assigned appropriate weights to each Contrast-Enhanced Ultrasound (CEUS) image by calculating the maximum mean divergence. Their experimental outcomes, based on a dataset of bi-modal images from liver cancer patients, indicated an impressive improvement in the Computer Aided Design (CAD) model's performance.</p><p>Zhao et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] proposed a pioneering machine learning methodology aimed at constructing an automated Hepatocellular Carcinoma staging system that incorporates a significantly larger number of clinical features than existing systems. Their approach, based on random survival trees, utilised B-splines to transform functions into vectors in a low-dimensional space, allowing for the grouping of similar patients into staging cohorts. The performance of their final staging system significantly surpassed the Barcelona Clinic Liver Cancer (BCLC) system in differentiating patients at diverse stages.</p><p>In the pursuit of early disease prediction using Machine Learning (ML) methods, Dritsas and Trigka [<xref ref-type="bibr" rid="ref_17">17</xref>] evaluated various ML models and Ensemble techniques. Their results demonstrated that the Voting Classifier outperformed other models in predicting liver disease occurrence, achieving an Area Under the Curve (AUC) of 88.4%, accuracy of 80.1%, recall of 80.1%, and F-measure of 80.1%.</p><p>Deshmukh et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] concentrated on the examination of images of two subtypes of cancer. Their framework, evaluated with 2871 images, achieved remarkable precision using a dual hybrid model. Their approach involved a result prioritizer that decided the most suitable model for image analysis based on the outputs of both neural networks. This deep learning system offered valuable insights into the defining features contributing to predictions.</p><p>Md et al. [<xref ref-type="bibr" rid="ref_19">19</xref>] employed a variety of data preparation techniques in their model, including accuracy with imputation to fill in gaps left by missing values. Their approach involved the application of the Log1p transformation on skewed columns, followed by normalization, and the use of feature selection techniques such as univariate analysis, feature importance, and correlation matrices. Their model, trained on enhanced preprocessed data using ensemble learning techniques such as Random Forest, Extra Tree, and Stacking, achieved a testing accuracy of 86.06%. This result outperformed those of prior models, thereby emphasizing the efficacy of their approach in identifying liver disease.</p><p>Huang et al. [<xref ref-type="bibr" rid="ref_20">20</xref>] exploited Raman spectroscopy to examine human hepatic tissue samples for in vitro cancer. The authors demonstrated the potential of Raman spectroscopy, coupled with cancer tissue identification, including subtype and stage, in differentiating carcinoma tissues from surrounding non-tumor tissues in a rapid, non-invasive, and label-free manner. The potential of a portable Raman device for real-time intraoperative diagnosis of human liver cancer was also highlighted.</p><p>Further, the correct diagnosis and determination of a patient's survival time rely critically on effective treatment [<xref ref-type="bibr" rid="ref_21">21</xref>], [<xref ref-type="bibr" rid="ref_22">22</xref>], [<xref ref-type="bibr" rid="ref_23">23</xref>]. Histopathological images, often deemed the gold standard for diagnosing liver cancer, accurately represent various stages of the disease. Research into the intelligent categorization of histological images of liver cancer with differing degrees of differentiation can significantly benefit liver cancer patients, despite the existing challenges associated with this categorization.</p><p>The aforementioned studies demonstrate the extensive application of machine learning and computational techniques in the detection and diagnosis of liver cancer. This paper seeks to build upon these contributions by proposing a bio-inspired approach for liver cancer detection using CT scans and BER-optimized deep learning models.</p>
    </sec>
    <sec sec-type="">
      <title>3. Methodology</title>
      <p>In this section, we delve into the databases leveraged by our proposed model. A comprehensive examination of the proposed methodology is also presented. <xref ref-type="fig" rid="fig_1">Figure 1</xref> provides a high-level schematic representation of the proposed method's architecture.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Working flow of the research work</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/7/img_E59W-Mv8clHiWtgV.png"/>
        </fig>
      
      
        <sec>
          
            <title>3.1. Dataset description</title>
          
          
            <sec>
              
                <title>3.1.1 Lits17 dataset</title>
              
              <p>Our proposed approach was evaluated using the LiTS17 dataset [<xref ref-type="bibr" rid="ref_24">24</xref>], specifically designed for the tumor lesion problem. This dataset provides a training set comprised of 130 CT scans.</p>
            </sec>
          
          
            <sec>
              
                <title>3.1.2 3d-ircadb-01 dataset</title>
              
              <p>The 3D-IRCADb-01 dataset incorporates 3D CT images from twenty patients, with liver tumors present in 75% of these cases. The typical liver densities range from 40 to 135. Each image is a square, with a resolution of 512 by 512 pixels. Segmentation was performed on the DICOM images with labels.</p><p>In this investigation, 10% of all CT scan images were reserved for testing purposes. This test set was comprised of 26 randomly selected images, chosen using Python packages. Additionally, 10% of the remaining training set was partitioned for validation, leaving 80% for subsequent training. Consequently, the final distribution for the study comprised 80% training, 10% validation, and 10% testing from the remaining images post-validation.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Ct image preprocessing</title>
          
          <p>The proposed method is designed for the segmentation of liver tumor images for clinical application. The process involves the utilization of data augmentation, preprocessing, and a Convolutional Neural Network (CNN) to diagnose tumors in the liver and its surrounding organs. During the preprocessing phase of CT scans, focus is placed predominantly on the liver, with surrounding organs disregarded.</p><p>Next, the image undergoes histogram equalization, a process aimed at enhancing image contrast. This procedure is followed by data amplification, employing insights derived from the augmentation steps to train the necessary invariant features.</p><p>Every medical image analysis system deploys image preprocessing to enhance the quality of the initial input image. This can involve a variety of techniques, including but not limited to noise reduction and enhancement methods. The necessity of preprocessing derives from its impact on subsequent processes, which depend heavily on image quality for defining blocks and feature extraction.</p><p>During normalization and scaling processes, the image values are adjusted and the range is narrowed, facilitating the precision of the classifier. Noise reduction improves the performance of image processing operations such as edge detection, segmentation, and compression. Contemporary medical imaging can advantageously employ either a spatial domain or a spectral domain method for noise reduction.</p><p>Mean filtering is a method in which each pixel is replaced by the average of its neighbors, resulting in a softened and blurred image. The adaptive mean filtering method maintains edges and features by utilizing local image statistics, such as mean, variance, and correlation. By changing the original value to the local mean, noise is significantly reduced.</p><p>This filter learns local image characteristics and assists in the removal of noise from specific regions. The statistic filter produces less blur than the mean filter while retaining edge sharpness. The values are estimated using a maximum a posteriori filter and an unobserved signal to maximize the Bayes theorem.</p><p>Noise in medical imaging can also be reduced using the Curvelet transform, an indexed frame multi-scale transformation that allows for scale, position, and element indexing. Histogram equalization is employed in image processing to improve the visibility of organs. This process enables more precise segmentation of the liver tumor.</p><p>Volumes of tumors and liver masks are stored separately for each CT imaging slice dataset, contributing to a comprehensive and detailed data repository for the study.</p>
          
            <sec>
              
                <title>3.2.1 Data augmentation</title>
              
              <p>The application of data augmentation techniques allows the envisaged system to encounter a broader diversity of tumors. <xref ref-type="fig" rid="fig_2">Figure 2</xref> showcases several instances of images that have been enhanced through this method. In this strategy, images with new features—derived from the original set but subjected to existing procedures—are incorporated into both the training and testing datasets. This supplemental information enables the assessment of the constructed models' capacity to identify images that have undergone rotation or magnification. If the models can recognize shapes in a specific orientation—or indeed any orientation—they retain these shapes, thereby generating accurate results.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. Proposed hybrid algorithm for liver tumor segmentation</title>
          
          <p>Various architectures are leveraged by Convolutional Neural Networks (CNN) to address classification problems. Recently, semantic segmentation has seen a notable increase in the usage of SegNet and UNet. However, the accuracy of the segmentation is intrinsically linked to the network hyperparameters. To achieve near-optimal segmentation results, adjustment of these hyperparameters is required. Selecting the appropriate hyperparameters necessitates the use of state-of-the-art optimization techniques. Such algorithms are capable of efficiently searching the space of potential solutions on a global scale. Our proposed hybrid method, termed SegNet-UNet-BER [<xref ref-type="bibr" rid="ref_23">23</xref>], amalgamates the advantages of SegNet and UNet deep learning architectures with the benefits of BER optimization to maximize efficiency.</p>
          
            <sec>
              
                <title>3.3.1 Liver segmentation from the ct image using network</title>
              
              <p>Abdominal CT scans encompass more than just the liver, necessitating the extraction of the liver as a crucial step in obtaining an accurate cancer diagnosis. In this context, a Convolutional Neural Network (CNN) utilizing a SegNet architecture is employed. This architecture has been effectively utilized for semantic segmentation tasks at the pixel level. The SegNet architecture is predicated on an encoder-decoder mechanism that culminates in a final layer.</p><p>The encoder constituent of SegNet is constructed from a sequence of stacked layers, with a max-pooling layer separating the convolutional layers within each cluster. This is demonstrated by the first 13 convolutional layers of the VGG16 architecture. In these layers, input from a filter bank undergoes convolution to generate the required number of feature maps. Subsequently, the resulting feature maps are subjected to batch normalization. The pixel-wise operation, the Rectified Linear Unit (ReLU) procedure, is then enacted, with the output being max(0, k). A max-pooling layer is utilized here to achieve downsampling by a factor of 2, as evidenced by the use of a 2x2 window and a stride of 2. SegNet relies extensively on max-pooling to attain translational invariance. However, the loss of boundary information during segmentation presents a challenge with this approach. Prior to the implementation of the max-pooling operation, the feature maps of the encoder are indexed to retain boundary information. In practice, the location of the highest value pixel in the feature map window is recorded.</p><p>The SegNet decoder mirrors the layer structure of the encoder but operates in reverse. The input maps are initially upsampled, resulting in a sparse feature map using the remembered max-pooling indices. The decoder's filter banks then convolve to generate a dense feature map. Batch normalization is applied subsequent to the convolution, as is the case in the encoder. In the final stage of the decoder, the pixels are activated with a softmax activation function before being relayed to the output layer. The desired segmentation is achieved by assigning each pixel to a specific category.</p>
              
                <fig id="fig_2">
                  <label>Figure 2</label>
                  <caption>
                    <title>Illustration of the SegNet-UNet-BER method, using UNet for liver lesion segmentation in CT images, with BER optimizing the segmentation results</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/7/img_iv8mMs7CTDd6NqPr.png"/>
                </fig>
              
            </sec>
          
          
            <sec>
              
                <title>3.3.2 Lesion segmentation from the liver tissue utilizing the unet system</title>
              
              <p>The isolation of lesions within liver tissue is a critical initial step for further examination. For this purpose, the UNet architecture is utilized. This framework has demonstrated effectiveness when applied to medical images. The input layer, as displayed in <xref ref-type="fig" rid="fig_2">Figure 2</xref>, accepts liver images in a 128×128×1 format. The UNet architecture is composed of three main components.</p><p>In the downsampling pathway, a max-pooling layer with a 2×2 window size and a stride value of 2 succeeds two convolutional layers. The input liver image undergoes two convolutions with a 3×3 filter, followed by activation with a ReLU filter. The output image retains the dimensions of the initial image, hence the padding value remains unchanged. Commencing with a value of eight in the convolution layer of the first group, the number of filters is incremented by a factor of two for each subsequent layer until the fifth layer.</p><p>Subsequently, upsampling is performed by halving the sample size of each group's feature maps. Within the UNet design, features from the layer are amalgamated into a single concatenation layer with the equivalent number of the group. This is followed by a ReLU activation function and then a pair of layers with a 3×3 convolutional filter. This series of layers is replicated from the sixth group through to the ninth. The tenth and final layer comprises a convolutional layer with a 1×1 filter and eight feature channels. In total, this architecture encompasses 27 layers: 18 convolutional layers paired with ReLU levels, 4 pooling layers, 4 up-convolution layers, and a solitary softmax layer.</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.3 Optimization of segmentation presentation using the ber procedure</title>
              
              <p>The BER method initially generates solution vectors. All feasible hyperparameter optimization values are included in each created vector. These numbers are used as inputs to the SegNet network's training procedure. Eq. (1) is used to calculate the fitness value of the BER's output hyperparameter vectors. To do this, we compare the predicted image <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>P</mi>
    </mrow>
  </math>
</inline-formula> to the ground truth image <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>G</mi>
    </mrow>
  </math>
</inline-formula> and calculate the contour matching score (<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>C</mi>
    </mrow>
  </math>
</inline-formula> score). Therefore, improving the F1-score, precision, and recall will determine the best approach for liver the abdominal CT scan. Three different masses <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <msub>
        <mi>w</mi>
        <mi>f</mi>
      </msub>
    </mrow>
  </math>
</inline-formula>, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <msub>
        <mi>w</mi>
        <mi>p</mi>
      </msub>
    </mrow>
  </math>
</inline-formula>, and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <msub>
        <mi>w</mi>
        <mi>r</mi>
      </msub>
    </mrow>
  </math>
</inline-formula> were recall, correspondingly.</p>
              
                <disp-formula>
                  <label>(1)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtext> Fitness </mtext>
                    <mo>=</mo>
                    <mo>−</mo>
                    <mo stretchy="false">(</mo>
                    <mo>,</mo>
                    <mo stretchy="false">)</mo>
                    <mi>C</mi>
                    <mi>score</mi>
                    <mi>P</mi>
                    <mi>G</mi>
                  </math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(2)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>C</mi>
                    <mi>score</mi>
                    <mi>P</mi>
                    <mi>G</mi>
                    <mi>F</mi>
                    <mo>−</mo>
                    <mo stretchy="false">(</mo>
                    <mo>,</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo>−</mo>
                    <mo stretchy="false">)</mo>
                    <mo>+</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>+</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <msub>
                      <mi>w</mi>
                      <mi>f</mi>
                    </msub>
                    <msub>
                      <mi>w</mi>
                      <mi>p</mi>
                    </msub>
                    <msub>
                      <mi>w</mi>
                      <mi>r</mi>
                    </msub>
                    <mn>1</mn>
                    <mtext> score </mtext>
                    <mtext> Precision </mtext>
                    <mtext> Recall </mtext>
                  </math>
                </disp-formula>
              
              <p>The precision and recall are totalled as shadows,</p>
              
                <disp-formula>
                  <label>(3)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtext> Precision </mtext>
                    <mo>=</mo>
                    <mfrac>
                      <mrow>
                        <mtext> True </mtext>
                        <mtext> Positive </mtext>
                      </mrow>
                      <mrow>
                        <mtext> True </mtext>
                        <mtext> Positive </mtext>
                        <mtext> False </mtext>
                        <mtext> Positive </mtext>
                        <mo>+</mo>
                      </mrow>
                    </mfrac>
                  </math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(4)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtext> Recall </mtext>
                    <mo>=</mo>
                    <mfrac>
                      <mtext> True Positive </mtext>
                      <mrow>
                        <mtext> True Positive </mtext>
                        <mtext> False Negative </mtext>
                        <mo>+</mo>
                      </mrow>
                    </mfrac>
                  </math>
                </disp-formula>
              
              <p>(1) Basic concepts and formulation</p><p>The goal of optimization procedures is to locate the best answer to a problem with a given set of restrictions. In BER, a vector can be used to represent a single member of the population, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mover>
        <mi>S</mi>
        <mo stretchy="false">→</mo>
      </mover>
    </mrow>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">{</mo>
      <mo>,</mo>
      <mo>,</mo>
      <mo>…</mo>
      <mo>;</mo>
      <mo data-mjx-texclass="CLOSE">}</mo>
      <msub>
        <mi>S</mi>
        <mn>1</mn>
      </msub>
      <msub>
        <mi>S</mi>
        <mn>2</mn>
      </msub>
      <msub>
        <mi>S</mi>
        <mi>d</mi>
      </msub>
    </mrow>
    <mo>=</mo>
    <mo>∈</mo>
    <msub>
      <mi>R</mi>
      <mi>d</mi>
    </msub>
  </math>
</inline-formula>, where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>d</mi>
    </mrow>
  </math>
</inline-formula> is the size and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <msub>
        <mi>S</mi>
        <mi>i</mi>
      </msub>
    </mrow>
  </math>
</inline-formula> is a parameter or feature in the optimization issue. The proposed method uses a fitness function f to assess an individual's performance up to point. The following stages of the optimization process are used to explore populations for a certain start with a collection of randomly chosen people (solutions). The fitness function, size are necessary for the optimization process to start.</p><p>(2) Exploration-exploitation balance</p><p>To better balance the demands of exploitation and exploration, the proposed strategy divides the population into subgroups and dynamically adapts the composition of each group. The population is divided into two groups at the outset of the process for exploration and exploitation. The exploration group makes up 70% of the population, whilst the misuse group makes up just 30% of the population. In order to increase the fitness values of people in each group, the number of persons in task is initially set at 30% and increased during the optimisation rounds to reach other hand, the exploratory group's membership falls from 70% to 30% during the course of the iterations. Thanks to this technique, the average level of people's fitness can be raised more clearly. Additionally, the elitism approach is employed if no better solution is discovered, preserving the process' leading solution to guarantee the population's convergence of the optimisation process. If utilising the BER optimisation process, another separate can be formed by performing the mutation procedure.</p><p>(3) Exploration-exploitation balance</p><p>• Exploration operation</p><p>In addition to finding interesting locations in the search space, exploration is in charge of moving away from local optima stagnation and towards the ideal answer, as will be detailed below.</p><p>• Heading towards the best solution</p><p>This method is used by the exploration group member to look for promising areas near its current location in the search space. This is done by repeatedly looking among nearby viable options for a better choice in terms of fitness value. The BER research uses the subsequent equations to achieve this.</p>
              
                <disp-formula>
                  <label>(5)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtext>r</mtext>
                    <mo>=</mo>
                    <mi>h</mi>
                    <mfrac>
                      <mrow>
                        <mi>cos</mi>
                        <mi>x</mi>
                        <mo data-mjx-texclass="NONE">⁡</mo>
                        <mo stretchy="false">(</mo>
                        <mo stretchy="false">)</mo>
                      </mrow>
                      <mrow>
                        <mn>1</mn>
                        <mo>−</mo>
                        <mo data-mjx-texclass="NONE">⁡</mo>
                        <mo stretchy="false">(</mo>
                        <mo stretchy="false">)</mo>
                        <mi>cos</mi>
                        <mi>x</mi>
                      </mrow>
                    </mfrac>
                  </math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(6)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>D</mi>
                        <mo stretchy="false">→</mo>
                      </mover>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>S</mi>
                        <mo stretchy="false">→</mo>
                      </mover>
                    </mrow>
                    <mo>=</mo>
                    <mo>⋅</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>−</mo>
                    <mo stretchy="false">)</mo>
                    <mover>
                      <msub>
                        <mi>r</mi>
                        <mn>1</mn>
                      </msub>
                      <mo>→</mo>
                    </mover>
                    <mi>t</mi>
                    <mn>1</mn>
                  </math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(7)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>S</mi>
                        <mo stretchy="false">→</mo>
                      </mover>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>S</mi>
                        <mo stretchy="false">→</mo>
                      </mover>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>D</mi>
                        <mo stretchy="false">→</mo>
                      </mover>
                    </mrow>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo>−</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mn>2</mn>
                      <mn>1</mn>
                      <msub>
                        <mrow data-mjx-texclass="ORD">
                          <mover>
                            <mi>r</mi>
                            <mo stretchy="false">→</mo>
                          </mover>
                        </mrow>
                        <mn>2</mn>
                      </msub>
                    </mrow>
                    <mo stretchy="false">(</mo>
                    <mo>+</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>+</mo>
                    <mo>⋅</mo>
                    <mi>t</mi>
                    <mi>t</mi>
                    <mn>1</mn>
                  </math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>S</mi>
      <mi>t</mi>
      <mo stretchy="false">(</mo>
      <mo stretchy="false">)</mo>
    </mrow>
  </math>
</inline-formula> is the solution vector at iteration, 0×180, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <msub>
        <mi>r</mi>
        <mn>1</mn>
      </msub>
    </mrow>
  </math>
</inline-formula> and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <msub>
        <mi>r</mi>
        <mn>2</mn>
      </msub>
    </mrow>
  </math>
</inline-formula> are coefficient vectors, and their values are given by Eq. (7). <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>h</mi>
    </mrow>
  </math>
</inline-formula> is a random number selected from the range [0, 2], where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>D</mi>
    </mrow>
  </math>
</inline-formula> is the diameter of the search agent's search region for gifted areas.</p><p>• Exploitation operation</p><p>The task of improving already implemented solutions falls to the exploitation team. The BER determines the best individual by calculating the fitness values of all participants at each cycle. To achieve exploitation, the BER uses two distinct strategies, which are described in more depth in the sections following.</p><p>• Heading towards the best solution</p><p>The subsequent reckonings are used to change the search agent to the best key:</p>
              
                <disp-formula>
                  <label>(8)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>S</mi>
                        <mo stretchy="false">→</mo>
                      </mover>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>S</mi>
                        <mo stretchy="false">→</mo>
                      </mover>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>D</mi>
                        <mo stretchy="false">→</mo>
                      </mover>
                    </mrow>
                    <mo stretchy="false">(</mo>
                    <mo>+</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>+</mo>
                    <mo stretchy="false">)</mo>
                    <mi>t</mi>
                    <mi>t</mi>
                    <mn>1</mn>
                    <msup>
                      <mi>r</mi>
                      <mn>2</mn>
                    </msup>
                  </math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(9)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>D</mi>
                        <mo stretchy="false">→</mo>
                      </mover>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>L</mi>
                        <mo stretchy="false">→</mo>
                      </mover>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>S</mi>
                        <mo stretchy="false">→</mo>
                      </mover>
                    </mrow>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>−</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo stretchy="false">)</mo>
                    <msub>
                      <mrow data-mjx-texclass="ORD">
                        <mover>
                          <mi>r</mi>
                          <mo stretchy="false">→</mo>
                        </mover>
                      </mrow>
                      <mn>3</mn>
                    </msub>
                    <mi>t</mi>
                    <mi>t</mi>
                  </math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mrow data-mjx-texclass="ORD">
        <mover>
          <mi>r</mi>
          <mo stretchy="false">→</mo>
        </mover>
      </mrow>
      <mn>3</mn>
    </msub>
  </math>
</inline-formula> is a chance vector intended using Eq. (7) that controls the drive ladders towards the best solution, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mover>
        <mi>S</mi>
        <mo stretchy="false">→</mo>
      </mover>
    </mrow>
    <mo stretchy="false">(</mo>
    <mo stretchy="false">)</mo>
    <mi>t</mi>
  </math>
</inline-formula> is the repetition <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>t</mi>
    </mrow>
  </math>
</inline-formula>, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mover>
        <mi>L</mi>
        <mo stretchy="false">→</mo>
      </mover>
    </mrow>
  </math>
</inline-formula> is the finest solution course, and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mover>
        <mi>D</mi>
        <mo stretchy="false">→</mo>
      </mover>
    </mrow>
  </math>
</inline-formula> refers to the distance vector.</p><p>• Investigating area around best solution</p><p>The area surrounding the best answer (leader) is the most talented. As a result, search close to the greatest option in the hopes of discovering a better one. The BER uses the subsequent equation to carry out this way.</p>
              
                <disp-formula>
                  <label>(10)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>S</mi>
                        <mo stretchy="false">→</mo>
                      </mover>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>k</mi>
                        <mo stretchy="false">→</mo>
                      </mover>
                    </mrow>
                    <mo stretchy="false">(</mo>
                    <mo>+</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>+</mo>
                    <mo stretchy="false">)</mo>
                    <mi>t</mi>
                    <mi>r</mi>
                    <mi>t</mi>
                    <mn>1</mn>
                    <mover>
                      <mrow>
                        <mi>S</mi>
                        <mo>∗</mo>
                      </mrow>
                      <mo>→</mo>
                    </mover>
                  </math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(11)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>k</mi>
                        <mo stretchy="false">→</mo>
                      </mover>
                    </mrow>
                    <mo>=</mo>
                    <mo>+</mo>
                    <mi>Z</mi>
                    <mfrac>
                      <mrow>
                        <mn>2</mn>
                        <mo>×</mo>
                        <msup>
                          <mi>t</mi>
                          <mn>2</mn>
                        </msup>
                      </mrow>
                      <msup>
                        <mi>N</mi>
                        <mn>2</mn>
                      </msup>
                    </mfrac>
                  </math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mover>
      <mrow>
        <mi>S</mi>
        <mo>∗</mo>
      </mrow>
      <mo>→</mo>
    </mover>
  </math>
</inline-formula> refers to the best answer, where <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>t</mi>
    </mrow>
  </math>
</inline-formula> is the iteration sum and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>N</mi>
    </mrow>
  </math>
</inline-formula> is the total sum of iterations, and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>z</mi>
    </mrow>
  </math>
</inline-formula> is a random number between [0, 1].</p><p>• Mutation operation</p><p>The BER uses the mutation as one more study method. The genetic operator responsible for maintaining and generating population variety. It may be seen as a local, probabilistic random disturbance of one or more distinct components. By aiding in the avoidance of local optima, it helps prevent early convergence; such a change in the search field serves as a launching pad for another intriguing subject. The strong exploration capability of the BER is affected by the mutation, in fact.</p>
              
                <disp-formula>
                  <label>(12)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>S</mi>
                        <mo stretchy="false">→</mo>
                      </mover>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mover>
                        <mi>k</mi>
                        <mo stretchy="false">→</mo>
                      </mover>
                    </mrow>
                    <mo stretchy="false">(</mo>
                    <mo>+</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo>∗</mo>
                    <mo>−</mo>
                    <mi>t</mi>
                    <mi>h</mi>
                    <mn>1</mn>
                    <msup>
                      <mi>z</mi>
                      <mn>2</mn>
                    </msup>
                    <mfrac>
                      <mrow>
                        <mi>cos</mi>
                        <mi>x</mi>
                        <mo data-mjx-texclass="NONE">⁡</mo>
                        <mo stretchy="false">(</mo>
                        <mo stretchy="false">)</mo>
                      </mrow>
                      <mrow>
                        <mn>1</mn>
                        <mo>−</mo>
                        <mo data-mjx-texclass="NONE">⁡</mo>
                        <mo stretchy="false">(</mo>
                        <mo stretchy="false">)</mo>
                        <mi>cos</mi>
                        <mi>x</mi>
                      </mrow>
                    </mfrac>
                  </math>
                </disp-formula>
              
              <p>• Selection of the best solution</p><p>The BER chooses the best choice for the next iteration in order to ensure the quality of the solutions identified. Although the elitism technique increases algorithm efficiency, it also increases the risk of multimodal functions convergent too soon. It is interesting that the BER offers exceptional exploration capabilities by using a mutation strategy and searching close to members of the exploration group. The BER can prevent early convergence because to its excellent exploration capabilities. Iterations, population size, mutation rate, and other input parameters are initially given to the BER. The people are then split up into the exploration group and the exploitation group by the BER. The BER method continuously controls the number of participants in each group while iteratively seeking the optimal solution. To complete their responsibilities, each group chooses one of two methods. To guarantee variety and high exploration, the BER randomly ranks findings after each cycle. A solution from the exploration group in one iteration can, for instance, join the exploitation group in the next. The elitist mindset of the BER aids in maintaining the leader over the iterations. <xref ref-type="table" rid="table_1">Table 1</xref> lists the hyper-parameters that the proposed model picked.</p>
              
                <table-wrap id="table_1">
                  <label>Table 1</label>
                  <caption>
                    <title>Optimized hyperparameters for lesions using SegNet and UNet, correspondingly</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="2"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Hyperparameter</span></p></td><td colspan="2" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Optimized Values</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">SegNet</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">UNet</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Early learning rate</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.01</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.05</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Shuffle</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Once</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Every epoch</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Minibatch size</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">11</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">16</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Momentum</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.9</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.9</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Supreme epochs</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">30</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">150</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">l2 regularization</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.0006019928</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.0004795852</span></p></td></tr></tbody></table>
                </table-wrap>
              
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.4. Classification using pre-trained alexnet model</title>
          
          <p>The AlexNet model is suggested as the dependable foundation of the suggested model. We chose AlexNet over other pre-trained models since we want to work with a straightforward model and test performances without sacrificing memory and testing time. AlexNet employs two distinct ideas. Both models have roughly the same amounts of layers, neurons, and filters of the same size. To address the issue with AlexNet, a better framework is presented by adding batch normalisation functions:</p><p>1. ReLU's insurmountable barrier, which can vanish and never act on point.</p><p>2. Adjust the local response normalisation (LRN) exercised in the conventional AlexNet's normalisation effect. Since batch normalisation (BN) is trainable but LRN is not, using the latter produces more encouraging results than using LRN.</p><p>The AlexNet model's repetition of the layers is due to the efficient use of the GPU for convolution and all other processing during CNN training to speed up training. Because AlexNet is distributed across two parallel GPUs, the model's processing speed is accelerated and model training time is reduced.</p>
          
            <sec>
              
                <title>3.4.1 Proposed cnn architecture</title>
              
              <p>On many datasets, the AlexNet perfect can yield accuracy capacities. However, removing any one of the convolutional layers must significantly reduce AlexNet's performance. Five convolutional layers, three fully linked layers, and eight successive layers make up the original AlexNet model network structure. AlexNet is one of the greatest options for forging because of its intricate nature. Except for the final completely connected layer, which uses the softmax function, other layers employ a max-out activation function. The following are the main contributions:</p><p>1. Replace RELU with function across all AlexNet layers.</p><p>2. Swap out LRN for batch normalisation.</p><p>3. The projected construction will be employed as a classifier for the output result to notice the bogus result.</p><p>As the input, an RBG image of 227 by 227 must be utilised. Due to the significant overfitting that would have occurred without this picture size, AlexNet would have had to use much lower network layers. If an RGB picture doesn't already exist, the supplied image is converted to one. If not, the input image's dimensions will be modified to 227×227. Convolution and max-pooling with BN are performed by 96 separate 11×11-sized filters in the first convolution layer. An input image with the dimensions 227×227×3 might be subjected to a convolution:</p><p>• There are (227×227×96) output nerve cells in L, one for each of the 227×227 input "pixels" and for each of the 96 yield maps.</p><p>• There are 96 kernels total, (11×11×3) per filter (the input size processed done the kernel), and (11×11×3) weights in total.</p><p>• It is possible to connect to (227×227×3×11×11×96) connections. For each of the (227×227×96) output units, a single filter procedure the (11×11×3) input values.</p><p>Through the following four convolution layers, this is repeated in a similar manner. Every layer has distinct input and filter sizes, as well as the matching numbers of maps.</p><p>On the connected layers that were used as the discriminant and trained using dropout followed by model.</p><p>A specific aspect will be briefly explained and will centre on the key terminology used in the model in order to aid families in understanding and explaining the intended work:</p><p>1. Max Pooling (MXP): To reduce the dimension, the suggested model uses a max-pooling strategy that retains just the extreme value in the filter.</p><p>2. Dropout: Using a predetermined probability, this approach turns off individual nodes. For the suggested AlexNet model, we kept the dropout rate at 50%. The 50% dropout rate was chosen since it will provide the model the most regularisation. This is so because a loss function that follows a distribution is minimised using the dropout.</p><p>3. Softmax Activation Function: Given a neuron, each neuron's softmax value results in an output as shown in Eq. (13).</p>
              
                <disp-formula>
                  <label>(13)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msub>
                      <mi>y</mi>
                      <mi>j</mi>
                    </msub>
                    <mo>=</mo>
                    <mfrac>
                      <msup>
                        <mi>e</mi>
                        <mrow data-mjx-texclass="ORD">
                          <msub>
                            <mi>x</mi>
                            <mi>j</mi>
                          </msub>
                        </mrow>
                      </msup>
                      <mrow>
                        <munderover>
                          <mo data-mjx-texclass="OP">∑</mo>
                          <mrow data-mjx-texclass="ORD">
                            <mi>k</mi>
                            <mo>=</mo>
                            <mn>1</mn>
                          </mrow>
                          <mrow data-mjx-texclass="ORD">
                            <mi>k</mi>
                            <mi>K</mi>
                            <mo>=</mo>
                          </mrow>
                        </munderover>
                        <msup>
                          <mi>e</mi>
                          <mrow data-mjx-texclass="ORD">
                            <msub>
                              <mi>x</mi>
                              <mi>k</mi>
                            </msub>
                          </mrow>
                        </msup>
                      </mrow>
                    </mfrac>
                  </math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>K</mi>
    </mrow>
  </math>
</inline-formula> is the length of <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>x</mi>
    </mrow>
  </math>
</inline-formula>, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>j</mi>
    </mrow>
  </math>
</inline-formula> stands for the output components, so <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>j</mi>
    </mrow>
  </math>
</inline-formula>=1, 2..., <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>K</mi>
    </mrow>
  </math>
</inline-formula>, and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>x</mi>
    </mrow>
  </math>
</inline-formula> is the layer.</p><p>4. Max-out Function: The projected model substitutes for ReLU because it is known to hasten the junction of big datasets. Eq. (14) can be used to represent the max-out function as follows.</p>
              
                <disp-formula>
                  <label>(14)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mo data-mjx-texclass="OP" movablelimits="true">max</mo>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo>+</mo>
                      <mo>,</mo>
                      <mo>+</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <msubsup>
                        <mi>w</mi>
                        <mi>T</mi>
                        <mn>1</mn>
                      </msubsup>
                      <msubsup>
                        <mi>w</mi>
                        <mi>T</mi>
                        <mn>2</mn>
                      </msubsup>
                      <mi>x</mi>
                      <mi>x</mi>
                      <msub>
                        <mi>b</mi>
                        <mn>1</mn>
                      </msub>
                      <msub>
                        <mi>b</mi>
                        <mn>2</mn>
                      </msub>
                    </mrow>
                  </math>
                </disp-formula>
              
              <p>where, the input vector <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>x</mi>
    </mrow>
  </math>
</inline-formula>, the matrix <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>w</mi>
    </mrow>
  </math>
</inline-formula>, and the bias <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mi>b</mi>
    </mrow>
  </math>
</inline-formula> are present. A well-known learning activation function is max-out. Known as a maxed-out special variant, ReLU. ReLU is an easy-to-train and simple-to-implement piecewise linear function. The model can be trained more quickly thanks to ReLU. As a function has none of the drawbacks (dying ReLU) and all the advantages of a ReLU.</p><p>5. A CNN that homogenises inputs for each mini-batch is trained using batch normalisation. As a result, the learning process is required to train CNNs is drastically reduced. The advantages of BN (ICS) and quickening network training. Before the output is passed to the activation function in BN, the following processing is done on the output:</p><p>i. Set the entire batch B to have a mean of 0 and a variance of 1.</p><p>• Compute the mean of the whole batch yield: <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>μ</mi>
      <mrow data-mjx-texclass="ORD">
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">B</mi>
        </mrow>
      </mrow>
    </msub>
  </math>
</inline-formula></p><p>• Compute the alteration of the whole batch output: <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msubsup>
      <mi>σ</mi>
      <mi>B</mi>
      <mn>2</mn>
    </msubsup>
  </math>
</inline-formula></p><p>• Regularize the batch by deducting the mean and in-between by the alteration.</p><p>ii. Suggest two training limits (<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>γ</mi>
  </math>
</inline-formula>: for scaling and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>β</mi>
  </math>
</inline-formula>: for instable).</p><p>iii. Smear the scaled and shifted regularized batch to a purpose.</p><p>By generating <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>μ</mi>
      <mrow data-mjx-texclass="ORD">
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">B</mi>
        </mrow>
      </mrow>
    </msub>
  </math>
</inline-formula> and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msubsup>
      <mi>σ</mi>
      <mi>B</mi>
      <mn>2</mn>
    </msubsup>
  </math>
</inline-formula> for a channel, batch normalisation normalises inputs <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <msub>
        <mi>x</mi>
        <mi>i</mi>
      </msub>
    </mrow>
  </math>
</inline-formula> before creating the normalised activation as in Eq. (15).</p>
              
                <disp-formula>
                  <label>(15)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msub>
                      <mrow data-mjx-texclass="ORD">
                        <mover>
                          <mi>x</mi>
                          <mo stretchy="false">^</mo>
                        </mover>
                      </mrow>
                      <mi>i</mi>
                    </msub>
                    <mo>=</mo>
                    <mfrac>
                      <mrow>
                        <msub>
                          <mi>x</mi>
                          <mi>i</mi>
                        </msub>
                        <msub>
                          <mi>μ</mi>
                          <mi>B</mi>
                        </msub>
                        <mo>−</mo>
                      </mrow>
                      <msqrt>
                        <msubsup>
                          <mi>σ</mi>
                          <mi>B</mi>
                          <mn>2</mn>
                        </msubsup>
                        <mo>+</mo>
                        <mi>ε</mi>
                      </msqrt>
                    </mfrac>
                  </math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>μ</mi>
      <mrow data-mjx-texclass="ORD">
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">B</mi>
        </mrow>
      </mrow>
    </msub>
  </math>
</inline-formula> is used to increase stability if the mini-batch's variation is small.</p><p>In the training process, mean and variance are calculated across the entire training dataset. By the conclusion of network training, the calculated mean and variance are preserved as properties. This contrasts with Local Response Normalization (LRN), which is a non-trainable layer. LRN primarily square-normalizes a feature map within a given neighbourhood. LRN reduces uniformly high activations within these neighbourhoods, leading to an enhancement of contrast in the feature map. This process is based on the principle of lateral inhibition, which involves achieving local maximum contrast. It's essential to note that LRN does not provide a regularization effect, whereas Batch Normalization (BN) does. The differences between LRN and BN are elaborated on in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
              
                <table-wrap id="table_2">
                  <label>Table 2</label>
                  <caption>
                    <title>Difference among local response normalization</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Normalization Category</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Trainable</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Regularization</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif"># of Trainable Limits</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">LRN</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">No</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">No</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">BN</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Yes</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Yes</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">2</span></p></td></tr></tbody></table>
                </table-wrap>
              
            </sec>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Results and discussion</title>
      <p>The existing procedures are considered and applied with our datasets and then, results are averaged in <xref ref-type="table" rid="table_3">Table 3</xref> and <xref ref-type="table" rid="table_4">Table 4</xref>. </p>
      
        <table-wrap id="table_3">
          <label>Table 3</label>
          <caption>
            <title>Analysis of projected model on LiTS17 dataset</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1" colwidth="266"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Model</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Precision (%)</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Recall (%)</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Accuracy (%)</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">F1-Score (%)</span></p></td></tr><tr><td colspan="1" rowspan="1" colwidth="266"><p style="text-align: center"><span style="font-family: Times New Roman, serif">LeNet</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">81.40</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">78.41</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">82.68</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">79.88</span></p></td></tr><tr><td colspan="1" rowspan="1" colwidth="266"><p style="text-align: center"><span style="font-family: Times New Roman, serif">ResNet</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">83.57</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">81.71</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">85.38</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">82.63</span></p></td></tr><tr><td colspan="1" rowspan="1" colwidth="266"><p style="text-align: center"><span style="font-family: Times New Roman, serif">VGGNet</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">87.14</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">85.62</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">87.90</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">86.37</span></p></td></tr><tr><td colspan="1" rowspan="1" colwidth="266"><p style="text-align: center"><span style="font-family: Times New Roman, serif">DenseNet</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">88.81</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">89.11</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">91.23</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">88.96</span></p></td></tr><tr><td colspan="1" rowspan="1" colwidth="266"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Hybrid Model with AlexNet</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">90.78</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">92.62</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">94.53</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">91.69</span></p></td></tr></tbody></table>
        </table-wrap>
      
      <p><xref ref-type="table" rid="table_3">Table 3</xref> illustrates an analysis of the projected model on the LiTS17 Dataset. The evaluation considers various models, and their respective performance metrics are as follows: LeNet Model: This model achieved a precision rate of 81.40%, a recall value of 78.41%, an accuracy value of 82.68%, and an F1 score of 79.88%. ResNet Model: The ResNet model performed better, with a precision rate of 83.57%, a recall value of 81.71%, an accuracy value of 85.38%, and an F1 score of 82.63%. VGGNet Model: This model demonstrated further improvement, achieving a precision rate of 87.14%, a recall value of 85.62%, an accuracy value of 87.90%, and an F1 score of 86.37%. DenseNet Model: The DenseNet model surpassed the others so far, with a precision rate of 88.81%, a recall value of 89.11%, an accuracy value of 91.23%, and an F1 score of 88.96%. Hybrid Model with AlexNet: The Hybrid model, when combined with AlexNet, outperformed all the previous models. It reached a precision rate of 90.78%, a recall value of 92.62%, an accuracy value of 94.53%, and an F1 score of 91.69%. This comparative analysis shows that the proposed Hybrid model, in combination with AlexNet, yields superior results compared to the other models (See <xref ref-type="fig" rid="fig_3">Figure 3</xref> and <xref ref-type="fig" rid="fig_4">Figure 4</xref>).</p>
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>Analysis of various models</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/7/img_sa1QC84eSBTrdpEC.png"/>
        </fig>
      
      
        <fig id="fig_4">
          <label>Figure 4</label>
          <caption>
            <title>Comparison performance of projected model</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/7/img_tu2lfTE0rEpwx4Dc.png"/>
        </fig>
      
      
        <table-wrap id="table_4">
          <label>Table 4</label>
          <caption>
            <title>Analysis of projected model on 3D-IRCADb-01 dataset</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Model</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Precision (%)</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Recall (%)</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Accuracy (%)</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">F1-Score (%)</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">LeNet</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">82.22</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">80.10</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">85.63</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">81.15</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">ResNet</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">85.41</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">82.30</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">87.18</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">83.83</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">VGGNet</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">88.98</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">87.80</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">89.08</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">88.39</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">DenseNet</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">89.20</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">91.78</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">92.25</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">90.47</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Hybrid Model with AlexNet</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">92.87</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">94.32</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">95.05</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">93.59</span></p></td></tr></tbody></table>
        </table-wrap>
      
      <p><xref ref-type="table" rid="table_4">Table 4</xref> presents an examination of the projected performance on the 3D-IRCADb-01 Dataset. This analysis assesses several distinct models. The LeNet model achieved a precision rate of 82.22%, a recall value of 80.10%, an accuracy value of 85.63%, and an F1-score of 81.15%. The ResNet model demonstrated a higher precision rate of 85.41%, a recall value of 82.30%, an accuracy value of 87.18%, and an F1-score of 83.83%. The VGGNet model showed further improvement, with a precision rate of 88.98%, a recall value of 87.80%, an accuracy value of 89.08%, and an F1-score of 88.39%. The DenseNet model surpassed the others, achieving a precision rate of 89.20%, a recall value of 91.78%, an accuracy value of 92.25%, and an F1-score of 90.47%. Finally, the Hybrid model, coupled with the AlexNet model, outperformed all the previous models. It reached a precision rate of 92.87%, a recall value of 94.32%, an accuracy value of 95.05%, and an F1-score of 93.59%. These results illustrate the comparative performance metrics of different models when applied to the 3D-IRCADb-01 Dataset (See <xref ref-type="fig" rid="fig_5">Figure 5</xref> and <xref ref-type="fig" rid="fig_6">Figure 6</xref>).</p>
      
        <fig id="fig_5">
          <label>Figure 5</label>
          <caption>
            <title>Validation analysis of different models</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/7/img__SDzwWtsPy5zoVNI.png"/>
        </fig>
      
      
        <fig id="fig_6">
          <label>Figure 6</label>
          <caption>
            <title>Graphical representation of various techniques</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/7/img_XK4aliSpcOUtPwRE.png"/>
        </fig>
      
      
        <table-wrap id="table_5">
          <label>Table 5</label>
          <caption>
            <title>Time required to obtain results using the proposed method on various hardware platforms</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Platform</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Time Obligatory to Get Result [s]</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">CPU, i3 processor, 8GB RAM</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.293</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">CPU, i5</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.193</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">CPU, I7</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.191</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">GPU, Nvidia K80</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.0026</span></p></td></tr></tbody></table>
        </table-wrap>
      
      <p> <xref ref-type="table" rid="table_5">Table 5</xref> depicts the time duration required to yield results using the proposed system architecture across various hardware platforms. This study evaluates the performance of the system on distinct platforms. The platform equipped with a CPU, specifically an i3 processor with 8GB RAM, took 0.293 seconds to produce the result. Subsequently, a platform with a CPU, an i5 processor, and 8GB RAM required 0.193 seconds. Additionally, a platform with a CPU, an i7 processor, and 8GB RAM necessitated a time of 0.191 seconds. The results obtained from the i5 and i7 processor-based platforms are nearly identical, suggesting similar performance levels for these CPUs. Lastly, a platform employing a GPU, namely the Nvidia K80, exhibited a significantly quicker result time of 0.0026 seconds. This stark contrast in result times underscores the superior performance of the GPU platform in this particular application.</p>
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>In this investigation, a unique methodology for liver lesion identification was introduced, predicated on the hybridization of multiple models alongside the BER optimization algorithm. This novel method, termed SegNet-UNet-BER, was proposed for the extraction of liver lesions from CT images. It represents a synthesis of SegNet, UNet, and the BER process. The BER algorithm was employed to refine the deep learning architectures, aiming to maximize the efficacy of liver lesion segmentation. In contrast to preceding attempts in liver cancer detection which utilized various approaches, the suggested methodology harnesses the AlexNet architecture of CNN as both a feature extractor and a classifier. Two openly accessible datasets were scrutinized in this research.</p><p>The potential of the proposed SegNet-UNet-BER algorithm in the segmentation and classification of CT images was first evaluated by contrasting its performance with existing state-of-the-art segmentation techniques. Future investigations into liver cancer detection will aim to incorporate multiple modalities, including ultrasound and CT imaging, to formulate a multimodal diagnostic method underpinned by deep learning. By leveraging the advantages offered by medical imaging, this approach is hypothesized to bolster diagnostic confidence. The significance of our findings in the context of liver lesion detection and categorization establishes a compelling case for further exploration and refinement of this methodology.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>238-242</page-range>
          <issue>6</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kherde</surname>
              <given-names>S. S.</given-names>
            </name>
            <name>
              <surname>Gurjar</surname>
              <given-names>A. A.</given-names>
            </name>
          </person-group>
          <article-title>Detection of abnormalities in liver using image processing techniques</article-title>
          <source>Int. J. Creat. Res. Thoughts.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>4765-4774</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hemalatha</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Sundar</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s12652-020-01885-4</pub-id>
          <article-title>Automatic liver cancer detection in abdominal liver images using soft optimization techniques</article-title>
          <source>J. Ambient. Intell. Humaniz. Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>39</volume>
          <page-range>1807-1814</page-range>
          <issue>5</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gavini</surname>
              <given-names>Venkateswarlu</given-names>
            </name>
            <name>
              <surname>Lakshmi</surname>
              <given-names>Gurusamy Ramasamy Jothi</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/ts.390540</pub-id>
          <article-title>CT image denoising model using image segmentation for image quality enhancement for liver tumor detection using CNN</article-title>
          <source>Trait. Signal</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>235</volume>
          <page-range>232-244</page-range>
          <issue>2</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Krishan</surname>
              <given-names>Abhay</given-names>
            </name>
            <name>
              <surname>Mittal</surname>
              <given-names>Deepti</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1177/0954411920971888</pub-id>
          <article-title>Ensembled liver cancer detection and classification using CT images</article-title>
          <source>J. Eng. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>37</volume>
          <page-range>257-265</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Inamanamelluri</surname>
              <given-names>Haritha Venkata Sai Lakshmi</given-names>
            </name>
            <name>
              <surname>Pulipati</surname>
              <given-names>Venkateswara Rao</given-names>
            </name>
            <name>
              <surname>Pradhan</surname>
              <given-names>Nrusingha Charan</given-names>
            </name>
            <name>
              <surname>Chintamaneni</surname>
              <given-names>Phanikanth</given-names>
            </name>
            <name>
              <surname>Manur</surname>
              <given-names>Manohar</given-names>
            </name>
            <name>
              <surname>Vatambeti</surname>
              <given-names>Ramesh</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/ria.370202</pub-id>
          <article-title>Classification of a new-born infant’s jaundice symptoms using a binary spring search algorithm with machine learning</article-title>
          <source>Rev. Intell. Artif.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>48-61</page-range>
          <issue>3</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mahalaxmi</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Tirupal</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Shanawaz</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Liver cancer detection using various image segmentation approaches: A review</article-title>
          <source>IUP J. Telecomm.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>37</volume>
          <page-range>2848-2857</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Thulasidass</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Soundari</surname>
              <given-names>D. V.</given-names>
            </name>
            <name>
              <surname>Chinnapparaj</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Naveen</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.matpr.2020.08.660</pub-id>
          <article-title>Liver tumor diagnosis by using hybrid watershed segmentation method</article-title>
          <source>Materials Today: Proceedings</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>36</volume>
          <page-range>551-560</page-range>
          <issue>3</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sato</surname>
              <given-names>Masaya</given-names>
            </name>
            <name>
              <surname>Tateishi</surname>
              <given-names>Ryosuke</given-names>
            </name>
            <name>
              <surname>Yatomi</surname>
              <given-names>Yutaka</given-names>
            </name>
            <name>
              <surname>Koike</surname>
              <given-names>Kazuhiko</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1111/jgh.15413</pub-id>
          <article-title>Artificial intelligence in the diagnosis and management of hepatocellular carcinoma</article-title>
          <source>J. Gastroenterol. Hepatol.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>27-30</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ikeda</surname>
              <given-names>Yusuke</given-names>
            </name>
            <name>
              <surname>Doma</surname>
              <given-names>Keisuke</given-names>
            </name>
            <name>
              <surname>Mekada</surname>
              <given-names>Yoshito</given-names>
            </name>
            <name>
              <surname>Nawano</surname>
              <given-names>Shigeru</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18178/joig.9.1.27-30</pub-id>
          <article-title>Lesion image generation using conditional GAN for metastatic liver cancer detection</article-title>
          <source>J. Eng. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>1-15</page-range>
          <issue>10</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Maqsood</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Bukhari</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Gillani</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Mehmood</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Rho</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Jung</surname>
              <given-names>Y.A.</given-names>
            </name>
          </person-group>
          <article-title>A residual-learning-based multi-scale parallel-convolutions-assisted efficient CAD system for liver tumor detection</article-title>
          <source>Mathematics</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ayalew</surname>
              <given-names>Yodit Abebe</given-names>
            </name>
            <name>
              <surname>Fante</surname>
              <given-names>Kinde Anlay</given-names>
            </name>
            <name>
              <surname>Mohammed</surname>
              <given-names>Mohammed Aliy</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s42490-021-00050-y</pub-id>
          <article-title>Modified U-Net for liver cancer segmentation from computed tomography images with a new class balancing method</article-title>
          <source>BMC Biomed. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>186</volume>
          <page-range>115686</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kaur</surname>
              <given-names>Amandeep</given-names>
            </name>
            <name>
              <surname>Chauhan</surname>
              <given-names>Ajay Pal Singh</given-names>
            </name>
            <name>
              <surname>Aggarwal</surname>
              <given-names>Ashwani Kumar</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2021.115686</pub-id>
          <article-title>An automated slice sorting technique for multi-slice computed tomography liver cancer images using convolutional network</article-title>
          <source>Expert Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>61-67</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Rehman</surname>
              <given-names>Aasia</given-names>
            </name>
            <name>
              <surname>Butt</surname>
              <given-names>Muheet Ahmed</given-names>
            </name>
            <name>
              <surname>Zaman</surname>
              <given-names>Majid</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/ataiml010108</pub-id>
          <article-title>Liver lesion segmentation using deep learning models</article-title>
          <source>Acad. Trans. AI Mach. Learn.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>44</volume>
          <page-range>4049-4062</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Nanda</surname>
              <given-names>Nalin</given-names>
            </name>
            <name>
              <surname>Kakkar</surname>
              <given-names>Prerna</given-names>
            </name>
            <name>
              <surname>Nagpal</surname>
              <given-names>Sushama</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s13369-019-03735-8</pub-id>
          <article-title>Computer-aided segmentation of liver lesions in CT scans using cascaded convolutional neural networks and genetically optimised classifier</article-title>
          <source>Arab. J. Sci. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>27</volume>
          <page-range>1512-1523</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Hui Li</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>Le Hang</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Jun</given-names>
            </name>
            <name>
              <surname>Ying</surname>
              <given-names>Shi Hui</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>Jun</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/jbhi.2022.3233717</pub-id>
          <article-title>Multi-view feature transformation based SVM+ for computer-aided diagnosis of liver cancers with ultrasound images</article-title>
          <source>IEEE J. Biomed. Health Inform.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>137</volume>
          <page-range>104266</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>Zheng Yun</given-names>
            </name>
            <name>
              <surname>Tian</surname>
              <given-names>Yi Chen</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>Zheng</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Peng</given-names>
            </name>
            <name>
              <surname>Xia</surname>
              <given-names>Feng</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>Sheng</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.jbi.2022.104266</pub-id>
          <article-title>A machine learning method for improving liver cancer staging</article-title>
          <source>J. Biomed. Inform.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>19</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Dritsas</surname>
              <given-names>Elias</given-names>
            </name>
            <name>
              <surname>Trigka</surname>
              <given-names>Maria</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/computers12010019</pub-id>
          <article-title>Supervised machine learning models for liver disease risk prediction</article-title>
          <source>Computers</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>30</volume>
          <page-range>151-165</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Deshmukh</surname>
              <given-names>Sunita P.</given-names>
            </name>
            <name>
              <surname>Choudhari</surname>
              <given-names>Dharmaveer</given-names>
            </name>
            <name>
              <surname>Amalraj</surname>
              <given-names>Shankar</given-names>
            </name>
            <name>
              <surname>Matte</surname>
              <given-names>Pravin N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.24423/CAMES.463</pub-id>
          <article-title>Hybrid deep learning method for detection of liver cancer</article-title>
          <source>Comp. Assist. Methods Eng. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>581</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Md</surname>
              <given-names>Abdul Quadir</given-names>
            </name>
            <name>
              <surname>Kulkarni</surname>
              <given-names>Sanika</given-names>
            </name>
            <name>
              <surname>Joshua</surname>
              <given-names>Christy Jackson</given-names>
            </name>
            <name>
              <surname>Vaichole</surname>
              <given-names>Tejas</given-names>
            </name>
            <name>
              <surname>Mohan</surname>
              <given-names>Senthilkumar</given-names>
            </name>
            <name>
              <surname>Iwendi</surname>
              <given-names>Celestine</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/biomedicines11020581</pub-id>
          <article-title>Enhanced preprocessing approach using ensemble machine learning algorithms for detecting liver disease</article-title>
          <source>Biomedicines</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <issue>48</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Huang</surname>
              <given-names>Li Ping</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>Hong Wei</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>Liang Bin</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>Ke Qing</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Yu Zhe</given-names>
            </name>
            <name>
              <surname>Ren</surname>
              <given-names>Xue Qian</given-names>
            </name>
            <name>
              <surname>Ge</surname>
              <given-names>Yuan Cai</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>Dan Feng</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Xiao Hu</given-names>
            </name>
            <name>
              <surname>Knoll</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Qing Wen</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Yi</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41467-022-35696-2</pub-id>
          <article-title>Rapid, label-free histopathological diagnosis of liver cancer based on Raman spectroscopy and deep learning</article-title>
          <source>Nat. Commun.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>5501</page-range>
          <issue>11</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Alawneh</surname>
              <given-names>Khaled</given-names>
            </name>
            <name>
              <surname>Alquran</surname>
              <given-names>Hiam</given-names>
            </name>
            <name>
              <surname>Alsalatie</surname>
              <given-names>Mohammed</given-names>
            </name>
            <name>
              <surname>Mustafa</surname>
              <given-names>Wan Azani</given-names>
            </name>
            <name>
              <surname>Al-Issa</surname>
              <given-names>Yazan</given-names>
            </name>
            <name>
              <surname>Alqudah</surname>
              <given-names>Amin</given-names>
            </name>
            <name>
              <surname>Badarneh</surname>
              <given-names>Alaa</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app12115501</pub-id>
          <article-title>LiverNet: Diagnosis of liver tumors in human CT images</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>33</volume>
          <page-range>14991-15025</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kaur</surname>
              <given-names>Amrita</given-names>
            </name>
            <name>
              <surname>Kaur</surname>
              <given-names>Lakhwinder</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>Ashima</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00521-021-06134-z</pub-id>
          <article-title>GA-UNet: UNet-based framework for segmentation of 2D and 3D medical images applicable on heterogeneous datasets</article-title>
          <source>Neural Comput. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>Chen</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Cheng</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>Ming Rui</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>Xiao Jian</given-names>
            </name>
            <name>
              <surname>Lv</surname>
              <given-names>Xiao Yi</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>Xiao Gang</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>Zi Wei</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Min</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Jia Jia</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s12911-022-01919-1</pub-id>
          <article-title>Classification of multi-differentiated liver cancer pathological images based on deep learning attention mechanism</article-title>
          <source>BMC Med. Inform. Decis. Mak.</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>80</page-range>
          <issue>2</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ghoniem</surname>
              <given-names>Rania M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/info11020080</pub-id>
          <article-title>A novel bio-inspired deep learning approach for liver cancer diagnosis</article-title>
          <source>Information</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>