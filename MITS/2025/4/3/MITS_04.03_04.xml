<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">MITS</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Mechatronics and Intelligent Transportation Systems</journal-title>
        <abbrev-journal-title abbrev-type="issn">Mechatron. Intell Transp. Syst.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">MITS</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-0218</issn>
      <issn publication-format="print">2958-020X</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-zDl0KnWD35Bzt1ci2qfvcgi_jJBAfk_K</article-id>
      <article-id pub-id-type="doi">10.56578/mits040304</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>A Color and Shape-Aware TSR Model Enhanced by Morphological Filtering and Fuzzy Logic</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8349-1313</contrib-id>
          <name>
            <surname>Ismail</surname>
            <given-names>Sharina</given-names>
          </name>
          <email>shahrinaismail@usim.edu.my</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2526-6927</contrib-id>
          <name>
            <surname>Yow</surname>
            <given-names>Kai Siong</given-names>
          </name>
          <email>ksyow@upm.edu.my</email>
        </contrib>
        <aff id="aff_1">Faculty of Science and Technology, Universiti Sains Islam Malaysia, 71800 Bandar Baru, Malaysia</aff>
        <aff id="aff_2">Department of Mathematics and Statistics, Faculty of Science, Universiti Putra Malaysia, 43400 Serdang, Malaysia</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>09</day>
        <month>09</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>3</issue>
      <fpage>144</fpage>
      <lpage>153</lpage>
      <page-range>144-153</page-range>
      <history>
        <date date-type="received">
          <day>14</day>
          <month>07</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>05</day>
          <month>09</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Real-time traffic sign recognition (TSR) plays a crucial role in intelligent transportation systems (ITS) and autonomous driving technologies. It enhances road safety, ensures efficient traffic rule enforcement, and supports the seamless operation of both autonomous and driver-assist systems. This paper proposes a hybrid TSR model that integrates mathematical morphology, edge detection, and fuzzy logic to accurately identify and classify traffic signs across diverse environmental conditions. The preprocessing stage applies contrast enhancement and Gaussian filtering to improve the visibility of key features. Next, shape- and color-based segmentation using mathematical morphology extracts regions of interest that are likely to contain traffic signs. These regions are then analyzed using a fuzzy inference system (FIS) that evaluates features such as color intensity, geometric shape ratios, and edge sharpness. The fuzzy system handles the inherent ambiguity in visual patterns, enabling robust decision-making. The entire model is developed in MATLAB R2015a, ensuring both computational efficiency and real-time performance. The integration of classical mathematical techniques with fuzzy reasoning allows the system to maintain high accuracy and reliability across a wide variety of traffic scenes. The proposed approach demonstrates significant potential for practical deployment in ITS applications, including smart vehicles and automated road safety systems.</p></abstract>
      <kwd-group>
        <kwd>Traffic sign recognition (TSR)</kwd>
        <kwd>Fuzzy logic</kwd>
        <kwd>Image processing</kwd>
        <kwd>Mathematical morphology</kwd>
        <kwd>Edge detection</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="2"/>
        <fig-count count="2"/>
        <table-count count="2"/>
        <ref-count count="20"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>TSR plays a vital role in the development of ITS and autonomous driving technologies. The ability to accurately detect and interpret traffic signs in real time not only enhances driver assistance systems but also significantly contributes to road safety. With increasing vehicle automation, reliable TSR models are essential to ensure that vehicles can correctly understand and react to regulatory and warning signs under diverse environmental conditions. Recent studies have explored a wide range of methodologies for improving TSR accuracy, including traditional image processing, machine learning, and deep learning frameworks [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>], [<xref ref-type="bibr" rid="ref_5">5</xref>]. These approaches emphasize robustness in challenging scenarios such as poor lighting, partial occlusion, and cluttered backgrounds, laying a solid foundation for the advancement of intelligent and context-aware driving systems.</p><p>Researchers have been studying deep learning techniques extensively to improve the accuracy and efficiency of traffic sign detection and recognition systems. Wang et al. [<xref ref-type="bibr" rid="ref_6">6</xref>] proposed an enhanced YOLOv5-based model that achieves a balance between detection performance and computational speed. In another approach, Barodi et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] developed an explainable artificial intelligence (XAI) model to ensure transparency and trustworthiness in TSR—a critical aspect in safety-critical domains like ITS. Nabou et al. [<xref ref-type="bibr" rid="ref_8">8</xref>] applied the YOLOv8 model and demonstrated its robustness across varied road conditions, showcasing its adaptability to real-world scenarios. Meanwhile, Zhu and Yan [<xref ref-type="bibr" rid="ref_9">9</xref>] highlighted the effectiveness of deep convolutional neural networks in achieving high recognition accuracy for traffic signs. Zhan and Temirgaziyeva [<xref ref-type="bibr" rid="ref_10">10</xref>] further contributed by optimizing convolutional neural network architectures for more efficient road sign detection. These contributions collectively reflect a growing trend toward deploying deep learning-driven solutions that outperform traditional methods in terms of scalability, robustness, and real-time performance.</p><p>Researchers have also focused on enhancing the preprocessing and detection stages of TSR to further optimize system performance. Liu and Yan [<xref ref-type="bibr" rid="ref_11">11</xref>] introduced an improved histogram equalization technique combined with bilinear interpolation to preprocess traffic sign images, resulting in enhanced image contrast and resolution. However, their approach demonstrated limited adaptability in handling varying illumination and complex backgrounds. Mani et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] proposed a deep learning-based traffic sign detection and recognition framework tailored for autonomous vehicles. Their model achieved high accuracy and real-time performance, yet the reliance on large annotated datasets remains a challenge, particularly in resource-constrained settings. In another effort, Jenifer and Balamanigandan [<xref ref-type="bibr" rid="ref_13">13</xref>] developed an advanced image processing technique for traffic sign detection, which showed promising results in terms of sign localization, though its robustness under noisy and occluded conditions was not fully addressed. Furthermore, Suresha et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] reviewed recent advancements in small traffic sign detection and highlighted state-of-the-art approaches and datasets. While their survey provides a valuable overview, it also emphasized the persistent issue of small sign misdetection due to scale variation and background clutter. These studies collectively reveal significant progress in TSR while also underlining the existing gaps that necessitate further research and refinement.</p><p>Considering the limitations identified in existing approaches—such as sensitivity to lighting variations in histogram-based methods, high computational demands in deep learning-based models, and limited generalizability of handcrafted techniques, this study proposes a novel real-time TSR model that integrates classical mathematical tools with fuzzy logic to achieve enhanced accuracy and robustness under diverse environmental conditions. <xref ref-type="fig" rid="fig_1">Figure 1</xref> illustrates the workflow from image acquisition and preprocessing to feature extraction and fuzzy logic-based classification, highlighting key stages such as edge detection, Hough transform-based shape isolation, and multi-criteria decision-making. Unlike previous works that rely heavily on either deep learning or simple thresholding methods, our model begins with a hybrid preprocessing stage involving histogram equalization and Gaussian filtering to improve contrast and suppress noise. It then applies shape- and color-based segmentation using mathematical morphology, tailored to reduce false positives in complex backgrounds. An FIS is employed for final classification, incorporating features such as color intensity, edge clarity, and geometric proportions—an approach aimed at overcoming the rigidity of traditional rule-based or SVM-based classifiers. To ensure computational feasibility, the entire framework is implemented in MATLAB R2015a with optimized routines for image filtering, candidate region extraction, and fuzzy classification. Experimental evaluations confirm that the proposed model delivers improved recognition accuracy and stability across diverse traffic and weather conditions, validating its effectiveness for real-time deployment in ITS.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Architecture of the proposed sign recognition model</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/8/img_QF7s2mMejg_FbYo0.png"/>
        </fig>
      
    </sec>
    <sec sec-type="">
      <title>2. Related work</title>
      <p>Researchers have extensively studied various techniques for traffic sign and gesture recognition, focusing on improving accuracy, real-time performance, and adaptability to complex environments. These studies encompass a range of approaches, including fuzzy logic, color and shape-based detection, and advanced image processing frameworks. The literature reveals significant progress in addressing challenges related to environmental variability, small object detection, and integration into ITS. This section high-lights key contributions and limitations of recent advancements in traffic sign and sign recognition.</p><p>Broadly, existing approaches can be categorized into three groups: (i) traditional image processing methods, (ii) deep learning-based methods, and (iii) fuzzy logic and hybrid methods. Each category has unique strengths but also notable weaknesses, which highlight the research gap addressed by our proposed model.</p>
      
        <sec>
          
            <title>2.1. Fuzzy logic-based approaches</title>
          
          <p>Ampadu and Huebner [<xref ref-type="bibr" rid="ref_15">15</xref>] proposed a novel approach to autonomous traffic gesture integrity monitoring by employing fuzzy logic to interpret complex road gestures in real-time. The model emphasizes the use of fuzzy decision-making to improve system flexibility and reduce dependency on rigid, rule-based frameworks. A significant achievement of their study is the ability to handle gesture ambiguity in dynamic traffic scenarios, thereby enhancing system reliability and safety. Their methodology also allows for adaptability in the presence of occlusions and varying gesture patterns.</p><p>Despite its strengths, the study is limited by its constrained dataset and lack of large-scale validation. The fuzzy logic-based approach, while robust to uncertainty, may suffer from performance drops in cases where gesture transitions are too subtle or vary significantly across regions. Furthermore, the implementation details remain limited, making replication and benchmarking against other models difficult.</p>
        </sec>
      
      
        <sec>
          
            <title>2.2. Traditional image processing approaches</title>
          
          <p>Li and Liu [<xref ref-type="bibr" rid="ref_16">16</xref>] investigated image processing technologies specific to traffic sign detection for autonomous vehicles. Their work integrates image denoising, color enhancement, and shape extraction techniques to accurately detect and classify traffic signs. One of the key achievements of their method is its robustness against motion blur and lighting variations, which are common in autonomous driving environments. The authors also demonstrated that their sign recognition algorithm could maintain high accuracy during both day and night conditions.</p><p>However, the research primarily focuses on static video frames and does not extend to real-time applications. While the preprocessing pipeline shows promising results, it lacks integration with end-to-end vehicle control systems. Additionally, the dataset used was not fully disclosed, raising concerns about the generalizability of their findings to diverse real-world scenarios.</p><p>Gan [<xref ref-type="bibr" rid="ref_17">17</xref>] presented a fast recognition method for pedestrian-related signs using a combination of color feature extraction and support vector machines (SVM). The model was particularly effective in detecting red-bordered triangular and circular signs that are common in pedestrian zones. A notable achievement of this approach is its balance between computational efficiency and classification accuracy, making it suitable for embedded vision systems in smart vehicles. The use of color segmentation significantly reduced the search space, while the SVM provided robust discrimination across sign types.</p><p>Nonetheless, the system exhibits certain limitations in recognizing faded or heavily occluded signs, where color cues become unreliable. Additionally, the method may not scale well when introduced to multilingual or highly cluttered traffic environments. The reliance on handcrafted features also limits adaptability compared to deep learning-based methods, which could offer improved performance on complex datasets.</p>
        </sec>
      
      
        <sec>
          
            <title>2.3. Comparative summary and research gap</title>
          
          <p> <xref ref-type="table" rid="table_1">Table 1</xref> summarizes the strengths and weaknesses of traditional, deep learning, and fuzzy logic-based methods. While traditional methods are computationally efficient, they struggle with occlusion and environmental variability. Deep learning approaches achieve high accuracy but demand large annotated datasets and high computational power. Fuzzy logic-based methods effectively handle uncertainty and interpretability but often lack large-scale benchmarking. Our proposed fuzzy logic-based road sign recognition model is designed to combine robustness against uncertainty with improved adaptability to real-world conditions, thereby addressing these gaps.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Comparison of existing approaches for road sign and gesture recognition</title>
              </caption>
              <table><tr><td >Category</td><td >Techniques</td><td >Strengths</td><td >Limitations</td></tr><tr><td >Traditional Image Processing</td><td >Color thresholding, shape detection, SVMs</td><td >Fast, low computational demand, suitable for embedded systems</td><td >Sensitive to lighting, noise, and occlusion; limited adaptability</td></tr><tr><td >Deep Learning</td><td >CNNs, transfer learning, large datasets</td><td >High accuracy, automatic feature extraction, scalable to complex datasets</td><td >Requires large annotated datasets, high computational cost, less interpretable</td></tr><tr><td >Fuzzy Logic / Hybrid</td><td >Rule-based fuzzy systems, neuro-fuzzy methods</td><td >Handles uncertainty, interpretable, adaptable to variations</td><td >Limited benchmarking, sometimes less scalable for real-time large-scale use</td></tr></table>
            </table-wrap>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Methodology</title>
      <p>The proposed methodology for real-time TSR is structured into a sequence of processing stages, each designed to enhance accuracy, speed, and robustness under real-world driving conditions. The system begins with image acquisition, preprocessing is then applied to remove noise and improve contrast, using Gaussian filtering and histogram equalization. Following this, color and shape-based segmentation is performed to isolate regions of interest that may contain traffic signs. Feature extraction is subsequently carried out, where each candidate region is analyzed for attributes such as shape descriptors, edge density, and color distribution. These features are then fed into an FIS, which evaluates the degree of membership of each region to different traffic sign classes. This rule-based fuzzy classifier enables the model to handle uncertainty and imprecise data, making it particularly effective in challenging environments such as fog, low light, or occlusion. Finally, the recognized traffic sign is highlighted and annotated on the original image in real-time.</p>
      
        <sec>
          
            <title>3.1. Image acquisition and preprocessing</title>
          
          <p>The proposed system processes video frames captured at a rate of 25 frames per second (fps), which ensures a smooth and near real-time performance suitable for TSR. Each frame is then subjected to a preprocessing pipeline aimed at enhancing color-based features critical for distinguishing traffic signs.</p><p>Color Transformation: Since RGB is highly sensitive to illumination, frames are transformed into the Hue, Saturation, Value (HSV) space [<xref ref-type="bibr" rid="ref_18">18</xref>], [<xref ref-type="bibr" rid="ref_19">19</xref>], which separates chromatic information from brightness and is more robust for traffic sign detection. The transformation highlights hue ($H<inline-formula>
  <mml:math id="mf9eeshjai">
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>S<inline-formula>
  <mml:math id="m48y9krbil">
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>V<inline-formula>
  <mml:math id="mn370cjrre">
    <mml:mo>)</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula> S=\frac{C}{\max \left(R^{\prime}, G^{\prime}, B^{\prime}\right)}, \quad V=\max \left(R^{\prime}, G^{\prime}, B^{\prime}\right) <inline-formula>
  <mml:math id="m1wja4gh8t">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>C=\max \left(R^{\prime}, G^{\prime}, B^{\prime}\right)-\min \left(R^{\prime}, G^{\prime}, B^{\prime}\right)<inline-formula>
  <mml:math id="ms1cgh5b79">
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>B</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>H</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>V</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>H \in\left[ 0^{\circ}, 10^{\circ}\right] \cup\left[ 170^{\circ}, 180^{\circ}\right]<inline-formula>
  <mml:math id="mxtdg45ces">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>B</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>H \in\left[ 100^{\circ}, 130^{\circ}\right]<inline-formula>
  <mml:math id="mw9nwsvbjd">
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>Y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>H \in\left[ 20^{\circ}, 40^{\circ}\right]<inline-formula>
  <mml:math id="mzov1augil">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>W</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
  </mml:math>
</inline-formula>S<inline-formula>
  <mml:math id="mj6uh55j8e">
    <mml:mo>,</mml:mo>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
  </mml:math>
</inline-formula>V<inline-formula>
  <mml:math id="mr5f6nzle0">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula>+<inline-formula>
  <mml:math id="mwskb6tjgk">
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula>+$ blue icon). This approach follows standard practices in traffic sign detection.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Edge detection and morphological operations</title>
          
          <p>To accurately isolate regions corresponding to traffic signs, edge detection followed by morphological operations is employed. These techniques are fundamental in identifying object boundaries and refining segmented regions.</p><p>Canny Edge Detector: The Canny operator is used for edge detection due to its robustness against noise and ability to preserve fine details. The process involves Gaussian smoothing, gradient computation, non-maximum suppression, and hysteresis thresholding [<xref ref-type="bibr" rid="ref_20">20</xref>]. This ensures reliable extraction of traffic sign contours under varying image conditions. </p><p>Morphological Operations: Basic morphological operations, including erosion and dilation, are applied with a disk-shaped structuring element to remove noise and enhance segmented sign regions. More complex operations such as opening and closing are further used to refine object boundaries and maintain shape consistency. These steps strengthen the binary masks before feature extraction, ensuring clearer and more accurate sign representation.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Shape and color-based segmentation</title>
          
          <p>To robustly localize traffic signs, the proposed model employs both shape detection and color thresholding. This combined strategy leverages the inherent circular structure of traffic signs and their distinctive color features in the HSV color space.</p><p>Hough Circle Transform: The Hough Circle Transform is utilized to detect circular objects in the edge-detected image. The standard form of the circle equation is:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mbv1qxcey6">
    <mml:msup>
      <mml:mi>x</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msup>
    <mml:msup>
      <mml:mi>y</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msup>
    <mml:msup>
      <mml:mi>r</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msup>
    <mml:mo>+</mml:mo>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula></p><p>In the Hough space, the presence of a circle is determined by accumulating votes in a three-dimensional parameter space <inline-formula>
  <mml:math id="mk6j616lg9">
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula>, where $x<inline-formula>
  <mml:math id="mgirkblqhr">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>y<inline-formula>
  <mml:math id="myrk61v8q6">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:msup>
      <mml:mi>e</mml:mi>
      <mml:mo>′</mml:mo>
    </mml:msup>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>r<inline-formula>
  <mml:math id="mr9nuz6ics">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula> H(x, y, r)=\sum_{i, j} f(i, j) \delta\left((i-x)^2+(j-y)^2-r^2\right) <inline-formula>
  <mml:math id="m0712pw6xk">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>f (i, j)<inline-formula>
  <mml:math id="md4ap3d0yd">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\delta<inline-formula>
  <mml:math id="m5n0xnquol">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>D</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>H(x, y, r)<inline-formula>
  <mml:math id="mfvr25eg1v">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>H</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>V</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>H</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>V</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>H<inline-formula>
  <mml:math id="mp71c856v5">
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>S<inline-formula>
  <mml:math id="m24d7dy9a5">
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>V<inline-formula>
  <mml:math id="mx05u8kjog">
    <mml:mo>)</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>H \in[ 0,10] \cup[ 160,180]<inline-formula>
  <mml:math id="mwqw4fifg0">
    <mml:mo>,</mml:mo>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
  </mml:math>
</inline-formula>S&gt;0.5<inline-formula>
  <mml:math id="mykxyd1dru">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>V&gt;0.5<inline-formula>
  <mml:math id="mot09vbigw">
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>H \in[ 100,130]<inline-formula>
  <mml:math id="mvebkqsaiu">
    <mml:mo>,</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
  </mml:math>
</inline-formula>S&gt;0.5<inline-formula>
  <mml:math id="my0atiopvw">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>V&gt;0.5$. These ranges are selected to improve robustness against shadows and illumination variations. This segmentation isolates colored regions that potentially represent traffic sign for further analysis.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Feature extraction</title>
          
          <p>After segmentation, feature extraction is applied to characterize the candidate traffic sign regions for classification.</p><p>Hu Moments: Hu moments are shape descriptors derived from image moments and are invariant to translation, rotation, and scaling. They are computed using normalized central moments <inline-formula>
  <mml:math id="m39os2n5yb">
    <mml:msub>
      <mml:mi>η</mml:mi>
      <mml:mrow>
        <mml:mi>p</mml:mi>
        <mml:mi>q</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula>, which are defined as:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mzctmqt63y">
    <mml:msub>
      <mml:mi>η</mml:mi>
      <mml:mrow>
        <mml:mi>p</mml:mi>
        <mml:mi>q</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mfrac>
      <mml:msub>
        <mml:mi>μ</mml:mi>
        <mml:mrow>
          <mml:mi>p</mml:mi>
          <mml:mi>q</mml:mi>
        </mml:mrow>
      </mml:msub>
      <mml:msubsup>
        <mml:mi>μ</mml:mi>
        <mml:mrow>
          <mml:mn>00</mml:mn>
        </mml:mrow>
        <mml:mrow>
          <mml:mn>1</mml:mn>
          <mml:mo>+</mml:mo>
          <mml:mfrac>
            <mml:mrow>
              <mml:mi>p</mml:mi>
              <mml:mi>q</mml:mi>
              <mml:mo>+</mml:mo>
            </mml:mrow>
            <mml:mn>2</mml:mn>
          </mml:mfrac>
        </mml:mrow>
      </mml:msubsup>
    </mml:mfrac>
  </mml:math>
</inline-formula></p><p>where, <inline-formula>
  <mml:math id="m80cz2jm8a">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:mi>p</mml:mi>
        <mml:mi>q</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> are the central moments of the image region. The seven Hu moment invariants <inline-formula>
  <mml:math id="m0efi1lt15">
    <mml:msub>
      <mml:mi>ϕ</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> are then calculated as nonlinear combinations of these normalized moments:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mj3q7f45iw">
    <mml:msub>
      <mml:mi>ϕ</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mi>f</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>η</mml:mi>
        <mml:mrow>
          <mml:mi>p</mml:mi>
          <mml:mi>q</mml:mi>
        </mml:mrow>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p>These features capture the geometric structure of traffic signs and help differentiate circular lights from other objects.</p><p>Color Histogram: To capture the color distribution within a segmented region, a normalized color histogram is computed. The image is typically quantized into 64 bins by dividing the HSV color space into levels. The histogram for bin $k<inline-formula>
  <mml:math id="m7vbjvlwmv">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula> H_k=\frac{n_k}{\sum_{j=1}^{64} n_j}, \quad k=1,2, \ldots, 64 <inline-formula>
  <mml:math id="mifpdlgvhn">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>n_k<inline-formula>
  <mml:math id="mpt8wpk7v3">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>k^{\text {th }}<inline-formula>
  <mml:math id="mrngvoxwd8">
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>R_s<inline-formula>
  <mml:math id="mew3vnxo88">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula> R_s=\frac{\left|A_{\text {left }}-A_{\text {right }}\right|}{A_{\text {left }}+A_{\text {right }}} <inline-formula>
  <mml:math id="mu29z37r9l">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>A_{\text {left }}<inline-formula>
  <mml:math id="mgbt4rux3b">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>A_{\text {right }}<inline-formula>
  <mml:math id="mon9aqpo7x">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>R_s \approx 0$ ) indicates a highly symmetrical object, which is a strong indicator of a traffic sign. This feature helps to eliminate asymmetrical artifacts and improves recognition reliability.</p>
        </sec>
      
      
        <sec>
          
            <title>3.5. Fis</title>
          
          <p>To robustly classify the detected traffic sign candidates, an FIS is utilized. The FIS models human-like reasoning by incorporating uncertainty and imprecision in input data, making it well-suited for real-world visual environments where sensor readings may not be crisp.</p><p>Inputs: The FIS takes three inputs: Shape Confidence (<inline-formula>
  <mml:math id="m8o7mqzgbk">
    <mml:mi>S</mml:mi>
    <mml:mi>C</mml:mi>
  </mml:math>
</inline-formula>), which measures the degree of similarity between the detected region and an ideal geometric shape (e.g., circle or triangle); Color Match (<inline-formula>
  <mml:math id="mzztn86h96">
    <mml:mi>C</mml:mi>
    <mml:mi>M</mml:mi>
  </mml:math>
</inline-formula>), which represents the degree of similarity between the region’s color histogram and the expected color profile of a traffic sign; Symmetry Ratio (<inline-formula>
  <mml:math id="mz80axeeza">
    <mml:mi>S</mml:mi>
    <mml:mi>R</mml:mi>
  </mml:math>
</inline-formula>), which measures the vertical symmetry of the region, as traffic signs are typically symmetric in shape.</p><p>Fuzzification: Fuzzification maps crisp numerical input values into fuzzy sets using predefined membership functions. For instance, the membership function for the fuzzy set “High” in Shape Confidence is defined as:</p><p style="text-align: center"><inline-formula>
  <mml:math id="m6a1v23p3q">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:mi>S</mml:mi>
        <mml:mi>C</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mrow>
      <mml:mo>{</mml:mo>
      <mml:mo fence="true"/>
      <mml:mtable columnalign="left left" columnspacing="1em" rowspacing=".2em">
        <mml:mtr>
          <mml:mtd>
            <mml:mn>1</mml:mn>
            <mml:mo>,</mml:mo>
          </mml:mtd>
          <mml:mtd>
            <mml:mi>a</mml:mi>
            <mml:mi>m</mml:mi>
            <mml:mi>p</mml:mi>
            <mml:mi>x</mml:mi>
            <mml:mo>;</mml:mo>
            <mml:mo>&amp;gt;</mml:mo>
            <mml:mn>0.9</mml:mn>
          </mml:mtd>
        </mml:mtr>
        <mml:mtr>
          <mml:mtd>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>x</mml:mi>
                <mml:mo>−</mml:mo>
                <mml:mn>0.7</mml:mn>
              </mml:mrow>
              <mml:mn>0.2</mml:mn>
            </mml:mfrac>
            <mml:mo>,</mml:mo>
          </mml:mtd>
          <mml:mtd>
            <mml:mi>a</mml:mi>
            <mml:mi>m</mml:mi>
            <mml:mi>p</mml:mi>
            <mml:mi>x</mml:mi>
            <mml:mo>;</mml:mo>
            <mml:mo>≤</mml:mo>
            <mml:mo>≤</mml:mo>
            <mml:mn>0.7</mml:mn>
            <mml:mn>0.9</mml:mn>
          </mml:mtd>
        </mml:mtr>
        <mml:mtr>
          <mml:mtd>
            <mml:mn>0</mml:mn>
            <mml:mo>,</mml:mo>
          </mml:mtd>
          <mml:mtd>
            <mml:mi>a</mml:mi>
            <mml:mi>m</mml:mi>
            <mml:mi>p</mml:mi>
            <mml:mi>x</mml:mi>
            <mml:mo>;</mml:mo>
            <mml:mo>&amp;lt;</mml:mo>
            <mml:mn>0.7</mml:mn>
          </mml:mtd>
        </mml:mtr>
      </mml:mtable>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p>This piecewise linear function means that if the shape confidence is greater than 0.9, it is fully considered “high.” If it lies between 0.7 and 0.9, it partially belongs to the “high” category, and below 0.7, it is not considered high at all. Similar fuzzification functions are defined for other inputs like <inline-formula>
  <mml:math id="mx9pw6ot9v">
    <mml:mi>C</mml:mi>
    <mml:mi>M</mml:mi>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mjn59pqobl">
    <mml:mi>S</mml:mi>
    <mml:mi>R</mml:mi>
  </mml:math>
</inline-formula>, with sets such as “low,” “medium,” and “high.”</p><p>Rule Base: Once the inputs are fuzzified, fuzzy rules are applied to derive conclusions.</p><p>These rules follow the IF–THEN format and are derived from expert knowledge or empirical observations. For example:</p><p style="text-align: center"><inline-formula>
  <mml:math id="m7ve4wmg9d">
    <mml:mtext> IF </mml:mtext>
    <mml:mtext> is High AND </mml:mtext>
    <mml:mtext> is High AND </mml:mtext>
    <mml:mtext> is Low THEN Class </mml:mtext>
    <mml:mtext> Circular Prohibitory </mml:mtext>
    <mml:mi>S</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula></p><p>This rule indicates that if the detected shape closely matches a circle (high shape confidence), has strong color similarity (high color match), and is symmetric (low asymmetry), then it is likely a circular prohibitory sign.</p><p>Another example is:</p><p style="text-align: center"><inline-formula>
  <mml:math id="m0h3tjxv3u">
    <mml:mtext> IF </mml:mtext>
    <mml:mtext> is Medium AND </mml:mtext>
    <mml:mtext> is High THEN Class = Warning Triangle </mml:mtext>
    <mml:mi>S</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>M</mml:mi>
  </mml:math>
</inline-formula></p><p>This rule captures cases where the shape does not strongly resemble a circle but has a high color match, possibly indicating a triangular warning sign.</p><p>Defuzzification: After evaluating the fuzzy rules, the fuzzy output needs to be converted into a crisp decision. This is achieved through defuzzification. The most commonly used method is the **centroid method**, which calculates the center of gravity of the output fuzzy set. It is given by:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mqoqemzuxg">
    <mml:mtext> Output </mml:mtext>
    <mml:mo>=</mml:mo>
    <mml:mfrac>
      <mml:mrow>
        <mml:mo>∫</mml:mo>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>z</mml:mi>
        <mml:mi>μ</mml:mi>
        <mml:mi>z</mml:mi>
        <mml:mi>d</mml:mi>
        <mml:mi>z</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mo>∫</mml:mo>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>μ</mml:mi>
        <mml:mi>z</mml:mi>
        <mml:mi>d</mml:mi>
        <mml:mi>z</mml:mi>
      </mml:mrow>
    </mml:mfrac>
  </mml:math>
</inline-formula></p><p>In this equation, $z<inline-formula>
  <mml:math id="mpl8bljuik">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\mu(z)$ is the aggregated membership function derived from the rule evaluations. The result is a crisp value used to classify the traffic sign into specific categories such as “Circular Prohibitory,” “Warning Triangle,” or others, depending on the output scale.</p><p>The FIS thus allows for smooth transitions between classes and handles uncertainty in the visual feature values, making it particularly effective in outdoor environments with noise, varying lighting, and occlusions.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Results and discussion</title>
      <p>To validate the effectiveness of the proposed TSR model, a series of experiments were carried out using the German Traffic Sign Recognition Benchmark (GTSRB) dataset. This benchmark dataset is widely used in TSR research and contains over 50,000 images of German traffic signs belonging to more than 40 categories. The images vary in terms of lighting, weather conditions, occlusions, and viewing angles, providing a comprehensive and realistic evaluation environment.</p><p>The dataset was divided into training and testing subsets, ensuring a balanced distribution of sign classes for model evaluation. The proposed method was implemented in MATLAB R2015a, following a structured pipeline involving preprocessing, segmentation, feature extraction, and classification. Performance metrics such as accuracy and recognition rate were calculated to objectively assess the model’s ability to identify traffic signs under diverse real-world scenarios.</p><p>To ensure reproducibility and transparency, the complete FIS rule set used in this study is fully documented, detailing the input membership functions, rule base, and output defuzzification strategy. While the full code implementation is not publicly hosted, it is available for research purposes upon request via email. Additionally, the dataset was divided into training and testing subsets according to standard GTSRB protocols, and all random initializations were controlled using fixed seeds to maintain consistent results across experiments. These measures collectively support replicability while maintaining accessibility for interested researchers.</p><p>The proposed TSR model was configured with carefully chosen parameters to ensure optimal performance across all processing stages. In the HSV color space transformation, hue thresholds were set to <inline-formula>
  <mml:math id="m70xydnovk">
    <mml:mo>[</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mo>∪</mml:mo>
    <mml:mo>[</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mn>0</mml:mn>
    <mml:mn>10</mml:mn>
    <mml:mn>160</mml:mn>
    <mml:mn>180</mml:mn>
  </mml:math>
</inline-formula> for red, and <inline-formula>
  <mml:math id="mqj07lq373">
    <mml:mo>[</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mn>100</mml:mn>
    <mml:mn>130</mml:mn>
  </mml:math>
</inline-formula> for blue, with both saturation and value thresholds greater than 0.5 to ensure vivid and bright color segmentation. These ranges were selected based on established traffic sign color standards in prior studies and verified empirically to remain robust under varying illumination conditions. For edge detection, a Gaussian kernel size of $5 \times 5<inline-formula>
  <mml:math id="mq6so5lmtv">
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
  </mml:math>
</inline-formula>\sigma=1.4<inline-formula>
  <mml:math id="mv2wdaqi52">
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>H</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>H</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>H</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>V</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>0.1</mml:mn>
    <mml:mn>0.3</mml:mn>
    <mml:mn>3</mml:mn>
    <mml:mn>10</mml:mn>
    <mml:mn>30</mml:mn>
    <mml:mn>20</mml:mn>
    <mml:mn>7</mml:mn>
    <mml:mn>64</mml:mn>
    <mml:mrow>
      <mml:mo>–</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>R_s&lt;0.2$ was considered indicative of high vertical symmetry. This threshold was empirically chosen after testing different values, where 0.2 provided the best trade-off between capturing sign symmetry and minimizing misclassification.</p><p>The proposed sign recognition model follows a systematic pipeline designed to robustly detect and classify traffic signs under varying conditions in <xref ref-type="fig" rid="fig_2">Figure 2</xref>. The process begins with image acquisition, where input images are captured for analysis. Next, preprocessing techniques are applied, including Gaussian filtering for noise reduction and histogram-based contrast enhancement to improve feature visibility. The model then transforms the image into a suitable color space (e.g., HSV) to facilitate color-based segmentation, enabling the isolation of sign regions based on hue and saturation.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Pipeline for sign recognition, comprising image acquisition, edge detection, morphological operations, color-based segmentation, geometric shape isolation, noise removal, contrast enhancement, and feature extraction using an FIS for shape and color confidence scoring</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/8/img_1BVDOmG5ZKPuw0Dg.png"/>
        </fig>
      
      <p>For feature extraction, the model employs edge detection combined with morphological operations (e.g., dilation and erosion) to refine shape boundaries. Geometric shapes are isolated using Hough transform approximations, allowing the detection of primitives such as circles, triangles, and rectangles. These steps ensure accurate localization of potential signs within the image.</p><p>In the classification phase, an FIS evaluates two key metrics: shape confidence (measuring geometric fit) and color match score (assessing hue consistency). By combining these scores probabilistically, the model achieves robust sign recognition even in challenging scenarios (e.g., occlusions or lighting variations). The final output classifies the detected sign into predefined categories (e.g., stop signs, speed limits) with high accuracy.</p><p>This modular approach ensures adaptability across diverse environments, making the model suitable for real-world applications such as autonomous driving and traffic management systems. The integration of traditional computer vision techniques with fuzzy logic enhances both precision and interpretability, addressing common challenges in sign recognition.</p>
      
        <sec>
          
            <title>4.1. Statistical analysis based on evaluation metrics</title>
          
          <p>To validate the performance of the proposed TSR model, we conduct a statistical analysis using standard evaluation metrics: Accuracy, Precision, Recall, and F1- score. These metrics are derived from the confusion matrix elements: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). The formulations are provided as follows:</p><p><inline-formula>
  <mml:math id="mgs5dwygyb">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Accuracy: Measures the overall correctness of the model’s predictions.</p><p style="text-align: center"><inline-formula>
  <mml:math id="m9immnsxth">
    <mml:mtext> Accuracy </mml:mtext>
    <mml:mo>=</mml:mo>
    <mml:mfrac>
      <mml:mrow>
        <mml:mi>T</mml:mi>
        <mml:mi>P</mml:mi>
        <mml:mi>T</mml:mi>
        <mml:mi>N</mml:mi>
        <mml:mo>+</mml:mo>
      </mml:mrow>
      <mml:mrow>
        <mml:mi>T</mml:mi>
        <mml:mi>P</mml:mi>
        <mml:mi>T</mml:mi>
        <mml:mi>N</mml:mi>
        <mml:mi>F</mml:mi>
        <mml:mi>P</mml:mi>
        <mml:mi>F</mml:mi>
        <mml:mi>N</mml:mi>
        <mml:mo>+</mml:mo>
        <mml:mo>+</mml:mo>
        <mml:mo>+</mml:mo>
      </mml:mrow>
    </mml:mfrac>
  </mml:math>
</inline-formula></p><p><inline-formula>
  <mml:math id="mu10atqaim">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Precision: Measures the proportion of correctly predicted positive observations to the total predicted positives.</p><p style="text-align: center"><inline-formula>
  <mml:math id="mbnahue4ft">
    <mml:mtext> Precision </mml:mtext>
    <mml:mo>=</mml:mo>
    <mml:mfrac>
      <mml:mrow>
        <mml:mi>T</mml:mi>
        <mml:mi>P</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mi>T</mml:mi>
        <mml:mi>P</mml:mi>
        <mml:mi>F</mml:mi>
        <mml:mi>P</mml:mi>
        <mml:mo>+</mml:mo>
      </mml:mrow>
    </mml:mfrac>
  </mml:math>
</inline-formula></p><p><inline-formula>
  <mml:math id="m0jntfyxh4">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Recall (Sensitivity): Measures the proportion of correctly predicted positive observations to all actual positives.</p><p style="text-align: center"><inline-formula>
  <mml:math id="ma85krg41l">
    <mml:mtext> Recall </mml:mtext>
    <mml:mo>=</mml:mo>
    <mml:mfrac>
      <mml:mrow>
        <mml:mi>T</mml:mi>
        <mml:mi>P</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mi>T</mml:mi>
        <mml:mi>P</mml:mi>
        <mml:mi>F</mml:mi>
        <mml:mi>N</mml:mi>
        <mml:mo>+</mml:mo>
      </mml:mrow>
    </mml:mfrac>
  </mml:math>
</inline-formula></p><p><inline-formula>
  <mml:math id="mf5gyyfyl6">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> F1-Score: Harmonic mean of Precision and Recall, offering a balance between the two.</p><p style="text-align: center"><inline-formula>
  <mml:math id="mcost7nod7">
    <mml:mtext> F1-Score </mml:mtext>
    <mml:mo>=</mml:mo>
    <mml:mo>×</mml:mo>
    <mml:mn>2</mml:mn>
    <mml:mfrac>
      <mml:mrow>
        <mml:mtext> Precision </mml:mtext>
        <mml:mtext> Recall </mml:mtext>
        <mml:mo>×</mml:mo>
      </mml:mrow>
      <mml:mrow>
        <mml:mtext> Precision </mml:mtext>
        <mml:mtext> Recall </mml:mtext>
        <mml:mo>+</mml:mo>
      </mml:mrow>
    </mml:mfrac>
  </mml:math>
</inline-formula></p><p>Using the above metrics, we evaluate the model’s ability to correctly recognize and classify traffic signs under various conditions. High values across all metrics (as shown in <xref ref-type="table" rid="table_2">Table 2</xref>) confirm the reliability, generalizability, and robustness of the proposed recognition system. These quantitative results validate that the model effectively minimizes false classifications while maintaining high sensitivity to true signs, making it suitable for deployment in real-time ITS.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Performance of the proposed model under different conditions with real-time metrics</title>
              </caption>
              <table><tr><td >Condition</td><td >Accuracy</td><td >Precision</td><td >Recall</td><td >F1-Score</td><td >FPS</td><td >Latency</td></tr><tr><td >Normal</td><td >98.7</td><td >98.5</td><td >98.6</td><td >98.55</td><td >32</td><td >31.2</td></tr><tr><td >Poor Lighting</td><td >96.2</td><td >95.8</td><td >96.0</td><td >95.9</td><td >30</td><td >33.5</td></tr><tr><td >Partial Occlusion</td><td >95.5</td><td >95.1</td><td >95.3</td><td >95.2</td><td >29</td><td >34.7</td></tr><tr><td >Rotated Signs</td><td >96.9</td><td >96.5</td><td >96.7</td><td >96.6</td><td >31</td><td >32.1</td></tr><tr><td >Blurred Images</td><td >94.7</td><td >94.2</td><td >94.5</td><td >94.3</td><td >28</td><td >36.0</td></tr><tr><td >Average</td><td >96.4</td><td >96.0</td><td >96.2</td><td >96.1</td><td >30</td><td >33.5</td></tr></table>
            </table-wrap>
          
          <p><xref ref-type="table" rid="table_2">Table 2</xref> presents the performance of the proposed TSR model under varying real-world conditions. To evaluate robustness under diverse conditions, additional test subsets were constructed from the GTSRB dataset. Specifically, poor lighting scenarios were simulated by reducing brightness and contrast levels in MATLAB, while partial occlusion was introduced by masking random portions of the traffic sign area with rectangular patches. Rotated signs were generated by applying rotation transformations within the range of <inline-formula>
  <mml:math id="mwgukiaqh2">
    <mml:mrow>
      <mml:mo>[</mml:mo>
      <mml:mo>−</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>+</mml:mo>
      <mml:mo>]</mml:mo>
      <mml:msup>
        <mml:mn>25</mml:mn>
        <mml:mrow>
          <mml:mo>∘</mml:mo>
        </mml:mrow>
      </mml:msup>
      <mml:msup>
        <mml:mn>25</mml:mn>
        <mml:mrow>
          <mml:mo>∘</mml:mo>
        </mml:mrow>
      </mml:msup>
    </mml:mrow>
  </mml:math>
</inline-formula>, and blurred images were obtained using a Gaussian blur with <inline-formula>
  <mml:math id="mlvreiukv5">
    <mml:mi>σ</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mn>2.0</mml:mn>
  </mml:math>
</inline-formula>. These subsets were created to systematically mimic real-world challenges while maintaining consistency with the original dataset distribution. Approximately 20% of the total test images were assigned to each condition, ensuring balanced evaluation across scenarios. The model demonstrates exceptional accuracy across all scenarios, achieving a peak accuracy of 98.7% in well-lit, clear conditions. Even under challenging environments such as poor lighting and partial occlusion, the model maintains high accuracy levels of 96.2% and 95.5%, respectively, indicating strong generalization and robustness.</p><p>Precision, recall, and F1-score metrics also remain consistently high, with average values of 96.0%, 96.2%, and 96.1%, respectively. Notably, the model performs reliably even with rotated signs (96.9% accuracy) and blurred inputs (94.7% accuracy), reflecting its effectiveness in real-world, unpredictable traffic conditions.</p><p>The inclusion of real-time performance metrics further validates the efficiency of the proposed model. As shown in <xref ref-type="table" rid="table_2">Table 2</xref>, the system achieves an average of 30 frames per second (FPS), which aligns with the typical requirement for real-time vision systems in intelligent transportation. Latency per frame is consistently maintained around 33.5 ms, even under challenging conditions such as poor lighting, occlusion, and motion blur. These results indicate that the model can process traffic signs at video frame rates without perceptible delays, ensuring smooth integration into real-world autonomous driving and traffic monitoring applications.</p><p>In summary, the proposed TSR model demonstrates high accuracy and robustness across various road conditions and sign types, effectively handling illumination changes, occlusions, and background complexities. The consistent performance observed in all evaluated scenarios confirms the reliability of the model in real-world applications. The use of integrated feature extraction, fuzzy logic inference, and optimized classification enhances both precision and computational efficiency. These results collectively validate the model’s suitability for deployment in ITS and real-time driver assistance frameworks, marking a significant advancement in the domain of TSR.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>This paper presented a hybrid real-time TSR framework that integrates mathematical morphology, color and shape-based modeling, and fuzzy logic inference. The framework can effectively process road scene images to detect and recognize traffic signs by first isolating candidate regions through morphological operations, followed by verification using shape and color features. The FIS enhanced the robustness of classification by handling uncertainties such as partial occlusion, poor lighting, or faded signs.</p><p>The proposed model demonstrated reliable performance in diverse road conditions, maintaining real-time responsiveness and low computational complexity. Its rule-based structure ensured interpretability, which is vital for applications in driver assistance systems and autonomous vehicles. Furthermore, the integration of fuzzy logic added flexibility to decision-making, making the system more tolerant of variations in input data.</p><p>However, the system has some limitations. The manually defined fuzzy rule base, while interpretable, lacks adaptability to dynamic or unseen environments. In addition, although the framework is real-time on standard systems, its performance on low-power embedded platforms remains to be fully optimized.</p><p>Future work will address these limitations by incorporating adaptive fuzzy rule learning techniques such as reinforcement learning or data-driven rule generation to improve model adaptability. Additionally, further research will explore lightweight hardware-friendly implementations, such as deploying the model on FPGA or Jetson platforms. The system will also be extended to support multilingual and region-specific signs to improve its usability in diverse geographic contexts.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that there are no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>1738-1752</page-range>
          <issue>4</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Babić</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Babić</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Fiolić</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Ferko</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/encyclopedia2040119</pub-id>
          <article-title>Road markings and signs in road safety</article-title>
          <source>Encyclopedia</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>201799–201805</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Hei</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Lai</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3032581</pub-id>
          <article-title>Image recognition and safety risk assessment of traffic sign based on deep convolution neural network</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-5</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gupta</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Verma</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kukreja</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Sharma</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICETCS61022.2024.10543721</pub-id>
          <article-title>Enhancing road safety and its efficiency: A comprehensive evaluation of traffic sign detection using yolov8</article-title>
          <source>2024 IEEE 4th International Conference on Smart Information Systems and Technologies (SIST), Bengaluru, India</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>56</volume>
          <page-range>241</page-range>
          <issue>5</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kandasamy</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Natarajan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Sri Preethaa</surname>
              <given-names>K. R.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>A. A. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11063-024-11693-y</pub-id>
          <article-title>A robust trafficsignnet algorithm for enhanced traffic sign recognition in autonomous vehicles under varying light conditions</article-title>
          <source>Neural Process. Lett.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="conf-paper">
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gupta</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Mhala</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Mangal</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Yadav</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Sharma</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.21203/rs.3.rs-3889986/v1</pub-id>
          <article-title>Traffic sign sensing: A deep learning approach for enhanced road safety</article-title>
          <source>2024 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT), Bangalore, India</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>54679–54691</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Q. Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>X. Y.</given-names>
            </name>
            <name>
              <surname>M. Lu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2023.3281551</pub-id>
          <article-title>An improved traffic sign detection and recognition deep model based on yolov5</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Barodi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Zemmouri</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Bajit</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Benbrahim</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Tamtaoui</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>An explainable model for detection and recognition of traffic road signs</article-title>
          <source>Explainable Artificial Intelligence for Intelligent Transportation Systems</source>
          <publisher-name>CRC Press</publisher-name>
          <year>2023</year>
          <page-range>171-206</page-range>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-7</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Nabou</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Salih</surname>
              <given-names>A. L.</given-names>
            </name>
            <name>
              <surname>Rabbaa</surname>
              <given-names>I. B.</given-names>
            </name>
            <name>
              <surname>Abdelwahed</surname>
              <given-names>E. H.</given-names>
            </name>
            <name>
              <surname>Ouhamou</surname>
              <given-names>M. A. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/SITA60746.2023.10373723</pub-id>
          <article-title>Road signs recognition by using yolov8 model</article-title>
          <source>2023 14th International Conference on Intelligent Systems: Theories and Applications (SITA), Casablanca, Morocco</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>81</volume>
          <page-range>17779–17791</page-range>
          <issue>13</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhu</surname>
              <given-names>Y. Z.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>W. Q.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-022-12163-0</pub-id>
          <article-title>Traffic sign recognition based on deep learning</article-title>
          <source>Multimed. Tools Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="conf-paper">
          <page-range>308-313</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhan</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Temirgaziyeva</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/SIST61555.2024.10629499</pub-id>
          <article-title>Development of deep convolutional neural network for road sign detection</article-title>
          <source>2024 IEEE 4th International Conference on Smart Information Systems and Technologies (SIST), Astana, Kazakhstan</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="conf-paper">
          <volume>13105</volume>
          <page-range>98-105</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1117/12.3026484</pub-id>
          <article-title>Preprocessing of traffic sign images based on improved histogram equalization and bilinear interpolation</article-title>
          <source>2023 International Conference on Computer Graphics, Artificial Intelligence, and Data Processing (ICCAID), Qingdao, China</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Mani</surname>
              <given-names>M. K.</given-names>
            </name>
            <name>
              <surname>Rajagopal</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Kavitha</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Ramachandran</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Deep learning-based traffic sign detection and recognition for autonomous vehicles</article-title>
          <source>Digital Twin and Blockchain for Smart Cities</source>
          <publisher-name>Wiley</publisher-name>
          <year>2024</year>
          <page-range>407-428</page-range>
          <pub-id pub-id-type="doi">10.1002/9781394303564.ch18</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="conf-paper">
          <page-range>650-655</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jenifer</surname>
              <given-names>A. M.</given-names>
            </name>
            <name>
              <surname>Balamanigandan</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICECA63461.2024.10800775</pub-id>
          <article-title>A novel method for traffic sign detection using advanced image processing techniques</article-title>
          <source>2024 8th International Conference on Electronics, Communication and Aerospace Technology (ICECA), Coimbatore, India</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>192840-192859</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Suresha</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Manohar</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Kumar</surname>
              <given-names>G. A.</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2024.3514692</pub-id>
          <article-title>Recent advancement in small traffic sign detection: Approaches and dataset</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>152</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ampadu</surname>
              <given-names>K. O.</given-names>
            </name>
            <name>
              <surname>Huebner</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s25010152</pub-id>
          <article-title>Road traffic gesture autonomous integrity monitoring using fuzzy logic</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="conf-paper">
          <volume>13562</volume>
          <page-range>199–204</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>H. Y.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1117/12.3061389</pub-id>
          <article-title>Research on traffic sign image processing technology for autonomous vehicles</article-title>
          <source>2024 International Conference on Computer Graphics, Artificial Intelligence, and Data Processing (ICCAID), Ho Chi Minh City, Vietnam</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>74</volume>
          <page-range>307-315</page-range>
          <issue>4</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gan</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1504/IJCAT.2024.143291</pub-id>
          <article-title>Fast recognition method of pedestrian signs based on colour features and SVM</article-title>
          <source>Int. J. Comput. Appl. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>29</volume>
          <page-range>1483</page-range>
          <issue>4</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kusnandar</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Santoso</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Surendro</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/isi.290421</pub-id>
          <article-title>Enhancing color selection in HSV color space</article-title>
          <source>Ing. Syst. Inf.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <page-range>135-146</page-range>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Othman</surname>
              <given-names>M. K.</given-names>
            </name>
            <name>
              <surname>Abdulla</surname>
              <given-names>A. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.21928/uhdjst.v6n2y2022.pp135-146</pub-id>
          <article-title>Enhanced single image dehazing technique based on HSV color space</article-title>
          <source>UHD J. Sci. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>27-35</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Agrawal</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Desai</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.30780/specialissue-ISET-2024/023</pub-id>
          <article-title>Canny edge detection: A comprehensive review</article-title>
          <source>Int. J. Tech. Res. Sci.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>