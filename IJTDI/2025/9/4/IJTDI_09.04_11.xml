<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IJTDI</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>International Journal of Transport Development and Integration</journal-title>
        <abbrev-journal-title abbrev-type="issn">Int. J. Transp. Dev. Integr.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IJTDI</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2058-8313</issn>
      <issn publication-format="print">2058-8305</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-Bq3wvZY88o6oXGcGYUMlfMDEbBHIehtW</article-id>
      <article-id pub-id-type="doi">10.56578/ijtdi090411</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Traffic Sign Classification Using Spatial Transformer Networks (STN) based Convolutional Neural Networks (CNN)</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-3496-0649</contrib-id>
          <name>
            <surname>Tiryaki</surname>
            <given-names>Burcu</given-names>
          </name>
          <email>burcutiryaki@atauni.edu.tr</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8120-9679</contrib-id>
          <name>
            <surname>Oral</surname>
            <given-names>Emin Argun</given-names>
          </name>
          <email>eminoral@atauni.edu.tr</email>
        </contrib>
        <aff id="aff_1">Department of Electrical and Electronics Engineering, Faculty of Engineering, Ataturk University, 25240 Erzurum, Turkey</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>28</day>
        <month>12</month>
        <year>2025</year>
      </pub-date>
      <volume>9</volume>
      <issue>4</issue>
      <fpage>822</fpage>
      <lpage>832</lpage>
      <page-range>822-832</page-range>
      <history>
        <date date-type="received">
          <day>21</day>
          <month>09</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>20</day>
          <month>11</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Recognition of traffic signs by drivers is essential for ensuring road safety. Recently, with the growing demand for driver assistance systems and autonomous vehicles, traffic sign recognition has become increasingly important. In this study, Spatial Transformer Networks (STN) integrated with Convolutional Neural Networks (CNN) were used to classify traffic signs. STNs minimize the effects of geometric distortions by applying affine transformations to images, thereby improving classification performance. This study focuses on adapting and optimizing an STN-based CNN model specifically for the Russian Traffic Signs Dataset (RTSD) to achieve higher classification accuracy. The proposed model was trained and tested on the RTSD. First, the proposed CNN model was trained on the RTSD-R1 and RTSD-R3 datasets, achieving accuracy rates of 89.15% and 94.3%, respectively. Then, by integrating STN into the CNN model, the proposed model was trained on the RTSD-R1 and RTSD-R3 datasets, achieving accuracy rates of 93% and 95%, respectively. These results demonstrate that incorporating STNs into the CNNs is effective in improving traffic sign classification performance.</p></abstract>
      <kwd-group>
        <kwd>STN</kwd>
        <kwd>CNN</kwd>
        <kwd>Traffic sign</kwd>
        <kwd>Classification</kwd>
        <kwd>RTSD</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="2"/>
        <fig-count count="7"/>
        <table-count count="2"/>
        <ref-count count="23"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Nowadays, identification and classification of traffic signs has become essential components of driver assistance systems and autonomous vehicles. Traffic sign recognition and detection are essential for road safety and advanced driver-assistance systems [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>]. Traffic signs provide essential information about road conditions and help maintain traffic order for both drivers and pedestrians. It plays a very important role in regulating traffic and facilitating safe driving. Therefore, traffic signs are vital for drivers and pedestrians. Traffic signs are difficult to recognize under poor illumination, partial occlusion, outdated signage, or adverse weather conditions, outdated traffic signs, adverse weather conditions such as rain, snow, fog, etc. and safety risks of following traffic signs while driving. These situations may also lead to accidents. In order to minimize the number of accidents, automatic systems have been developed that automatically detect traffic signs and warn drivers during driving. In this sense, driver assistance systems are utilized to detect and recognize traffic signs and make real-time decisions [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>]. With the increasing interest in autonomous vehicles in recent years, on the other hand, detection of traffic signs has become very important for environmental perception. In this context, traffic sign detection is required to locate the signs in the scene before classifying them into specific categories.</p><p>Various environmental factors and the visual similarity of traffic signs can adversely affect real-time performance of such autonomous systems. To resolve this, machine learning methods are used both to recognize and classify traffic signs processing image related features. In order to obtain such features from images, deep learning methods, a novel machine learning approach, can be used to extract general and hidden features. For this purpose, Convolutional Neural Networks (CNN) are employed. CNNs have been used in many studies such as visual recognition, speech recognition and natural language processing in recent years. A traditional convolutional neural network consists of one or more blocks of convolution and pooling layers, followed by one or more fully connected (FC) layers and an output layer. Convolutional layers consist of filters to calculate different feature maps. The pooling layer, on the other hand, takes a small region as input and produces a single output for downsampling. The fully connected layer, produces output from the inputs it receives from the final pooling or convolutional layer [<xref ref-type="bibr" rid="ref_5">5</xref>].</p><p>In the literature, there are some studies on recognizing and classifying traffic signs. Luo et al proposed a new data-driven system to recognize all traffic sign categories, including both symbol-based and text-based signs, in video, captured by a car-mounted camera. Li and Wang [<xref ref-type="bibr" rid="ref_6">6</xref>] designed the sensor using faster R-CNN neural network and MobileNet model. In the study, shape and color information was used to improve recognition of small traffic signs. An efficient CNN with an asymmetric core is used as the classifier of traffic signs. Song et al.  [<xref ref-type="bibr" rid="ref_7">7</xref>] proposed a convolutional neural network that reduces parameters and speeds up networks. Network performance is evaluated on the Tsinghua–Tencent 100K dataset. Experimental results showed that the model outperformed Fast R-CNN and Faster R-CNN. Zhu and Yan [<xref ref-type="bibr" rid="ref_8">8</xref>] evaluated YOLOv5 performance for traffic sign recognition in their study. They showed that YOLOv5 has better performance when compared with SSD detector.</p><p>In the study conducted by Alawaji et al. [<xref ref-type="bibr" rid="ref_9">9</xref>], the aim was to enhance the safety of driverless transportation systems operating on fixed routes by detecting and recognizing traffic signs using computer vision and deep learning techniques. The research primarily employed CNN-based approaches. In this framework, a structure was developed in which convolutional layer parameters were shared across different tasks, utilizing pre-trained models such as InceptionResNetV2 and DenseNet201. Initially, single-task models were compared on symbol-based traffic signs, and subsequently, a multi-task learning approach was applied to further improve accuracy. In the study conducted by Kandasamy et al., the focus was on recent advancements in machine perception, particularly in the context of autonomous vehicles. The accurate detection and interpretation of road signs by autonomous systems are critically important for ensuring safety and efficiency on the roads. In this research, a novel algorithm called TrafficSignNet was developed using a custom dataset that reflects the diversity of traffic signs in India, consisting of 12 main categories and 7 subcategories. The algorithm was specifically designed to recognize traffic signs indicating speed limits, turns, zones, and road bumps. The model was trained on 4,962 images and evaluated on 705 real traffic images, demonstrating high accuracy under varying lighting conditions [<xref ref-type="bibr" rid="ref_10">10</xref>]. In the study conducted by Enan and Chowdhury [<xref ref-type="bibr" rid="ref_11">11</xref>], a defense method was developed against adversarial patch attacks (APA), a type of physical attack targeting the computer vision module of autonomous vehicles. The researchers proposed a GAN-based single-stage defense strategy to enhance the robustness of traffic sign classifiers against such attacks. The proposed method can protect different classes of traffic signs without requiring prior knowledge and increases classification accuracy by up to 80.8% under APA conditions. Moreover, since the approach is model-agnostic, it can be applied to any traffic sign classification model.</p><p>Although existing deep learning models achieve high accuracy, their performance may vary under geometric distortions, non-uniform lighting, and viewpoint changes. Spatial Transformer Networks (STN) have been proposed to address these issues by learning spatial transformations that normalize input images before classification. In this study, a CNN architecture integrated with STN modules is developed and optimized for the Russian Traffic Signs Dataset (RTSD) dataset. Unlike previous works, this study does not introduce a new algorithmic framework but rather designs a more suitable network architecture specifically adapted to the RTSD dataset. The proposed model aims to enhance classification accuracy by mitigating geometric variations while maintaining computational efficiency.</p><p>The rest of this article is organized as follows: Section 2 describes our proposed method for traffic sign classification. Section 3 shows the experimental results. Section 4 shows the discussion and finally Section 5 shows the conclusion and future work.</p>
    </sec>
    <sec sec-type="">
      <title>2. Materials and methods</title>
      <p>In this study, a traffic sign recognition network integrating a convolutional neural network and a spatial transformer module is proposed. <xref ref-type="fig" rid="fig_1">Figure 1</xref> illustrates the main block diagram of the proposed traffic sign recognition system. It is composed of two main blocks: a STN and classifier network. In the traffic sign recognition framework, traffic sign images are first resized to different resolutions of 20 <inline-formula>
  <mml:math id="m31kif66a8">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 20, 60 <inline-formula>
  <mml:math id="msqu5ir54z">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 60, and 90 <inline-formula>
  <mml:math id="mz76vx0qop">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 90 before being fed into the spatial transformer network. The STN performs an affine transformation on the input image to improve classification performance by removing unnecessary background elements and enhancing focus [<xref ref-type="bibr" rid="ref_12">12</xref>]. The STN consists of a proposed localization network to detect the necessary affine transformation parameters, including cropping, translation, rotation, scale, and skew, a grid generator and a sampler, as shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>.</p><p>STN identify the appropriate region in the input image and generate a set of affine parameters (<inline-formula>
  <mml:math id="mvhqaftt3z">
    <mml:mi>θ</mml:mi>
  </mml:math>
</inline-formula>) for the transform. This information is then forwarded to the grid generator. Using the supplied affine transformation parameters, the system performs operations such as normalization, cropping, and scaling on the relevant region of the input image. During the sampling phase, the output image is reconstructed using the parameters obtained from the grid generator. As a result, input traffic sign images captured at non-ideal angles or containing geometric distortions are corrected.</p><p>The proposed architecture integrates the STN directly with the CNN classifier in an end-to-end manner. The STN module first receives the input image and performs affine transformations, including translation, scaling, rotation, and cropping, to correct geometric distortions and emphasize the region of interest. The transformed output feature maps generated by the STN are then passed as tensors to the initial convolutional layer of the CNN classifier. This sequential tensor flow allows the network to automatically learn spatially normalized representations before feature extraction and classification. </p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Block diagram of the proposed traffic sign recognition system</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/11/img_yBsHCdu0fracUryx.png"/>
        </fig>
      
      <p>In the final classification step, the output image of the STN is processed by proposed classification network of traffic signs. This proposed classification network is based on CNN and described in the following sub-sections.</p>
      
        <sec>
          
            <title>2.1. Dataset</title>
          
          <p>The RSTD consists of frames captured at a rate of 5 frames per second from a video recorder provided by the company Geocenter Consulting [<xref ref-type="bibr" rid="ref_13">13</xref>]. The video was captured at different times of a day, in different seasons and in different weather conditions. For the evaluation of the detectors, the dataset was divided into three groups as RTSD-D1, RTSD-D2 and RTSD-D3. For the evaluation of classifiers, the dataset is divided into two groups: RTSD-R1 and RTSD-R3. Here, RTSD-R1 consists of traffic signs cropped from the images in the RTSD-D1 dataset while RTSD-R3 consists of traffic signs cropped from the images in the RTSD-D3 dataset. Among these, RTSD-R1 consists of 66 classes with 25.432 training and 7.551 test images, and RTSD-R3 consists of 106 classes that include 70.687 training and 22.967 test images [<xref ref-type="bibr" rid="ref_12">12</xref>]. <xref ref-type="fig" rid="fig_2">Figure 2</xref> shows examples of images in the RTSD-R1 and RTSD-R3 datasets. The RTSD-R1 and RTSD-R3 datasets were used with their predefined training and test partitions as provided by the original RTSD benchmark. Both subsets are class-balanced, ensuring proportional representation of each traffic-sign category.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Example images of (a) RTSD-R1 and (b) RTSD-R3 dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/11/img_cjXZ4j28XHwJzfvL.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>2.2. Matconvnet</title>
          
          <p>MatConvNet is an open source MATLAB toolbox developed by the authors of the VLfeat library [<xref ref-type="bibr" rid="ref_14">14</xref>]. It uses CNN in image processing applications. Since it is designed as simple and flexible, it enables rapid prototyping of CNN architectures. Convolutional neural network applications handle large amounts of data. MatConvNet supports working with large datasets and training of complex models by enabling calculations on both CPU and GPU. Therefore, the main reason for the development of the MatConvNet toolbox is to ensure that the work is done in a more efficient environment. Integration into the MATLAB working environment is possible [<xref ref-type="bibr" rid="ref_15">15</xref>]. Layers are easily combined and extended when building the CNN architecture. The layers in the neural network are arranged in arrays. Interconnections can also be made. Basically, the network structure created in MatConvNet is shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>MatConvNet chain block structure [<xref ref-type="bibr" rid="ref_15">15</xref>]</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/11/img_iJq-VoT-prBjtBis.png"/>
            </fig>
          
          <p>In the $f<inline-formula>
  <mml:math id="mcguw8lej0">
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>X_0$ shows the input data and W shows the parameter values. The function obtained when the network is evaluated from left to right;</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="m344m0is0r">
                <mml:msub>
                  <mml:mi>X</mml:mi>
                  <mml:mi>L</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mi>f</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>;</mml:mo>
                  <mml:mo>;</mml:mo>
                  <mml:mo>…</mml:mo>
                  <mml:mo>;</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>X</mml:mi>
                    <mml:mn>0</mml:mn>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>W</mml:mi>
                    <mml:mi>l</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>W</mml:mi>
                    <mml:mi>L</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>MatConvNet includes two wrappers, SimpleNN and DagNN, designed to accommodate different types of network architectures. While SimpleNN is suitable for networks with linear structure, DagNN structure is used for more complex networks.</p><p>MatConvNet performs a MATLAB function as $y<inline-formula>
  <mml:math id="mh3y3zof7j">
    <mml:mo>=</mml:mo>
    <mml:mi>v</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:msub>
      <mml:mi>l</mml:mi>
      <mml:mi>n</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>&lt;<inline-formula>
  <mml:math id="m1557mg111">
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>k</mml:mi>
  </mml:math>
</inline-formula>&gt;(x, w)<inline-formula>
  <mml:math id="mq10ct6nag">
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>x<inline-formula>
  <mml:math id="m1hog7lg4f">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>w<inline-formula>
  <mml:math id="mtqroujz81">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>y<inline-formula>
  <mml:math id="mt94oli2or">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>V</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>U</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:msub>
      <mml:mi>l</mml:mi>
      <mml:mi>n</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>y<inline-formula>
  <mml:math id="m60516458z">
    <mml:mo>=</mml:mo>
    <mml:mi>v</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:msub>
      <mml:mi>l</mml:mi>
      <mml:mi>n</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>(x, f, b)<inline-formula>
  <mml:math id="mnqfierpuz">
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>x<inline-formula>
  <mml:math id="mmc2wkine8">
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>f<inline-formula>
  <mml:math id="mhjngk7w99">
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>b$ the slope. The filter is circulated over the image according to the specified number of steps.</p><p>Activation function: Two activation functions are supported:</p><p>ReLU: vl_nnrelu calculates the rectified linear unit layer.</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="mwpv6xga94">
                <mml:msub>
                  <mml:mi>y</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mi>j</mml:mi>
                    <mml:mi>d</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>max</mml:mo>
                <mml:mrow>
                  <mml:mo>{</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>}</mml:mo>
                  <mml:mn>0</mml:mn>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mi>j</mml:mi>
                      <mml:mi>d</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>Sigmoid: vl_nnsigmoid calculates sigmoid.</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="mz6aqvcdop">
                <mml:msub>
                  <mml:mi>y</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mi>j</mml:mi>
                    <mml:mi>d</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mi>σ</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mi>j</mml:mi>
                      <mml:mi>d</mml:mi>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
                <mml:mfrac>
                  <mml:mn>1</mml:mn>
                  <mml:mrow>
                    <mml:mn>1</mml:mn>
                    <mml:mo>+</mml:mo>
                    <mml:msup>
                      <mml:mi>e</mml:mi>
                      <mml:mrow>
                        <mml:mo>−</mml:mo>
                        <mml:msub>
                          <mml:mi>x</mml:mi>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                            <mml:mi>j</mml:mi>
                            <mml:mi>d</mml:mi>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:msup>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
        </sec>
      
      
        <sec>
          
            <title>2.3. Stn</title>
          
          <p>The spatial transformation mechanism consists of a localization network, grid generator, and sampler. While CNNs have limited ability to perform spatial transformations on images, STNs overcome this limitation by applying operations such as scaling, rotation, and shearing to each input data. The transformations are applied across the entire feature map, eliminating the need for manual scaling of the data [<xref ref-type="bibr" rid="ref_16">16</xref>]. STNs identify important regions in images and adjust their poses to be processed effectively in subsequent layers, thereby reducing geometric variability in the data. Transformation networks placed in CNNs are trained by back propagation, allowing the CNN architecture to be trained end-to-end. </p><p>STNs consist of three parts: Localization Network, Grid Generator and Sampling.</p>
          
            <sec>
              
                <title>2.3.1 Localization network</title>
              
              <p>The purpose of the network is to determine the affine transformation parameters to be applied on the input map. The network can have any number of layers, but the last layer must be a regression layer with six output neurons to generate the transformation parameters (<inline-formula>
  <mml:math id="mpmi4dly7p">
    <mml:mi>θ</mml:mi>
  </mml:math>
</inline-formula>). Affine transform parameters are used to correct images with geometric distortions. Spatial transformations perform transformation operations in the relevant regions, and the transformation information is compressed in the weights of the localization network.</p><p>The localization network receives the input feature map <inline-formula>
  <mml:math id="mjkv8a3epu">
    <mml:mi>U</mml:mi>
    <mml:mi>ϵ</mml:mi>
    <mml:msup>
      <mml:mi>R</mml:mi>
      <mml:mrow>
        <mml:mi>H</mml:mi>
        <mml:mi>W</mml:mi>
        <mml:mi>C</mml:mi>
        <mml:mo>×</mml:mo>
        <mml:mo>×</mml:mo>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula>. Where $U<inline-formula>
  <mml:math id="m97dbgx9sn">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>H<inline-formula>
  <mml:math id="mhsxu7z04u">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>W<inline-formula>
  <mml:math id="m7rofmd3bb">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>C$ is the number of channels [<xref ref-type="bibr" rid="ref_16">16</xref>]. The output of the localization network is represented by Eq. (4).</p>
              
                <disp-formula>
                  <label>(4)</label>
                  <mml:math id="micff4jel8">
                    <mml:mi>θ</mml:mi>
                    <mml:mi>U</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msub>
                      <mml:mi>f</mml:mi>
                      <mml:mrow>
                        <mml:mi>l</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>c</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:math>
                </disp-formula>
              
              <p> where, <inline-formula>
  <mml:math id="mzgk78qxtd">
    <mml:msub>
      <mml:mi>f</mml:mi>
      <mml:mrow>
        <mml:mi>l</mml:mi>
        <mml:mi>o</mml:mi>
        <mml:mi>c</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> is the localization network function and <inline-formula>
  <mml:math id="mgch87cacx">
    <mml:mi>θ</mml:mi>
  </mml:math>
</inline-formula> denotes the 6 -dimensional affine transformation parameters.</p>
            </sec>
          
          
            <sec>
              
                <title>2.3.2 Grid generator</title>
              
              <p>Creates a coordinate grid corresponding to each pixel in the output image using the transformation predicted by the localization network in the input image. Each output pixel is computed by applying a sampling kernel focused on a specific location in the input feature map. In general, the output pixels are defined to be located on a regular grid as in Eq. (5).</p>
              
                <disp-formula>
                  <label>(5)</label>
                  <mml:math id="m49ebjk7t1">
                    <mml:mi>G</mml:mi>
                    <mml:mi>G</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mi>G</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mo fence="false">{</mml:mo>
                    <mml:mo fence="false">}</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msubsup>
                        <mml:mi>x</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                      </mml:msubsup>
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi data-mjx-variant="-tex-calligraphic">Y</mml:mi>
                        </mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                      </mml:msubsup>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where, $G<inline-formula>
  <mml:math id="mer16fe8ih">
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>x_i^t<inline-formula>
  <mml:math id="m213q5gumo">
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>y_i^t<inline-formula>
  <mml:math id="m9xwg6gx3e">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>V \epsilon R^{H^{\prime} \times W^{\prime} \times C}<inline-formula>
  <mml:math id="mmgkhvkq71">
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>H^{\prime}<inline-formula>
  <mml:math id="m8xlua54hx">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>W^{\prime}$ indicate the height and width of the grid. The number of channels (C) is the same for input and output [<xref ref-type="bibr" rid="ref_17">17</xref>].</p>
              <p><xref ref-type="fig" rid="fig_4">Figure 4</xref> shows how to obtain the $V<inline-formula>
  <mml:math id="mdyxm3sare">
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>U<inline-formula>
  <mml:math id="mnrvtgmfm6">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>4</mml:mn>
  </mml:math>
</inline-formula>G=\tau_{\imath}(G)<inline-formula>
  <mml:math id="myu4ng0a4b">
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>I</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mn>4</mml:mn>
  </mml:math>
</inline-formula>\tau_\theta(\mathrm{G})<inline-formula>
  <mml:math id="mqexaz1qan">
    <mml:mo>.</mml:mo>
    <mml:mi>H</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>I<inline-formula>
  <mml:math id="m4jpizdoo5">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\tau_\theta$ is the affine transformation parameter. Eq. (6) illustrates the affine transformation.</p>
              
                <fig id="fig_4">
                  <label>Figure 4</label>
                  <caption>
                    <title>Grid generator [<xref ref-type="bibr" rid="ref_17">17</xref>]</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/11/img_wxxgYCngDx42ePgp.png"/>
                </fig>
              
              
                <disp-formula>
                  <label>(6)</label>
                  <mml:math id="mb4k79zyqh">
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mo minsize="2.047em" maxsize="2.047em">(</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo minsize="2.047em" maxsize="2.047em">)</mml:mo>
                      </mml:mrow>
                      <mml:mfrac linethickness="0">
                        <mml:msubsup>
                          <mml:mi>x</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mi>s</mml:mi>
                        </mml:msubsup>
                        <mml:msubsup>
                          <mml:mi>y</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mi>s</mml:mi>
                        </mml:msubsup>
                      </mml:mfrac>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mtable columnspacing="1em" rowspacing="4pt">
                        <mml:mtr>
                          <mml:mtd>
                            <mml:msubsup>
                              <mml:mi>x</mml:mi>
                              <mml:mi>i</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msubsup>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd>
                            <mml:msubsup>
                              <mml:mi>y</mml:mi>
                              <mml:mi>i</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msubsup>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd>
                            <mml:mn>1</mml:mn>
                          </mml:mtd>
                        </mml:mtr>
                      </mml:mtable>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>[</mml:mo>
                      <mml:mo>]</mml:mo>
                      <mml:mtable columnalign="left left left" columnspacing="1em" rowspacing="4pt">
                        <mml:mtr>
                          <mml:mtd>
                            <mml:msub>
                              <mml:mi>θ</mml:mi>
                              <mml:mrow>
                                <mml:mn>11</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd>
                            <mml:msub>
                              <mml:mi>θ</mml:mi>
                              <mml:mrow>
                                <mml:mn>12</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd>
                            <mml:msub>
                              <mml:mi>θ</mml:mi>
                              <mml:mrow>
                                <mml:mn>13</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd>
                            <mml:msub>
                              <mml:mi>θ</mml:mi>
                              <mml:mrow>
                                <mml:mn>21</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd>
                            <mml:msub>
                              <mml:mi>θ</mml:mi>
                              <mml:mrow>
                                <mml:mn>22</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                          <mml:mtd>
                            <mml:msub>
                              <mml:mi>θ</mml:mi>
                              <mml:mrow>
                                <mml:mn>23</mml:mn>
                              </mml:mrow>
                            </mml:msub>
                          </mml:mtd>
                        </mml:mtr>
                      </mml:mtable>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mtable columnspacing="1em" rowspacing="4pt">
                        <mml:mtr>
                          <mml:mtd>
                            <mml:msubsup>
                              <mml:mi>x</mml:mi>
                              <mml:mi>i</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msubsup>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd>
                            <mml:msubsup>
                              <mml:mi>y</mml:mi>
                              <mml:mi>i</mml:mi>
                              <mml:mi>t</mml:mi>
                            </mml:msubsup>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd>
                            <mml:mn>1</mml:mn>
                          </mml:mtd>
                        </mml:mtr>
                      </mml:mtable>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:msub>
                      <mml:mi>τ</mml:mi>
                      <mml:mrow>
                        <mml:mi>θ</mml:mi>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:msub>
                            <mml:mi>G</mml:mi>
                            <mml:mi>i</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mo>=</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mi>A</mml:mi>
                          </mml:mrow>
                          <mml:mi>θ</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                    </mml:msub>
                  </mml:math>
                </disp-formula>
              
              <p> where, <inline-formula>
  <mml:math id="mcuibwixox">
    <mml:msub>
      <mml:mrow>
        <mml:mi>A</mml:mi>
      </mml:mrow>
      <mml:mi>θ</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is the affine transformation. (<inline-formula>
  <mml:math id="m5fhnzscqq">
    <mml:msubsup>
      <mml:mi>x</mml:mi>
      <mml:mi>i</mml:mi>
      <mml:mi>s</mml:mi>
    </mml:msubsup>
    <mml:msubsup>
      <mml:mi>y</mml:mi>
      <mml:mi>i</mml:mi>
      <mml:mi>s</mml:mi>
    </mml:msubsup>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>), defines the coordinates in the input feature map, (<inline-formula>
  <mml:math id="mx66ow4xki">
    <mml:msubsup>
      <mml:mi>x</mml:mi>
      <mml:mi>i</mml:mi>
      <mml:mi>t</mml:mi>
    </mml:msubsup>
    <mml:msubsup>
      <mml:mi>y</mml:mi>
      <mml:mi>i</mml:mi>
      <mml:mi>t</mml:mi>
    </mml:msubsup>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>), defines the positions in the output feature map. Since the affine transformation in Eq. (6) allows clipping, translation, scaling, rotation and warping operations, 6 parameters must be generated by the localization network [<xref ref-type="bibr" rid="ref_16">16</xref>].</p>
            </sec>
          
          
            <sec>
              
                <title>2.3.3 Sampling</title>
              
              <p>The feature map and sampling grid are used as inputs to the sampler, and the sampler produces the output map by sampling from the input at the grid points. It produces the output V based on the new coordinate sets (<inline-formula>
  <mml:math id="m43haxtjmu">
    <mml:msubsup>
      <mml:mi>x</mml:mi>
      <mml:mi>i</mml:mi>
      <mml:mi>t</mml:mi>
    </mml:msubsup>
    <mml:msubsup>
      <mml:mi>y</mml:mi>
      <mml:mi>i</mml:mi>
      <mml:mi>t</mml:mi>
    </mml:msubsup>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>), coming from the grid generator. Sampling is defined by Eq. (7).</p>
              
                <disp-formula>
                  <label>(7)</label>
                  <mml:math id="m2fvl8f7x4">
                    <mml:msubsup>
                      <mml:mi>V</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>c</mml:mi>
                    </mml:msubsup>
                    <mml:msubsup>
                      <mml:mi>U</mml:mi>
                      <mml:mi>c</mml:mi>
                      <mml:mrow>
                        <mml:mi>n</mml:mi>
                        <mml:mi>m</mml:mi>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>=</mml:mo>
                    <mml:munderover>
                      <mml:mo>∑</mml:mo>
                      <mml:mi>n</mml:mi>
                      <mml:mi>H</mml:mi>
                    </mml:munderover>
                    <mml:munderover>
                      <mml:mo>∑</mml:mo>
                      <mml:mi>m</mml:mi>
                      <mml:mi>W</mml:mi>
                    </mml:munderover>
                    <mml:mi>k</mml:mi>
                    <mml:mi>k</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>;</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msubsup>
                        <mml:mi>x</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>s</mml:mi>
                      </mml:msubsup>
                      <mml:mi>m</mml:mi>
                      <mml:msub>
                        <mml:mi>Φ</mml:mi>
                        <mml:mi>x</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>;</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msubsup>
                        <mml:mrow>
                          <mml:mi data-mjx-variant="-tex-calligraphic">Y</mml:mi>
                        </mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mi>s</mml:mi>
                      </mml:msubsup>
                      <mml:mi>n</mml:mi>
                      <mml:msub>
                        <mml:mi>Φ</mml:mi>
                        <mml:mi>y</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where, (<inline-formula>
  <mml:math id="mn5g407kvq">
    <mml:msubsup>
      <mml:mi>x</mml:mi>
      <mml:mi>i</mml:mi>
      <mml:mi>s</mml:mi>
    </mml:msubsup>
    <mml:msubsup>
      <mml:mi>y</mml:mi>
      <mml:mi>i</mml:mi>
      <mml:mi>s</mml:mi>
    </mml:msubsup>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>) defines the positions of the $V<inline-formula>
  <mml:math id="m9ibkz269y">
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
  </mml:math>
</inline-formula>i<inline-formula>
  <mml:math id="mpyt4if14t">
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\Phi_x<inline-formula>
  <mml:math id="mz93sadmhs">
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>\Phi_y<inline-formula>
  <mml:math id="m5pqt85b14">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>U_{n m}^c<inline-formula>
  <mml:math id="mlhw1obqa2">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>n<inline-formula>
  <mml:math id="mwjlq183pf">
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>m<inline-formula>
  <mml:math id="mbtjn9lbfo">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
  </mml:math>
</inline-formula>c<inline-formula>
  <mml:math id="mcxztqu32h">
    <mml:mo>,</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>V_i^c<inline-formula>
  <mml:math id="mvjm6xzv9l">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
  </mml:math>
</inline-formula>i<inline-formula>
  <mml:math id="m4ca4kfud8">
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>(</mml:mo>
  </mml:math>
</inline-formula>x_i^t, y_i^t<inline-formula>
  <mml:math id="m897jlch4l">
    <mml:mo>)</mml:mo>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
  </mml:math>
</inline-formula>c$ . Sampling is done the identical way for each channel of the input, so each channel is converted the identical way [<xref ref-type="bibr" rid="ref_17">17</xref>].</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>2.4. Proposed localization network</title>
          
          <p>In the proposed localization network, a small convolutional neural network model is used. This model consists of a convolutional structure with layers such as Maxpool, Conv, ReLU, Maxpool, Conv, ReLU, followed by a Fully Connected Layer, ReLU, and an output layer. To ensure that the localization network remains compatible with all three input resolutions (20 <inline-formula>
  <mml:math id="m530a3vvs6">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 20, 60 <inline-formula>
  <mml:math id="m0awpyuxn5">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 60, and 90 <inline-formula>
  <mml:math id="msmydd0i12">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 90), the spatial dimensions of the intermediate feature maps were recalculated for each input size. The kernel sizes, strides, and pooling operations were checked against each resolution to confirm that the resulting feature maps retain valid spatial dimensions before entering the fully connected layer. The architecture itself—number of layers and channel depths—remains unchanged across resolutions; only the spatial dimensions vary according to the input size. This adjustment prevents the feature maps from collapsing to overly small or non-integer sizes and allows the same localization network structure to operate consistently at different resolutions. The parameters of the proposed localization network are detailed in <xref ref-type="table" rid="table_1">Table 1</xref>. In <xref ref-type="table" rid="table_1">Table 1</xref>, the parameters calculated for the 90 <inline-formula>
  <mml:math id="m0gq3elp85">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 90 size are given as an example. The same parameters were used for the RTSD-R1 and RTSD-R2 datasets of the same size.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Parameters of the proposed STN architecture</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Layers</p></th><th colspan="1" rowspan="1"><p>Kernel Size</p></th><th colspan="1" rowspan="1"><p>Filters</p></th><th colspan="1" rowspan="1"><p>Units</p></th><th colspan="1" rowspan="1"><p>Stride</p></th><th colspan="1" rowspan="1"><p>Padding</p></th><th colspan="1" rowspan="1"><p><mml:math id="mfz1fp3dno">
  <mml:mi>θ</mml:mi>
</mml:math></p></th></tr><tr><td colspan="1" rowspan="1"><p>Max Pooling</p></td><td colspan="1" rowspan="1"><p>2 <mml:math id="mgyqgc49cd">
  <mml:mo>×</mml:mo>
</mml:math> 2</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>0</p></td><td colspan="1" rowspan="1"></td></tr><tr><td colspan="1" rowspan="1"><p>Convolution</p></td><td colspan="1" rowspan="1"><p>12 <mml:math id="mj9swuo2hh">
  <mml:mo>×</mml:mo>
</mml:math> 12</p></td><td colspan="1" rowspan="1"><p>70</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>0</p></td><td colspan="1" rowspan="1"></td></tr><tr><td colspan="1" rowspan="1"><p>ReLU</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td></tr><tr><td colspan="1" rowspan="1"><p>Max Pooling</p></td><td colspan="1" rowspan="1"><p>2 <mml:math id="m57t9wbvus">
  <mml:mo>×</mml:mo>
</mml:math> 2</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>0</p></td><td colspan="1" rowspan="1"></td></tr><tr><td colspan="1" rowspan="1"><p>Convolution</p></td><td colspan="1" rowspan="1"><p>11 <mml:math id="mxncvbtfba">
  <mml:mo>×</mml:mo>
</mml:math> 11</p></td><td colspan="1" rowspan="1"><p>110</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td></tr><tr><td colspan="1" rowspan="1"><p>ReLU</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td></tr><tr><td colspan="1" rowspan="1"><p>Fully Connected</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>180</p></td><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>0</p></td><td colspan="1" rowspan="1"></td></tr><tr><td colspan="1" rowspan="1"><p>ReLU</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td></tr><tr><td colspan="1" rowspan="1"><p>Output</p></td><td colspan="1" rowspan="1"><p>1 <mml:math id="mdngtq4fse">
  <mml:mo>×</mml:mo>
</mml:math> 1</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>0</p></td><td colspan="1" rowspan="1"><p>6</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>2.5. Proposed classification network</title>
          
          <p>We constructed a CNN model without spatial transformer layers to evaluate their impact on the recognition of traffic signs. The model consists of a convolutional structure with Conv, ReLU, MaxPool, Conv, ReLU, and MaxPool layers. Then, following these layers, a fully connected layer and a ReLU layer are added, respectively. With the convolutional layers used in the network, features in the input image are identified, a feature map is created, and the data is processed. With the MaxPooling layer, the computational load is reduced by decreasing the size of the feature maps. A fully connected layer was used to make the features from these layers available for classification. A classifier layer has been added to classify traffic signs.</p><p>The layers used in the proposed model and the parameter values for each layer are shown in <xref ref-type="table" rid="table_2">Table 2</xref>. In <xref ref-type="table" rid="table_2">Table 2</xref>, the parameters calculated for the 90 <inline-formula>
  <mml:math id="mz88kwt1qa">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 90 size are given as an example. Layer size, stride, and padding values are calculated based on the specified image size. The stride value of each convolutional layer is set to 1, while the stride value of each max pooling layer is set to 2, allowing for spatial subsampling calculations in this layer. Additionally, the kernel size values for each layer have been adjusted according to these calculations.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Parameters of the proposed CNN architecture</title>
              </caption>
              <table><tr><th >Layers</th><th >Kernel Size</th><th >Filters</th><th >Units</th><th >Stride</th><th >Padding</th><th >Number of Class</th></tr><tr><td >Convolution</td><td >10 <mml:math id="mbghopijr8">
  <mml:mo>×</mml:mo>
</mml:math> 10</td><td >32</td><td ></td><td >1</td><td >0</td><td ></td></tr><tr><td >ReLU</td><td ></td><td ></td><td ></td><td ></td><td ></td><td ></td></tr><tr><td >Max Pooling</td><td >2 <mml:math id="mw1cg8tqmf">
  <mml:mo>×</mml:mo>
</mml:math> 2</td><td ></td><td ></td><td >2</td><td >0</td><td ></td></tr><tr><td >Convolution</td><td >7 <mml:math id="m9kw4acizh">
  <mml:mo>×</mml:mo>
</mml:math> 7</td><td >32</td><td ></td><td >1</td><td >0</td><td ></td></tr><tr><td >ReLU</td><td ></td><td ></td><td ></td><td ></td><td ></td><td ></td></tr><tr><td >Max Pooling</td><td >2 <mml:math id="m4wo3y3gj9">
  <mml:mo>×</mml:mo>
</mml:math> 2</td><td ></td><td ></td><td >2</td><td >0</td><td ></td></tr><tr><td >Convolution</td><td >5 <mml:math id="mj4qdbr1qw">
  <mml:mo>×</mml:mo>
</mml:math> 5</td><td >64</td><td ></td><td >1</td><td >0</td><td ></td></tr><tr><td >Fully Connected</td><td ></td><td ></td><td >128</td><td >1</td><td >0</td><td ></td></tr><tr><td >ReLU</td><td ></td><td ></td><td ></td><td ></td><td ></td><td ></td></tr><tr><td >Classifiers</td><td >1 <mml:math id="mdb1e7rauh">
  <mml:mo>×</mml:mo>
</mml:math> 1</td><td ></td><td ></td><td >1</td><td >0</td><td >66 / 106</td></tr></table>
            </table-wrap>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Experimental results</title>
      <p>The experimental results are presented for two cases: one involving the application of the CNN-based classification network and the other involving the application of the STN architecture to improve performance. All experiments were run on a computer equipped with an Intel Core i7-7700HQ CPU, 16 GB of RAM, and a Nvidia GeForce GTX 1050 discrete GPU with 8 GB of internal memory. In the optimization parameters used during all training stages, Stochastic Gradient Descent with Momentum (SGDM) was employed as the optimization algorithm, the mini-batch size was set to 80, and the number of epochs was set to 50. The learning rate was set to 1 <inline-formula>
  <mml:math id="mldbmzqnpp">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 10<inline-formula>
  <mml:math id="mk5j6l4jpg">
    <mml:msup>
      <mml:mi/>
      <mml:mrow>
        <mml:mo>−</mml:mo>
        <mml:mn>3</mml:mn>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula>, a momentum of 0.9 was used, and an L2 regularization term with a weight decay of 1 <inline-formula>
  <mml:math id="mgo4iwn9z8">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 10<inline-formula>
  <mml:math id="mrh2renr5q">
    <mml:msup>
      <mml:mi/>
      <mml:mrow>
        <mml:mo>−</mml:mo>
        <mml:mn>4</mml:mn>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> was applied to prevent overfitting.</p><p>The model contains approximately 3 million parameters (12.2 MB). The average inference time per image was measured to be 11.0 ms per image, corresponding to roughly 90 frames per second (FPS). These results indicate that the proposed network achieves real-time performance while maintaining a moderate computational load.</p><p>All experiments were conducted using the predefined RTSD-R1 and RTSD-R3 training–test splits provided by the official benchmark. Since no additional random sampling or reshuffling was applied, the results are deterministic. Each experiment was run once.</p><p>The confidence intervals (CI) of the accuracy rates were calculated according to Eq. (8), where $z$ denotes the constant value corresponding to a 95% confidence level (1.96) and n represents the size of the test dataset [<xref ref-type="bibr" rid="ref_18">18</xref>], [<xref ref-type="bibr" rid="ref_19">19</xref>], [<xref ref-type="bibr" rid="ref_20">20</xref>].</p>
      
        <disp-formula>
          <label>(8)</label>
          <mml:math id="maszxejixe">
            <mml:mi>C</mml:mi>
            <mml:mi>I</mml:mi>
            <mml:mi>A</mml:mi>
            <mml:mi>c</mml:mi>
            <mml:mi>c</mml:mi>
            <mml:mi>z</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mo>±</mml:mo>
            <mml:msqrt>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mi>A</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:mi>A</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:mo>(</mml:mo>
                  <mml:mo>−</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
                <mml:mi>n</mml:mi>
              </mml:mfrac>
            </mml:msqrt>
          </mml:math>
        </disp-formula>
      
      <p>To observe the effect of the STN architecture on classification performance, each dataset was first classified using the proposed Traffic Sign Classification Network (TSCN), and then the performances were compared by adding the STN mechanism to this network. When the RTSD-R1 dataset was resized to 20 <inline-formula>
  <mml:math id="mwabb2j9u0">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 20 and classified using the TSCN network, an accuracy of 87.11 <inline-formula>
  <mml:math id="mj4hhm61y7">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula> 0.0076% was achieved, while an accuracy of 92.01 <inline-formula>
  <mml:math id="m0uysxyrzc">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula> 0.006% was obtained by adding the STN mechanism to the network. Similarly, when the RTSD-R1 dataset was resized to 60 <inline-formula>
  <mml:math id="mj9303ucsr">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 60, an accuracy of 87.95 <inline-formula>
  <mml:math id="m0732o368k">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula> 0.0074% was achieved using the TSCN network, while an accuracy of 91.46 <inline-formula>
  <mml:math id="m3l79kjd3h">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula> 0.006% was obtained by adding the STN mechanism to the network. When the RTSD-R1 dataset was resized to 90 <inline-formula>
  <mml:math id="mf7iaedfjc">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 90, an accuracy of 89.15 <inline-formula>
  <mml:math id="mzg1cizxlk">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula> 0.007% was achieved using the TSCN network, while an accuracy of 93 <inline-formula>
  <mml:math id="mibljrnrx1">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula> 0.0057% was obtained by adding the STN mechanism to the network. These results are shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>.</p>
      
        <fig id="fig_5">
          <label>Figure 5</label>
          <caption>
            <title>Classification results of RTSD-R1 dataset</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/11/img_HJSOUvuPiZpkweRX.png"/>
        </fig>
      
      <p>Similarly, the RTSD-R3 dataset was resized and classified into three different dimensions. When the RTSD-R3 dataset was resized to 20 <inline-formula>
  <mml:math id="mgoc24t449">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 20 and classified using the TSCN network, an accuracy of 85.65 <inline-formula>
  <mml:math id="m7aamrd1n5">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula> 0.0045% was achieved, while an accuracy of 87.62 <inline-formula>
  <mml:math id="mh0krgui4a">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula> 0.0043% was obtained by adding the STN mechanism to the network. When the RTSD-R3 dataset was resized to 60 <inline-formula>
  <mml:math id="m5c90pdmtd">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 60, an accuracy of 87.1 <inline-formula>
  <mml:math id="ml1ccdxgri">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula> 0.0043% was achieved using the TSCN network, while an accuracy of 88.71 <inline-formula>
  <mml:math id="mh3cl1520r">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula> 0.0041% was obtained by adding the STN mechanism to the network. When the RTSD-R3 dataset was resized to 90 <inline-formula>
  <mml:math id="mfmcfaqgy6">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 90, an accuracy of 94.3 <inline-formula>
  <mml:math id="mk6bsm3o5x">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula> 0.003% was achieved using the TSCN network, while an accuracy of 95.69 <inline-formula>
  <mml:math id="mtncufbat1">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula> 0.0026% was obtained by adding the STN mechanism to the network. These results are shown in <xref ref-type="fig" rid="fig_6">Figure 6</xref>. Image examples obtained from the output of the STN mechanism are also shown in <xref ref-type="fig" rid="fig_7">Figure 7</xref>.</p>
      
        <fig id="fig_6">
          <label>Figure 6</label>
          <caption>
            <title>Classification results of RTSD-R3 dataset</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/11/img_LZetIgvhHf9Cwycq.png"/>
        </fig>
      
      
        <fig id="fig_7">
          <label>Figure 7</label>
          <caption>
            <title>(a) Original images and (b) STN resulting images</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/11/img_lWF81McuiN_NOk7H.png"/>
        </fig>
      
      <p>To provide a clearer understanding of failure cases, several misclassified examples obtained from the STN + TSCN model were examined. A large proportion of the errors occurred in traffic sign images that were recorded at smaller scales and lower resolutions. In the analysis conducted on the RTSD-R1 subset, accuracy exceeded 95% for traffic signs in the prohibitory and warning categories, whereas accuracy rates remained within the 85–90% range for classes that included information signs with lower resolution. Similarly, in the RTSD-R3 dataset, misclassifications were predominantly observed in small-scale signs captured under low-light conditions or partially occluded. In the information sign category, accuracy rates were observed to remain between 85% and 90%. These findings indicate that although the STN module enhances geometric robustness, challenges persist in fine-grained classes characterized by high intra-class similarity or low-quality visual features.</p><p>Overall, the results indicate that the Spatial Transformer Network positively impacts classification performance.</p>
    </sec>
    <sec sec-type="discussion">
      <title>4. Discussion</title>
      <p>Traffic signs were classified by adding a classification network to the STN mechanism, which was created using the MatConvNet toolbox. The results indicate that the STN mechanism positively impacted classification performance.</p><p>In previous studies, the STN mechanism was employed to classify traffic signs, resulting in strong classification performance on benchmark datasets. In the study by Haloi [<xref ref-type="bibr" rid="ref_21">21</xref>], a network was designed to classify traffic signs using a spatial transformer layer and a custom initialization module. In the study conducted on the German Traffic Signs Recognition Benchmark (GTSRB) dataset, an accuracy of 99.81% was achieved. In the study conducted by Zhang et al. [<xref ref-type="bibr" rid="ref_22">22</xref>], a multi-column STN consisting of CNNs and STNs was proposed to address the issue of CNNs not adapting well to the spatial diversity of images. In their research on the GTSRB dataset, an impressive classification accuracy of 99.75% was achieved. In the study by Arcos-García et al. [<xref ref-type="bibr" rid="ref_16">16</xref>], a Deep Neural Network with convolutional layers and STNs was used to perform various classification experiments on public traffic sign datasets from Germany and Belgium. In the classification conducted on the GTSRB dataset using a CNN structure with the SGD optimization algorithm and without STNs, an accuracy of 98.31% was achieved. When three STN blocks were added to this CNN structure, an accuracy of 99.49% was achieved, demonstrating the positive impact of the STN mechanism on classification. Lim et al. [<xref ref-type="bibr" rid="ref_23">23</xref>] propose a new convolutional neural network consisting of a spatial transformer network and a multi-structure convolutional neural network. The classification performances of the basic CNN without the STN mechanism and the multi-column convolutional neural network with STNs (SPMCNN) on the GTSRB dataset were achieved as 91% and 97.45%, respectively.</p><p>In our study, the RTSD-R1 and RTSD-R3 traffic sign recognition datasets were used to evaluate the performance of the STN mechanism on classification. In the study conducted by Shakhuro and Konushin [<xref ref-type="bibr" rid="ref_12">12</xref>], a dataset consisting of Russian traffic sign images was developed. RTSD-R1 and RTSD-R3 datasets created for classification of this dataset were classified in a proposed convolutional neural network and an accuracy of 90.78% and 92.9% was obtained, respectively. In our study, both datasets were resized to 20 <inline-formula>
  <mml:math id="mo63atw137">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 20, 60 <inline-formula>
  <mml:math id="mdo8ij9d0d">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 60, and 90 <inline-formula>
  <mml:math id="m2v463nbpl">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 90, respectively, and classified using the TSCN and STN + TSCN models. The localization network of the STN mechanism and the proposed TSCN parameters were recalculated for each dimension. In the RTSD-R1 dataset, the addition of the STN mechanism increased the classification accuracy by 5.6% for the 20 <inline-formula>
  <mml:math id="m6kqcb4kcl">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 20 size, 3.9% for the 60 <inline-formula>
  <mml:math id="mtb6qzbgmk">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 60 size, and 4.3% for the 90 <inline-formula>
  <mml:math id="m4ibz9dibm">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 90 size. In the RTSD-R3 dataset, the addition of STN mechanism increased the classification accuracy by 2.3% for the 20 <inline-formula>
  <mml:math id="m13gi2jz41">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 20 size, 1.9% for the 60 <inline-formula>
  <mml:math id="m0skq739gt">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 60 size and 1.5% for the 90 <inline-formula>
  <mml:math id="mlairynrjg">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 90 size. The highest accuracy rates were achieved as 93% for the RTSD-R1 dataset and 95.69% for the RTSD-R3 dataset in the STN + TSCN model with 90 <inline-formula>
  <mml:math id="m7u00jcbm1">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 90 dimensions.</p><p>The results obtained at different image sizes (20 <inline-formula>
  <mml:math id="m20np55dfm">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 20, 60 <inline-formula>
  <mml:math id="mxeod5i3zj">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 60, and 90 <inline-formula>
  <mml:math id="mhhpedsmsy">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 90) show that image resolution has a noticeable effect on the contribution of the STN mechanism. At smaller image sizes, such as 20<inline-formula>
  <mml:math id="m0lbvammpj">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula>20, the STN provided a greater improvement in accuracy, especially for the RTSD-R1 dataset. This is because low-resolution images lose spatial details, and the STN helps to correct geometric distortions and focus on the most relevant regions. As the image size increased, the improvement provided by the STN became smaller, since higher-resolution images already contained enough spatial information for the base network (TSCN) to perform well. These results indicate that the STN mechanism is particularly useful when images are low in resolution or not well aligned, while its effect decreases when the input data already provide clear spatial information.</p><p>The improvement in performance can be attributed to the ability of the STN to learn spatial transformations such as rotation, scaling, and translation automatically, allowing the network to correct geometric distortions and focus on the most informative regions of the image. This effectively reduces intra-class variability caused by different viewing angles, lighting conditions, or partial occlusions, leading to more stable feature extraction by the convolutional layers.</p><p>However, it was also observed that when the dataset images were already well-aligned and contained limited spatial variations (as in the RTSD-R3 subset), the incremental gain from adding STN was relatively smaller. This suggests that while the STN mechanism enhances robustness under non-ideal conditions, its contribution becomes less significant when the training data are already normalized or when excessive transformations cause the loss of fine-grained details. These findings highlight that the effectiveness of STN is closely tied to the geometric diversity of the dataset and the complexity of the visual distortions present in the input images.</p><p>Compared with recent lightweight detection models such as YOLOv8 or transformer-based traffic-sign recognizers, the proposed STN-enhanced CNN model serves a different purpose and operates with a substantially smaller computational footprint. YOLOv8-nano, for example, contains approximately 3–4 million parameters and is designed for real-time object detection, whereas transformer-based architectures typically exceed tens of millions of parameters and require higher computational resources for multi-head attention operations. In contrast, our model contains only ~3 million parameters with a 12.2 MB model size and achieves an average inference time of 11 ms, making it suitable for resource-constrained or embedded environments where detection is not required and classification alone is sufficient. While YOLO-based or transformer-based systems offer end-to-end detection-plus-classification pipelines, the goal of this study is to enhance classification accuracy under geometric distortions. Thus, the proposed STN-integrated CNN can be considered a lightweight and distortion-robust alternative when a standalone classification module is required or when computational resources are limited.</p><p>The results indicate that while the TSCN model exhibited high classification performance on its own, the addition of the STN mechanism further improved the classification performance.</p>
    </sec>
    <sec sec-type="">
      <title>5. Conclusion and future work</title>
      <p>CNNs provide powerful architectures for image processing applications; however, their ability to perform spatial transformations on data is limited. Correcting geometrically corrupted or distorted images is crucial for improving classification performance. Although fully connected layers in CNNs partially handle this task, their effectiveness remains limited on large and complex datasets due to the small receptive fields of convolutional filters. In this study, STNs were employed to identify the region of interest by applying transformations such as rotation, scaling, and cropping, while simultaneously removing irrelevant areas from the image. These transformations reduce the need for extensive manual data augmentation. Spatial transformations apply geometric transformations to the input feature map, enabling CNNs to achieve computationally efficient spatial invariance. Thus, the conventional need for data augmentation typically used to improve performance when training data is limited is effectively eliminated.</p><p>In the study, STNs were integrated into the TSCN architectures. The TSCN and STN+TSCN structures were tested on the RTSD-R1 and RTSD-R3 datasets, and it was observed that STNs had a positive effect on classification performance. This work contributes to the field of AI by demonstrating the practical integration of STN with CNNs. In our future work, we aim to achieve improved classification accuracy and robustness by enhancing the proposed localization network and traffic sign classification network for other AI applications.</p><p>Although MatConvNet provides a simple and MATLAB-integrated environment for rapid prototyping, it lacks some of the advanced features, flexibility, and computational efficiency available in modern deep learning frameworks such as PyTorch and TensorFlow. In future work, we plan to reimplement the proposed STN-based CNN architecture using these frameworks to enable faster training, more efficient GPU utilization, and easier integration with state-of-the-art modules.</p><p>Additionally, the model will be benchmarked against state-of-the-art architectures such as YOLOv8 and Transformer-based detectors to assess its relative performance and identify potential limitations. Future studies will further focus on incorporating attention mechanisms to enhance spatial feature discrimination and robustness against occlusions. Moreover, lightweight and real-time variants of the proposed architecture will be developed to optimize its applicability for embedded systems and autonomous driving platforms.</p><p>Through these improvements, the current framework is expected to evolve into a more adaptive, efficient, and deployable solution for intelligent transportation and other real-world computer vision applications.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>Software, B.T.; Data curation, B.T.; formal analysis, B.T. and E.A.O.; writing—review and editing, B.T. and E.A.O. All authors have read and agreed to the published version of the manuscript.</p>
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>23</volume>
          <page-range>5150-5162</page-range>
          <issue>6</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ahmed</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Kamal</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Hasan</surname>
              <given-names>M. K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.13140/RG.2.2.18341.86249</pub-id>
          <article-title>DFR-TSD: A deep learning based framework for robust traffic sign detection under challenging weather conditions</article-title>
          <source>IEEE Trans. Intell. Transp. Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Preeti</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Kunjal</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.22214/ijraset.2022.40224</pub-id>
          <article-title>Traffic sign classification using CNN</article-title>
          <source>Int. J. Appl. Sci. Eng. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>178798-178810</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ou</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Xiong</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2019.2959015</pub-id>
          <article-title>FAMN: Feature aggregation multipath network for small traffic sign detection</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>19</volume>
          <page-range>282-292</page-range>
          <issue>4</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cinar</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Taspinar</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Saritas</surname>
              <given-names>M. M.</given-names>
            </name>
            <name>
              <surname>Koklu</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Feature extraction and recognition on traffic sign ımages</article-title>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Ghosh</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sufian</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sultana</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Chakrabarti</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>De</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Fundamental concepts of convolutional neural network</article-title>
          <source>Recent Trends and Advances in Artificial Intelligence and Internet of Things, Intelligent Systems Reference Library (ISRL, vol.172)</source>
          <year>2019</year>
          <page-range>519-567</page-range>
          <pub-id pub-id-type="doi">10.1007/978-3-030-32644-9_36</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>20</volume>
          <page-range>975-984</page-range>
          <issue>3</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TITS.2018.2843815</pub-id>
          <article-title>Real-time traffic sign recognition based on efficient CNNs in the wild</article-title>
          <source>IEEE Trans. Intell. Transp. Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>97</volume>
          <page-range>269-277</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Song</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Que</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Hou</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Du</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.sysarc.2019.01.012</pub-id>
          <article-title>An efficient convolutional neural network for small traffic sign detection</article-title>
          <source>J. Syst. Archit.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>81</volume>
          <page-range>17779-17791</page-range>
          <issue>13</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>W. Q.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-022-12163-0</pub-id>
          <article-title>Traffic sign recognition based on deep learning</article-title>
          <source>Multimed. Tools Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>24</volume>
          <page-range>3282</page-range>
          <issue>11</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Alawaji</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Hedjar</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Zuair</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s24113282</pub-id>
          <article-title>Traffic sign recognition using multi-task deep learning for self-driving vehicles</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>56</volume>
          <page-range>241</page-range>
          <issue>5</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kandasamy</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Natarajan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Preethaa</surname>
              <given-names>K. R. Sri</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>A. A. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11063-024-11693-y</pub-id>
          <article-title>A robust TrafficSignNet algorithm for enhanced traffic sign recognition in autonomous vehicles under varying light conditions</article-title>
          <source>Neural Process. Lett.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Enan</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Chowdhury</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2503.12567</pub-id>
          <article-title>GAN-based single-stage defense for traffic sign classification under adversarial patch attack</article-title>
          <source>arXiv preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>40</volume>
          <page-range>294-300</page-range>
          <issue>2</issue>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shakhuro</surname>
              <given-names>V. I.</given-names>
            </name>
            <name>
              <surname>Konushin</surname>
              <given-names>A. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18287/2412-6179-2016-40-2-294-300</pub-id>
          <article-title>Russian traffic sign images dataset</article-title>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="webpage">
          <article-title>Traffic sign recognition</article-title>
          <source>, https://graphics.cs.msu.ru/projects/traffic-sign-recognition.html</source>
          <year>2025</year>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="webpage">
          <article-title>MatConvNet: CNNs for MATLAB</article-title>
          <source>, https://www.vlfeat.org/matconvnet/</source>
          <year>2025</year>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="conf-paper">
          <page-range>689-692</page-range>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Vedaldi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Lenc</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/2733373.2807412</pub-id>
          <article-title>MatConvNet: Convolutional neural networks for MATLAB</article-title>
          <source>Proceedings of the 23rd ACM International Conference on Multimedia</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>99</volume>
          <page-range>158-165</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Arcos-García</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Alvarez-García</surname>
              <given-names>J. A.</given-names>
            </name>
            <name>
              <surname>Soria-Morillo</surname>
              <given-names>L. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neunet.2018.01.005</pub-id>
          <article-title>Deep neural network for traffic sign recognition systems: An analysis of spatial transformers and stochastic optimisation methods</article-title>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>28</volume>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jaderberg</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Simonyan</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zisserman</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kavukcuoglu</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>Spatial transformer networks</article-title>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>177647-177666</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Babukarthik</surname>
              <given-names>R. G.</given-names>
            </name>
            <name>
              <surname>Adiga</surname>
              <given-names>V. A. K.</given-names>
            </name>
            <name>
              <surname>Sambasivam</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Chandramohan</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Amudhavel</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3025164</pub-id>
          <article-title>Prediction of COVID-19 using genetic deep learning convolutional neural network (GDCNN)</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>74</page-range>
          <issue>4</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Moung</surname>
              <given-names>E. G.</given-names>
            </name>
            <name>
              <surname>Hou</surname>
              <given-names>C. J.</given-names>
            </name>
            <name>
              <surname>Sufian</surname>
              <given-names>M. M.</given-names>
            </name>
            <name>
              <surname>Hijazi</surname>
              <given-names>M. H. A.</given-names>
            </name>
            <name>
              <surname>Dargham</surname>
              <given-names>J. A.</given-names>
            </name>
            <name>
              <surname>Omatu</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/bdcc5040074</pub-id>
          <article-title>Fusion of moment invariant method and deep learning algorithm for COVID-19 classification</article-title>
          <source>Big Data Cogn. Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>120</volume>
          <page-range>105897</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Omeroglu</surname>
              <given-names>A. N.</given-names>
            </name>
            <name>
              <surname>Mohammed</surname>
              <given-names>H. M.</given-names>
            </name>
            <name>
              <surname>Oral</surname>
              <given-names>E. A.</given-names>
            </name>
            <name>
              <surname>Aydin</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.engappai.2023.105897</pub-id>
          <article-title>A novel soft attention-based multi-modal deep learning framework for multi-label skin lesion classification</article-title>
          <source>Eng. Appl. Artif. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Haloi</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1511.02992</pub-id>
          <article-title>Traffic sign classification using deep inception based convolutional networks</article-title>
          <source>arXiv preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Duan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Zou</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>Multi-column spatial transformer convolution neural network for traffic sign recognition</article-title>
          <source>Advances in Neural Networks—ISNN 2018</source>
          <year>2018</year>
          <page-range>593-600</page-range>
          <pub-id pub-id-type="doi">10.1007/978-3-319-92537-0_68</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>118-122</page-range>
          <issue>4</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lim</surname>
              <given-names>M. T.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>D. W.</given-names>
            </name>
            <name>
              <surname>Chung</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Ahn</surname>
              <given-names>W. J.</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>S. K.</given-names>
            </name>
            <name>
              <surname>Kang</surname>
              <given-names>T. K.</given-names>
            </name>
            <name>
              <surname>Dongnam-gu</surname>
              <given-names>C. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.36224/ijes.120401</pub-id>
          <article-title>Traffic sign recognition using spatial transformer network with multi-structure convolutional neural network</article-title>
          <source>Int. J. Eng. Sci.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>