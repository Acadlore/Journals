<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-_FwtHiKpAEqOxaxqN9Qf-9B3DWNYRfXg</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml030103</article-id>
      <title-group>
        <article-title>Enhanced Real-Time Facial Expression Recognition Using Deep Learning</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Haq</surname>
            <given-names>Hafiz Burhan Ul</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3026-3728</contrib-id>
          <email>hafiz.burh@kmutt.ac.th</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="2">2</xref>
          <name>
            <surname>Akram</surname>
            <given-names>Waseem</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2734-1844</contrib-id>
          <email>waseemakram@ucp.edu.pk</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Irshad</surname>
            <given-names>Muhammad Nauman</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1782-4658</contrib-id>
          <email>muhammadnauman.irsh@kmutt.ac.th</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="3">3</xref>
          <name>
            <surname>Kosar</surname>
            <given-names>Amna</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0006-1735-929X</contrib-id>
          <email>amnakosar@lgu.edu.pk</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="4">4</xref>
          <name>
            <surname>Abid</surname>
            <given-names>Muhammad</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0006-8619-6666</contrib-id>
          <email>mabid@ncsu.edu</email>
        </contrib>
        <aff id="1">Department of Electronics and Telecommunication Engineering, Faculty of Engineering, King Mongkut’s University of Technology Thonburi (KMUTT), 10140 Bangkok, Thailand</aff>
        <aff id="2">Faculty of Information Technology, University of Central Punjab, 54000 Lahore, Pakistan</aff>
        <aff id="3">Department of Computer Sciences, Faculty of Computer Sciences, Lahore Garrison University, 54000 Lahore, Pakistan</aff>
        <aff id="4">Department of Mathematics, North Carolina State University, 27695 NC Raleigh, USA</aff>
      </contrib-group>
      <year>2024</year>
      <volume>3</volume>
      <issue>1</issue>
      <fpage>24</fpage>
      <lpage>35</lpage>
      <page-range>24-35</page-range>
      <history>
        <date date-type="received">
          <month>10</month>
          <day>22</day>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <month>01</month>
          <day>08</day>
          <year>2024</year>
        </date>
        <date date-type="pub">
          <month>01</month>
          <day>24</day>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the authors</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license>. Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the <a href='https://creativecommons.org/licenses/by/4.0/' target='_blank' class='text-yellow-700 hover:underline'>CC BY 4.0 license</a>.</license>
      </permissions>
      <abstract><p>In the realm of facial expression recognition (FER), the identification and classification of seven universal emotional states, surprise, disgust, fear, happiness, neutrality, anger, and contempt, are of paramount importance. This research focuses on the application of convolutional neural networks (CNNs) for the extraction and categorization of these expressions. Over the past decade, CNNs have emerged as a significant area of research in human-computer interaction, surpassing previous methodologies with their superior feature learning capabilities. While current models demonstrate exceptional accuracy in recognizing facial expressions within controlled laboratory datasets, their performance significantly diminishes when applied to real-time, uncontrolled datasets. Challenges such as degraded image quality, occlusions, variable lighting, and alterations in head pose are commonly encountered in images sourced from unstructured environments like the internet. This study aims to enhance the recognition accuracy of FER by employing deep learning techniques to process images captured in real-time, particularly those of lower resolution. The objective is to augment the accuracy of FER in real-world datasets, which are inherently more complex and collected under less controlled conditions, compared to laboratory-collected data. The effectiveness of a deep learning-based approach to emotion detection in photographs is rigorously evaluated in this work. The proposed method is exhaustively compared with manual techniques and other existing approaches to assess its efficacy. This comparison forms the foundation for a subjective evaluation methodology, focusing on validation and end-user satisfaction. The findings conclusively demonstrate the method's proficiency in accurately recognizing emotions in both laboratory and real-world scenarios, thereby underscoring the potential of deep learning in the domain of facial emotion identification.</p></abstract>
      <kwd-group>
        <kwd>Facial expression recognition</kwd>
        <kwd>CNNs</kwd>
        <kwd>MobileNet</kwd>
        <kwd>Feature extraction</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors">5</count>
        <fig-count>8</fig-count>
        <table-count>2</table-count>
        <ref-count>37</ref-count>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec disp-level="level1" sec-type="intro">
      <title>1. Introduction</title>
      <p>Over the past decade, the field of artificial intelligence (AI), which aims to emulate human cognitive processes, has undergone significant advancements and encountered intriguing challenges. Among these, the analysis of subtle facial expressions represents a complex task. It is observed that the manifestation of a single emotion can vary considerably across individuals, influenced by factors such as ethnicity, age, or gender. Moreover, the interpretation of an individual's emotional state is subject to contextual variables including lighting, posture, and background. This paper delves into the intricacies of facial expression analysis in the era of AI, exploring the multitude of aspects impacting the accuracy of human emotion detection. Expression, encompassing a broad spectrum of behaviors, actions, thoughts, and feelings, is ultimately a subjective and intimate mental and physical state. The foundational work of Charles Darwin, particularly his book "The Expression of the Emotions in Man and Animals," laid the groundwork for early emotion studies. Subsequent research, notably by Ekman and Friesen in 1969, identified cross-cultural consistencies in emotional expressions, establishing six universal emotional states: happiness, sadness, anger, contempt, surprise, and fear [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>].</p><p>Conversely, facial expression, a non-verbal form of communication, is crucial to human perception, behavior, and interaction. Facial expressions represent morphological alterations in the face [<xref ref-type="bibr" rid="ref_4">4</xref>], and it is estimated that only about 7% of the information conveyed is through words, with vocal intonation accounting for 55% and body language for 38%. The use and interpretation of body language and facial expressions often occur subconsciously, yet they play a vital role in effective communication. The increasing relevance of emotions in human-robot interaction (HRI) has sparked interest in equipping social robots with FER capabilities. HRI amalgamates disciplines such as social sciences, robotics, AI, and natural language processing [<xref ref-type="bibr" rid="ref_5">5</xref>]. This interdisciplinary approach underlines the growing need to understand and accurately interpret facial expressions, not only in human-to-human interactions but also in the evolving domain of human-robot communication.</p><p>Emotions are fundamental in HRI, rendering social robots an increasingly studied subject due to their potential in FER. The exploration of HRI necessitates a multidisciplinary approach, incorporating fields such as AI, robotics, natural language processing, design, and social sciences. Within this scope, facial recognition technologies are crucial yet encounter several limitations including restricted processing capabilities, speed, duration, and accuracy. Challenges in 2D, 3D, and temporal facial recognition methods are prevalent, primarily owing to spatial alterations, occlusions, lighting variances, and the intensive demand for computational resources. Efforts to refine classification accuracy have been observed, with some researchers opting to simplify methodologies by minimizing feature points or adopting a more objective approach. In the realm of computer vision, traditional machine learning techniques previously demonstrated efficacy but were hindered by their inability to process direct photo inputs. Contemporary face recognition systems continue to face challenges due to varying lighting conditions, backgrounds, and postures, which can significantly alter appearances and obstruct precise expression detection. The advent of deep learning has been pivotal in addressing these challenges, enhancing the recognition performance of the six core emotional expressions—sadness, disgust, anger, happiness, fear, and surprise. However, the application of deep learning models to faces captured under divergent conditions from the training dataset remains a significant limitation. A comparative analysis of traditional and deep learning techniques in facial recognition is presented in <xref ref-type="fig" rid="fig_1">Figure 1</xref>.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>Comparative analysis between traditional machine learning and the deep learning model</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_4RWZd80nrycZAiQA.png"/>
        </fig>
      
      <p>In this study, a novel deep learning-based framework is introduced, designed to surmount the challenges inherent in real-time facial emotion recognition. The system employs deep learning algorithms for detection, coupled with CNNs for the extraction of features, thereby recognizing a spectrum of seven emotional states: happiness, sadness, anger, fear, surprise, disgust, and neutrality. The methodology incorporates current techniques while introducing several key enhancements:</p><ul><li><p>Expanded recognition capability: The model is engineered to differentiate between seven emotional categories, thereby broadening its scope to capture a more extensive range of human emotions. This expansion enables a more precise and nuanced analysis of facial expressions.</p></li><li><p>Streamlined and resilient architecture: The system is designed with simplicity and robustness, facilitating effective real-time processing. This feature ensures the model's applicability in real-world scenarios without excessively taxing computational resources.</p></li><li><p>Enhanced accuracy: By leveraging advanced deep learning techniques, the model achieves elevated levels of accuracy in facial emotion detection. This improvement is critical for reliable outcomes, particularly in fields such as human-computer interaction, market research, and mental health assessments.</p></li><li><p>Rigorous evaluation and validation: The efficacy of the proposed model will be rigorously assessed using predefined datasets. This evaluation process is aimed at empirically demonstrating the system's proficiency and its capacity to yield valuable insights across various applications. The methodology outlined in this research encompasses several critical steps, each contributing to the development of an advanced real-time facial expression detection system:</p></li><li><p>Data collection: Initially, a comprehensive and varied dataset of facial expressions is compiled. This dataset is meticulously curated to ensure diversity and representativeness, laying a foundational basis for subsequent model training.</p></li><li><p>Model training and evaluation: Deep learning models are then rigorously trained on the assembled datasets. The focus of this training is to enhance the models' proficiency in accurately identifying the seven predefined emotional categories. Subsequent extensive testing is conducted to refine and validate the models' performance.</p></li><li><p>Application in real-time detection: Designed for practical, real-time scenarios, the system operates by selecting an image from the collected dataset as input. It then rapidly processes this image for emotion recognition, aiming to deliver prompt and reliable results.</p></li></ul><p>In summation, the approach proposed in this study marks a significant advancement in real-time FER. It integrates innovative features, including simplicity, robustness, and high accuracy, making it a valuable asset in diverse applications where understanding human emotions is essential. These applications range from enhancing human-computer interaction to providing insights in market research and mental health assessments.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>2. Literature review</title>
      <p>The implementation of facial recognition technology in smart devices has become increasingly prevalent, yet it imposes significant demands on storage and processing capabilities. In response to these challenges, a range of strategies and systems for expression recognition have been developed and are briefly reviewed herein:</p><p>Guo et al. [<xref ref-type="bibr" rid="ref_6">6</xref>] introduced an innovative approach utilizing DNNs with relativity learning (DNNRL). This method aims to contract the distances in the embedding space between samples representing the same expression, while concurrently expanding the gap between those of differing expressions. The training process involves the selection of an anchor, a positive sample (bearing the same expression as the anchor), and a negative sample (exhibiting a different expression). The core objective is to minimize the triplet loss, which effectively reduces the distance between the anchor and the positive instance in the embedding space, ensuring that it remains narrower than that between the anchor and the negative sample. DNNRL notably assigns greater weight to challenging instances based on the network's output, allowing for more nuanced learning. The efficacy of DNNRL has been validated using the SFEW and FER2013 datasets.</p><p>Feature extraction, a critical step following face detection in FER, is heavily dependent on the quality of the features extracted. Subtle or pronounced deformations in facial features such as eyebrows, lips, eyes, and nose can induce changes in facial expressions. Feature extraction methods are categorized into two types: non-geometric and geometric-based features [<xref ref-type="bibr" rid="ref_7">7</xref>]. Geometric feature extraction focuses on quantifying the size and position of facial features, including the nose, lips, forehead, chin, and eyes. These attributes are encapsulated within a facial geometry feature vector. Geometric feature extraction employs various geometric interactions, such as points, stretches, and angles, between these components to encode the features. In contrast, appearance-based feature extraction employs either a single image filter or a combination of filters applied to the entire image or specific regions to discern changes in texture and shape [<xref ref-type="bibr" rid="ref_8">8</xref>]. Furthermore, a range of computational models and methods for processing visual data are employed in feature extraction. These include tools like fuzzy logic and neural networks. Feature extraction strategies are broadly classified into four types: feature-based, appearance-based, template-based, and part-based approaches [<xref ref-type="bibr" rid="ref_9">9</xref>].</p><p>Li et al. [<xref ref-type="bibr" rid="ref_10">10</xref>] have explored the application of the k-nearest neighbor (KNN) strategy, augmented by center loss and locality-preserving loss (LP-loss), for clustering deep features and ensuring intra-class compactness. The employed deep locality-preserving CNN (DLP-CNN) maintains the local representation of each sample in the embedding space. During training, Euclidean distance is utilized to ascertain the KNN for each data point, aiming to minimize the sample's distance from the mean of its KNNs. The effectiveness of LP-loss has been evaluated using datasets such as CK+, SFEW, MMI, and RAF-DB. Center loss, while promoting intra-class compactness and consequently aiding in inter-class separation, may still permit overlap among feature regions in the embedding space. Building on this concept, Cai et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] enhanced center loss by integrating an additional objective function. This modified center loss, termed as island loss, merges the original center loss with the pairwise cosine distance between class centers in the embedding space. The approach aims to increase cosine distance, thereby angularly separating the class centers. Island loss has been assessed using datasets including CK+, MMI, and Oulu-CASI. Recent advancements in facial emotion recognition have been significantly influenced by deep learning algorithms. Jain et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] introduced single deep neural network (DNN) incorporating convolution layers and deep residual blocks. Lopes et al. [<xref ref-type="bibr" rid="ref_13">13</xref>], in a similar vein, presented a multiple CNN framework, complemented by a specialized image pre-processing stage for emotion recognition.</p><p>The application of FER in dynamic environments was addressed by Jain et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] through the deployment of a hybrid convolution-recurrent neural network technique. A comparative analysis was conducted by Sajjanhar et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] on the performance of pre-trained facial recognition algorithms, Visual Geometry Group (VGG)-facial and Inception, both initially developed for object detection. Wen et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] employed a convolutional rectified linear layer as the initial layer in their CNN aggregate for facial emotion recognition, incorporating multiple hidden maxout layers to modify the architecture of each CNN. Despite notable advancements in the field of FER, research predominantly focuses on devising strategies to enhance outcomes presented in one or more datasets independently. The investigation into the impact of cross-dataset fine-tuning on performance was conducted by Zavarez et al. [<xref ref-type="bibr" rid="ref_17">17</xref>]. For this purpose, the VGG face deep CNN model was adapted for facial emotion recognition. Cross-dataset experiments were meticulously designed, utilizing one dataset as the test set while employing others for training, to ensure the reliability of the results. Wang et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] proposed an innovative approach integrating FER technology with online course platforms. In this method, student facial expressions were captured using device cameras during an online course and processed through a FER algorithm (CNN model), categorizing them into eight emotional states: anger, disgust, fear, happiness, sadness, surprise, contempt, and neutral. This approach was tested in an online course using Tencent Meeting with 27 students, demonstrating consistent performance across diverse scenarios. The applicability of this concept extends beyond online educational settings, suggesting potential in various interactive environments.</p><p>Pise et al. [<xref ref-type="bibr" rid="ref_19">19</xref>] have applied contemporary deep learning models to the evolving field of automated emotion recognition within computational intelligence. This research demonstrates the integration of deep learning-based FER with architectural methods and databases, yielding highly accurate results. A diverse array of machine learning and deep learning methodologies are employed in this investigation. Saeed et al. [<xref ref-type="bibr" rid="ref_20">20</xref>] discussed a technique to enhance accuracy in facial recognition. Their proposed CNN method (fall detection-CNN), incorporating two hidden layers and four convolutional layers, serves as an automated framework. Utilizing the expanded Cohn-Kanade (CK+) dataset, which includes images portraying a range of emotions from various individuals, the process encompasses pre-processing, feature extraction, and categorization. The model's effectiveness is evaluated through metrics such as F1-score, recall, and precision, with respective values of 84.07%, 78.22%, and 94.09%. Additionally, numerous studies employing machine learning methods have contributed to this field [<xref ref-type="bibr" rid="ref_21">21</xref>], [<xref ref-type="bibr" rid="ref_22">22</xref>], [<xref ref-type="bibr" rid="ref_23">23</xref>], [<xref ref-type="bibr" rid="ref_24">24</xref>], [<xref ref-type="bibr" rid="ref_25">25</xref>], [<xref ref-type="bibr" rid="ref_26">26</xref>], [<xref ref-type="bibr" rid="ref_27">27</xref>]. Despite advancements, certain facial recognition approaches encounter challenges, including poor lighting, shadows, partial facial visibility, camera orientation issues, and lower recognition rates. This project aims to develop a CNN-based FER system enhanced with data augmentation. The proposed system is designed to classify the seven principal emotions, anger, contempt, fear, happiness, neutrality, sadness, and surprise, from visual data.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>3. Proposed methodology</title>
      <p> <xref ref-type="fig" rid="fig_2">Figure 2</xref> delineates the architecture of the proposed emotion recognition model. The methodology comprises the following principal components:</p><p>(a) Data collection: This phase involves the accumulation of a diverse dataset, encompassing images that represent a range of emotions.</p><p>(b) Data preprocessing: The dataset undergoes classification, categorizing images into seven emotional states: anger, happiness, fear, disgust, neutrality, sadness, and surprise.</p><p>(c) Emotion prediction: Utilizing a deep learning model, emotion predictions are executed on the images.</p><p>(d) Performance evaluation: The final stage involves assessing the model's performance in accurately predicting emotions.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>Architectural diagram of the proposed model</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_GbTMFYhVjT_ewNIO.png"/>
        </fig>
      
      
        <sec disp-level="level2">
          
            <title>3.1. Data collection</title>
          
          <p>The data collection process was facilitated by a data acquisition layer, responsible for aggregating data from various online sources. This research utilized information gathered from links, data repositories, and additional internet resources. <xref ref-type="fig" rid="fig_3">Figure 3</xref> presents a selection of the data samples amassed for this study. The methodology incorporated two primary datasets: the FER-2013 [<xref ref-type="bibr" rid="ref_28">28</xref>] and a Random dataset [<xref ref-type="bibr" rid="ref_29">29</xref>]. The FER-2013 dataset comprises grayscale images, each measuring 48×48 pixels. It encompasses a training set of 28,000 labeled images, a development set consisting of 3,500 labeled images, and a test set with another 3,500 labeled images. This dataset encapsulates seven emotional states: happiness, sadness, anger, fear, surprise, disgust, and neutral. In contrast, the Random dataset includes a compilation of 350 images, both in color and grayscale, further categorized into six emotional categories: happiness, sadness, anger, fear, surprise, disgust, and neutral. <xref ref-type="fig" rid="fig_3">Figure 3</xref> showcases representative images from both datasets, illustrating the diversity and range of emotions covered.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>Sample images from the collected datasets</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_7rxJPzkmpqxviGfr.png"/>
            </fig>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.2. Proposed model</title>
          
          <p>In this phase of the research, the focus is on the utilization of deep learning models, specifically MobileNet, for real-time prediction of seven emotional categories: happiness, sadness, anger, fear, surprise, disgust, and neutrality. The MobileNet architecture is leveraged due to its efficiency in processing and reduced parameter count compared to conventional convolutional networks. Bounding boxes are employed to highlight the facial regions where emotions are detected. MobileNet, a variant of CNNs developed by Google, employs depth-separable convolutions, significantly reducing the number of parameters required. This reduction enables the deployment of DNNs on portable devices, making MobileNet an ideal foundation for compact and rapid classifiers. The architecture of MobileNet comprises several depth-separable convolutional layers, each consisting of a depth-wise convolution followed by a point-wise convolution. In total, a MobileNet architecture contains 28 layers when depth-wise and point-wise convolutions are considered separately. Furthermore, the adaptability of MobileNet is enhanced by the width multiplier hyperparameter, which allows for the adjustment of the network's complexity. Typically, a standard MobileNet comprises approximately 4.2 million parameters, with input dimensions of 224×224×3. <xref ref-type="fig" rid="fig_4">Figure 4</xref> presents the architectural diagram of MobileNet, highlighting its structural components [<xref ref-type="bibr" rid="ref_30">30</xref>], [<xref ref-type="bibr" rid="ref_31">31</xref>].</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>MobileNet architecture [<xref ref-type="bibr" rid="ref_31">31</xref>]</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_37fbVxPb3md-1U35.png"/>
            </fig>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.3. Speed comparison between mobilenet and other models</title>
          
          <p>In the evaluation of object detection models, MobileNet is distinguished by its exceptional speed performance. Contrasting with its counterparts, which typically operate at a frame rate of 5 frames per second, MobileNet excels by achieving a remarkable 22 frames per second. This rapid processing capability significantly elevates MobileNet above other models in terms of efficiency. To illustrate this, consider the comparison with models such as regions with CNN features (R-CNN) and its enhanced version, Fast R-CNN. While these models exhibit higher accuracy rates, capturing more detailed information than MobileNet, they lag in processing speed. The defining advantage of MobileNet lies in its speed, making it a preferred option for applications where prompt and efficient object detection is crucial. This aspect is particularly vital in real-world scenarios where time-sensitive detection is paramount [<xref ref-type="bibr" rid="ref_32">32</xref>]. <xref ref-type="fig" rid="fig_5">Figure 5</xref> provides a comparative analysis of MobileNet against various object detection techniques, emphasizing the speed differential.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>Speed comparison of deep learning models</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_Ag1EERhKCxFxaeiu.png"/>
            </fig>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.4. Experimental analysis and results</title>
          
          <p>The experimental evaluation was conducted on a computer equipped with an Intel Core i5-6200U CPU, operating at 2.4 GHz and supported by 8 gigabytes of RAM. Python, chosen for its versatility and efficiency, served as the programming language for the implementation of the models. To assess the accuracy of the developed models, a comprehensive evaluation was performed using a test dataset with well-established target features. The model outputs were systematically compared against these known ground truths, facilitating a detailed analysis of their performance. A key instrument in this evaluation was the utilization of a confusion matrix. This matrix provided both a visual and numerical representation of the model's performance, indicating not only the predicted instances for each class but also the accuracy of these predictions. Moreover, various assessment parameters were calculated using specific mathematical formulae, further elucidating the models' effectiveness. These calculations and their corresponding formulae are detailed in Eq. (1), which offers a comprehensive view of the analytical methods employed in this study.</p>
          
            <disp-formula>
              <label>(1)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
                  <mtr>
                    <mtd/>
                    <mtd>
                      <mtext>Accuracy</mtext>
                      <mo>=</mo>
                      <mo stretchy="false">(</mo>
                      <mo>+</mo>
                      <mo stretchy="false">)</mo>
                      <mo stretchy="false">(</mo>
                      <mo>+</mo>
                      <mo>+</mo>
                      <mo>+</mo>
                      <mo stretchy="false">)</mo>
                      <mi>T</mi>
                      <mi>P</mi>
                      <mi>T</mi>
                      <mi>N</mi>
                      <mi>T</mi>
                      <mi>P</mi>
                      <mi>T</mi>
                      <mi>N</mi>
                      <mi>F</mi>
                      <mi>P</mi>
                      <mi>F</mi>
                      <mi>N</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mo>/</mo>
                      </mrow>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd/>
                    <mtd>
                      <mtext>Precision</mtext>
                      <mo>=</mo>
                      <mo stretchy="false">(</mo>
                      <mo>+</mo>
                      <mo stretchy="false">)</mo>
                      <mi>T</mi>
                      <mi>P</mi>
                      <mi>T</mi>
                      <mi>P</mi>
                      <mi>F</mi>
                      <mi>P</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mo>/</mo>
                      </mrow>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd/>
                    <mtd>
                      <mtext>Recall</mtext>
                      <mo>=</mo>
                      <mo stretchy="false">(</mo>
                      <mo>+</mo>
                      <mo stretchy="false">)</mo>
                      <mi>T</mi>
                      <mi>P</mi>
                      <mi>T</mi>
                      <mi>P</mi>
                      <mi>F</mi>
                      <mi>N</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mo>/</mo>
                      </mrow>
                    </mtd>
                  </mtr>
                </mtable>
              </math>
            </disp-formula>
          
          <p>In this research, the model's performance was evaluated using a subjective assessment approach, wherein manually created images and frames depicting emotions were compared individually. The datasets utilized for testing and training encompassed the following emotional classes: happiness, sadness, anger, fear, surprise, disgust, and neutrality. <xref ref-type="table" rid="table_1">Table 1</xref> provides an extensive description of the dataset.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>Dataset composition for testing/training</caption>
              <abstract/>
              <table><tr><td >Classes</td><td > FER-2013 Dataset</td><td >Random Dataset</td></tr><tr><td >Happiness</td><td >879</td><td >50</td></tr><tr><td >Sadness</td><td >594</td><td >50</td></tr><tr><td >Anger</td><td >491</td><td >50</td></tr><tr><td >Fear</td><td >528</td><td >50</td></tr><tr><td >Surprise</td><td >416</td><td >50</td></tr><tr><td >Disgust</td><td >55</td><td >50</td></tr><tr><td >Neutrality</td><td >626</td><td >50</td></tr></table>
            </table-wrap>
          
          <p>For the experimental analysis, images were sourced from online platforms, system directories, and those specifically collected for this study. While some images from the training data were used preliminarily to check for duplicates, the primary focus was on images not included in the training set. The proposed model's efficacy was tested across a range of image resolutions. A unique aspect of the assessment involved contrasting the proposed model with a manual approach, where an individual subjectively classified images into respective emotional categories. This method entailed manual predictions which, despite initial accuracy, exhibited uncertainty in certain cases due to behavioral similarities, such as mistaking an image of a newborn for fear when it might also be interpreted as surprise. Subsequently, these images were processed through the proposed model, and the outcomes from both methods were compared. This process constituted an image-level comparison. <xref ref-type="fig" rid="fig_6">Figure 6</xref> illustrates this comparison, showcasing how each method categorized images across the seven emotional classes: happiness, sadness, anger, fear, surprise, disgust, and neutrality.</p><p>As depicted in <xref ref-type="fig" rid="fig_7">Figure 7</xref>, it was observed that the proposed model did not accurately identify certain emotional states. This limitation was primarily due to the visual similarities between different emotions. For instance, an image that predominantly exhibited characteristics of fear was erroneously classified under the category of surprise by the model. Similarly, another image, which ideally belonged to the disgust category, was incorrectly identified as sadness. This misclassification stemmed from the visual resemblance of the image to those typically associated with sadness, as perceived by the unaided eye. <xref ref-type="fig" rid="fig_7">Figure 7</xref> presents a selection of instances where the model's predictions were impeded by such behavioral similarities. These examples highlight the challenges faced in accurately distinguishing between emotions that share common visual traits.</p>
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>Comparative outputs (a) Manual method; (b) Proposed model</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_kwn9MjxwgYtvYhmZ.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_IOuF7YN0VxKTFeMW.png"/>
            </fig>
          
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>Speed comparison of deep learning models</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_2dk5OAO6S18Xkab6.png"/>
            </fig>
          
          
            <sec disp-level="level3">
              
                <title>3.4.1 Results</title>
              
              <p>The results of the proposed model, as depicted in <xref ref-type="fig" rid="fig_8">Figure 8</xref>, demonstrated a remarkable accuracy of 100% during validation and 97.9% during training. These statistics indicate the model's successful generalization from the training dataset to the validation dataset. However, challenges arose when the model was applied to real-world images sourced from various platforms such as online resources, system directories, and captured photographs. These images encompassed a spectrum of emotional states: happiness, sadness, anger, fear, surprise, disgust, and neutrality.</p><p>The manual assessment method, which relied on human judgment to classify images, also faced difficulties in accurately identifying emotions, especially in instances where images exhibited similar emotional traits. For example, an image of a baby, which might appear fearful, could also be interpreted as showing surprise due to the ambiguity in facial expressions. This issue underscores the subjective nature of human visual perception in emotion recognition tasks. Discrepancies were noted between the model's predictions and manual assessments. The model, due to its focus on behavioral similarities, misclassified certain images into incorrect emotional categories. An image that visually suggested fear was sometimes predicted as surprise, while an image that initially appeared sad was classified as disgust. These misclassifications were attributed to the model's challenge in discerning subtle differences in emotional expressions. <xref ref-type="fig" rid="fig_8">Figure 8</xref> illustrates instances where the model struggled with reliable predictions due to the proximity of the emotional expressions depicted in the images. Despite its high accuracy in training and validation settings, the model's performance in real-world scenarios highlighted the intricacies of emotion recognition in photographs, particularly when dealing with minute variations and nuances.</p>
              <p>In summary, while the model exhibited commendable performance during training and validation phases, it encountered difficulties in accurately discerning emotions in real-world images. The findings emphasize the significance of acknowledging the inherent challenges and limitations in emotion detection tasks, particularly with images exhibiting a range of similar emotional expressions. Further research and refinement may be necessary to enhance the model's capability in such scenarios.</p>
              
                <fig id="fig_8">
                  <label>Figure 8</label>
                  <caption>Accuracy curve and validation loss</caption>
                  <abstract/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/0/img_7yzl4jm1rnooRKOE.png"/>
                </fig>
              
            </sec>
          
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="results">
      <title>4. Results</title>
      <p> <xref ref-type="table" rid="table_2">Table 2</xref> presents a comparative analysis between the proposed MobileNet-V1 model and existing techniques in emotion recognition. The comparison, based on accuracy, is drawn from a range of studies and models:</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>Comparative analysis with other techniques</caption>
          <abstract/>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Authors</p></td><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td></tr><tr><td colspan="1" rowspan="1"><p>Barsoum et al. [<xref ref-type="bibr" rid="ref_33">33</xref>]</p></td><td colspan="1" rowspan="1"><p>VGG13(MV)</p></td><td colspan="1" rowspan="1"><p>83.86 %</p></td></tr><tr><td colspan="1" rowspan="1"><p>Li et al. [<xref ref-type="bibr" rid="ref_34">34</xref>]</p></td><td colspan="1" rowspan="1"><p>TFE-JL</p></td><td colspan="1" rowspan="1"><p>84.29%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Georgescu et al. [<xref ref-type="bibr" rid="ref_35">35</xref>]</p></td><td colspan="1" rowspan="1"><p>CNNs and BOVM + global SVM</p></td><td colspan="1" rowspan="1"><p>87.76%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Huang [<xref ref-type="bibr" rid="ref_36">36</xref>]</p></td><td colspan="1" rowspan="1"><p>ResNet + VGG</p></td><td colspan="1" rowspan="1"><p>87.4%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Wang et al. [<xref ref-type="bibr" rid="ref_18">18</xref>]</p></td><td colspan="1" rowspan="1"><p>SCN + ResNet18</p></td><td colspan="1" rowspan="1"><p>88.01%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Nan et al. [<xref ref-type="bibr" rid="ref_37">37</xref>]</p></td><td colspan="1" rowspan="1"><p>A-MobileNet</p></td><td colspan="1" rowspan="1"><p>88.11%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Proposed model</p></td><td colspan="1" rowspan="1"><p>MobileNet-V1</p></td><td colspan="1" rowspan="1"><p>97. 9%</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>The proposed MobileNet-V1 model demonstrates a significantly higher accuracy rate of 97.9%, surpassing the accuracy levels of other models cited in the comparative study. This superior accuracy rate is indicative of the model's effectiveness in emotion recognition tasks. The comparative analysis underscores the compactness and reliability of the proposed model, especially in contrast to the existing methodologies.</p>
    </sec>
    <sec disp-level="level1" sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>The method presented in this article for real-time emotion recognition incorporates user behavior analysis and is capable of differentiating between seven behavioral categories: happiness, sadness, neutrality, disgust, fear, surprise, and anger. Existing methods in the literature for content extraction and behavior recognition, while useful, are often hindered by high hardware demands and slow processing speeds. A detailed comparison of emotion recognition techniques is provided, demonstrating the simplicity, optimization, precision, and reliability of the proposed model relative to current methods. A key innovation in this study is the accurate and efficient extraction of seven distinct behaviors. The primary objective of developing an emotion detection system based on seven classifications using images or shots has been successfully achieved with the proposed approach. For assessment purposes, the experimental study utilized two datasets, FER2013 and Random datasets, both comprising images categorized into seven emotional states. In comparison to the subjective evaluation method, where an observer manually identifies the behavior from an image, the proposed model demonstrated superior performance. Extensive experiments have shown that the proposed method achieved an accuracy of 97.7% while maintaining high processing speed and efficiency. Future research directions include exploring additional classes, enhancing accuracy, and implementing real-time facial emotion recognition using cameras. A user-friendly interface is also planned for integration into an application utilizing the developed model.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p></p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range/>
          <issue/>
          <year>2006</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Grimm</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Dastidar</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Kroschel</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi"/>
          <article-title>Recognizing emotions in spontaneous facial expressions</article-title>
          <source>Proceedings of the International Conference on Intelligent Systems and Computing, Cyprus</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>48</volume>
          <page-range>384-392</page-range>
          <issue/>
          <year>1993</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Paul</given-names>
              <surname>Ekman</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1037/0003-066X.48.4.384</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Facial expression and emotion</article-title>
          <source>Am. Psychol.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>205-211</page-range>
          <issue>04</issue>
          <year>2004</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Carlos</given-names>
              <surname>Busso</surname>
            </name>
            <name>
              <given-names>Zhigang G.</given-names>
              <surname>Deng</surname>
            </name>
            <name>
              <given-names>Serdar</given-names>
              <surname>Yildirim</surname>
            </name>
            <name>
              <given-names>Murtaza</given-names>
              <surname>Bulut</surname>
            </name>
            <name>
              <given-names>Chul Min</given-names>
              <surname>Lee</surname>
            </name>
            <name>
              <given-names>Abe</given-names>
              <surname>Kazemzadeh</surname>
            </name>
            <name>
              <given-names>Sungbok</given-names>
              <surname>Lee</surname>
            </name>
            <name>
              <given-names>Ulrich</given-names>
              <surname>Neumann</surname>
            </name>
            <name>
              <given-names>Shrikanth</given-names>
              <surname>Narayanan</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1145/1027933.1027968</pub-id>
          <article-title>Analysis of emotion recognition using facial expressions, speech and multimodal information</article-title>
          <source>Proceedings of the 6th International Conference on Multimodal Interfaces, State College, PA, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>170</volume>
          <page-range>234-243</page-range>
          <issue>3</issue>
          <year>2009</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Goodfellow</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Nowicki Jr</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/00221320903218281</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Social adjustment, academic adjustment, and the ability to identify emotion in facial expressions of 7-year-old children</article-title>
          <source>J. Genet. Psychol.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="book">
          <volume/>
          <page-range/>
          <issue/>
          <year>2016</year>
          <publisher-name>Cham: Springer</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>F.</given-names>
              <surname>Furrer</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Burri</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Achtelik</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Siegwart</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1007/978-3-319-26054-9_23</pub-id>
          <article-title>RotorS—A modular Gazebo MAV simulator framework</article-title>
          <source>ROS: Studies in Computational Intelligence</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>1-6</page-range>
          <issue/>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Yanan</given-names>
              <surname>Guo</surname>
            </name>
            <name>
              <given-names>Da Peng</given-names>
              <surname>Tao</surname>
            </name>
            <name>
              <given-names>Jun</given-names>
              <surname>Yu</surname>
            </name>
            <name>
              <given-names>Hao</given-names>
              <surname>Xiong</surname>
            </name>
            <name>
              <given-names>Yao Tang</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>Da Cheng</given-names>
              <surname>Tao</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/ICMEW.2016.7574736</pub-id>
          <article-title>Deep neural networks with relativity learning for facial expression recognition</article-title>
          <source>2016 IEEE International Conference on Multimedia &amp; Expo Workshops (ICMEW), Seattle, WA</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>263</volume>
          <page-range>042092</page-range>
          <issue/>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Sharma</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Anuradha</surname>
            </name>
            <name>
              <given-names>H. K.</given-names>
              <surname>Manne</surname>
            </name>
            <name>
              <given-names>G. S. C.</given-names>
              <surname>Kashyap</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1088/1757-899X/263/4/042092</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Facial detection using deep learning</article-title>
          <source>IOP Conf. Ser.: Mater. Sci. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range/>
          <issue/>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A. J.</given-names>
              <surname>Shepley</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1907.12739</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Deep learning for face recognition: A critical analysis</article-title>
          <source>arXiv preprint arXiv:1907.12739</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>1-6</page-range>
          <issue/>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A. K.</given-names>
              <surname>Sharma</surname>
            </name>
            <name>
              <given-names>U.</given-names>
              <surname>Kumar</surname>
            </name>
            <name>
              <given-names>S. K.</given-names>
              <surname>Gupta</surname>
            </name>
            <name>
              <given-names>U.</given-names>
              <surname>Sharma</surname>
            </name>
            <name>
              <given-names>S. Lakshmi</given-names>
              <surname>Agrwal</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/CCAA.2018.8777550</pub-id>
          <article-title>A survey on feature extraction technique for facial expression recognition system</article-title>
          <source>2018 4th International Conference on Computing Communication and Automation (ICCCA), Greater Noida, India</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>2584-2593</page-range>
          <issue/>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Si Cheng</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>Wei Hong</given-names>
              <surname>Deng</surname>
            </name>
            <name>
              <given-names>Jun</given-names>
              <surname>Du</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/CVPR.2017.277</pub-id>
          <article-title>Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild</article-title>
          <source>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="conf-paper">
          <volume>61</volume>
          <page-range>302-309</page-range>
          <issue/>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J.</given-names>
              <surname>Cai</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Meng</surname>
            </name>
            <name>
              <given-names>A. S.</given-names>
              <surname>Khan</surname>
            </name>
            <name>
              <given-names>Z. Y.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>O'Reilly</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Tong</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/FG.2018.00051</pub-id>
          <article-title>Island loss for learning discriminative features in facial expression recognition</article-title>
          <source>2018 13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018), Xi'an, China</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>120</volume>
          <page-range>69-74</page-range>
          <issue/>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Dhruv Kumar</given-names>
              <surname>Jain</surname>
            </name>
            <name>
              <given-names>Pourya</given-names>
              <surname>Shamsolmoali</surname>
            </name>
            <name>
              <given-names>Pankaj</given-names>
              <surname>Sehdev</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.patrec.2019.01.008</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Extended DNN for facial emotion recognition</article-title>
          <source>Pattern Recognit. Lett.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>61</volume>
          <page-range>610-628</page-range>
          <issue/>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A. T.</given-names>
              <surname>Lopes</surname>
            </name>
            <name>
              <given-names>E.</given-names>
              <surname>De Aguiar</surname>
            </name>
            <name>
              <given-names>A.  F.</given-names>
              <surname>De Souza</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Oliveira-Santos</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.patcog.2016.07.026</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Facial expression recognition with convolutional neural networks: Coping with few data and the training sample order</article-title>
          <source>Pattern Recognit.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>115</volume>
          <page-range>101-106</page-range>
          <issue/>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Naveen</given-names>
              <surname>Jain</surname>
            </name>
            <name>
              <given-names>Surender</given-names>
              <surname>Kumar</surname>
            </name>
            <name>
              <given-names>Anil</given-names>
              <surname>Kumar</surname>
            </name>
            <name>
              <given-names>Pourya</given-names>
              <surname>Shamsolmoali</surname>
            </name>
            <name>
              <given-names>Masoumeh</given-names>
              <surname>Zareapoor</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.patrec.2018.04.010</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Hybrid DNNs for face emotion recognition</article-title>
          <source>Pattern Recognit. Lett.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>1-6</page-range>
          <issue/>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Atul</given-names>
              <surname>Sajjanhar</surname>
            </name>
            <name>
              <given-names>Zhen Yu</given-names>
              <surname>Wu</surname>
            </name>
            <name>
              <given-names>Qi</given-names>
              <surname>Wen</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/DICTA.2018.8615843</pub-id>
          <article-title>Deep learning models for facial expression recognition</article-title>
          <source>2018 Digital Image Computing: Techniques and Applications (DICTA), Canberra, ACT, Australia</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>597-610</page-range>
          <issue/>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <given-names>G. H.</given-names>
              <surname>Wen</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Hou</surname>
            </name>
            <name>
              <given-names>H. H.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>Y. D. Y.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>L. J.</given-names>
              <surname>Jiang</surname>
            </name>
            <name>
              <given-names>E.</given-names>
              <surname>Xun</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s12559-017-9472-6</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Ensemble of DNNs with probability-based fusion for facial expression recognition</article-title>
          <source>Cognit. Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>405-412</page-range>
          <issue/>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Marcelo V.</given-names>
              <surname>Zavarez</surname>
            </name>
            <name>
              <given-names>Rodrigo F.</given-names>
              <surname>Berriel</surname>
            </name>
            <name>
              <given-names>Thiago</given-names>
              <surname>Oliveira-Santos</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/SIBGRAPI.2017.60</pub-id>
          <article-title>Cross-database facial expression recognition based on fine-tuned deep convolutional network</article-title>
          <source>2017 30th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI), Niteroi, Brazil</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>6897-6906</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>K. P.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>X. H.</given-names>
              <surname>Peng</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Yang</surname>
            </name>
            <name>
              <given-names>S. J.</given-names>
              <surname>Lu</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Qiao</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/CVPR42600.2020.00693</pub-id>
          <article-title>Suppressing uncertainties for large-scale facial expression recognition</article-title>
          <source>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>2022</volume>
          <page-range/>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A. A.</given-names>
              <surname>Pise</surname>
            </name>
            <name>
              <given-names>Mejdal A.</given-names>
              <surname>Alqahtani</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Verma</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Dimitrios  Karras</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Halifa</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2022/9261438</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Methods for facial expression recognition with applications in challenging situations</article-title>
          <source>Comput. Intell. Neurosci.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>2022</volume>
          <page-range/>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Saeed</surname>
            </name>
            <name>
              <given-names>A. A.</given-names>
              <surname>Shah</surname>
            </name>
            <name>
              <given-names>M. K.</given-names>
              <surname>Ehsan</surname>
            </name>
            <name>
              <given-names>M. R.</given-names>
              <surname>Amirzada</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Mahmood</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Mezgebo</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2022/5707930</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Automated facial expression recognition framework using deep learning</article-title>
          <source>J. Healthc. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>136-147</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Hammad Bin Ul</given-names>
              <surname>Haq</surname>
            </name>
            <name>
              <given-names>Muhammad</given-names>
              <surname>Saqlain</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/jii010301</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Iris detection for attendance monitoring in educational institutes amidst a pandemic: A machine learning approach</article-title>
          <source>J. Ind. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range/>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>D.</given-names>
              <surname>Baig</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Akram</surname>
            </name>
            <name>
              <given-names>H. B. U.</given-names>
              <surname>Haq</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Asif</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.47852/bonviewAIA32021378</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Cloud gaming approach to learn programming concepts</article-title>
          <source>Artif. Intell. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>17-28</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Nawaz</surname>
            </name>
            <name>
              <given-names>A. N.</given-names>
              <surname>Akhtar</surname>
            </name>
            <name>
              <given-names>H. B. U.</given-names>
              <surname>Haq</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.54692/lgurjcsit.2023.0702459</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Cloud computing services and security challenges: A review</article-title>
          <source>Lahore Garrison Univ. Res. J. Comput. Sci. Inf. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>1-14</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>H. B. U.</given-names>
              <surname>Haq</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Saqlain</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.31181/taci1120232</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>An implementation of effective machine learning approaches to perform sybil attack detection (SAD) in IoT network</article-title>
          <source>Theor. Appl. Comput. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>130-138</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Saqlain</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/jimd020303</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Sustainable hydrogen production: A decision-making approach using VIKOR and intuitionistic hypersoft sets</article-title>
          <source>J. Intell. Manag. Deci.</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>1-12</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Abid</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Saqlain</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.31181/sems1120235a</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Decision-making for the bakery product transportation using linear programming</article-title>
          <source>Spectr. Eng. Manag. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>73-82</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.N.</given-names>
              <surname>Jafar</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Muniba</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Saqlain</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.31181/sems1120238u</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Enhancing diabetes diagnosis through an intuitionistic fuzzy soft matrices-based algorithm</article-title>
          <source>Spectr. Eng. Manag. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="report">
          <article-title>FER-2013</article-title>
          <source>, https://www.kaggle.com/datasets/msambare/fer2013</source>
          <year>2013</year>
          <publisher-name>Kaggle</publisher-name>
          <publisher-loc/>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="report">
          <article-title>Dataset</article-title>
          <source>, https://www.dropbox.com/s/w3zlhing4dkgeyb/train.zip?dl=0</source>
          <year/>
          <publisher-name>DropBox</publisher-name>
          <publisher-loc/>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="report">
          <article-title>MobileNet V1 Architecture</article-title>
          <source>, https://iq.opengenus.org/mobilenet-v1-architecture</source>
          <year/>
          <publisher-name>OpenGenus IQ</publisher-name>
          <publisher-loc/>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="webpage">
          <article-title>Image classification with MobileNet</article-title>
          <source>, https://medium.com/analytics-vidhya/image-classification-using-mobilenet-in-the-browser-b69f2f57abf</source>
          <year>2020</year>
          <uri/>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="webpage">
          <article-title>Object detection: Speed and accuracy comparison (Faster R-CNN, R-FCN, SSD, FPN, RetinaNet and YOLOv3)</article-title>
          <source>, https://medium. com/@ jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359</source>
          <year/>
          <uri/>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>279-283</page-range>
          <issue/>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <given-names>E.</given-names>
              <surname>Barsoum</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>C. C.</given-names>
              <surname>Ferrer</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Zhang</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1145/2993148.2993165</pub-id>
          <article-title>Training deep networks for facial expression recognition with crowd-sourced label distribution</article-title>
          <source>Proceedings of the 18th ACM International Conference on Multimodal Interaction, Tokyo, Japan</source>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>544-550</page-range>
          <issue/>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Xu</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Huang</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Song</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Li</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TAFFC.2018.2880201</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Facial expression recognition with identity and emotion joint learning</article-title>
          <source>IEEE Trans. Affect. Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_35">
        <label>35.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>64827-64836</page-range>
          <issue/>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M. I.</given-names>
              <surname>Georgescu</surname>
            </name>
            <name>
              <given-names>R. T.</given-names>
              <surname>Ionescu</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Popescu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1804.10892</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Local learning with deep and handcrafted features for facial expression recognition</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_36">
        <label>36.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>1-4</page-range>
          <issue/>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Chao</given-names>
              <surname>Huang</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/URTC.2017.8284175</pub-id>
          <article-title>Combining CNNs for emotion recognition</article-title>
          <source>2017 IEEE MIT Undergraduate Research Technology Conference (URTC), Cambridge, MA, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_37">
        <label>37.</label>
        <element-citation publication-type="journal">
          <volume>61</volume>
          <page-range>4435-4444</page-range>
          <issue>6</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Y. H.</given-names>
              <surname>Nan</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Jiang Grace  Ju</surname>
            </name>
            <name>
              <given-names>Q.</given-names>
              <surname>Hua</surname>
            </name>
            <name>
              <given-names>H. M.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Wang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.aej.2021.09.066</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A-MobileNet: An approach of facial expression recognition</article-title>
          <source>Alex. Eng. J.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>