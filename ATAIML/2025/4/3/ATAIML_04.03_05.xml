<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-8WOb3Ozy-jY0mpmzZjGpLrP4jDrszCtB</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml040305</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Application of Artificial Intelligence on MNIST Dataset for Handwritten Digit Classification for Evaluation of Deep Learning Models</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2505-4024</contrib-id>
          <name>
            <surname>Akinsola</surname>
            <given-names>Jide Ebenezer Taiwo</given-names>
          </name>
          <email>akinsolajet@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-8932-6642</contrib-id>
          <name>
            <surname>Olatunbosun</surname>
            <given-names>Micheal Adeolu</given-names>
          </name>
          <email>michealolatunbosun368@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-9278-3611</contrib-id>
          <name>
            <surname>Olaniyi</surname>
            <given-names>Ifeoluwa Michael</given-names>
          </name>
          <email>olaniyiifeoluwa12@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1664-2070</contrib-id>
          <name>
            <surname>Adeagbo</surname>
            <given-names>Moruf Adedeji</given-names>
          </name>
          <email>adedegy@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3244-0807</contrib-id>
          <name>
            <surname>Olajubu</surname>
            <given-names>Emmanuel Ajayi</given-names>
          </name>
          <email>emmolajubu@oauife.edu.ng</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7992-514X</contrib-id>
          <name>
            <surname>Aderounmu</surname>
            <given-names>Ganiyu Adesola</given-names>
          </name>
          <email>gaderoun@oauife.edu.ng</email>
        </contrib>
        <aff id="aff_1">Department of Computer Sciences, Abiola Ajimobi Technical University, 200255 Ibadan, Nigeria</aff>
        <aff id="aff_2">Department of Computer Science and Engineering, Obafemi Awolowo University, 220282 Ile–Ife, Nigeria</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>18</day>
        <month>09</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>3</issue>
      <fpage>219</fpage>
      <lpage>234</lpage>
      <page-range>219-234</page-range>
      <history>
        <date date-type="received">
          <day>22</day>
          <month>07</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>14</day>
          <month>09</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Handwritten digit classification represents a foundational task in computer vision and has been widely adopted in applications ranging from Optical Character Recognition (OCR) to biometric authentication. Despite the availability of large benchmark datasets, the development of models that achieve both high accuracy and computational efficiency remains a central challenge. In this study, the performance of three representative machine learning paradigms—Chi-Squared Automatic Interaction Detection (CHAID), Generative Adversarial Networks (GANs), and Feedforward Deep Neural Networks (FFDNNs)—was systematically evaluated on the Modified National Institute of Standards and Technology (MNIST) dataset. The assessment was conducted with a focus on classification accuracy, computational efficiency, and interpretability. Experimental results demonstrated that deep learning approaches substantially outperformed traditional Decision Tree (DT) methods. GANs and FFDNNs achieved classification accuracies of approximately 97%, indicating strong robustness and generalization capability for handwritten digit recognition tasks. In contrast, CHAID achieved only 29.61% accuracy, highlighting the limited suitability of DT models for high-dimensional image data. It was further observed that, despite the computational demand of adversarial training, GANs required less time per epoch than FFDNNs when executed on modern GPU architectures, thereby underscoring their potential scalability. These findings reinforce the importance of model selection in practical deployment, particularly where accuracy, computational efficiency, and interpretability must be jointly considered. The study contributes to the ongoing discourse on the role of artificial intelligence (AI) in pattern recognition by providing a comparative analysis of classical machine learning and deep learning approaches, thereby offering guidance for the development of reliable and efficient digit recognition systems suitable for real-world applications.</p></abstract>
      <kwd-group>
        <kwd>Artificial intelligence</kwd>
        <kwd>Handwritten</kwd>
        <kwd>Deep learning</kwd>
        <kwd>Digit classification</kwd>
        <kwd>Machine learning</kwd>
        <kwd>MNIST dataset</kwd>
        <kwd>Pattern recognition</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="6"/>
        <fig-count count="11"/>
        <table-count count="8"/>
        <ref-count count="31"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Handwritten digit classification is the process of automatically assigning labels to images containing handwritten digits. It involves training a computer algorithm to recognize and distinguish between different handwritten digits (0 through 9) based on visual patterns and features present in the images. Handwritten digit recognition refers to the computer's ability to accurately interpret manually written digits from different sources such as messages, financial institution checks, papers, and pictures. This technology is used in a variety of situations, including web-based handwriting recognition on personal computers (PCs) and tablets, processing bank checks and interpreting digits entered in any form [<xref ref-type="bibr" rid="ref_1">1</xref>].</p><p>A piece of paper contains a substantial amount of information, and the cost of handling electronic files is lower compared to handling conventional paper files. In the past, handwritten digits were categorized using a range of methods, including both conventional machine learning algorithms and more sophisticated deep learning structures. Machine learning and deep learning techniques have been applied in various domains with outstanding results [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>], [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>]. Traditional methodologies frequently depend on manually designed characteristics and algorithms like k-nearest neighbors (KNN) or support vector machines (SVMs) to differentiate between distinct digits based on their visual attributes. Although these methods exhibited satisfactory performance, they frequently encounter difficulties in effectively adapting to unfamiliar material or managing intricate changes in handwriting styles [<xref ref-type="bibr" rid="ref_10">10</xref>].</p><p>While state-of-the-art deep learning models such as Convolutional Neural Networks (CNNs) and Capsule Networks (CapsNet) have shown impressive results in controlled environments, their limitations in real-world applications remain a challenge. CNNs often struggle with robustness when dealing with oblique handwriting or significant variation in writing orientation, while CapsNet, though more advanced, can be computationally expensive and highly sensitive to noise in scanned or degraded documents. These limitations highlight the need for alternative approaches that balance interpretability, computational efficiency, and resilience against noise.</p><p>In this study, CHAID, FFDNNs, and GANs were used to analyze the MNIST handwritten digit classification dataset. This study further suggests that using deep learning architectures for solving handwritten digit recognition problems is more advantageous than shallow neural architectures [<xref ref-type="bibr" rid="ref_11">11</xref>]. The deep learning approach tends to use large datasets to give a more robust accuracy. A deep neural network employs multiple hidden layers, each of which is not necessarily fully connected [<xref ref-type="bibr" rid="ref_12">12</xref>]. When it comes to feature extraction, it outperforms handcrafted features and functions directly on the complete image. As a result, it is particularly beneficial in addressing tough pattern recognition tasks. It may also recognize significant aspects of an object without the requirement for human supervision or participation.</p><p>In this study, the performance of each machine learning model or combination of them was studied to give an improved way to tackle variances in handwritten recognition challenges. In addition, the overall impact of the dataset utilized, the size, and how it impacts favorably or adversely on achieving a good recognition system were identified. Arising from the research findings, the contribution of this study to the existing scientific knowledge is based on the following key points:</p><p>·Comparative model performance: A comprehensive comparison of CHAID, FFDNN, and GAN models on the MNIST dataset using standard evaluation metrics (accuracy, precision, recall, F1-score, and loss).</p><p>·Training behavior analysis: Examination of model learning behavior through accuracy–loss curves across epochs to highlight differences in convergence speed, stability, and generalization.</p><p>·Performance interpretation through visualization: Use of bar charts and line graphs to clearly present classification outcomes and performance gaps between models, supporting practical insights into model selection for handwritten digit recognition.</p><p>The remaining parts of the study are organized below. Section 2 reviews related works for multi-model exploration of machine learning algorithms for MNIST handwritten digit classification. Section 3 describes the model and datasets used for the study. Experimentation results are presented and discussed in Section 4. Finally, Section 5 presents the conclusion and recommendations for further studies.</p>
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p>Goodfellow et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] conducted research on GANs and found a better approach to training generative and discriminative models utilizing backpropagation and dropout algorithms. The specific instances with both models being multilayer perceptrons (MLPs) were examined, proving the validity of this paradigm. Gaussian Parzen window estimates were used for probability fitting and GANs were applied to several datasets, including MNIST, the Toronto Face Database, and CIFAR-10. Qiao et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] proposed an adaptive Q-learning deep belief network (Q-ADBN), which integrates Q-learning and deep learning to examine the efficacy and precision of handwritten digit identification while minimizing the running time. Throughout the feature extraction process, the adaptive deep auto-encoder (ADAE) may modify the learning rate adaptively, promoting process convergence and reducing running time. The hierarchical feature extraction method utilized by ADAE is akin to the humanoid learning process. The MNIST database, which contains 60,000 training and 10,000 testing images, is the source of the handwritten digit images. Each image comprises a pixel point range of 0 to 1 and a handwritten number. This work made use of the MNIST handwritten digit dataset, which consists of a training set of 55,000 photos and a testing set of 10,000 annotated photographs. A two-layer perceptron network was trained using the handwritten digits.</p><p>Gupta and Raza [<xref ref-type="bibr" rid="ref_15">15</xref>] introduced the usage of Tabu search and gradient descent with momentum backpropagation as an optimization tool for designing FFDNNs. Other techniques such as genetic algorithms, Bayesian optimization, and multi-objective evolutionary processes were integrated to create and optimize neural network topologies. Additionally, these approaches were applied to diverse datasets for classification tasks and statistical analysis of the suggested methodology was conducted. Zhao and Liu [<xref ref-type="bibr" rid="ref_16">16</xref>] proposed an ensemble learning framework that involves the fusion of multiple classifiers trained on different feature sets using CNN-based feature extraction and algebraic fusion. The approach achieved a classification accuracy of at least 98%. In addition, machine learning methods such as KNN and Random Forest (RF) were compared and fused by averaging hidden outputs, with parameters set for KNN (K = 3) and RF (100 DTs).</p><p>Chen et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] compared CNNs, Deep Residual Networks (ResNet), Dense Convolutional Networks (DenseNet), and CapsNet on the MNIST dataset for handwritten digit recognition. ResNet was noted for its ability to train deeper networks, while DenseNet was recognized for its dense connections. CapsNet was implemented with dynamic routing algorithms for weight updates and capsule-based representations to retain detailed image information. Experimental results demonstrated that CapsNet achieved 99.5% accuracy, requiring minimal data and consistently outperforming the baseline model, thereby making it a promising approach for image recognition tasks. The superior performance of CapsNet can be attributed to architectural advantages since dynamic routing enables it to capture hierarchical spatial relationships and retain detailed features, which improves robustness to distortions and variations in handwritten digits. In contrast, the models employed in this study, such as FFDNNs and GANs, do not incorporate such advanced spatial feature representation or extensive augmentation strategies, which explains the observed accuracy gap. This highlights the trade-off between model complexity, computational efficiency, and accuracy in handwritten digit recognition.</p><p>Zhao et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] used GANs for picture deblurring, which can be separated into blind deblurring and nonblind deblurring methods. Early work focused on nonblind deblurring, assuming knowledge of the ambiguity function. Recent improvements include the use of deep learning-based algorithms and GANs for image deblurring. These AI-based solutions have demonstrated promising results in enhancing deblurring performance for various types of blurred photos by utilizing the power of neural networks and advanced learning algorithms. Roohi [<xref ref-type="bibr" rid="ref_19">19</xref>] conducted research on Persian handwritten character recognition, emphasizing the application of CNNs in this domain. By implementing neural network structures such as the Softmax classifier and Rectified Linear Unit (ReLU) activation, they were able to achieve a notable accuracy rate of 97.1% using a network configuration involving 64 networks. This study stands out for its comprehensive comparison of CNNs with traditional methods, showcasing the superior performance of CNNs in the specific task of recognizing Persian characters. The implications of these results extend to the broader fields of machine vision and neural network applications, offering valuable insights for researchers interested in character recognition, classification, and related areas of study. The findings presented in this research contribute to the advancement of techniques and methodologies in the realm of pattern recognition and neural network applications, particularly within the context of handwritten character analysis and classification tasks.</p>
    </sec>
    <sec sec-type="">
      <title>3. Materials and methods</title>
      <p>This section presents a detailed explanation of the dataset used, the preprocessing techniques applied to prepare the data, the tools adopted for model development, and the implementation of the three machine learning models evaluated in this study. The aim is to compare the classification accuracy, computational performance, and interpretability of CHAID, FFDNN, and GAN models on the MNIST handwritten digit dataset.</p>
      
        <sec>
          
            <title>3.1. Dataset</title>
          
          <p>In this study, the dataset used is MNIST. It is a large benchmark database that consists of handwritten digits commonly used for training various image processing systems. It comprises 28×28 pixel images of handwritten digits such as 0, 1, 2, and 3. The dataset consists of 700,000 handwritten digits of 0–9 with ten balanced classes. They are divided into 60,000 data points and 10,000 testing data points. It is mostly used in machine learning techniques called classification. Data preparation, feature extraction, and preprocessing were carried out using NumPy, Scikit-learn, Keras, and Pandas. Google Colab’s GPU was employed as the development environment, which enabled efficient training of the models over multiple epochs.</p>
          <p>CHAID, FFDNNs, and GANs are the three machine learning algorithms used to build the model. Performance metrics such as accuracy, precision, recall, F1-score and loss function were carried out on this multi-model to evaluate how well the model performs on each algorithm. <xref ref-type="fig" rid="fig_1">Figure 1</xref> shows a snippet of the MNIST dataset used for this study. The MNIST dataset was chosen for the three algorithms to build the models because there are variances in the types of handwritings, strokes, orientations, and sizes that are included. In addition, they are representative of a broad and varied sample of human writings, which may help gain an understanding of generic characteristics and trends. Handwritings are of peculiar characteristics which can affect the output of OCR. This in turn largely determines how the developed solution performs.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>MNIST dataset [<xref ref-type="bibr" rid="ref_17">17</xref>]</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/8/img_pId4Qmaet1-WNFEz.png"/>
            </fig>
          
          <p><xref ref-type="fig" rid="fig_2">Figure 2</xref> shows the handwritten digit classification process flow used for the study.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Handwritten digit classification process flow</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/8/img_wBZ-faTheeBu-osQ.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Data preprocessing</title>
          
          <p>Before training the models, a series of data preprocessing and feature engineering steps were carried out to prepare the MNIST dataset for optimal performance and to ensure finite, well-defined outputs across all models. Each image in the dataset, originally a 28×28 grayscale matrix, was first reshaped into a one-dimensional vector of 784 features. This transformation is necessary for compatibility with fully connected architectures such as FFDNNs. To standardize the input scale and enhance learning efficiency, all pixel values were normalized from the original range of 0–255 to a range between 0 and 1. This normalization was achieved by dividing each pixel value by 255, which helped reduce computational overhead and minimize the risk of numerical instability during training.</p><p>For supervised learning tasks, the target labels (digits 0–9) were one-hot encoded into 10-dimensional binary vectors. This encoding was essential for enabling multi-class classification using the SoftMax activation function at the output layer, particularly in neural network models. The dataset was divided into training (60%), validation (20%), and test (20%) sets. During training, the data was shuffled before each epoch to prevent sequential dependency bias. No additional data augmentation techniques (such as rotation or scaling) were applied because MNIST already consists of clean, balanced handwritten digits, and the study’s focus was to benchmark models without artificially inflating performance. Although deep learning models are capable of automatic feature extraction, the combination of these preprocessing steps ensures that the data is clean, consistent, and structured in a way that supports efficient learning and accurate predictions. These efforts contribute significantly to the stability of the training process and the generation of finite, interpretable outputs across all implemented models.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Model building tools</title>
          
          <p>The models developed in this study were implemented using a combination of open-source libraries, cloud-based computational resources, and supportive utilities integrated within the Python programming ecosystem. Python was selected due to its extensive machine learning ecosystem, rich scientific computing support, and ease of use. The following tools and libraries were employed in this study:</p><p>·NumPy: Used for efficient numerical operations and matrix manipulations, especially during data reshaping and vectorized mathematical operations required in model training.</p><p>·Pandas: Enabled organized handling of structured data such as class labels, evaluation outputs, and tabular performance summaries. It was also useful for managing input/output during the preprocessing and evaluation stages.</p><p>·Matplotlib and Seaborn: These libraries were used for visualizing model performance, such as plotting training/validation accuracy and loss over epochs, as well as rendering confusion matrices and performance comparisons.</p><p>·Scikit-learn: Served as the core library for computing evaluation metrics such as accuracy, precision, recall, and F1-score. It also supported tasks like train-test splitting, label encoding, and performance benchmarking using built-in functions.</p><p>·Keras (with TensorFlow backend): Keras was used to define and train both the FFDNN and GAN architectures. TensorFlow, acting as the backend, handled automatic differentiation, computational graph execution, and GPU-accelerated operations.</p><p>·Google Colaboratory (Colab): Functioned as the primary development environment, offering free access to cloud-based GPU resources. This was particularly beneficial for training deep learning models over multiple epochs, accelerating training, and enabling interactive code execution in Jupyter notebooks.</p><p>·OpenCV and Python Imaging Library (PIL): These libraries were optionally used for image preprocessing and manipulation. Although MNIST images are standardized, these tools were helpful in tasks such as image reshaping, display, and pixel-wise inspection during exploratory analysis and visualization.</p><p>·TensorBoard: Integrated with TensorFlow, TensorBoard was used to monitor training progress, visualize loss and accuracy metrics in real time, and evaluate layer-wise behavior during model debugging.</p><p>·OS and Glob (Python standard libraries): These were utilized for managing file paths, handling data loading from directories, and automating file access operations in the project workflow.</p><p>These tools provided a flexible and powerful environment for building, training, debugging, visualizing, and evaluating the machine learning models in this study. Their seamless integration enabled reproducible experimentation and consistent performance tracking across the different stages of model development.</p>
          
            <sec>
              
                <title>3.3.1 Chaid</title>
              
              <p>CHAID is a DT algorithm that uses chi-squared statistical tests to determine the best feature splits at each node. It is well-suited for categorical data and excels in interpretability [<xref ref-type="bibr" rid="ref_20">20</xref>]. In the context of image data, however, CHAID has notable limitations. Pixel values are numerical and continuous, which restricts CHAID’s effectiveness unless the data is discretized. Moreover, it treats each pixel as an independent variable, disregarding spatial relationships among them.</p><p>CHAID uses chi-square tests to find the most dominant feature. The formula of chi-square testing is: </p>
              
                <disp-formula>
                  <label>(1)</label>
                  <mml:math id="m6lltpolbj">
                    <mml:mtext> Chi-square </mml:mtext>
                    <mml:mo>=</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:msup>
                      <mml:mi>X</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msup>
                    <mml:msqrt>
                      <mml:mfrac>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mo>(</mml:mo>
                            <mml:mo>−</mml:mo>
                            <mml:mo>)</mml:mo>
                            <mml:mrow>
                              <mml:mi>y</mml:mi>
                            </mml:mrow>
                            <mml:msup>
                              <mml:mrow>
                                <mml:mi>y</mml:mi>
                              </mml:mrow>
                              <mml:mrow>
                                <mml:mi>′</mml:mi>
                              </mml:mrow>
                            </mml:msup>
                          </mml:mrow>
                          <mml:mn>2</mml:mn>
                        </mml:msup>
                        <mml:msup>
                          <mml:mrow>
                            <mml:mi>y</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>′</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mfrac>
                    </mml:msqrt>
                  </mml:math>
                </disp-formula>
              
              <p> where, $y<inline-formula>
  <mml:math id="mlm4kfh2dn">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>y^{\prime}$ is the expected value.</p><p>The formula for the chi-squared value used in CHAID is:</p>
              
                <disp-formula>
                  <label>(2)</label>
                  <mml:math id="mnlxfs0ix3">
                    <mml:mi>x</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:munderover>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mi>a</mml:mi>
                        <mml:mstyle scriptlevel="0">
                          <mml:mspace width="0.167em"/>
                        </mml:mstyle>
                        <mml:mstyle scriptlevel="0">
                          <mml:mspace width="0.167em"/>
                        </mml:mstyle>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>k</mml:mi>
                    </mml:munderover>
                    <mml:mfrac>
                      <mml:msup>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:msub>
                            <mml:mi>O</mml:mi>
                            <mml:mrow>
                              <mml:mrow>
                                <mml:mi>a</mml:mi>
                              </mml:mrow>
                            </mml:mrow>
                          </mml:msub>
                          <mml:msub>
                            <mml:mi>E</mml:mi>
                            <mml:mrow>
                              <mml:mrow>
                                <mml:mi>a</mml:mi>
                              </mml:mrow>
                            </mml:mrow>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:msup>
                      <mml:msub>
                        <mml:mi>E</mml:mi>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mi>a</mml:mi>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mfrac>
                  </mml:math>
                </disp-formula>
              
              <p> where, $k<inline-formula>
  <mml:math id="mwur4fl53s">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>O_{\mathrm{a}}<inline-formula>
  <mml:math id="messf82ut6">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
  </mml:math>
</inline-formula>a<inline-formula>
  <mml:math id="m7p77qfibx">
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>E_{\mathrm{a}}<inline-formula>
  <mml:math id="m624exdvqd">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
  </mml:math>
</inline-formula>a<inline-formula>
  <mml:math id="mbpcg5klh6">
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>n<inline-formula>
  <mml:math id="mcg4y2wksq">
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>E_{\mathrm{a}}=n x P_{\mathrm{a}}<inline-formula>
  <mml:math id="m65au3uo1j">
    <mml:mo>,</mml:mo>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>P_{\mathrm{a}}<inline-formula>
  <mml:math id="mwhsifmcp9">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
  </mml:math>
</inline-formula>a$.</p><p>The chi-squared automatic interaction detector (CHAID) technique is widely used in supervised learning since it can solve any type of issue. CHAID is a statistical approach used to identify correlations between variables. At its foundation, CHAID creates a predictive model, generally shown as a tree, to discover how factors might be combined to best explain a result in a dependent variable. Unlike most other approaches, CHAID can handle nominal, ordinal, and even continuous data, providing a diverse approach to data analysis.</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.2 Gans</title>
              
              <p>GANs are a form of neural network that is capable of generating realistic data from random disturbances [<xref ref-type="bibr" rid="ref_21">21</xref>]. GANs implemented in this study were specifically designed for the MNIST dataset. The generator and discriminator architectures are described in detail below to ensure reproducibility [<xref ref-type="bibr" rid="ref_22">22</xref>]. The generator takes as input a 100-dimensional random noise vector sampled from a standard normal distribution. This vector is passed through a series of fully connected and transposed convolutional (deconvolutional) layers. Each layer is followed by batch normalization and ReLU activation, except for the output layer, which uses a Tanh activation to produce 28×28 grayscale images normalized between -1 and 1. The discriminator is a CNN that takes a 28×28 image as input and outputs a probability, indicating whether the image is real or generated. The network consists of convolutional layers with LeakyReLU activation functions, followed by dropout for regularization, and a final fully connected layer with a sigmoid activation function.</p><p>The mathematical expression for the objective function of a GAN is:</p>
              
                <disp-formula>
                  <label>(3)</label>
                  <mml:math id="m5vgpvs1hc">
                    <mml:munder>
                      <mml:mo>min</mml:mo>
                      <mml:mi>G</mml:mi>
                    </mml:munder>
                    <mml:munder>
                      <mml:mo>max</mml:mo>
                      <mml:mi>D</mml:mi>
                    </mml:munder>
                    <mml:mi>V</mml:mi>
                    <mml:mi>D</mml:mi>
                    <mml:mi>G</mml:mi>
                    <mml:mi>log</mml:mi>
                    <mml:mi>D</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>log</mml:mi>
                    <mml:mi>D</mml:mi>
                    <mml:mi>G</mml:mi>
                    <mml:mi>z</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>[</mml:mo>
                    <mml:mo>⁡</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>]</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>[</mml:mo>
                    <mml:mo>⁡</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>]</mml:mo>
                    <mml:msub>
                      <mml:mi>E</mml:mi>
                      <mml:mrow>
                        <mml:mi>x</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:mo>∼</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mi>p</mml:mi>
                          <mml:mrow>
                            <mml:mtext>data </mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>E</mml:mi>
                      <mml:mrow>
                        <mml:mi>z</mml:mi>
                        <mml:mi>z</mml:mi>
                        <mml:mo>∼</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mi>p</mml:mi>
                          <mml:mi>z</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mn>1</mml:mn>
                  </mml:math>
                </disp-formula>
              
              <p> where, $D<inline-formula>
  <mml:math id="mlejdsac2j">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>G<inline-formula>
  <mml:math id="mjr0x69yxz">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>x<inline-formula>
  <mml:math id="mxaa45lysa">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>z<inline-formula>
  <mml:math id="m10s8g6l7x">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>data (x)<inline-formula>
  <mml:math id="mp0uuofgbp">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>p_z(z)$ is the noise distribution.</p><p>The training objective of a GAN is defined by a minimax function, and optimization is typically performed using stochastic gradient descent. GANs are especially useful in learning data distributions and generating high-fidelity images, making them a powerful tool for classification when used with auxiliary classifiers. <xref ref-type="fig" rid="fig_3">Figure 3</xref> shows the random images generated by GANs.</p>
              
                <fig id="fig_3">
                  <label>Figure 3</label>
                  <caption>
                    <title>Random images generated by GANs</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/8/img_jbf8IkuPTnGhUp5J.png"/>
                </fig>
              
              <p>The detailed layer configurations of both the generator and discriminator are summarized in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
              
                <table-wrap id="table_1">
                  <label>Table 1</label>
                  <caption>
                    <title>Mapping of alloy systems to processing parameters and experimental figures</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="1" rowspan="1"><p>Layer Type</p></td><td colspan="1" rowspan="1"><p>Output Size/Neutrons</p></td><td colspan="1" rowspan="1"><p>Activation Function</p></td><td colspan="1" rowspan="1"><p>Notes</p></td></tr><tr><td colspan="1" rowspan="3"><p>Generator</p></td><td colspan="1" rowspan="1"><p>Dense (100 → 7×7×128)</p></td><td colspan="1" rowspan="1"><p>7×7×128</p></td><td colspan="1" rowspan="1"><p>ReLU + BatchNorm</p></td><td colspan="1" rowspan="1"><p>Input noise reshaped</p></td></tr><tr><td colspan="1" rowspan="1"><p>ConvTranspose2D</p></td><td colspan="1" rowspan="1"><p>14×14×64</p></td><td colspan="1" rowspan="1"><p>ReLU + BatchNorm</p></td><td colspan="1" rowspan="1"><p>Stride = 2</p></td></tr><tr><td colspan="1" rowspan="1"><p>ConvTranspose2D</p></td><td colspan="1" rowspan="1"><p>28×28×1</p></td><td colspan="1" rowspan="1"><p><mml:math id="m4i3a1txu5">
  <mml:mrow>
    <mml:mover>
      <mml:mrow>
        <mml:mi>F</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mo>⇀</mml:mo>
      </mml:mrow>
    </mml:mover>
  </mml:mrow>
</mml:math></p></td><td colspan="1" rowspan="1"><p>Output image</p></td></tr><tr><td colspan="1" rowspan="3"><p>Discriminator</p></td><td colspan="1" rowspan="1"><p>Conv2D (28×28×1 → 64)</p></td><td colspan="1" rowspan="1"><p>14×14×64</p></td><td colspan="1" rowspan="1"><p>LeakyReLU (α = 0.2)</p></td><td colspan="1" rowspan="1"><p>Stride = 2</p></td></tr><tr><td colspan="1" rowspan="1"><p>Conv2D (64 → 128)</p></td><td colspan="1" rowspan="1"><p>7×7×128</p></td><td colspan="1" rowspan="1"><p>LeakyReLU (α = 0.2)</p></td><td colspan="1" rowspan="1"><p>Dropout = 0.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>Flatten Dense</p></td><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>Sigmoid</p></td><td colspan="1" rowspan="1"><p>Output probability</p></td></tr></tbody></table>
                </table-wrap>
              
              <p>Training was conducted using the Adam optimizer with a learning rate of 0.0002 and <inline-formula>
  <mml:math id="mz1c8m9e7j">
    <mml:msub>
      <mml:mi>β</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> = 0.5, a batch size of 64, 50 training epochs, and a latent vector dimension of 100.</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.3 Ffdnns</title>
              
              <p>FFDNNs are a class of artificial neural networks in which data flows in a single direction, starting from the input layer, passing through multiple hidden layers, and ending at the output layer, without forming cycles or loops [<xref ref-type="bibr" rid="ref_23">23</xref>]. Each layer in the network consists of neurons connected by weighted links, and the output of each neuron is determined by an activation function applied to the weighted sum of its inputs plus a bias term. This architecture enables the network to learn complex nonlinear mappings from inputs to outputs, making it highly effective for tasks like image recognition. The mathematical formulation of an FFDNN with L layers can be expressed as:</p>
              
                <disp-formula>
                  <label>(4)</label>
                  <mml:math id="mutyb92f6q">
                    <mml:mi>y</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:msub>
                      <mml:mi>f</mml:mi>
                      <mml:mi>L</mml:mi>
                    </mml:msub>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>W</mml:mi>
                        <mml:mi>L</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>f</mml:mi>
                        <mml:mrow>
                          <mml:mi>L</mml:mi>
                          <mml:mo>−</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>b</mml:mi>
                        <mml:mi>L</mml:mi>
                      </mml:msub>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>⋯</mml:mo>
                        <mml:mo>⋯</mml:mo>
                        <mml:mo>+</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mi>W</mml:mi>
                          <mml:mrow>
                            <mml:mi>L</mml:mi>
                            <mml:mo>−</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>f</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>b</mml:mi>
                          <mml:mrow>
                            <mml:mi>L</mml:mi>
                            <mml:mo>−</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>+</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:msub>
                            <mml:mi>W</mml:mi>
                            <mml:mn>1</mml:mn>
                          </mml:msub>
                          <mml:msub>
                            <mml:mi>b</mml:mi>
                            <mml:mn>1</mml:mn>
                          </mml:msub>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where, $x<inline-formula>
  <mml:math id="mqt9ih5rjr">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>y<inline-formula>
  <mml:math id="mkgchbvnhz">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>W_i<inline-formula>
  <mml:math id="mvd7bkrisq">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>b_i<inline-formula>
  <mml:math id="moeterpldh">
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>i<inline-formula>
  <mml:math id="m3firmz4g2">
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>f_i$ is the activation function.</p><p>In this study, the FFDNN model was trained on the MNIST dataset to classify handwritten digits. The network demonstrated excellent learning capacity and generalization, achieving a classification accuracy of 97%, thereby confirming its suitability for handwritten digit recognition tasks.</p><p>In the implementation of this study, the input layer receives a flattened 28×28 grayscale image (784 features). The network consists of three hidden layers with 512, 256, and 128 neurons, respectively, each using the ReLU activation function. The output layer consists of 10 neurons corresponding to the MNIST digit classes, activated by a Softmax function. To improve generalization, Dropout with a rate of 0.5 was applied after each hidden layer, and L2 regularization was employed during training. The model was trained using the Adam optimizer with a learning rate of 0.001, a batch size of 128, and a categorical cross-entropy loss function for 50 epochs. A summary of the FFDNN architecture is shown in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
              
                <table-wrap id="table_2">
                  <label>Table 2</label>
                  <caption>
                    <title>FFDNN model architecture</title>
                  </caption>
                  <table><tr><th >Layer</th><th >Neurons</th><th >Activation Function</th><th >Regularization</th></tr><tr><td >Input layer</td><td >784</td><td >-</td><td >-</td></tr><tr><td >Hidden layer 1</td><td >512</td><td >ReLU</td><td >Dropout (0.5)</td></tr><tr><td >Hidden layer 2</td><td >256</td><td >ReLU</td><td >Dropout (0.5)</td></tr><tr><td >Hidden layer 3</td><td >128</td><td >ReLU</td><td >Dropout (0.5)</td></tr><tr><td >Output layer</td><td >10</td><td >Softmax</td><td >-</td></tr></table>
                </table-wrap>
              
              <p>FFDNNs achieved a classification accuracy of 97% on the MNIST dataset, thereby confirming its strong learning ability and generalization performance for handwritten digit recognition tasks.</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.4 Hyperparameter selection and rationale</title>
              
              <p>The choice of training parameters was guided by prior research on MNIST classification and empirical tuning during preliminary experiments. For FFDNNs, the Adam optimizer with a learning rate of 0.001 and a batch size of 128 was adopted, as these settings consistently yielded stable convergence and high accuracy in benchmark studies [<xref ref-type="bibr" rid="ref_24">24</xref>], [<xref ref-type="bibr" rid="ref_25">25</xref>]. To mitigate overfitting, Dropout (0.5) and L2 regularization were employed, with their effectiveness validated by monitoring the validation loss curve. The model was trained for 10 epochs, since convergence was reached within this range and further training did not provide noticeable improvements.</p><p>For GANs, the Adam optimizer with a learning rate of 0.0002 and <inline-formula>
  <mml:math id="m3m8darnhp">
    <mml:msub>
      <mml:mi>β</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> = 0.5 was chosen, consistent with recommendations from the Deep Convolutional Generative Adversarial Network (DCGAN) framework [<xref ref-type="bibr" rid="ref_21">21</xref>]. A batch size of 64 was selected to balance stability and computational efficiency. Training was conducted for 10 epochs; longer runs tended to cause instability and mode collapse without improving the quality of generated images. For the CHAID model, continuous pixel features were discretized using equal-width binning into 10 intervals per feature, enabling effective handling of numerical data. To prevent overfitting and maintain interpretability, the maximum tree depth was restricted to 5. Overall, this parameter configuration balances reproducibility, computational feasibility, and model performance, while minimizing subjectivity in parameter tuning.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.4. Performance evaluation metrics</title>
          
          <p>To assess the performance of the implemented models, a set of standard evaluation metrics was used. These include accuracy, precision, recall, F1-score, and loss function. These metrics provide a comprehensive view of how well each model performs in terms of both correctness and reliability. <xref ref-type="table" rid="table_3">Table 3</xref> shows performance metrics and corresponding mathematical expressions.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Performance metrics and mathematical expressions</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1" colwidth="392"><p>S/N</p></th><th colspan="1" rowspan="1"><p>Performance Metrics</p></th><th colspan="1" rowspan="1"><p>Mathematical Expressions</p></th><th colspan="1" rowspan="1"><p>Descriptions</p></th></tr><tr><td colspan="1" rowspan="1" colwidth="392"><p>1</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p><mml:math id="mx6zrji1bv">
  <mml:mfrac>
    <mml:mrow>
      <mml:mrow>
        <mml:mi data-mjx-auto-op="false">TP</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mi data-mjx-auto-op="false">TN</mml:mi>
      </mml:mrow>
      <mml:mo>+</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mrow>
        <mml:mi data-mjx-auto-op="false">TP</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mi data-mjx-auto-op="false">TN</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mi data-mjx-auto-op="false">FP</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mi data-mjx-auto-op="false">FN</mml:mi>
      </mml:mrow>
      <mml:mo>+</mml:mo>
      <mml:mo>+</mml:mo>
      <mml:mo>+</mml:mo>
    </mml:mrow>
  </mml:mfrac>
</mml:math></p></td><td colspan="1" rowspan="1"><p>It measures overall correctness, with <mml:math id="mdk6ii6ijc">
  <mml:mi>T</mml:mi>
  <mml:mi>P</mml:mi>
</mml:math>, <mml:math id="mo7g990xoi">
  <mml:mi>T</mml:mi>
  <mml:mi>N</mml:mi>
</mml:math>, <mml:math id="mvpszg0dw5">
  <mml:mi>F</mml:mi>
  <mml:mi>P</mml:mi>
</mml:math>, and <mml:math id="mfe7ra63ww">
  <mml:mi>F</mml:mi>
  <mml:mi>N</mml:mi>
</mml:math> representing true positive, true negative, false positive, and false negative, respectively.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="392"><p>2</p></td><td colspan="1" rowspan="1"><p>F1-score</p></td><td colspan="1" rowspan="1"><p><mml:math id="mcu9lt84fk">
  <mml:mfrac>
    <mml:mrow>
      <mml:mn>2</mml:mn>
      <mml:mo>×</mml:mo>
      <mml:mo>×</mml:mo>
      <mml:mtext> Precision </mml:mtext>
      <mml:mtext> Recall </mml:mtext>
    </mml:mrow>
    <mml:mrow>
      <mml:mtext> Precision </mml:mtext>
      <mml:mtext> Recall </mml:mtext>
      <mml:mo>+</mml:mo>
    </mml:mrow>
  </mml:mfrac>
</mml:math></p></td><td colspan="1" rowspan="1"><p>It balances precision and recall.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="392"><p>3</p></td><td colspan="1" rowspan="1"><p>Precision</p></td><td colspan="1" rowspan="1"><p><mml:math id="mdp4d2sgr6">
  <mml:mfrac>
    <mml:mrow>
      <mml:mi data-mjx-auto-op="false">TP</mml:mi>
    </mml:mrow>
    <mml:mrow>
      <mml:mrow>
        <mml:mi data-mjx-auto-op="false">TP</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mi data-mjx-auto-op="false">FP</mml:mi>
      </mml:mrow>
      <mml:mo>+</mml:mo>
    </mml:mrow>
  </mml:mfrac>
</mml:math></p></td><td colspan="1" rowspan="1"><p>It evaluates the proportion of true positives among predicted positives.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="392"><p>4</p></td><td colspan="1" rowspan="1"><p>Recall</p></td><td colspan="1" rowspan="1"><p><mml:math id="masu62gqgs">
  <mml:mfrac>
    <mml:mrow>
      <mml:mi data-mjx-auto-op="false">TP</mml:mi>
    </mml:mrow>
    <mml:mrow>
      <mml:mrow>
        <mml:mi data-mjx-auto-op="false">TP</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mi data-mjx-auto-op="false">FN</mml:mi>
      </mml:mrow>
      <mml:mo>+</mml:mo>
    </mml:mrow>
  </mml:mfrac>
</mml:math></p></td><td colspan="1" rowspan="1"><p>It assesses the model's ability to find all actual positives.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="392"><p>5</p></td><td colspan="1" rowspan="1"><p>Loss function</p></td><td colspan="1" rowspan="1"><p><mml:math id="m6ieo1jtq0">
  <mml:mi>L</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mo>=</mml:mo>
  <mml:mo>∑</mml:mo>
  <mml:mo>(</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mfrac>
    <mml:mn>1</mml:mn>
    <mml:mi>N</mml:mi>
  </mml:mfrac>
  <mml:mrow>
    <mml:mover>
      <mml:mrow>
        <mml:mi>y</mml:mi>
      </mml:mrow>
      <mml:mo>^</mml:mo>
    </mml:mover>
  </mml:mrow>
  <mml:msup>
    <mml:mo>)</mml:mo>
    <mml:mn>2</mml:mn>
  </mml:msup>
</mml:math></p></td><td colspan="1" rowspan="1"><p>It quantifies the model's prediction error during the optimization training process. In the equation, $y<mml:math id="m8m7v4ofbf">
  <mml:mi>r</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>h</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>u</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>v</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>u</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>f</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>h</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mo>,</mml:mo>
</mml:math>\hat{y}<mml:math id="mjm11cakni">
  <mml:mi>r</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>h</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>v</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>u</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>f</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>h</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mo>,</mml:mo>
</mml:math>N$ represents the number of samples.</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Results and discussion</title>
      <p>This section presents the results obtained by evaluating the three models on the MNIST dataset. After building the models using the three algorithms, training them and performing evaluation metrics on them, it was deduced that CHAID, FFDNNs, and GANs can achieve accuracies of 29.61%, 97%, and 97%, respectively, on the MNIST dataset. This means that only one out of five digits was correctly classified using CHAID, which is very low performance compared to GANs and FFDNNs that can achieve over 90% accuracy on the same dataset. This is attributed to the fact that:</p><p>·CHAID is not suited for numerical or continuous data, such as pixel values. It can only handle categorical variables with a restricted number of levels. Therefore, it may lose a lot of information and resolution while transforming the pixel values into categories.</p><p>·CHAID is not able to capture the spatial connections and patterns among the pixels. It considers each pixel as an independent variable, disregarding the reality that surrounding pixels are closely connected and generate important forms and features. Therefore, it may fail to identify the distinctions and similarities among the digits.</p><p>To build this type of model, a higher GPU is required due to the number of epochs that GANs and FFDNNs need to run. It was also deduced that CHAID is more useful in deciding the best algorithms that suit a model. The table below shows the performance evaluation metrics on the models.</p>
      
        <sec>
          
            <title>4.1. Comparative analysis of the three models</title>
          
          <p>The results of the performance evaluation on the models are detailed in <xref ref-type="table" rid="table_4">Table 4</xref>, which assesses four performance evaluation metrics rating the attributes of these approaches. <xref ref-type="fig" rid="fig_4">Figure 4</xref> shows the comparative analysis of the CHAID, FFDNN, and GAN models.</p>
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Performance evaluation metrics for the three models</title>
              </caption>
              <table><tr><th >S / N</th><th >Algorithm</th><th >Loss Function</th><th >Validation Accuracy</th><th >F1-Score</th><th >Precision</th><th >Recall</th></tr><tr><td >1</td><td >CHAID</td><td >0.057</td><td >0.2961</td><td >0.2490</td><td >0.2178</td><td >0.2909</td></tr><tr><td >2</td><td >FFDNN</td><td >0.0901</td><td >0.9774</td><td >0.9772</td><td >0.9775</td><td >0.9771</td></tr><tr><td >3</td><td >GAN</td><td >0.056</td><td >0.9774</td><td >0.9775</td><td >0.9771</td><td >0.9772</td></tr></table>
            </table-wrap>
          
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Comparative analysis of the CHAID, FFDNN, and GAN models across metrics</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/8/img_pnca9AJkWaMkIVDl.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.2. Performance of the three models and visualization</title>
          
          <p>This section presents a visual analysis of the training performance of the models implemented in this study, focusing on two key metrics: accuracy and loss, both plotted against the number of training epochs. These visualizations help to interpret how well each model learned from the MNIST dataset over time and provide insight into convergence behavior and overall model stability.</p>
          
            <sec>
              
                <title>4.2.1 Chaid entropy and class features</title>
              
              <p> <xref ref-type="fig" rid="fig_5">Figure 5</xref> illustrates the CHAID implementation by visualizing how feature thresholds split the dataset across several layers to classify samples. Each split reduces uncertainty (entropy) and moves toward more confident class predictions, enabling clearer insight into feature importance and classification structure. CHAID is a statistical approach aimed at uncovering correlations between variables. At its foundation, CHAID produces a prediction model, sometimes depicted as a tree. The CHAID (Chi-square Automatic Interaction Detector) analysis is a technique for detecting correlations between categorical responses.</p>
              
                <fig id="fig_5">
                  <label>Figure 5</label>
                  <caption>
                    <title>CHAID implementation showing the visualization of insights from the model</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/8/img_kYaPAuLxXCP0CXay.png"/>
                </fig>
              
              <p> <xref ref-type="fig" rid="fig_5">Figure 5</xref> shows a DT generated by the CHAID model, used to classify handwritten digits from the MNIST dataset. Each node represents a decision based on a specific feature (pixel value), with splits determined by threshold conditions. The entropy value at each node measures the level of uncertainty or impurity. Lower entropy indicates a more confident classification. The samples show how many data points have reached that node, while the value array suggests the distribution of digits (0 to 9) among those samples. The class shown is the predicted digit at that node, based on the highest count in the value array. While the tree structure makes the model easy to interpret, the wide value distributions and relatively high entropy at several nodes reflect its struggle to make accurate decisions on complex image data.</p>
            </sec>
          
          
            <sec>
              
                <title>4.2.2 Model accuracy versus epochs</title>
              
              <p>The accuracy curve in <xref ref-type="fig" rid="fig_6">Figure 6</xref> and <xref ref-type="fig" rid="fig_7">Figure 7</xref> demonstrates the progressive improvement in classification performance for both the FFDNN and GAN models during training. At the outset, both models showed a rapid increase in accuracy within the first few epochs, indicating their ability to quickly learn and extract essential features from the handwritten digit images. This early progress reflects how effectively the models adapted to the underlying structure of the dataset. As training continued, the rate of improvement began to slow, and the accuracy values gradually stabilized around 97%. This plateau suggests that the models reached a point of convergence, where additional training yielded minimal performance gain. The consistent accuracy levels toward the final epochs also imply that both models were able to generalize well to unseen data without significant overfitting. However, in <xref ref-type="fig" rid="fig_8">Figure 8</xref>, there is a total divergence in the results of both training accuracy and validation accuracy for CHAID, largely because of the non-numerical nature of the dataset.</p>
              
                <fig id="fig_6">
                  <label>Figure 6</label>
                  <caption>
                    <title>Model accuracy vs. epoch for the FFDNN model</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/8/img_p4LMncOrckDiCi1S.png"/>
                </fig>
              
              
                <fig id="fig_7">
                  <label>Figure 7</label>
                  <caption>
                    <title>Model accuracy vs. epoch for the GAN model</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/8/img_ogwmcOJRkFfL460q.png"/>
                </fig>
              
              
                <fig id="fig_8">
                  <label>Figure 8</label>
                  <caption>
                    <title>Model accuracy vs. epoch for the CHAID model</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/8/img_pU_aGaUCxJYCculp.png"/>
                </fig>
              
              <p><xref ref-type="table" rid="table_5">Table 5</xref>, <xref ref-type="table" rid="table_6">Table 6</xref> and <xref ref-type="table" rid="table_7">Table 7</xref> highlight the training behavior of the FFDNN, GAN, and CHAID models across 10 epochs, focusing on both accuracy and loss for the training and validation sets. From the tables, it is clear that both FFDNNs and GANs demonstrated strong learning performance. The FFDNN model showed a steady improvement in training accuracy, which surpassed 99% by the final epoch, alongside a consistent drop in training loss. Validation accuracy also remained stable around 97%, suggesting that the model was able to generalize well to unseen data despite a slight increase in validation loss after the fourth epoch. Similarly, the GAN model experienced a rapid rise in training accuracy within the first few epochs and maintained low loss values in both training and validation, reflecting efficient learning and early convergence.</p>
              
                <table-wrap id="table_5">
                  <label>Table 5</label>
                  <caption>
                    <title>Epoch-wise accuracy and loss for the FFDNN model</title>
                  </caption>
                  <table><tbody><tr><th colspan="1" rowspan="1"><p>Epochs</p></th><th colspan="1" rowspan="1"><p>Train Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Validation Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Train Loss</p></th><th colspan="1" rowspan="1"><p>Validation Loss</p></th></tr><tr><td colspan="1" rowspan="1"><p>0</p></td><td colspan="1" rowspan="1"><p>93.3</p></td><td colspan="1" rowspan="1"><p>94.0</p></td><td colspan="1" rowspan="1"><p>0.23</p></td><td colspan="1" rowspan="1"><p>0.19</p></td></tr><tr><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>96.0</p></td><td colspan="1" rowspan="1"><p>95.8</p></td><td colspan="1" rowspan="1"><p>0.13</p></td><td colspan="1" rowspan="1"><p>0.16</p></td></tr><tr><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>97.1</p></td><td colspan="1" rowspan="1"><p>96.5</p></td><td colspan="1" rowspan="1"><p>0.08</p></td><td colspan="1" rowspan="1"><p>0.15</p></td></tr><tr><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p>97.8</p></td><td colspan="1" rowspan="1"><p>96.7</p></td><td colspan="1" rowspan="1"><p>0.05</p></td><td colspan="1" rowspan="1"><p>0.15</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>98.3</p></td><td colspan="1" rowspan="1"><p>96.9</p></td><td colspan="1" rowspan="1"><p>0.04</p></td><td colspan="1" rowspan="1"><p>0.16</p></td></tr><tr><td colspan="1" rowspan="1"><p>5</p></td><td colspan="1" rowspan="1"><p>98.6</p></td><td colspan="1" rowspan="1"><p>97.0</p></td><td colspan="1" rowspan="1"><p>0.03</p></td><td colspan="1" rowspan="1"><p>0.17</p></td></tr><tr><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p>98.8</p></td><td colspan="1" rowspan="1"><p>97.0</p></td><td colspan="1" rowspan="1"><p>0.02</p></td><td colspan="1" rowspan="1"><p>0.18</p></td></tr><tr><td colspan="1" rowspan="1"><p>7</p></td><td colspan="1" rowspan="1"><p>99.0</p></td><td colspan="1" rowspan="1"><p>96.8</p></td><td colspan="1" rowspan="1"><p>0.02</p></td><td colspan="1" rowspan="1"><p>0.19</p></td></tr><tr><td colspan="1" rowspan="1"><p>8</p></td><td colspan="1" rowspan="1"><p>99.1</p></td><td colspan="1" rowspan="1"><p>96.9</p></td><td colspan="1" rowspan="1"><p>0.01</p></td><td colspan="1" rowspan="1"><p>0.19</p></td></tr><tr><td colspan="1" rowspan="1"><p>9</p></td><td colspan="1" rowspan="1"><p>99.22</p></td><td colspan="1" rowspan="1"><p>97.4</p></td><td colspan="1" rowspan="1"><p>0.01</p></td><td colspan="1" rowspan="1"><p>0.20</p></td></tr></tbody></table>
                </table-wrap>
              
              
                <table-wrap id="table_6">
                  <label>Table 6</label>
                  <caption>
                    <title>Epoch-wise accuracy and loss for the GAN model</title>
                  </caption>
                  <table><tbody><tr><th colspan="1" rowspan="1"><p>Epochs</p></th><th colspan="1" rowspan="1"><p>Train Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Validation Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Train Loss</p></th><th colspan="1" rowspan="1"><p>Validation Loss</p></th></tr><tr><td colspan="1" rowspan="1"><p>0</p></td><td colspan="1" rowspan="1"><p>92.0</p></td><td colspan="1" rowspan="1"><p>91.5</p></td><td colspan="1" rowspan="1"><p>0.41</p></td><td colspan="1" rowspan="1"><p>0.38</p></td></tr><tr><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>94.6</p></td><td colspan="1" rowspan="1"><p>94.0</p></td><td colspan="1" rowspan="1"><p>0.22</p></td><td colspan="1" rowspan="1"><p>0.25</p></td></tr><tr><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>95.8</p></td><td colspan="1" rowspan="1"><p>95.2</p></td><td colspan="1" rowspan="1"><p>0.14</p></td><td colspan="1" rowspan="1"><p>0.17</p></td></tr><tr><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p>96.5</p></td><td colspan="1" rowspan="1"><p>96.0</p></td><td colspan="1" rowspan="1"><p>0.09</p></td><td colspan="1" rowspan="1"><p>0.12</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>96.9</p></td><td colspan="1" rowspan="1"><p>96.5</p></td><td colspan="1" rowspan="1"><p>0.06</p></td><td colspan="1" rowspan="1"><p>0.08</p></td></tr><tr><td colspan="1" rowspan="1"><p>5</p></td><td colspan="1" rowspan="1"><p>97.2</p></td><td colspan="1" rowspan="1"><p>96.7</p></td><td colspan="1" rowspan="1"><p>0.05</p></td><td colspan="1" rowspan="1"><p>0.07</p></td></tr><tr><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p>97.4</p></td><td colspan="1" rowspan="1"><p>96.9</p></td><td colspan="1" rowspan="1"><p>0.04</p></td><td colspan="1" rowspan="1"><p>0.07</p></td></tr><tr><td colspan="1" rowspan="1"><p>7</p></td><td colspan="1" rowspan="1"><p>97.6</p></td><td colspan="1" rowspan="1"><p>97.0</p></td><td colspan="1" rowspan="1"><p>0.03</p></td><td colspan="1" rowspan="1"><p>0.06</p></td></tr><tr><td colspan="1" rowspan="1"><p>8</p></td><td colspan="1" rowspan="1"><p>97.7</p></td><td colspan="1" rowspan="1"><p>97.1</p></td><td colspan="1" rowspan="1"><p>0.02</p></td><td colspan="1" rowspan="1"><p>0.05</p></td></tr><tr><td colspan="1" rowspan="1"><p>9</p></td><td colspan="1" rowspan="1"><p>97.8</p></td><td colspan="1" rowspan="1"><p>97.1</p></td><td colspan="1" rowspan="1"><p>0.02</p></td><td colspan="1" rowspan="1"><p>0.05</p></td></tr></tbody></table>
                </table-wrap>
              
              
                <table-wrap id="table_7">
                  <label>Table 7</label>
                  <caption>
                    <title>Epoch-wise accuracy and loss for the CHAID model</title>
                  </caption>
                  <table><tbody><tr><th colspan="1" rowspan="1"><p>Split Level/Iteration</p></th><th colspan="1" rowspan="1"><p>Train Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Validation Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Train Error (%)</p></th><th colspan="1" rowspan="1"><p>Validation Error (%)</p></th></tr><tr><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>25.0</p></td><td colspan="1" rowspan="1"><p>24.0</p></td><td colspan="1" rowspan="1"><p>75.0</p></td><td colspan="1" rowspan="1"><p>76.0</p></td></tr><tr><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>26.0</p></td><td colspan="1" rowspan="1"><p>25.0</p></td><td colspan="1" rowspan="1"><p>74.0</p></td><td colspan="1" rowspan="1"><p>75.0</p></td></tr><tr><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p>26.8</p></td><td colspan="1" rowspan="1"><p>25.5</p></td><td colspan="1" rowspan="1"><p>73.2</p></td><td colspan="1" rowspan="1"><p>74.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>27.5</p></td><td colspan="1" rowspan="1"><p>26.2</p></td><td colspan="1" rowspan="1"><p>72.5</p></td><td colspan="1" rowspan="1"><p>73.8</p></td></tr><tr><td colspan="1" rowspan="1"><p>5</p></td><td colspan="1" rowspan="1"><p>28.0</p></td><td colspan="1" rowspan="1"><p>26.8</p></td><td colspan="1" rowspan="1"><p>72.0</p></td><td colspan="1" rowspan="1"><p>73.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p>28.5</p></td><td colspan="1" rowspan="1"><p>27.3</p></td><td colspan="1" rowspan="1"><p>71.5</p></td><td colspan="1" rowspan="1"><p>72.7</p></td></tr><tr><td colspan="1" rowspan="1"><p>7</p></td><td colspan="1" rowspan="1"><p>29.0</p></td><td colspan="1" rowspan="1"><p>28.0</p></td><td colspan="1" rowspan="1"><p>71.0</p></td><td colspan="1" rowspan="1"><p>72.0</p></td></tr><tr><td colspan="1" rowspan="1"><p>8</p></td><td colspan="1" rowspan="1"><p>29.2</p></td><td colspan="1" rowspan="1"><p>28.5</p></td><td colspan="1" rowspan="1"><p>70.8</p></td><td colspan="1" rowspan="1"><p>71.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>9</p></td><td colspan="1" rowspan="1"><p>29.4</p></td><td colspan="1" rowspan="1"><p>29.0</p></td><td colspan="1" rowspan="1"><p>70.6</p></td><td colspan="1" rowspan="1"><p>71.0</p></td></tr><tr><td colspan="1" rowspan="1"><p>10</p></td><td colspan="1" rowspan="1"><p>29.6</p></td><td colspan="1" rowspan="1"><p>29.61</p></td><td colspan="1" rowspan="1"><p>70.4</p></td><td colspan="1" rowspan="1"><p>70.39</p></td></tr></tbody></table>
                </table-wrap>
              
              <p>The FFDNN model, as shown in <xref ref-type="table" rid="table_5">Table 5</xref>, achieved a training accuracy of 99.22% and a validation accuracy of 97.1% at the 9<inline-formula>
  <mml:math id="ml7j3hzu3e">
    <mml:msup>
      <mml:mi/>
      <mml:mrow>
        <mml:mtext>th</mml:mtext>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> epoch, with a gap of approximately 2.12%. This indicates a slight tendency toward overfitting. To mitigate this, Dropout (rate = 0.5) and L2 regularization were employed during training, which helped reduce variance between training and validation accuracy. The remaining overfitting may be attributed to the model’s relatively high complexity compared to the dataset size. Future improvements could involve the use of early stopping, learning rate scheduling, or data augmentation to further enhance generalization and stability across epochs.</p><p>Since CHAID does not train in epochs like neural networks, its performance is best represented as a single final result. However, for the purpose of comparison with FFDNNs and GANs, the results can be presented in a table format that mimics iterative evaluation. A common approach is to use tree depth or successive splitting iterations in place of epochs, showing how accuracy evolves as the tree grows. <xref ref-type="table" rid="table_7">Table 7</xref> provides a clear redesign based on this approach.</p>
            </sec>
          
          
            <sec>
              
                <title>4.2.3 Model loss versus epoch</title>
              
              <p>The loss curves in <xref ref-type="fig" rid="fig_9">Figure 9</xref> and <xref ref-type="fig" rid="fig_10">Figure 10</xref> illustrate how the FFDNN and GAN models progressively improved during training. Both models showed a consistent decline in training and validation loss over the epochs, indicating effective learning. The GAN model, in particular, experienced a sharp loss reduction in the initial epochs, suggesting rapid early convergence and strong initial learning on the MNIST dataset. FFDNNs exhibited a more gradual but steady optimization process, eventually achieving low and stable loss values, thereby demonstrating successful convergence. In contrast, the CHAID model shown in <xref ref-type="fig" rid="fig_11">Figure 11</xref> exhibited minimal change across training iterations. Training accuracy plateaued around 75.6%, while validation accuracy remained nearly flat at approximately 74.2%. Loss values also stayed constant, which is consistent with CHAID’s rule-based, non-iterative nature. Since CHAID does not adjust its parameters through backpropagation or gradient descent, it lacks the ability to improve performance over time, especially when applied to pixel-level image data. Although CHAID is valued for its interpretability, it falls short in this context compared to deep learning models like FFDNNs and GANs.</p>
              
                <fig id="fig_9">
                  <label>Figure 9</label>
                  <caption>
                    <title>Model loss vs. epoch for the FFDNN model</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/8/img_n4vu-IVGmF6v1Ehz.png"/>
                </fig>
              
              
                <fig id="fig_10">
                  <label>Figure 10</label>
                  <caption>
                    <title>Model loss vs. epoch for GAN model</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/8/img_nHMxUVf94N_yw4B0.png"/>
                </fig>
              
              
                <fig id="fig_11">
                  <label>Figure 11</label>
                  <caption>
                    <title>Model loss vs. epoch for the CHAID model</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/8/img_-VU2-hY_ION_qQrz.png"/>
                </fig>
              
              <p>Finally, cost metrics such as log loss are crucial for evaluating model reliability. Ideally, these values should trend toward zero, indicating greater confidence and precision in predictions. Both the FFDNN and GAN models demonstrated this trend, reinforcing their suitability for complex classification tasks like MNIST. </p>
            </sec>
          
          
            <sec>
              
                <title>4.2.4 Computational efficiency analysis</title>
              
              <p>In addition to accuracy and loss, the computational efficiency of the models was evaluated. On the Google Colab GPU environment, the GAN model completed 10 epochs in approximately 7 minutes (average approximately 0.7 minutes per epoch), while the FFDNN model required about 10 minutes (average approximately 1 minute per epoch). The faster training time of GANs can be attributed to their comparatively lower parameter size and more efficient gradient updates. CHAID, being a tree-based algorithm, had negligible training time (less than 1 minute) but lacked scalability to handle pixel-based image data effectively. These quantitative results confirm that GANs not only converged faster but also demonstrated better computational efficiency than FFDNNs, supporting the observations discussed earlier.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>4.3. Comparison of the proposed models with similar works</title>
          
          <p><xref ref-type="table" rid="table_8">Table 8</xref> compares the models with other related works that used the MNIST dataset. While the FFDNN and GAN models achieved strong accuracy around 97%, similar studies like LeNet-5 and CNN ensembles reported higher accuracy, even up to 99.9% in some cases. Other works using methods like SVM, RF, and MLP also performed well, especially when extra preprocessing or data augmentation was used. The CHAID model in this study didn’t perform as well, but it was still useful for showing how simpler, interpretable models compare to more complex deep learning approaches. Feedforward neural networks have a unidirectional data flow from input to output via stacked layers (input, hidden, and output), which is designed for simplicity and efficiency. They analyze input using weighted sums and non-linear activation functions in each neuron to learn complicated patterns without feedback loops or memory components, making them adaptable for tasks like classification and regression. The information usually flows in one direction in FNNN from the input layer through the hidden layers. There is no feedback loops from the transmitting signals.</p>
          
            <table-wrap id="table_8">
              <label>Table 8</label>
              <caption>
                <title>Comparison of the proposed models with related MNIST classification works</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Performance Metrics</p></th><th colspan="1" rowspan="1"><p>Proposed Models (FFDNNs/GANs/CHAID)</p></th><th colspan="1" rowspan="1"><p>Medium-Scale MLP (Neural Network vs. DT and RF) [<xref ref-type="bibr" rid="ref_26">26</xref>]</p></th><th colspan="1" rowspan="1"><p>RF vs. DT [<xref ref-type="bibr" rid="ref_27">27</xref>]</p></th></tr><tr><td colspan="1" rowspan="1"><p>Training accuracy</p></td><td colspan="1" rowspan="1"><p>FFDNNs: 99.22% GANs: 97.8% CHAID: 75.6%</p></td><td colspan="1" rowspan="1"><p>93.7%</p></td><td colspan="1" rowspan="1"><p>DT: 87.16% RF: 96.78%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Validation accuracy</p></td><td colspan="1" rowspan="1"><p>FFDNNsamp; GANs: 97.7%, CHAID: 29.61%</p></td><td colspan="1" rowspan="1"><p>SVM: 97.7% RF: 96.7% (others from 85% to 97.7%)</p></td><td colspan="1" rowspan="1"><p>Similar to training: RF: 96.7% DT: 87.2%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Training epochs and iterations</p></td><td colspan="1" rowspan="1"><p>Fixed at 10 epochs</p></td><td colspan="1" rowspan="1"><p>Up to 1,000 iterations reported</p></td><td colspan="1" rowspan="1"><p>Not specified</p></td></tr><tr><td colspan="1" rowspan="1"><p>Data preprocessing techniques</p></td><td colspan="1" rowspan="1"><p>Normalization, flattening, and one-hot encoding</p></td><td colspan="1" rowspan="1"><p>Normalization, no mention of deep image processing</p></td><td colspan="1" rowspan="1"><p>Standard feature scaling/encoding</p></td></tr><tr><td colspan="1" rowspan="1"><p>Core strength</p></td><td colspan="1" rowspan="1"><p>Balanced: high accuracy and explanatory ability of CHAID included</p></td><td colspan="1" rowspan="1"><p>Demonstrates the performance of classical machine learning vs. neural networks</p></td><td colspan="1" rowspan="1"><p>Highlight RF’s superiority over DT in structured input</p></td></tr><tr><td colspan="1" rowspan="1"><p>Limitation</p></td><td colspan="1" rowspan="1"><p>CHAID is weak on pixelbased patterns</p></td><td colspan="1" rowspan="1"><p>Deep neural network accuracy is still lower than CNN-based methods</p></td><td colspan="1" rowspan="1"><p>Tree-based models are not suited for high-dimensional image data</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>4.4. Limitations of the study</title>
          
          <p>Although the proposed multi-model framework demonstrates strong performance on the MNIST dataset, several limitations may affect the generalizability and robustness of the results. First, MNIST contains only numeric digits, which restricts the models’ ability to generalize to non-numeric handwriting; employing a more diversified dataset, such as EMNIST, could provide a better evaluation of generalization to letters and symbols. Second, the models were trained and tested on relatively clean images, leaving their robustness to noisy, distorted, or inconsistent handwriting unexamined. Finally, the models’ performance in small-sample scenarios or with imbalanced datasets has not been evaluated, which may impact their applicability in real-world situations with limited labeled data. Addressing these limitations in future work could improve the reliability, robustness, and broader applicability of the proposed framework.</p>
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>This study explored the use of AI techniques for handwritten digit classification on the MNIST dataset by implementing and comparing three models: GANs, FFDNNs, and CHAID. The results clearly show that deep learning models, particularly GANs and FFDNNs, significantly outperformed CHAID, with both achieving classification accuracies of around 97%. While GANs showed faster convergence during training, FFDNNs maintained a more stable and gradual learning process. CHAID, although easy to interpret, struggled with image data due to its inability to handle continuous pixel features and spatial dependencies. The analysis of accuracy and loss trends across training epochs further confirmed the learning behaviors and generalization capabilities of the models. These findings highlight the advantages of deep learning for pattern recognition tasks and offer practical insights for researchers working on real-world classification problems. The study also emphasizes the importance of proper data preparation and model selection in achieving reliable outcomes.</p><p>Future research should investigate hybrid strategies that combine the strengths of statistical and neural methods. CHAID may serve as a feature selection module before training FFDNNs to reduce dimensionality and improve interpretability. Synthetic data generated by GANs could be employed to augment CHAID’s training samples, thereby enhancing accuracy on high-dimensional image inputs. Ensemble approaches that integrate the predictive power of FFDNNs with CHAID’s rule-based decision paths also represent a promising direction, enabling both strong performance and transparency. Such methods hold potential for bridging the gap between accuracy and interpretability, thereby advancing the adoption of AI in sensitive real-world applications. In addition, future work could consider expanding to more diverse datasets, optimizing model architectures, or combining statistical and neural methods to improve both accuracy and interpretability. Multi-Criteria Decision Making (MCDM) could be used for optimal model selection [<xref ref-type="bibr" rid="ref_28">28</xref>] using software engineering paradigms [<xref ref-type="bibr" rid="ref_29">29</xref>], [<xref ref-type="bibr" rid="ref_30">30</xref>] with disruptive technology considerations [<xref ref-type="bibr" rid="ref_29">29</xref>] while not neglecting privacy issues [<xref ref-type="bibr" rid="ref_31">31</xref>] in the digital world.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="conf-paper">
          <volume>3307</volume>
          <page-range>1526-1530</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yu</surname>
              <given-names>S. Y.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>J. M.</given-names>
            </name>
            <name>
              <surname>Shu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Cheng</surname>
              <given-names>Z. W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TOCS56154.2022.10016012</pub-id>
          <article-title>Handwritten digit recognition using deep learning networks</article-title>
          <source>2022 IEEE Conference on Telecommunications, Optics and Computer Science (TOCS)</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="conference-proceedings">
          <volume>2031</volume>
          <page-range>148-163</page-range>
          <year>2024</year>
          <publisher-name>Springer</publisher-name>
          <person-group person-group-type="author">
            <name>
              <surname>Akinsola</surname>
              <given-names>J. E. T.</given-names>
            </name>
            <name>
              <surname>Onipede</surname>
              <given-names>F. O.</given-names>
            </name>
            <name>
              <surname>Olajubu</surname>
              <given-names>E. A.</given-names>
            </name>
            <name>
              <surname>Aderounmu</surname>
              <given-names>G. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-031-53728-8_12</pub-id>
          <article-title>Machine learning enabled image classification using k-nearest neighbour and learning vector quantization</article-title>
          <source>, undefined</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-7</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Akinsola</surname>
              <given-names>J. E. T.</given-names>
            </name>
            <name>
              <surname>Efiong</surname>
              <given-names>J. E.</given-names>
            </name>
            <name>
              <surname>Olajubu</surname>
              <given-names>E. A.</given-names>
            </name>
            <name>
              <surname>Aderounmu</surname>
              <given-names>G. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICECET58911.2023.10389469</pub-id>
          <article-title>Artificial intelligence-based model for data security and mitigation against SQL injection attacks in web applications</article-title>
          <source>International Conference on Electrical, Computer and Energy Technologies (ICECET), Cape Town, South Africa</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Akinsola</surname>
              <given-names>J. E. T.</given-names>
            </name>
            <name>
              <surname>Onipede</surname>
              <given-names>F. O.</given-names>
            </name>
            <name>
              <surname>Osonuga</surname>
              <given-names>S. O.</given-names>
            </name>
            <name>
              <surname>Abdul-Yakeen</surname>
              <given-names>S. O.</given-names>
            </name>
            <name>
              <surname>Olopade</surname>
              <given-names>R. O.</given-names>
            </name>
            <name>
              <surname>Eyitayo</surname>
              <given-names>A. O.</given-names>
            </name>
            <name>
              <surname>Badmus</surname>
              <given-names>H. A.</given-names>
            </name>
          </person-group>
          <article-title>Network vulnerability analysis for Internet of Things (IoT)-based cyber physical systems (CPS) using digital forensics</article-title>
          <source>Internet of Things and Cyber Physical Systems</source>
          <publisher-name>CRC Press</publisher-name>
          <year>2022</year>
          <page-range>2022</page-range>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1011-1015</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Akinsola</surname>
              <given-names>J. E. T.</given-names>
            </name>
            <name>
              <surname>Olajubu</surname>
              <given-names>E. A.</given-names>
            </name>
            <name>
              <surname>Aderounmu</surname>
              <given-names>G. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CSCI58124.2022.00179</pub-id>
          <article-title>Development of threat hunting model using machine learning algorithms for cyber-attacks mitigation</article-title>
          <source>2022 International Conference on Computational Science and Computational Intelligence (CSCI), Las Vegas, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>e24506</page-range>
          <issue>3</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Olawoye</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Fagbohun</surname>
              <given-names>O. F.</given-names>
            </name>
            <name>
              <surname>Popoola-Akinola</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Akinsola</surname>
              <given-names>J. E. T.</given-names>
            </name>
            <name>
              <surname>Akanbi</surname>
              <given-names>C. T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.heliyon.2024.e24506</pub-id>
          <article-title>A supervised machine learning approach for the prediction of antioxidant activities of Amaranthus viridis seed</article-title>
          <source>Heliyon</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>1320-1330</page-range>
          <issue>5</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Efiong</surname>
              <given-names>J. E.</given-names>
            </name>
            <name>
              <surname>Akinsola</surname>
              <given-names>J. E. T.</given-names>
            </name>
            <name>
              <surname>Akinyemi</surname>
              <given-names>B. O.</given-names>
            </name>
            <name>
              <surname>Olajubu</surname>
              <given-names>E. A.</given-names>
            </name>
            <name>
              <surname>Aderounmu</surname>
              <given-names>G. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.12928/TELKOMNIKA.v22i5.26000</pub-id>
          <article-title>A contrived dataset of substation automation for cybersecurity research in the smart grid networks based on IEC61850</article-title>
          <source>Telkomnika (Telecommun. Comput. Electron. Control)</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Awoseyi</surname>
              <given-names>A. A.</given-names>
            </name>
            <name>
              <surname>Akinsola</surname>
              <given-names>J. E. T.</given-names>
            </name>
            <name>
              <surname>Oladoja</surname>
              <given-names>O. M.</given-names>
            </name>
            <name>
              <surname>Adeagbo</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Adebowale</surname>
              <given-names>O. O.</given-names>
            </name>
          </person-group>
          <article-title>Hybridization of decision tree algorithm using sequencing predictive model for COVID-19</article-title>
          <source>Emerging Technologies for Combatting Pandemics</source>
          <publisher-name>Auerbach Publications</publisher-name>
          <year>2022</year>
          <page-range>201-221</page-range>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>3307</volume>
          <page-range>229-235</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kuyoro</surname>
              <given-names>S. O.</given-names>
            </name>
            <name>
              <surname>Eluwa</surname>
              <given-names>J. M.</given-names>
            </name>
            <name>
              <surname>Akinsola</surname>
              <given-names>J. E. T.</given-names>
            </name>
            <name>
              <surname>Ayankoya</surname>
              <given-names>F. Y.</given-names>
            </name>
            <name>
              <surname>Omotunde</surname>
              <given-names>A. A.</given-names>
            </name>
            <name>
              <surname>Adegbenjo</surname>
              <given-names>A. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.32628/CSEIT206547</pub-id>
          <article-title>Intelligent essay grading system using hybrid text processing techniques</article-title>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>102</volume>
          <page-range>2172-2180</page-range>
          <issue>5</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Serap</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Rabia</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.16984/saufenbilder.801684</pub-id>
          <article-title>Handwritten digit recognition using machine learning</article-title>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-6</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Saabni</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICAIPR.2016.7585206</pub-id>
          <article-title>Recognizing handwritten single digits and digit strings using deep architecture of neural networks</article-title>
          <source>3rd International Conference on Artificial Intelligence and Pattern Recognition (AIPR), Lodz, Poland</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <page-range>arXiv:1711.09846</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jaderberg</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Dalibard</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Osindero</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Czarnecki</surname>
              <given-names>W. M.</given-names>
            </name>
            <name>
              <surname>Donahue</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Razavi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Vinyals</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Green</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Dunning</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Simonyan</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Fernando</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Kavukcuoglu</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1711.09846</pub-id>
          <article-title>Population based training of neural networks</article-title>
          <source>arXiv preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>112</volume>
          <page-range>5135-5161</page-range>
          <issue>12</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>S. F.</given-names>
            </name>
            <name>
              <surname>Qian</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>K. Z.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>J. M.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>C. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10994-023-06367-0</pub-id>
          <article-title>Robust generative adversarial network</article-title>
          <source>Mach. Learn.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>107</volume>
          <page-range>61-71</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Qiao</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neunet.2018.02.010</pub-id>
          <article-title>An adaptive deep Q-learning strategy for handwritten digit recognition</article-title>
          <source>Neural Netw.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>51</volume>
          <page-range>2855-2870</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gupta</surname>
              <given-names>T. K.</given-names>
            </name>
            <name>
              <surname>Raza</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11063-020-10234-7</pub-id>
          <article-title>Optimizing deep neural network architecture: A Tabu search based approach</article-title>
          <source>Neural Process. Lett.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>411-418</page-range>
          <issue>3</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>H. H.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s41066-019-00158-6</pub-id>
          <article-title>Multiple classifiers fusion and CNN feature extraction for handwritten digits recognition</article-title>
          <source>Granular Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>F. Y.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Mao</surname>
              <given-names>H. Y.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>H. L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1811.08278</pub-id>
          <article-title>Assessing four neural networks on handwritten digit recognition dataset (MNIST)</article-title>
          <source>arXiv</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>2021</volume>
          <page-range>9946809</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>Y. Q.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>G. Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H. Q.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>S. L.</given-names>
            </name>
            <name>
              <surname>Yue</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2021/9946809</pub-id>
          <article-title>Infrared image deblurring based on generative adversarial networks</article-title>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="conf-paper">
          <page-range>247-251</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Roohi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Alizadehashrafi</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Persian handwritten character recognition using convolutional neural network</article-title>
          <source>10th Iranian Conference on Machine Vision and Image Processing (MVIP), Isfahan, Iran</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>275-282</page-range>
          <issue>3</issue>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Díaz-Pérez</surname>
              <given-names>F. M.</given-names>
            </name>
            <name>
              <surname>Bethencourt-Cejas</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.jdmm.2016.01.006</pub-id>
          <article-title>CHAID algorithm as an appropriate analytical method for tourism market segmentation</article-title>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Salimans</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Goodfellow</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Zaremba</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Cheung</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Radford</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>Improved techniques for training GANs</article-title>
          <source>Advances in Neural Information Processing Systems 29 (NIPS 2016)</source>
          <publisher-name>Curran Associates, Inc.</publisher-name>
          <year>2016</year>
          <page-range>2234-2242</page-range>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2009.00169</pub-id>
          <article-title>A mathematical introduction to generative adversarial nets (GAN)</article-title>
          <source>arXiv</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Pajankar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Joshi</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Feedforward neural networks</article-title>
          <source>Hands-on Machine Learning with Python: Implement Neural Network Solutions with Scikit-learn and PyTorch</source>
          <publisher-name>Springer</publisher-name>
          <year>2022</year>
          <page-range>227-260</page-range>
          <pub-id pub-id-type="doi">10.1007/978-1-4842-7921-2_13</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>86</volume>
          <page-range>2278-2324</page-range>
          <issue>11</issue>
          <year>1998</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lecun</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Bottou</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Bengio</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Haffner</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/5.726791</pub-id>
          <article-title>Gradient-based learning applied to document recognition</article-title>
          <source>Proc. IEEE</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>3207-3220</page-range>
          <issue>12</issue>
          <year>2010</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cireşan</surname>
              <given-names>D. C.</given-names>
            </name>
            <name>
              <surname>Meier</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Gambardella</surname>
              <given-names>L. M.</given-names>
            </name>
            <name>
              <surname>Schmidhuber</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1162/NECO_a_00052</pub-id>
          <article-title>Deep, big, simple neural nets for handwritten digit recognition</article-title>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="webpage">
          <article-title>Implementation and performance evaluation of standard multi-class classification algorithms using MNIST dataset</article-title>
          <source>, https://medium.com/@sayanrik1996_34278/implementation-and-performance-evaluation-of-standard-multi-class-classification-algorithms-using-8d2dc917143</source>
          <year>2023</year>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>79</volume>
          <page-range>182-199</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Pan</surname>
              <given-names>T. Y.</given-names>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.54254/2755-2721/79/20241622</pub-id>
          <article-title>Analysis of classification algorithms: Insights from MNIST and WDBC datasets</article-title>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="conf-paper">
          <page-range>17-34</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Akinsola</surname>
              <given-names>J. E. T.</given-names>
            </name>
            <name>
              <surname>Awodele</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Kuyoro</surname>
              <given-names>S. O.</given-names>
            </name>
            <name>
              <surname>Kasali</surname>
              <given-names>F. A.</given-names>
            </name>
          </person-group>
          <article-title>Performance evaluation of supervised machine learning algorithms using multi-criteria decision making techniques</article-title>
          <source>International Conference on Information Technology in Education and Development</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Akinsola</surname>
              <given-names>J. E. T.</given-names>
            </name>
            <name>
              <surname>Adeagbo</surname>
              <given-names>M. A.</given-names>
            </name>
            <name>
              <surname>Akinseinde</surname>
              <given-names>S. A.</given-names>
            </name>
            <name>
              <surname>Onipede</surname>
              <given-names>F. O.</given-names>
            </name>
            <name>
              <surname>Yusuf</surname>
              <given-names>A. A.</given-names>
            </name>
          </person-group>
          <article-title>Applications of Blockchain technology in cyber attacks prevention</article-title>
          <source>Sustainable and Advanced Applications of Blockchain in Smart Computational Technologies</source>
          <publisher-name>Chapman and Hall/CRC</publisher-name>
          <year>2022</year>
          <page-range>129-159</page-range>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <volume>20</volume>
          <page-range>17-23</page-range>
          <issue>1</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Akinsola</surname>
              <given-names>J. E. T.</given-names>
            </name>
            <name>
              <surname>Kuyoro</surname>
              <given-names>A. O.</given-names>
            </name>
            <name>
              <surname>A Adeagbo</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Awoseyi</surname>
              <given-names>A. A.</given-names>
            </name>
          </person-group>
          <article-title>Performance evaluation of software using formal methods</article-title>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>22-33</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Akinsola</surname>
              <given-names>J. E. T.</given-names>
            </name>
            <name>
              <surname>Awodele</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Adebayo</surname>
              <given-names>A. O.</given-names>
            </name>
            <name>
              <surname>Onipede</surname>
              <given-names>F. O.</given-names>
            </name>
            <name>
              <surname>Muhammad</surname>
              <given-names>B. A.</given-names>
            </name>
          </person-group>
          <article-title>Netiquette of cyberbullying and privacy issues</article-title>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>