<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IDA</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Information Dynamics and Applications</journal-title>
        <abbrev-journal-title abbrev-type="issn">Inf. Dyn. Appl.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IDA</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-1494</issn>
      <issn publication-format="print">2958-1486</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-IEf2LDmH8Q1SmxmkJzpS9vJvqBaZrLPr</article-id>
      <article-id pub-id-type="doi">10.56578/ida040201</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Crowd Density Estimation via a VGG-16-Based CSRNet Model</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-4934-222X</contrib-id>
          <name>
            <surname>TatlıCan</surname>
            <given-names>Damla</given-names>
          </name>
          <email>215509023@firat.edu.tr</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0006-3438-7401</contrib-id>
          <name>
            <surname>Apaydin</surname>
            <given-names>Nafiye Nur</given-names>
          </name>
          <email>241144110@firat.edu.tr</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9623-2284</contrib-id>
          <name>
            <surname>Yaman</surname>
            <given-names>Orhan</given-names>
          </name>
          <email>orhanyaman@firat.edu.tr</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3276-3788</contrib-id>
          <name>
            <surname>Karakose</surname>
            <given-names>Mehmet</given-names>
          </name>
          <email>mkarakose@firat.edu.tr</email>
        </contrib>
        <aff id="aff_1">Department of Digital Forensics Engineering, College of Technology, Firat University, 23119 Elazig, Turkey</aff>
        <aff id="aff_2">Department of Computer Engineering, Faculty of Engineering, Firat University, 23119 Elazig, Turkey</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>29</day>
        <month>04</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>2</issue>
      <fpage>66</fpage>
      <lpage>75</lpage>
      <page-range>66-75</page-range>
      <history>
        <date date-type="received">
          <day>12</day>
          <month>03</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>24</day>
          <month>04</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Accurate crowd density estimation has become critical in applications ranging from intelligent urban planning and public safety monitoring to marketing analytics and emergency response. In recent developments, various methods have been used to enhance the precision of crowd analysis systems. In this study, a Convolutional Neural Network (CNN)-based approach was presented for crowd density detection, wherein the Congested Scene Recognition Network (CSRNet) architecture was employed with a Visual Geometry Group (VGG)-16 backbone. This method was applied to two benchmark datasets—Mall and Crowd-UIT—to assess its effectiveness in real-world crowd scenarios. Density maps were generated to visualize spatial distributions, and performance was quantitatively evaluated using Mean Squared Error (MSE) and Mean Absolute Error (MAE) metrics. For the Mall dataset, the model achieved an MSE of 0.08 and an MAE of 0.10, while for the Crowd-UIT dataset, an MSE of 0.05 and an MAE of 0.15 were obtained. These results suggest that the proposed VGG-16-based CSRNet model yields high accuracy in crowd estimation tasks across varied environments and crowd densities. Additionally, the model demonstrates robustness in generalizing across different dataset characteristics, indicating its potential applicability in both surveillance systems and public space management. The outcomes of this investigation offer a promising direction for future research in data-driven crowd analysis, particularly in enhancing predictive reliability and real-time deployment capabilities of deep learning models for population monitoring tasks.</p></abstract>
      <kwd-group>
        <kwd>Crowd density estimation</kwd>
        <kwd>CSRNet</kwd>
        <kwd>VGG-16</kwd>
        <kwd>Mall dataset</kwd>
        <kwd>Crowd-UIT dataset</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="4"/>
        <fig-count count="9"/>
        <table-count count="3"/>
        <ref-count count="20"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Crowds are predominantly observed in public and private spaces where people interact, engage in leisure activities, travel, and exchange information, such as shopping malls, cinemas, concerts, sports halls, public transportation, traffic, schools, and hospitals. Crowd density varies depending on the number of people present. For instance, in rural areas, due to the limited space and lower population density, crowd intensity remains low. In contrast, in metropolitan cities, the combination of large urban areas and high population density results in significantly higher crowd concentrations. Crowd density detection offers multiple benefits. For example, in areas with high crowd density, early intervention can help prevent security threats, such as public disturbances or fights, by enabling timely precautions. In the event of natural disasters, it facilitates effective search and rescue operations. In traffic management, red light durations can be adjusted based on the presence of vehicles, optimizing traffic flow. In shopping malls, crowd density detection allows for the identification of frequently visited stores, enabling strategic placement of less-visited stores next to high-traffic ones to attract more customers and increase revenue. Additionally, analyzing customer behavior in highly visited stores helps determine which products are frequently purchased together, allowing for strategic shelf placement to enhance marketing strategies. Moreover, crowd density detection can be utilized to monitor gatherings and implement preventive measures to mitigate the spread of infectious diseases. Due to these advantages, crowd density detection remains a crucial research area, with various studies continuing to be conducted in the literature.</p><p>Despite the wide range of applications, crowd density detection still faces several challenges, particularly in scenarios involving high crowd density, severe congestion, and varying illumination or perspective conditions. To address these problems, various approaches have been proposed in the literature, ranging from traditional image processing techniques to modern deep learning-based methods.</p><p><xref ref-type="table" rid="table_1">Table 1</xref> provides a comparative overview of several studies concerning crowd density detection. The methods employed in these studies range from traditional machine learning algorithms, such as random forests and regression models, to deep learning architectures, including CNN-based and attention-based models. While classical models offer advantages in terms of simplicity and interpretability, they often fall short in effectively extracting features from densely populated images.</p><p>Conventional CNN-based models often fail to effectively extract features from highly congested scenes, resulting in poor performance in detecting individuals within dense crowds. Therefore, in this study, CSRNet, a CNN architecture specifically designed for crowd counting and density map estimation, was proposed. The proposed method integrates a VGG-16-based frontend with an extended convolutional backend, enabling CSRNet to capture multi-scale spatial features effectively without losing resolution. This allows for accurate localization and counting of individuals even in complex and densely crowded scenarios. The primary objective of this study is to develop a fast and accurate crowd density detection model capable of robustly estimating the number and positions of people in an image, particularly under challenging conditions. Another aim of this work is to contribute to the advancement of the existing CSRNet model in the literature. Moreover, due to the limited number of existing studies utilizing the Crowd-UIT dataset, this work is expected to serve as a valuable reference for future research efforts.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Some studies for crowd density detection</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1" colwidth="159"><p>References</p></td><td colspan="1" rowspan="1" colwidth="115"><p>Year</p></td><td colspan="1" rowspan="1"><p>Purpose of the Studies</p></td><td colspan="1" rowspan="1" colwidth="378"><p>Advantages</p></td><td colspan="1" rowspan="1"><p>Disadvantages</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="159"><p>Wei et al. [<xref ref-type="bibr" rid="ref_1">1</xref>]</p></td><td colspan="1" rowspan="1" colwidth="115"><p>2019</p></td><td colspan="1" rowspan="1"><p>In this study, a novel algorithm was proposed to address the counting of fast-moving crowds by utilizing deep cumulative feature learning, support vector regression, and spatiotemporal features.</p></td><td colspan="1" rowspan="1" colwidth="378"><p>Handles motion features and uses temporal information.</p></td><td colspan="1" rowspan="1"><p>Computationally intensive and less effective in static scenes.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="159"><p>Zhang et al. [<xref ref-type="bibr" rid="ref_2">2</xref>]</p></td><td colspan="1" rowspan="1" colwidth="115"><p>2015</p></td><td colspan="1" rowspan="1"><p>In this study, a label distribution learning method was introduced for crowd counting in public video surveillance.</p></td><td colspan="1" rowspan="1" colwidth="378"><p>Performs well under varying density levels.</p></td><td colspan="1" rowspan="1"><p>Limited performance with perspective distortion.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="159"><p>Chen et al. [<xref ref-type="bibr" rid="ref_3">3</xref>]</p></td><td colspan="1" rowspan="1" colwidth="115"><p>2012</p></td><td colspan="1" rowspan="1"><p>In this study, a multi-output regression model was developed for crowd counting in spatially localized regions.</p></td><td colspan="1" rowspan="1" colwidth="378"><p>Effective in structured and partitioned scenes.</p></td><td colspan="1" rowspan="1"><p>Less accurate in highly dense scenarios.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="159"><p>Chen et al. [<xref ref-type="bibr" rid="ref_4">4</xref>]</p></td><td colspan="1" rowspan="1" colwidth="115"><p>2013</p></td><td colspan="1" rowspan="1"><p>In this study, age and crowd density estimation were performed by transforming high-dimensional feature vectors into low-dimensional scalar values. To achieve this, a cumulative feature space was utilized to reduce data imbalance.</p></td><td colspan="1" rowspan="1" colwidth="378"><p>Handles high-dimensional data and reduces imbalance.</p></td><td colspan="1" rowspan="1"><p>Information loss during reduction and sensitivity to feature selection.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="159"><p>Pham et al. [<xref ref-type="bibr" rid="ref_5">5</xref>]</p></td><td colspan="1" rowspan="1" colwidth="115"><p>2015</p></td><td colspan="1" rowspan="1"><p>In this study, a random forest-based model was developed to estimate crowd density in scenes with dense human crowds.</p></td><td colspan="1" rowspan="1" colwidth="378"><p>Robust to noise and outliers and interpretable.</p></td><td colspan="1" rowspan="1"><p>May underperform in real-time applications; limited spatial modeling.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="159"><p>Tomar et al. [<xref ref-type="bibr" rid="ref_6">6</xref>]</p></td><td colspan="1" rowspan="1" colwidth="115"><p>2022</p></td><td colspan="1" rowspan="1"><p>In this study, a dynamic kernel-based CNN-Linear Regression (LR) model was proposed for human counting in crowd scenes. This model is specifically designed to address the overlapping issues in dense crowds.</p></td><td colspan="1" rowspan="1" colwidth="378"><p>Tackles overlap issues; dynamic kernel improves learning.</p></td><td colspan="1" rowspan="1"><p>Requires large datasets for training and complex parameter tuning.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="159"><p>Abdullah &amp;amp; Jalal [<xref ref-type="bibr" rid="ref_7">7</xref>]</p></td><td colspan="1" rowspan="1" colwidth="115"><p>2023</p></td><td colspan="1" rowspan="1"><p>In this study, a fuzzy classifier (neuro-fuzzy classifier) and a semantic segmentation-based method were developed for crowd tracking and anomaly detection in intelligent surveillance systems. The method aims to monitor behaviors in crowded areas and detect anomalies.</p></td><td colspan="1" rowspan="1" colwidth="378"><p>Interpretable decision-making; good for anomaly detection.</p></td><td colspan="1" rowspan="1"><p>Sensitive to noise; limited scalability in large scenes.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="159"><p>Tripathy &amp;amp; Srivastava [<xref ref-type="bibr" rid="ref_8">8</xref>]</p></td><td colspan="1" rowspan="1" colwidth="115"><p>2021</p></td><td colspan="1" rowspan="1"><p>In this study, an Attention-based Multi-Stream (AMSCNN)-CNN was developed for video-based crowd counting, considering both spatial and temporal features.</p></td><td colspan="1" rowspan="1" colwidth="378"><p>Captures both spatial and temporal features effectively.</p></td><td colspan="1" rowspan="1"><p>High computational cost; real-time deployment is challenging.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="159"><p>Xiong et al. [<xref ref-type="bibr" rid="ref_9">9</xref>]</p></td><td colspan="1" rowspan="1" colwidth="115"><p>2017</p></td><td colspan="1" rowspan="1"><p>In this study, a model was developed for crowd counting in videos that captures temporal and spatial information, enabling more accurate predictions.</p></td><td colspan="1" rowspan="1" colwidth="378"><p>Improves accuracy in dynamic scenes.</p></td><td colspan="1" rowspan="1"><p>Limited performance in still images; temporal data dependency.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="159"><p>Maktoof et al. [<xref ref-type="bibr" rid="ref_10">10</xref>]</p></td><td colspan="1" rowspan="1" colwidth="115"><p>2023</p></td><td colspan="1" rowspan="1"><p>In this study, different models from the You Only Look Once version 5 (YOLOv5) family were compared to detect human crowds more accurately.</p></td><td colspan="1" rowspan="1" colwidth="378"><p>High detection accuracy and real-time capability.</p></td><td colspan="1" rowspan="1"><p>Struggles in extremely dense scenes; limited counting ability.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="159"><p>Maktoof et al. [<xref ref-type="bibr" rid="ref_11">11</xref>]</p></td><td colspan="1" rowspan="1" colwidth="115"><p>2023</p></td><td colspan="1" rowspan="1"><p>In this study, a real-time system combining YOLOv5 and Kernel Correlation Filter (KCF) algorithms was developed for detecting and counting human crowds.</p></td><td colspan="1" rowspan="1" colwidth="378"><p>Efficient real-time tracking and detection.</p></td><td colspan="1" rowspan="1"><p>Accuracy drops in highly occluded scenes.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="159"><p>Deng at al. [<xref ref-type="bibr" rid="ref_12">12</xref>]</p></td><td colspan="1" rowspan="1" colwidth="115"><p>2024</p></td><td colspan="1" rowspan="1"><p>In this study, a new dataset was presented in the field by performing human crowd counting from videos.</p></td><td colspan="1" rowspan="1" colwidth="378"><p>Enriches the field with new data sources.</p></td><td colspan="1" rowspan="1"><p>No new method proposed; limited to data contribution.</p></td></tr></tbody></table>
        </table-wrap>
      
    </sec>
    <sec sec-type="">
      <title>2. Methodology</title>
      
        <sec>
          
            <title>2.1. Dataset</title>
          
          <p>In this study, the Mall and Crowd-UIT datasets from the literature were used [<xref ref-type="bibr" rid="ref_13">13</xref>].</p>
          
            <sec>
              
                <title>2.1.1 Mall dataset</title>
              
              <p>The Mall dataset consists of low-resolution, crowded human images captured in a shopping mall. This dataset contains 2,000 images, with the number of people in each image provided in a CSV file along with the image name and corresponding person count. <xref ref-type="fig" rid="fig_1">Figure 1</xref> presents sample images from the Mall dataset along with the CSV file.</p>
              
                <fig id="fig_1">
                  <label>Figure 1</label>
                  <caption>
                    <title>Mall dataset (a) Original images (b) CSV file</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_TDXcDLyswFvDyKzv.png"/>
                </fig>
              
            </sec>
          
          
            <sec>
              
                <title>2.1.2 Crowd-uit dataset</title>
              
              <p>The Crowd-UIT dataset consists of ten videos [<xref ref-type="bibr" rid="ref_12">12</xref>]. Using these videos, ten different repositories have been created for each video. In this study, only repositories 1 and 2 from the Crowd-UIT dataset were used. Each repository contains frames from the videos and the locations of people in those frames as JSON labels. The first repository contains 145 images, while the second repository contains 144 images. <xref ref-type="fig" rid="fig_2">Figure 2</xref> presents the sample image and the JSON file for repository 1, while <xref ref-type="fig" rid="fig_3">Figure 3</xref> presents the sample image and the JSON file for repository 2. </p>
              
                <fig id="fig_2">
                  <label>Figure 2</label>
                  <caption>
                    <title>Sample image for repository 1 of the Crowd-UIT dataset and corresponding JSON tag</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_1aYN4_KHF8SDi2De.png"/>
                </fig>
              
              
                <fig id="fig_3">
                  <label>Figure 3</label>
                  <caption>
                    <title>Sample image for repository 2 of the Crowd-UIT dataset and corresponding JSON tag</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_DIwh0XeZaR4LAohT.png"/>
                </fig>
              
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>2.2. Method</title>
          
          <p>The proposed method in this study was applied separately to the repositories in the Mall and Crowd-UIT datasets to obtain results. After training, the resulting model was tested on sample images. The block diagram of the proposed method is shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Block diagram of the proposed CSRNet model</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_jhKkyyg9ucMXZ-fr.png"/>
            </fig>
          
          <p>As shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>, the images and labels from the dataset were first fed as input to the VGG-16 model. VGG-16 was used to extract the basic features and subsequently generate feature maps. These feature maps were then passed to the convolutional layers of CSRNet, where more complex operations were performed to detect the density [<xref ref-type="bibr" rid="ref_14">14</xref>], [<xref ref-type="bibr" rid="ref_15">15</xref>], [<xref ref-type="bibr" rid="ref_16">16</xref>], [<xref ref-type="bibr" rid="ref_17">17</xref>], [<xref ref-type="bibr" rid="ref_18">18</xref>], [<xref ref-type="bibr" rid="ref_19">19</xref>], [<xref ref-type="bibr" rid="ref_20">20</xref>]. Following this, the information from a larger area was captured by passing the feature maps through dilated convolution layers. After these steps, density maps were generated. The resulting density maps were then upsampled to higher resolution through upsampling operations. To determine the accuracy of the model and compare it with other studies, MSE and MAE metrics were calculated. These values were used to assess the model's learning process and performance.</p><p>The MSE and MAE metrics are given in Eq. (1) and Eq. (2):</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="m8ayde41na">
                <mml:mi>M</mml:mi>
                <mml:mi>A</mml:mi>
                <mml:mi>E</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mn>1</mml:mn>
                  <mml:mi>N</mml:mi>
                </mml:mfrac>
                <mml:munderover>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>N</mml:mi>
                </mml:munderover>
                <mml:mrow>
                  <mml:mo>|</mml:mo>
                  <mml:mo>−</mml:mo>
                  <mml:mo>|</mml:mo>
                  <mml:msub>
                    <mml:mi>X</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                  <mml:mi>X</mml:mi>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="mehp5dakxi">
                <mml:mi>M</mml:mi>
                <mml:mi>S</mml:mi>
                <mml:mi>E</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mn>1</mml:mn>
                  <mml:mi>N</mml:mi>
                </mml:mfrac>
                <mml:munderover>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>N</mml:mi>
                </mml:munderover>
                <mml:msup>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msub>
                      <mml:mi>Y</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>Y</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mn>2</mml:mn>
                </mml:msup>
              </mml:math>
            </disp-formula>
          
          
            <sec>
              
                <title>2.2.1 Mdensity maps</title>
              
              <p>Density maps visualize the concentrations of people in images, clearly showing which areas of the crowd are more densely populated. The model predicts the human density in specific areas of the given image and provides this as a density map. These maps are generated through convolutional operations via deep learning models. In the maps, blue colors represent areas with low density. As the density increases, the colors transition to yellow and orange. The regions with the highest crowd density are shown in red. The working principle of the density map is illustrated in <xref ref-type="fig" rid="fig_5">Figure 5</xref>.</p><p>CSRNet is a deep learning model specifically designed for estimating crowd density. Its most distinctive feature lies in its utilization of dilated convolutional layers, which enable effective crowd counting, particularly in densely populated scenarios. In this study, the VGG-16 architecture was employed as the frontend of the CSRNet model. VGG-16 is a pre-trained deep CNN, originally trained on the ImageNet dataset, and is widely used for feature extraction due to its strong generalization capability. For the purposes of this study, only the convolutional layers of VGG-16 were retained, while the fully connected layers were removed. This modification allows the model to accept input images of arbitrary sizes and produce corresponding density maps, making it suitable for practical crowd analysis applications. The convolutional blocks in VGG-16 extract hierarchical features, where the early layers learn low-level visual information such as edges and textures, while the deeper layers capture high-level semantic features. This progressive feature representation enhances the model’s capability in recognizing and interpreting complex crowd patterns. The architecture of the modified VGG-16 frontend and CSRNet backend is illustrated in <xref ref-type="fig" rid="fig_6">Figure 6</xref>.</p>
              
                <fig id="fig_5">
                  <label>Figure 5</label>
                  <caption>
                    <title>Working principle of the density map</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_DlbEktaciuhIVCaJ.png"/>
                </fig>
              
              
                <fig id="fig_6">
                  <label>Figure 6</label>
                  <caption>
                    <title>Architecture of the VGG-16 model</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_8BRFH4s1tJ4UrEW6.png"/>
                </fig>
              
              <p>The CSRNet backend comprises five consecutive dilated convolutional layers, each designed to expand the receptive field without reducing the feature map resolution. By increasing the dilation rate, these layers effectively aggregate more contextual information while maintaining the integrity of fine-grained spatial details—a critical factor in the dense crowd analysis.</p><p>In addition to these core components, the following layers were utilized:</p><p>• Convolutional layers: Feature maps were extracted from the input by applying a set of learnable filters. The size of these filters determines the level of detail captured in the feature maps.</p><p>• MaxPooling layers: The feature maps were downsampled by selecting the maximum value from subregions, thus preserving the most salient features while reducing computational complexity.</p><p>• Sequential layer configuration: Layers were structured sequentially to streamline the architectural design, allowing for organized and efficient forward propagation.</p><p>This enhanced architecture allows the model to process images of varying sizes and to make accurate and real-time predictions even in complex and crowded environments. All these layers and the input-output values are shown in <xref ref-type="fig" rid="fig_7">Figure 7</xref>.</p><p><xref ref-type="fig" rid="fig_7">Figure 7</xref> provides a comprehensive visualization of the layer-by-layer architecture of the CSRNet model enhanced with VGG-16 as the frontend. The diagram outlines the exact input and output dimensions of each layer, starting from the input layer and continuing through the convolutional and pooling blocks of VGG-16, followed by the dilated convolutional layers and the final output layer. The frontend, consisting of five convolutional blocks (Block1 to Block5), is derived from VGG-16 and is responsible for feature extraction. These blocks utilize multiple Conv2D layers followed by MaxPooling2D layers, which progressively reduce the spatial resolution while increasing the depth (i.e., number of feature maps), capturing both low- and high-level features effectively. After the VGG-16 blocks, the backend (CSRNet) begins with a set of dilated convolutional layers (represented in the model with three Conv2D layers post-Block5), which preserve spatial resolution while expanding the receptive field. This allows the model to integrate broader contextual information for precise density estimation. The final sequential layer produces a single-channel output representing the predicted density map. This detailed structure illustrates how the model maintains a balance between spatial accuracy and semantic richness, enabling robust performance in dense crowd counting tasks.</p>
              
                <fig id="fig_7">
                  <label>Figure 7</label>
                  <caption>
                    <title>Layers and input-output values of the CSRNet model enhanced with VGG-16</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_C-rUvq9WOy1gplQx.png"/>
                </fig>
              
            </sec>
          
        </sec>
      
    </sec>
    <sec sec-type="results">
      <title>3. Results</title>
      
        <sec>
          
            <title>3.1. Mall dataset results</title>
          
          <p> <xref ref-type="fig" rid="fig_8">Figure 8</xref> presents a test example of the model created using the Mall dataset.</p>
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>Test example on the MALL dataset (a) Original image (b) Density detection image</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_lrG-gB8Ywpnk3yOB.png"/>
            </fig>
          
          <p>This dataset only provides the number of people along with the images and does not give the locations of the people. Therefore, the model attempts to predict the locations of people on its own.</p><p>The results of the proposed method for the MALL dataset were compared with the studies in the literature in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Comparison of the MALL dataset results with existing studies</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1" colwidth="199"><p>References</p></td><td colspan="1" rowspan="1" colwidth="366"><p>Methods</p></td><td colspan="1" rowspan="1" colwidth="115"><p>MSE</p></td><td colspan="1" rowspan="1" colwidth="104"><p>MAE</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="199"><p>Wei et al. [<xref ref-type="bibr" rid="ref_1">1</xref>]</p></td><td colspan="1" rowspan="1" colwidth="366"><p>Boosting DAL-SVR</p></td><td colspan="1" rowspan="1" colwidth="115"><p>9.57</p></td><td colspan="1" rowspan="1" colwidth="104"><p>2.40</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="199"><p>Zhang et al. [<xref ref-type="bibr" rid="ref_2">2</xref>]</p></td><td colspan="1" rowspan="1" colwidth="366"><p>IIS-LDL</p></td><td colspan="1" rowspan="1" colwidth="115"><p>12.1</p></td><td colspan="1" rowspan="1" colwidth="104"><p>2.69</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="199"><p>Chen et al. [<xref ref-type="bibr" rid="ref_3">3</xref>]</p></td><td colspan="1" rowspan="1" colwidth="366"><p>Multi-output ridge regression (MORR)</p></td><td colspan="1" rowspan="1" colwidth="115"><p>15.7</p></td><td colspan="1" rowspan="1" colwidth="104"><p>3.15</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="199"><p>Chen et al. [<xref ref-type="bibr" rid="ref_4">4</xref>]</p></td><td colspan="1" rowspan="1" colwidth="366"><p>CA-RR</p></td><td colspan="1" rowspan="1" colwidth="115"><p>17.7</p></td><td colspan="1" rowspan="1" colwidth="104"><p>3.43</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="199"><p>Pham et al. [<xref ref-type="bibr" rid="ref_5">5</xref>]</p></td><td colspan="1" rowspan="1" colwidth="366"><p>Patch-based Gaussian kernel</p></td><td colspan="1" rowspan="1" colwidth="115"><p>10.0</p></td><td colspan="1" rowspan="1" colwidth="104"><p>2.50</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="199"><p>Tomar et al. [<xref ref-type="bibr" rid="ref_6">6</xref>]</p></td><td colspan="1" rowspan="1" colwidth="366"><p>DKCNN-LR</p></td><td colspan="1" rowspan="1" colwidth="115"><p>2.76</p></td><td colspan="1" rowspan="1" colwidth="104"><p>1.65</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="199"><p>Abdullah &amp;amp; Jalal [<xref ref-type="bibr" rid="ref_7">7</xref>]</p></td><td colspan="1" rowspan="1" colwidth="366"><p>Semantic segmentation-based crowd tracking method</p></td><td colspan="1" rowspan="1" colwidth="115"><p>4.34</p></td><td colspan="1" rowspan="1" colwidth="104"><p>2.57</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="199"><p>Tripathy &amp;amp; Srivastava [<xref ref-type="bibr" rid="ref_8">8</xref>]</p></td><td colspan="1" rowspan="1" colwidth="366"><p>AMS-CNN</p></td><td colspan="1" rowspan="1" colwidth="115"><p>3.08</p></td><td colspan="1" rowspan="1" colwidth="104"><p>2.47</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="199"><p>Xiong et al. [<xref ref-type="bibr" rid="ref_9">9</xref>]</p></td><td colspan="1" rowspan="1" colwidth="366"><p>ConvLSTM</p><p>Bidirectional ConvLSTM</p></td><td colspan="1" rowspan="1" colwidth="115"><p>7.6</p></td><td colspan="1" rowspan="1" colwidth="104"><p>2.10</p></td></tr><tr><td colspan="2" rowspan="1" colwidth="199,366"><p>VGG-16 + CSRNet (proposed model)</p></td><td colspan="1" rowspan="1" colwidth="115"><p>0.08</p></td><td colspan="1" rowspan="1" colwidth="104"><p>0.10</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>Upon examining <xref ref-type="table" rid="table_2">Table 2</xref>, it can be seen that the CSRNet model enhanced with VGG-16 achieves significant success in crowd density prediction, with an MSE of 0.08 and an MAE of 0.10, outperforming other methods in the literature.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Crowd-uit dataset results</title>
          
          <p> <xref ref-type="fig" rid="fig_9">Figure 9</xref> shows the test result with an example image after applying the proposed method to the Crowd-UIT dataset.</p>
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>
                <title>Test example on the Crowd-UIT dataset (a) Original image (b) Density detection image</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_GYR1wdVWc2E0HgDv.png"/>
            </fig>
          
          <p>The Crowd-UIT dataset not only contains images of crowded scenes but also includes precise location annotations for each individual. These annotations were directly utilized during the training phase of the proposed method, enabling more precise learning of spatial patterns associated with crowd distribution. By incorporating positional labels into the supervised learning process, the model could effectively minimize the loss function by explicitly correlating input image features with ground-truth spatial positions. This significantly improved the optimization process during training, as the model received clear, structured signals about where people are located, rather than only how many people are in the scene. As a result, it could better localize individuals even in densely populated scenes or in areas with high background complexity.</p><p> <xref ref-type="table" rid="table_3">Table 3</xref> presents a comparative analysis of the Crowd-UIT dataset results with existing studies in the literature. As shown, there are very few prior works utilizing the Crowd-UIT dataset, and all the known studies are summarized in <xref ref-type="table" rid="table_3">Table 3</xref>. Due to this scarcity, the findings of this study are anticipated to contribute as a meaningful benchmark for future researchers working on crowd analysis.</p>
          <p>Upon examining <xref ref-type="table" rid="table_3">Table 3</xref>, it can be seen that the CSRNet model enhanced with VGG-16 achieves significant success in crowd density prediction, with an MSE of 0.05 and an MAE of 0.15, outperforming other studies in the literature.</p><p>In conclusion, the Crowd-UIT dataset includes crowd images along with annotated human positions. Since the proposed method was trained with these positional annotations, it was able to more accurately detect individuals regardless of crowd density. In contrast, the Mall dataset only provides the total number of people in each image without their exact positions. Therefore, the proposed method had to autonomously identify individuals in the scenes. However, as illustrated in the test image shown in <xref ref-type="fig" rid="fig_8">Figure 8</xref>, due to the high scene complexity in the Mall dataset—such as lighting angles and the presence of non-human objects like trees—the method was not as successful as it was on the Crowd-UIT dataset. Therefore, when compared to the Mall dataset, the Crowd-UIT dataset yielded better results due to its higher image clarity and the availability of directly annotated human positions.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Comparison of the Crowd-UIT dataset results with existing studies</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1" colwidth="130"><p>References</p></td><td colspan="1" rowspan="1" colwidth="114"><p>Methods</p></td><td colspan="1" rowspan="1" colwidth="88"><p>MSE</p></td><td colspan="1" rowspan="1" colwidth="89"><p>MAE</p></td><td colspan="1" rowspan="1" colwidth="117"><p>Accuracy</p></td></tr><tr><td colspan="1" rowspan="2" colwidth="130"><p>Deng et al. [<xref ref-type="bibr" rid="ref_12">12</xref>]</p></td><td colspan="1" rowspan="1" colwidth="114"><p>FairMOT</p></td><td colspan="1" rowspan="1" colwidth="88"><p>5232.8</p></td><td colspan="1" rowspan="1" colwidth="89"><p>50.4</p></td><td colspan="1" rowspan="2" colwidth="117"><p>-</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="114"><p>JDE</p></td><td colspan="1" rowspan="1" colwidth="88"><p>439425.5</p></td><td colspan="1" rowspan="1" colwidth="89"><p>445.5</p></td></tr><tr><td colspan="1" rowspan="4" colwidth="130"><p>Maktoof et al. [<xref ref-type="bibr" rid="ref_10">10</xref>]</p></td><td colspan="1" rowspan="1" colwidth="114"><p>YOLOv5s</p></td><td colspan="1" rowspan="4" colwidth="88"><p>-</p></td><td colspan="1" rowspan="4" colwidth="89"><p>-</p></td><td colspan="1" rowspan="1" colwidth="117"><p>93.22%</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="114"><p>YOLOv5m</p></td><td colspan="1" rowspan="1" colwidth="117"><p>90.96%</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="114"><p>YOLOv5l</p></td><td colspan="1" rowspan="1" colwidth="117"><p>96.41%</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="114"><p>YOLOv5x</p></td><td colspan="1" rowspan="1" colwidth="117"><p>96.53%</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="130"><p>Maktoof et al. [<xref ref-type="bibr" rid="ref_11">11</xref>]</p></td><td colspan="1" rowspan="1" colwidth="114"><p>YOLOv5 + KCF</p></td><td colspan="1" rowspan="1" colwidth="88"><p>-</p></td><td colspan="1" rowspan="1" colwidth="89"><p>-</p></td><td colspan="1" rowspan="1" colwidth="117"><p>97.61%</p></td></tr><tr><td colspan="2" rowspan="1" colwidth="130,114"><p>VGG-16 + CSRNet (proposed model)</p></td><td colspan="1" rowspan="1" colwidth="88"><p>0.05</p></td><td colspan="1" rowspan="1" colwidth="89"><p>0.15</p></td><td colspan="1" rowspan="1" colwidth="117"><p>-</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>4. Conclusions</title>
      <p>In this study, crowd density detection was performed using the CSRNet model enhanced with VGG-16 on the Mall and Crowd-UIT datasets. The model was applied separately to each dataset, and the results were obtained. The density maps generated by the model effectively visualized areas with high crowd density. The MSE and MAE metrics obtained in the model showed very low values. For the Mall dataset, an MSE of 0.08 and an MAE of 0.10 were achieved. For the Crowd-UIT dataset, an MSE of 0.05 and an MAE of 0.15 were obtained. When comparing the Crowd-UIT dataset with the Mall dataset, better results were observed in the Crowd-UIT dataset due to the clearer images and the direct labeling of people's positions. The low error values provided by the CSRNet model enhanced with VGG-16 demonstrate that the model performs better than existing methods in the literature for crowd density detection. Since other algorithms have higher error metrics, it is evident that the CSRNet model enhanced with VGG-16 is an effective method for crowd density detection.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>The authors contributed equally to the article.</p>
    </notes>
    <notes>
      <title>Funding</title>
      <p>The paper was funded by the Scientific and Technological Research Council of Turkey (Grant No.: 5220154).</p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>1. The data [image] supporting our research results are deposited in [Mall Dataset], which does not issue DOIs. The data can be accessed at [<a target="_blank" rel="noopener noreferrer nofollow" href="https://personal.ie.cuhk.edu.hk/">https://personal.ie.cuhk.edu.hk/</a> ccloy/downloads mall dataset.html].</p><p>2. The data [image] supporting our research results are deposited in [Crowd-UIT], which does not issue DOIs. The data can be accessed at [<a target="_blank" rel="noopener noreferrer nofollow" href="https://www.kaggle.com/datasets/khitthanhnguynphan/crowduit?select=Crowd-UIT">https://www.kaggle.com/datasets/khitthanhnguynphan/crowduit?select=Crowd-UIT</a>].</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>119</volume>
          <page-range>12-23</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wei</surname>
              <given-names>Xin Lei</given-names>
            </name>
            <name>
              <surname>Du</surname>
              <given-names>Jun Ping</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>Mei Yu</given-names>
            </name>
            <name>
              <surname>Ye</surname>
              <given-names>Ling Fei</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.patrec.2017.12.002</pub-id>
          <article-title>Boosting deep attribute learning via support vector regression for fast moving crowd counting</article-title>
          <source>Pattern Recognit. Lett.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>166</volume>
          <page-range>151-163</page-range>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Zhao Xiang</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Mo</given-names>
            </name>
            <name>
              <surname>Geng</surname>
              <given-names>Xin</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neucom.2015.03.083</pub-id>
          <article-title>Crowd counting in public video surveillance by label distribution learning</article-title>
          <source>Neurocomputing</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Loy</surname>
              <given-names>C. C.</given-names>
            </name>
            <name>
              <surname>Gong</surname>
              <given-names>S. G.</given-names>
            </name>
            <name>
              <surname>Xiang</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Feature mining for localised crowd counting</article-title>
          <source>BMVC</source>
          <year>2012</year>
          <page-range>1-11</page-range>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>Ke</given-names>
            </name>
            <name>
              <surname>Gong</surname>
              <given-names>Shao Gang</given-names>
            </name>
            <name>
              <surname>Xiang</surname>
              <given-names>Tao</given-names>
            </name>
            <name>
              <surname>Change Loy</surname>
              <given-names>Chen</given-names>
            </name>
          </person-group>
          <article-title>Cumulative attribute space for age and crowd density estimation</article-title>
          <source>2013 IEEE Conference on Computer Vision and Pattern Recognition, Portland, OR, USA</source>
          <year>2013</year>
          <page-range>2467--2474</page-range>
          <pub-id pub-id-type="doi">10.1109/CVPR.2013.319</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Pham</surname>
              <given-names>V. Q.</given-names>
            </name>
            <name>
              <surname>Kozakaya</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Yamaguchi</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Okada</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>COUNT forest: CO-voting uncertain number of targets using random forest for crowd density estimation</article-title>
          <source>Proceedings of the IEEE International Conference on Computer Vision, Santiago, Chile</source>
          <year>2015</year>
          <page-range>3253--3261</page-range>
          <pub-id pub-id-type="doi">10.1109/ICCV.2015.372</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>52</volume>
          <page-range>55-70</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tomar</surname>
              <given-names>Ankit</given-names>
            </name>
            <name>
              <surname>Kumar</surname>
              <given-names>Santosh</given-names>
            </name>
            <name>
              <surname>Pant</surname>
              <given-names>Bhaskar</given-names>
            </name>
            <name>
              <surname>Tiwari</surname>
              <given-names>Umesh Kumar</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10489-021-02375-6</pub-id>
          <article-title>Dynamic kernel CNN-LR model for people counting</article-title>
          <source>Appl. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>48</volume>
          <page-range>2173-2190</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Abdullah</surname>
              <given-names>Faisal</given-names>
            </name>
            <name>
              <surname>Jalal</surname>
              <given-names>Ahmad</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s13369-022-07092-x</pub-id>
          <article-title>Semantic segmentation based crowd tracking and anomaly detection via neuro-fuzzy classifier in smart surveillance system</article-title>
          <source>Arab. J. Sci. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>239-254</page-range>
          <issue>4</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tripathy</surname>
              <given-names>Santosh Kumar</given-names>
            </name>
            <name>
              <surname>Srivastava</surname>
              <given-names>Rajeev</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s13735-021-00220-7</pub-id>
          <article-title>AMS-CNN: Attentive multi-stream CNN for video-based crowd counting</article-title>
          <source>Int. J. Multimed. Inf. Retr.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Xiong</surname>
              <given-names>Feng</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>Xing Jian</given-names>
            </name>
            <name>
              <surname>Yeung</surname>
              <given-names>Dit Yan</given-names>
            </name>
          </person-group>
          <article-title>Spatiotemporal modeling for crowd counting in videos</article-title>
          <source>Proceedings of the IEEE International Conference on Computer Vision (ICCV)</source>
          <year>2017</year>
          <page-range>5151--5159</page-range>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>19</volume>
          <page-range>94-108</page-range>
          <issue>4</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Maktoof</surname>
              <given-names>M. A. J.</given-names>
            </name>
            <name>
              <surname>Al_attar</surname>
              <given-names>I. T. A.</given-names>
            </name>
            <name>
              <surname>Ibraheem</surname>
              <given-names>I. N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3991/ijoe.v19i04.39095</pub-id>
          <article-title>Comparison YOLOv5 family for human crowd detection</article-title>
          <source>Int. J. Online Biomed. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>92-101</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Maktoof</surname>
              <given-names>M. A. J.</given-names>
            </name>
            <name>
              <surname>Ibraheem</surname>
              <given-names>I. N.</given-names>
            </name>
            <name>
              <surname>Al_attar</surname>
              <given-names>I. T. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.21533/pen.v11.i2.102</pub-id>
          <article-title>Crowd counting using Yolov5 and KCF</article-title>
          <source>Period. Eng. Nat. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>1043-1077</page-range>
          <issue>5</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Deng</surname>
              <given-names>L. J.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Q. H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>S. H.</given-names>
            </name>
            <name>
              <surname>Górriz</surname>
              <given-names>J. M.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y. D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1049/cit2.12241</pub-id>
          <article-title>Deep learning in crowd counting: A survey</article-title>
          <source>CAAI Trans. Intell. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Change Loy</surname>
              <given-names>Chen</given-names>
            </name>
            <name>
              <surname>Gong</surname>
              <given-names>Shao Gang</given-names>
            </name>
            <name>
              <surname>Xiang</surname>
              <given-names>Tao</given-names>
            </name>
          </person-group>
          <article-title>From semi-supervised to transfer counting of crowds</article-title>
          <source>Proceedings of the IEEE International Conference on Computer Vision, Sydney, NSW, Australia</source>
          <year>2013</year>
          <page-range>2256--2263</page-range>
          <pub-id pub-id-type="doi">10.1109/ICCV.2013.270</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Kaur</surname>
              <given-names>Taranjit</given-names>
            </name>
            <name>
              <surname>Gandhi</surname>
              <given-names>Tapan Kumar</given-names>
            </name>
          </person-group>
          <article-title>Automated brain image classification based on VGG-16 and transfer learning</article-title>
          <source>2019 International Conference on Information Technology (ICIT), Bhubaneswar, India</source>
          <year>2019</year>
          <page-range>94--98</page-range>
          <pub-id pub-id-type="doi">10.1109/ICIT48102.2019.00023</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Yu Hong</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Xiao Fan</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>De Ming</given-names>
            </name>
          </person-group>
          <article-title>CSRNet: Dilated convolutional neural networks for understanding the highly congested scenes</article-title>
          <source>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Lake City, UT, USA</source>
          <year>2018</year>
          <page-range>1091-1100</page-range>
          <pub-id pub-id-type="doi">10.1109/CVPR.2018.00120</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Chang Lin</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Tao Jian Nan</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Si Jie</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Chen</given-names>
            </name>
            <name>
              <surname>Guan</surname>
              <given-names>Shan Yue</given-names>
            </name>
          </person-group>
          <article-title>Density map guided object detection in aerial images</article-title>
          <source>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), Seattle, WA, USA</source>
          <year>2020</year>
          <page-range>737-746</page-range>
          <pub-id pub-id-type="doi">10.1109/CVPRW50498.2020.00103</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>143-150</page-range>
          <issue>10</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tammina</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.29322/IJSRP.9.10.2019.p9420</pub-id>
          <article-title>Transfer learning using VGG-16 with Deep Convolutional Neural Network for classifying images</article-title>
          <source>Int. J. Sci. Res. Publ.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Wan</surname>
              <given-names>Jia</given-names>
            </name>
            <name>
              <surname>Chan</surname>
              <given-names>Antoni</given-names>
            </name>
          </person-group>
          <article-title>Adaptive density map generation for crowd counting</article-title>
          <source>2019 IEEE/CVF International Conference on Computer Vision (ICCV), Seoul, Korea (South)</source>
          <year>2019</year>
          <page-range>1130-1139</page-range>
          <pub-id pub-id-type="doi">10.1109/ICCV.2019.00122</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>211</volume>
          <page-range>118537</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Xiong</surname>
              <given-names>Jing Jing</given-names>
            </name>
            <name>
              <surname>Po</surname>
              <given-names>Lai Man</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>Wing Yin</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Chang</given-names>
            </name>
            <name>
              <surname>Xian</surname>
              <given-names>Peng Fei</given-names>
            </name>
            <name>
              <surname>Ou</surname>
              <given-names>Wei Feng</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2022.118537</pub-id>
          <article-title>CSRNet: Cascaded selective resolution network for real-time semantic segmentation</article-title>
          <source>Expert Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>51</volume>
          <page-range>2850-2863</page-range>
          <issue>5</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sitaula</surname>
              <given-names>Chiranjibi</given-names>
            </name>
            <name>
              <surname>Hossain</surname>
              <given-names>Mohammad Belayet</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10489-020-02055-x</pub-id>
          <article-title>Attention-based VGG-16 model for COVID-19 chest X-ray image classification</article-title>
          <source>Appl. Intell.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>