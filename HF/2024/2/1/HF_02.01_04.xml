<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">HF</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Healthcraft Frontiers</journal-title>
        <abbrev-journal-title abbrev-type="issn">Healthcraft. Front.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">HF</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">3005-799X</issn>
      <issn publication-format="print">3005-7981</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-pwGunny8_3DMl4MUmOM6aXkqlwSPInLv</article-id>
      <article-id pub-id-type="doi">10.56578/hf020104</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>NC2C-TransCycleGAN: Non-Contrast to Contrast-Enhanced CT Image Synthesis Using Transformer CycleGAN</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-8106-4017</contrib-id>
          <name>
            <surname>Hou</surname>
            <given-names>Xiaoxue</given-names>
          </name>
          <email>1971047@stu.neu.edu.cn</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8813-0752</contrib-id>
          <name>
            <surname>Liu</surname>
            <given-names>Ruibo</given-names>
          </name>
          <email>1910445@stu.neu.edu.cn</email>
          <xref ref-type="aff" rid="aff_2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-0014-528X</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Youzhi</given-names>
          </name>
          <email>20237337@stu.neu.edu.cn</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-8911-6869</contrib-id>
          <name>
            <surname>Han</surname>
            <given-names>Xuerong</given-names>
          </name>
          <email>20217231@stu.neu.edu.cn</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-6580-686X</contrib-id>
          <name>
            <surname>He</surname>
            <given-names>Jiachuan</given-names>
          </name>
          <email>1701240@stu.neu.edu.cn</email>
          <xref ref-type="aff" rid="aff_2">2</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5054-3586</contrib-id>
          <name>
            <surname>Ma</surname>
            <given-names>He</given-names>
          </name>
          <email>mahe@bmie.neu.edu.cn</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <aff id="aff_1">College of Medicine and Biological Information Engineering, Northeastern University, 110000 Shenyang, China</aff>
        <aff id="aff_2">Department of Radiology, The First Hospital of China Medical University, 110000 Shenyang, China</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>21</day>
        <month>03</month>
        <year>2024</year>
      </pub-date>
      <volume>2</volume>
      <issue>1</issue>
      <fpage>34</fpage>
      <lpage>45</lpage>
      <page-range>34-45</page-range>
      <history>
        <date date-type="received">
          <day>11</day>
          <month>12</month>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <day>10</day>
          <month>03</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Background: Lung cancer poses a great threat to human life and health. Although the density differences between lesions and normal tissues shown on enhanced CT images is very helpful for doctors to characterize and detect lesions, contrast agents and radiation may cause harm to the health of patients with lung cancer. By learning the mapping relationship between plain CT image and enhanced CT image through deep learning methods, high quality synthetic CECT image results can be generated based on plain scan CT image. It has great potential to help save treatment time and cost of lung cancer patients, reduce radiation dose and expand the medical image dataset in the field of deep learning. Methods: In this study, plain and enhanced CT images of 71 lung cancer patients were retrospectively collected. The data from 58 lung cancer patients were randomly assigned to the training set, and the other 13 cases formed the test set. The Convolution Vison Transformer structure and PixelShuffle operation were combined with CycleGAN, respectively, to help generate clearer images. After random erasing, image scaling and flipping to enhance the training data, paired plain and enhanced CT slices of each patient are input into the network as input and labeled, respectively, for model training. Finally, the peak signal-to-noise ratio, structural similarity and mean square error are used to evaluate the image quality and similarity. Results: The performance of our proposed method is compared with CycleGAN and Pix2Pix on the test set, respectively. The results show that the SSIM value of the enhanced CT images generated by the proposed method improve by 2.00% and 1.39%, the PSNR values improve by 2.05% and 1.71%, and the MSE decreases by 12.50% and 8.53%, respectively, compared with Pix2Pix and CycleGAN. Conclusions: The experimental results show that the improved algorithm based on CylceGAN proposed in this paper can synthesize high-quality lung cancer synthetic enhanced CT images, which is helpful to expand the lung cancer image data set in the deep learning research. More importantly, this method can help lung cancer patients save medical treatment time and cost.</p></abstract>
      <kwd-group>
        <kwd>Contrast-enhanced CT image synthesis</kwd>
        <kwd>Cycle-Consistent adversarial networks improvement</kwd>
        <kwd>Data augmentation</kwd>
        <kwd>Deep learning</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="6"/>
        <fig-count count="8"/>
        <table-count count="4"/>
        <ref-count count="24"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p style="text-align: justify">Globally, cancer incidence and death are rising, with lung cancer being the most commonly diagnosed form of cancer (11.6% of the total cases) (<xref ref-type="bibr" rid="ref_13">Lahiri et al., 2023</xref>). The rapid development of computed tomography (CT) has been proven that it can significantly help the diagnosis of lung diseases (<xref ref-type="bibr" rid="ref_3">Bushara et al., 2023</xref>). CT can be divided into non-contrast CT(NCCT) and contrast-enhanced CT(CECT), and different types of CT have shown different advantages in various applications. CECT increases the density difference between lesions and normal tissues by injecting contrast media into blood vessels (<xref ref-type="bibr" rid="ref_11">Kojima et al., 2010</xref>). This helps doctors to understand the blood supply of the disease and the relationship between mediastinal lesions and cardiac macrovessels, thus improving the accuracy of differentiating benign from malignant lung diseases. However, the disadvantage of CECT is that it increases the scanning time for patients, the examination cost, and cannot be used in patients with contraindications to iodine contrast media. In addition, needle insertion into the human body during the use of contrast agents in high-pressure syringes may cause discomfort (<xref ref-type="bibr" rid="ref_23">Wu et al., 2023</xref>) and may result in leakage of contrast agents or greater irritation to the patient’s local skin. Instead, NCCT shows a low contrast between tumor areas and tissues, which is not conducive to the localization and qualitative diagnosis of lung lesions. The heart and surrounding tissues such as the chest wall, spine, and pulmonary vessels move several millimeters, producing a phenomenon of interleaved or staircase artifacts known as respiratory artifacts (<xref ref-type="bibr" rid="ref_14">Maier et al., 2021</xref>; <xref ref-type="bibr" rid="ref_9">Hertanto et al., 2012</xref>). Axial slices of NCCT from a lung cancer patient and CECT from an injected intravenous contrast agent are shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>. Stripes, artifacts and motion artifacts are common phenomena in clinical work (<xref ref-type="bibr" rid="ref_21">Wang et al., 2023a</xref>). Post-processing features such as scan reconstruction parameters or ECG editing cannot eliminate this phenomenon, so the effect of artifacts on image quality cannot be ignored. In addition, the situation of blood supply in lung cancer patients is complicated on CT imaging, which makes the generation of CECT more difficult. With the wide application of Generative Adversarial Networks (GAN) in image generation (<xref ref-type="bibr" rid="ref_17">Skandarani et al., 2023</xref>), these problems are expected to be solved.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>(A) Axial NCCT of lung cancer. (B) Axial CECT of lung cancer. During the image acquisition, due to the patient’s autonomous or respiratory movement and other non autonomous movement, there are staggered layers between slice (A) and (B) and the generation of stripes and shadows in (B). NCCT, non-contrast computed tomography; CECT, contrast-enhanced CT computed tomography</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_F1F6UYLg0XMgEEES.png"/>
        </fig>
      
      <p style="text-align: justify">In recent years GAN has been widely used in medical image tasks such as image segmentation (<xref ref-type="bibr" rid="ref_2">Beji et al., 2023</xref>; <xref ref-type="bibr" rid="ref_6">Dash et al., 2024</xref>; <xref ref-type="bibr" rid="ref_17">Skandarani et al., 2023</xref>; <xref ref-type="bibr" rid="ref_24">Zhong et al., 2023</xref>), lesion classification (<xref ref-type="bibr" rid="ref_5">Chen et al., 2023</xref>; <xref ref-type="bibr" rid="ref_8">Fan et al., 2023</xref>), and lesion detection (<xref ref-type="bibr" rid="ref_7">Esmaeili et al., 2023</xref>; <xref ref-type="bibr" rid="ref_20">Vyas &amp;amp; Rajendran, 2023</xref>). And the study of GAN in medical image synthesis tasks has dominated. The high-quality medical image data synthesised by GAN is now being validated by radiologists and can be used for radiological teaching or for big data deep network training (<xref ref-type="bibr" rid="ref_10">Kelkar et al., 2023</xref>). In addition, it has successfully remedied the problems of missing medical images and unbalanced data for classification and labeling due to difficulties in medical image data acquisition and involving patient privacy. CycleGAN is widely used for cross domain or cross-modality medical image synthesis due to its ability to process unpaired data. CycleGAN can be trained with unpaired data to simulate the implicit distribution of real data distribution to generate real images (<xref ref-type="bibr" rid="ref_19">Torbunov et al., 2023</xref>), which makes it very suitable for the interleaving problem caused by respiratory motion in this task. <xref ref-type="bibr" rid="ref_4">Chandrashekar et al. (2020)</xref> used CycleGAN to learn the relationship between soft tissue components to mimic contrast-enhanced CTA without contrast agents. It is also assumed that the raw data from non-contrast material CT contains enough information to distinguish blood and other soft tissue components. The accuracy of network output is assessed by comparing it with a contrast image. The test results show that the CTA generated from the non-contrast images are very similar to the ground truth. <xref ref-type="bibr" rid="ref_1">Agrawal et al. (2020)</xref> accomplished bidirectional exchange of content and style between two image modalities, CT and MR, on a pelvic dataset by means of an improved CycleGAN network. The validation results by radiologists showed that the subtle variations in MR and CT images generated by the improved network may not be identical to the real images, but can be used for medical purposes. In this work, it has been shown that cycleGAN can be used for medical image translation tasks.Many unpaired medical image synthesis tasks can achieve good results by using CycleGAN, but most of these methods are completed by CNN. Although CNN can well extract the local information in the feature map, such as image edge, textture and rich context semantic information, the receptive field of CNN is limited. In addition, the transpose convolution used in the up-sampling portion of the CycleGAN generator results in a checkerboard artifacts that greatly reduces the quality of the generated image.</p><p style="text-align: justify">The Convolution Vision Transformer structure merges the advantages of Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Convolutional Neural Networks (CNNs) are recognised for their efficiency in processing local features through their convolutional layers, while Vision Transformers (ViTs) excel at capturing global dependencies in an image through self-attention mechanisms (<xref ref-type="bibr" rid="ref_15">Maurício et al., 2023</xref>). The PixelShuffle operation, also known as sub-pixel convolution, is a technique mainly used for upscaling images in super-resolution tasks (<xref ref-type="bibr" rid="ref_22">Wang et al., 2023b</xref>). The combination of Convolutional Vision Transformer structures and PixelShuffle operations in CycleGANs is a powerful tool for advanced image processing tasks. This approach leverages the strengths of convolutional operations, transformer models, and efficient upscaling to significantly enhance the quality and effectiveness of generated images.</p><p style="text-align: justify">In this paper, we propose the NC2C-TransCycleGAN, which integrates the CVT (<xref ref-type="bibr" rid="ref_12">Komorowski et al., 2023</xref>) structure that can perceive long distances range. In addition, the PixelShufflfle operation (<xref ref-type="bibr" rid="ref_18">Sun et al., 2023</xref>) is added into the CycleGAN to reduce the serious impact of the checkerboard artifacts caused by transposed convolution operation on the image generation quality. The advantage of NC2C-TransCycleGAN is that it can learn the complex information of lung cancer enhanced CT images in a specific way without manually annotating the lung cancer regions, generate more accurate enhanced regions, and then synthesize CECT images of lung cancer patients and avoid the appearance of checkerboard artifacts.</p>
    </sec>
    <sec sec-type="">
      <title>2. Materials and methods</title>
      
        <sec>
          
            <title>2.1. Materials</title>
          
          <p>This study retrospectively collected CT images of lung cancer patients who underwent unenhanced and enhanced examinations between February 2017 and October 2020. The patient inclusion criteria were: (I) patients with lung cancer requiring lung CT enhancement imaging; (II) the CT sequence of lung cancer has its corresponding complete imaging data, clinical data and paired plain and enhanced chest images; (III) contraindication for iodinated contrast medium (CM); (IV) Imaging of patients with initial diagnosis of lung cancer on preoperative CT who had not undergone previous lung cancer surgery and other treatments. Exclusion criteria were as follows: (I) CT imaging sequences or clinical data of the lungs were incomplete, and paired plain and enhanced CT sequences were missing; (II) CT images with severe motion artifacts or other artifacts such as poor image quality affect the analysis of the model and experimental results; (III) known severe allergy to iodinated CM injection; (IV) patients who have undergone treatment such as tumor resection.</p><p><xref ref-type="table" rid="table_1">Table 1</xref> shows that CT images in the dataset have different tube voltage (in kVp), tube current (in mAS), volume CT dose index (in mGy), and slice scan thickness parameters when acquired. The data set used in this study contained image data of 71 lung cancer patients, including 44 men and 27 women, accounting for 58% and 42% of the total data, respectively. The age of the patients ranged from 42 to 86 years with a mean age of 59.85 +8.67 years. The number of patients aged 42-50 accounted for 11% of the total data, of which 34% were 50-60 years old, 41% were 60-70 years old, and 14% were over 70 years old. Thirty-one percent of patients had metastasis, including pancreas, liver, kidney, brain, bone, chest wall, adrenal gland, lung, etc. Each patient in the dataset has two types of CT images, axial unenhanced CT and corresponding enhanced CT images. These images are all from two different CT manufacturers, Toshiba and Siemens. The data coverage of this experiment is comprehensive and diverse.</p><p>The study was conducted in accordance with the Declaration of Helsinki (as revised in 2013). The study was approved by the First Hospital of China Medical University, and individual consent for this retrospective analysis was waived.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Acquisition parameters of NCCT and CECT</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Parameters</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">NCCT</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">CECT</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">CTDIvol (mGy)</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">15.27(3.87-22.40)</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">16.01(3.75-25.31)</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Voltage (kVp)</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">90-120</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">90-120</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Tubecurrent (mA)</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">111-671</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">111-743</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">WindowCenter (HU)</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">35-40</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">40-80</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">WindowWidth (HU)</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">180-400</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">180-400</span></p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>2.2. Methods</title>
          
          <p>In this paper, we improve CycleGAN and propose NC2C-TransCycleGAN for the virtual lung cancer enhanced CT image synthesis task. The overall network structure of NC2C-TransCycleGAN continues the dual structure of vanilla CycleGAN in <xref ref-type="fig" rid="fig_2">Figure 2</xref>. The generator consists of an encoder, a transformer, and a decoder. Generator GCECT or FNCCT attempts to make the generated image as similar as possible to the samples in the underlying real domain of the image. The discriminators DCECT and DNCCT retain the PatchGAN structure from CycleGAN, which outputs a 30*30 image to discriminate whether the input image is from a fake image or a real image generated by the generator.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>(A) forward cycle-consistency loss of CycleGAN in our task: NCCT to synthetic CECT to NCCT. (B) back cycle-consistency loss: CECT to synthetic NCCT to CECT</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_ZjJLoSpLVAPZoDMS.png"/>
            </fig>
          
          <p style="text-align: justify">The NC2C-TransCycleGAN is still consistent with the cycle structure of CycleGAN, but its internal structure has been adjusted and updated to make it applicable to more complex medical image generation tasks.</p>
          
            <sec>
              
                <title>2.2.1 Image preprocessing</title>
              
              <p style="text-align: justify">Each patient in the dataset had CT images in both modalities. The two modalities were axial non-enhanced and corresponding enhanced CT images, respectively. Due to the influence of respiratory movement, the layers of these paired data will be separated from each other, and the location, size and shape of tumors are very different, which brings challenges to the task of medical image synthesis. We randomly selected 3253 pairs of data from 50 patients as the training set, 663 pairs of images from 8 patients as the validiation set and 813 pairs of data from 13 patients as the test set. The data of each patient were divided into multiple slices according to the axial scan direction. These two-dimensional slices were used to train the model. The 512*512 resolution downsampled from the original NCCT and CECT images in this experiment is 256*256 resolution. The CT images of lung cancer for each patient contained an average of more than 60 slices of 2-dimensional axial images. Based on Eq. (1) on the pixel values of NCCT and CECT, resampled to a uniform distribution in the 256 gray scale range. And data enhancement operations such as horizontal flipping are performed. After that, we normalize the original image and keep the information of the original image as much as possible. The normalization operation smoothes the distribution of the input layer, helps stochastic gradient descent, and helps the network converge quickly during the training process. In order to make the model training more generalized and robust, this experiment expands the training data by online data augmentation operation. Data augmentation methods are widely used in deep learning and provide an effective means to overcome the overfitting phenomenon caused by the scarcity of training data for the model. Random erasing and random flipping are selected in this experiment. Random erasing refers to randomly selecting a part of a rectangular region in an image and erasing the pixels in that region using random values. Random flipping means the operation of flipping the image by a certain angle along the horizontal or vertical direction. After preprocessing, the enhanced and non-enhanced CT images of lung cancer are input into the network in pairs for training. And real enhanced CT image of the same patient will be used as the ground truth data for network.</p>
              
                <disp-formula>
                  <label>(1)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtext>Image</mtext>
                    <mo>=</mo>
                    <mo>∗</mo>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mfrac>
                        <mrow>
                          <mi>I</mi>
                          <mi>I</mi>
                          <mo>−</mo>
                          <mo data-mjx-texclass="OP" movablelimits="true">min</mo>
                          <mo stretchy="false">(</mo>
                          <mo stretchy="false">)</mo>
                        </mrow>
                        <mrow>
                          <mo data-mjx-texclass="OP" movablelimits="true">max</mo>
                          <mo stretchy="false">(</mo>
                          <mo stretchy="false">)</mo>
                          <mo>−</mo>
                          <mo data-mjx-texclass="OP" movablelimits="true">min</mo>
                          <mo stretchy="false">(</mo>
                          <mo stretchy="false">)</mo>
                          <mi>I</mi>
                          <mi>I</mi>
                        </mrow>
                      </mfrac>
                    </mrow>
                    <mn>255</mn>
                  </math>
                </disp-formula>
              
            </sec>
          
          
            <sec>
              
                <title>2.2.2 Nc2c-transcyclegan architecture</title>
              
              <p>The encoder of the generator in NC2C-TransCycleGAN is composed of three convolutional layers. The input lung cancer plain CT image is down sampled through these convolution layers, so that the number of channels of the feature map is expanded and the size is gradually reduced.</p><p>The encoder output is then passed sequentially to four residual blocks, two stage of CvT and three residual blocks in the converter module, which is used to convert the image domain between the two images, each of which is composed of two convolutional layers. The purpose of the residual structure is to add part of the image features obtained from the input data after feature extraction to the output by means of residuals by introducing a constant mapping jump connection, which ensures that the information of the feature map of the previous layer can also be used in the later feature layers, thus better preserving the semantic information of the image and reducing the deviation between the output of the later layers of the network and the original input image content, which can both supplement information while avoiding gradient disappearance or gradient explosion when the network is deeper. After four residual blocks are extracted to the key features in the input CT image, two stage structure in CvT are connected to help obtain the global information.</p><p>A single stage of CvT is shown in (A) of <xref ref-type="fig" rid="fig_3">Figure 3</xref>. The stage consists of two parts, the Convolutional Token Embedding layer and the Convolutional Transformer Block. The feature map with the shape of (N, C, H, W) is used as the input of the Convolutional Token Embedding layer. The details are as follows: the first step is to downsample the feature map after the input passes through a two-dimensional convolution layer, and the second step is to transform the two-dimensional feature map into a sequence shape (N, H*W, C) by reshape operation for layer normalization and input into the Transformer Token Embedding module, where the length of the sequence becomes shorter. Similar to convolution, without padding, the size of the feature map decreases after downsampling. The length of the sequence generated by the signature graph after the Convolutional Token Embedding module is reduced to simulate the convolution down-sampling operation. Unlike convolution, the Convolutional Token Embedding layer not only extracts local information from the image, but also long-range information with the participation of sequence. The sequence vector is reshaped to get a feature map, and the reshape after padding and convolution operations is the previous sequence shape, resulting in Q, K, V, respectively. Q is obtained when stride=1, K is obtained when step size is increased to 2, V is obtained separately. This can be formulated as:</p>
              
                <disp-formula>
                  <label>(2)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msubsup>
                      <mi>x</mi>
                      <mi>i</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mi>q</mi>
                        <mi>k</mi>
                        <mi>v</mi>
                        <mrow data-mjx-texclass="ORD">
                          <mo>/</mo>
                        </mrow>
                        <mrow data-mjx-texclass="ORD">
                          <mo>/</mo>
                        </mrow>
                      </mrow>
                    </msubsup>
                    <mo>=</mo>
                    <mi>Flatten</mi>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mrow data-mjx-texclass="OP">
                        <mi>Conv</mi>
                        <mi mathvariant="normal">d</mi>
                        <mn>2</mn>
                      </mrow>
                      <mrow data-mjx-texclass="INNER">
                        <mo data-mjx-texclass="OPEN">(</mo>
                        <mo>,</mo>
                        <mo data-mjx-texclass="CLOSE">)</mo>
                        <mi>Reshape</mi>
                        <mi>D</mi>
                        <mi>s</mi>
                        <mn>2</mn>
                        <mrow data-mjx-texclass="INNER">
                          <mo data-mjx-texclass="OPEN">(</mo>
                          <mo data-mjx-texclass="CLOSE">)</mo>
                          <msub>
                            <mi>x</mi>
                            <mi>i</mi>
                          </msub>
                        </mrow>
                      </mrow>
                    </mrow>
                  </math>
                </disp-formula>
              
              <p style="text-align: justify"><inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msubsup>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="normal">x</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">i</mi>
        </mrow>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">q</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mo>/</mo>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">k</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mo>/</mo>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="normal">v</mi>
        </mrow>
      </mrow>
    </msubsup>
  </math>
</inline-formula> is the token input for Q/K/V matrices at layer i. xi is the original token before Convolution Projection. S is the size of convolution kernel size. Conv2d refers to a deep-wise separable convolution. (B) shows this process, which is called Convolutional Projection. Due to the change of step size, K and V are shortened after reshape. As shown in (C), when they enter the Multi-Head Self-Attention module and MLP, the number of neurons and the algorithm of network model will be greatly reduced. In summary, CVT not only guarantees the dynamic attention and global context of Transformer, but also preserves the invariance of translation, scaling and distortion in convolution neural networks. So we introduced CVT into our research work, taking out the individual stages in the network and fusing them into CycleGAN. And the head parameter of multi-head attention (MHA) is set to 2, which helps our model to focus on different aspects of information and capture abundant features. The number of perations of the Convolutional Transformer Block is also set to 2 in our model. Based on the global information of CvT, three residual blocks are used for further feature extraction. This part is called transformer module, which can transform the image from one domain to another.</p>
              
                <fig id="fig_3">
                  <label>Figure 3</label>
                  <caption>
                    <title>(A) one of the stages of the hierarchical multi-stagestructure facilitated by the Convolutional Token Embedding layer. (B) Convolutional projection with stride = 1 and stride = 2 to generate query(Q), key(K), value(V). (C) details of the Convolutional Transformer Block, where the first layer is Convolution Projection NC2C-TransCycleGAN optimizes the decoder of the vanilla CycleGAN</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_s7JxpPgR4N9T22-t.png"/>
                </fig>
              
              <p>The decoder is similar to the inverse process of the encoder. The vanilla CycleGAN completes the upsampling operation using the transposed convolution operation shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>, with the aim of expanding the feature map size while reducing the low-level features from the feature vector, and thus obtaining the synthetic CT generated by the network. However, since the size of the convolution kernel represented by the blue box is 3, it is not divisible by the step size of 2. Therefore, it can be seen in the feature map of the input of this figure that the region slid by the blue convolution kernel will have the uneven overlapping trajectories indicated in green in the figure, which is the checkerboard artifacts caused by transposed convolution. The actual performance of checkerboard artifacts on the synthetic enhanced CT task is shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>. Although checkerboard artifacts can theoretically be avoided by learning the weights, they are still an insurmountable problem in the actual image reconstruction process. However, since transposed convolution has the property of learning parameters and makes a valuable contribution to the final result, NC2C-TransCycleGAN uses PixelShuffle operation in another branch parallel to transposed convolution. The PixShuffle operation, also known as sub-pixel convolution, is an up-sampling method suitable for image super-resolution tasks. It converts a low-resolution feature map into a high-resolution feature map by using ordinary convolution coupled with a pixel shuffle operation. To further enhance image recovery, the Channel Unit (CA) module and the Positional Unit (POS) module are often introduced when using the PixShuffle operation. Unlike the traditional direct interpolation method, the PixelShuffle operation is based on a low-resolution feature image of size H*W*C, convolved to obtain a feature image with r2 channels, where r denotes the magnification factor, and finally a high-resolution feature map of rH*rW*C is reconstructed by periodic shuffling. The output result generated by this branch of PixelShuffle in NC2C-TransCycleGAN performs concat operation with the result generated by the transposed convolution branch to generate the final synthetic CT image. <xref ref-type="fig" rid="fig_5">Figure 5</xref> shows the implementation flow of the NC2C-TransCycleGAN algorithm.</p>
              
                <fig id="fig_4">
                  <label>Figure 4</label>
                  <caption>
                    <title>The principle of checkerboard artifacts in two-dimensional image</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_3u4rk944mjRs83cq.png"/>
                </fig>
              
              
                <fig id="fig_5">
                  <label>Figure 5</label>
                  <caption>
                    <title>The pipeline of the Generator network architecture of proposed NC2C-TransCycleGAN</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_OfIT7eIanoh2Xy8r.png"/>
                </fig>
              
              
                <fig id="fig_6">
                  <label>Figure 6</label>
                  <caption>
                    <title>The pipeline of the Generator network architecture of proposed NC2C-TransCycleGAN</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_xk_nrZAJP4DCgdc9.png"/>
                </fig>
              
              <p>The discriminator is a PatchGAN network consisting of five convolution layers. The image of the input discriminator passes through multiple convolution layers to produce a 30*30 probability matrix output, where each convolution layer is composed of convolution, instanceNorm, and LeakyReLU operations in turn. each element of the probability matrix has a true probability for a 70*70 area of the original input NCCT image. This method is more efficient than trying to view the entire input. The discriminator network structure is shown in the <xref ref-type="fig" rid="fig_6">Figure 6</xref>.</p><p>The experimental environment for this study is configured as follows: the graphics card model is NVIDIA Tesla P100; the video memory is 16 GB; CUDA version is 11.3; the operating system is Linux; the development language and version is Python 3.9; the development framework and version is PyTorch 1.8.1. In the training process of the experiment, we set the hyperparameters of the network as follows: for the learning rate scheduler, we use ReduceLROnPlateau in Pytorch to adaptively adjust the learning rate, which can detect the dynamics of the specified indicators within a specified number of times, and automatically decay the learning rate when the loss curve is no longer falling or the accuracy indicator is no longer rising, allowing the network to obtain better learning results, where the main parameters corresponding to the adaptive method. The initial learning rate of ReduceLROnPlateau is set to 0.005, and the patience is 7, the learning rate reduction coefficient is 0.01, batchsize is set to 1, the beta value of the Adam optimizer is set to 0.5, and the weighing parameter λ1 is set to 0.5. The loss function for network training uses the same loss function used for training in the vanilla CycleGAN.</p>
            </sec>
          
          
            <sec>
              
                <title>2.2.3 Evaluation method</title>
              
              <p>The CT image synthesis task can essentially be viewed as an image processing problem, so the quality of images generated by GAN networks can be measured using commonly used image quality evaluation criteria. In order to objectively compare the image synthesis effects of different networks, this study will introduce three evaluation metrics, PSNR (peak signal-to-noise ratio), SSIM (structural similarity), and MSE (mean squared error), which are commonly used internationally (<xref ref-type="bibr" rid="ref_16">Prodan et al., 2023</xref>) to quantitatively evaluate and compare the performance of three networks, NC2C-TransCycleGAN, original CycleGAN, and Pix2Pix. These indicators are widely used in the evaluation of medical image synthesis. The MSE formula is as follows, where sCECT represents the virtual image synthesized by the network:</p>
              
                <disp-formula>
                  <label>(3)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>M</mi>
                    <mi>S</mi>
                    <mi>E</mi>
                    <mi>CECT</mi>
                    <mi>i</mi>
                    <mi>s</mi>
                    <mi>CECT</mi>
                    <mi>i</mi>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>−</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mfrac>
                      <mn>1</mn>
                      <mi>N</mi>
                    </mfrac>
                    <munderover>
                      <mo data-mjx-texclass="OP">∑</mo>
                      <mrow data-mjx-texclass="ORD">
                        <mi>i</mi>
                        <mo>=</mo>
                        <mn>1</mn>
                      </mrow>
                      <mi>N</mi>
                    </munderover>
                    <msup>
                      <mo stretchy="false">)</mo>
                      <mn>2</mn>
                    </msup>
                  </math>
                </disp-formula>
              
              <p>where, <italic>N </italic>denotes the total number of pixels in the image area, and i is the index of the aligned pixels in the image area.</p><p>PSNR is a commonly used index for evaluating image restoration. It is used to represent the ratio between the maximum signal of the image and the background noise. The larger the value, the better the image quality. The PSNR is defined as:</p>
              
                <disp-formula>
                  <label>(4)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>P</mi>
                    <mi>S</mi>
                    <mi>N</mi>
                    <mi>R</mi>
                    <mo>=</mo>
                    <mo>⋅</mo>
                    <mo data-mjx-texclass="NONE">⁡</mo>
                    <mn>10</mn>
                    <msub>
                      <mi>log</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mn>10</mn>
                      </mrow>
                    </msub>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">(</mo>
                      <mo data-mjx-texclass="CLOSE">)</mo>
                      <mfrac>
                        <mrow>
                          <mi>M</mi>
                          <mi>A</mi>
                          <msup>
                            <mi>X</mi>
                            <mn>2</mn>
                          </msup>
                        </mrow>
                        <mrow>
                          <mi>M</mi>
                          <mi>S</mi>
                          <mi>E</mi>
                        </mrow>
                      </mfrac>
                    </mrow>
                  </math>
                </disp-formula>
              
              <p>MAX represents the maximum pixel value of ground truth CECT and sCECT images.</p><p>SSIM measures image similarity from three aspects: brightness, contrast, and structure. It is the most commonly used index to measure the performance of image reconstruction. The closer the SSIM value is to 1.0, the closer the generated enhanced CT is to the real enhanced CT and its formula can be expressed as:</p>
              
                <disp-formula>
                  <label>(5)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>SSIM</mi>
                    <mo>=</mo>
                    <mfrac>
                      <mrow>
                        <mrow data-mjx-texclass="INNER">
                          <mo data-mjx-texclass="OPEN">(</mo>
                          <mo>+</mo>
                          <mo data-mjx-texclass="CLOSE">)</mo>
                          <mn>2</mn>
                          <msub>
                            <mi>μ</mi>
                            <mrow data-mjx-texclass="ORD">
                              <mi>C</mi>
                              <mi>E</mi>
                              <mi>C</mi>
                              <mi>T</mi>
                            </mrow>
                          </msub>
                          <msub>
                            <mi>μ</mi>
                            <mrow data-mjx-texclass="ORD">
                              <mi>s</mi>
                              <mi>C</mi>
                              <mi>E</mi>
                              <mi>C</mi>
                              <mi>T</mi>
                            </mrow>
                          </msub>
                          <msub>
                            <mi>c</mi>
                            <mn>1</mn>
                          </msub>
                        </mrow>
                        <mrow data-mjx-texclass="INNER">
                          <mo data-mjx-texclass="OPEN">(</mo>
                          <mo>+</mo>
                          <mo data-mjx-texclass="CLOSE">)</mo>
                          <mn>2</mn>
                          <msub>
                            <mi>σ</mi>
                            <mrow data-mjx-texclass="ORD">
                              <mi>C</mi>
                              <mi>E</mi>
                              <mi>C</mi>
                              <mi>T</mi>
                              <mi>S</mi>
                              <mi>C</mi>
                              <mi>E</mi>
                              <mi>C</mi>
                              <mi>T</mi>
                              <mo>⋅</mo>
                            </mrow>
                          </msub>
                          <msub>
                            <mi>c</mi>
                            <mn>2</mn>
                          </msub>
                        </mrow>
                      </mrow>
                      <mrow>
                        <mrow data-mjx-texclass="INNER">
                          <mo data-mjx-texclass="OPEN">(</mo>
                          <mo>+</mo>
                          <mo>+</mo>
                          <mo data-mjx-texclass="CLOSE">)</mo>
                          <msubsup>
                            <mi>μ</mi>
                            <mrow data-mjx-texclass="ORD">
                              <mi>C</mi>
                              <mi>E</mi>
                              <mi>C</mi>
                              <mi>T</mi>
                            </mrow>
                            <mn>2</mn>
                          </msubsup>
                          <msubsup>
                            <mi>μ</mi>
                            <mrow data-mjx-texclass="ORD">
                              <mi>s</mi>
                              <mi>C</mi>
                              <mi>E</mi>
                              <mi>C</mi>
                              <mi>T</mi>
                            </mrow>
                            <mn>2</mn>
                          </msubsup>
                          <msub>
                            <mi>c</mi>
                            <mn>1</mn>
                          </msub>
                        </mrow>
                        <mrow data-mjx-texclass="INNER">
                          <mo data-mjx-texclass="OPEN">(</mo>
                          <mo>+</mo>
                          <mo>+</mo>
                          <mo data-mjx-texclass="CLOSE">)</mo>
                          <msubsup>
                            <mi>σ</mi>
                            <mrow data-mjx-texclass="ORD">
                              <mi>C</mi>
                              <mi>E</mi>
                              <mi>C</mi>
                              <mi>T</mi>
                            </mrow>
                            <mn>2</mn>
                          </msubsup>
                          <msubsup>
                            <mi>σ</mi>
                            <mrow data-mjx-texclass="ORD">
                              <mi>s</mi>
                              <mi>C</mi>
                              <mi>E</mi>
                              <mi>C</mi>
                              <mi>T</mi>
                            </mrow>
                            <mn>2</mn>
                          </msubsup>
                          <msub>
                            <mi>c</mi>
                            <mn>2</mn>
                          </msub>
                        </mrow>
                      </mrow>
                    </mfrac>
                  </math>
                </disp-formula>
              
            </sec>
          
        </sec>
      
    </sec>
    <sec sec-type="results">
      <title>3. Results</title>
      <p>We compare the NC2C-TransCycleGAN with Pix2Pix and CycleGAN, which perform well in image generation. The average value of all test slice indexes is calculated, and the result of the average value is shown in <xref ref-type="table" rid="table_2">Table 2</xref>. It can be seen from the results that our proposed method is higher than other methods in these three indicators. Among them, the SSIM value of the image generated by our method is 2.00% and 1.39% improved by the Pix2Pix and CycleGAN, and the PSNR value is 2.05% and 1.71% better than that of the Pix2Pix and CycleGAN, respectively. Compared with Pix2Pix and CycleGAN, our method improved MSE value by 12.50% and 8.53% respectively. This is due to the existence of cycle loss, which reduces CycleGAN’s requirements for data pairing. Although each pair of data in our data set comes from the same patient, the layer-to-layer mismatch caused by the breathing movement causes the layer-to-layer correspondence to be incomplete. This will have a great impact on the results of the Pix2Pix network. Because our method introduces the CvT structure, the network can also focus on more global information while extracting features, making the prediction of tumor blood supply more accurate than CycleGAN. Compared with CycleGAN using transposed convolution as upsampling, PixShuffle method makes a great contribution to the clarity of the generated image. <xref ref-type="fig" rid="fig_7">Figure 7</xref> is helpful to observe the improvement of checkerboard artifacts by our network, and we compared different lung locations in patients with lung cancer. We compare the effects of four groups of lung tomography, where each group contains tomographic images of the same positon from two patients with lung cancer. The first and second rows in (A) are CT images of the same location layer in two patients, as are (B), (C), and (D). The first column in the figure is NNCT, the second column is the real CECT, the third column is the result of synthetic CECT generated by Pix2Pix, the fourth column is the result generated by CycleGAN, and the last column is the result of NC2C-TransCycleGAN. The enlarged area and whole CT slice show that the better effect of our method is not accidental, but has good wide applicability. As can be seen in the images of the first patients of (A) and (C), the virtual CECT images generated by our network have an enhanced effect on the presence of a blood supply location that is closer to the ground truth. From the images of the first patient in (B) and (D), it can be seen that our proposed method can effectively reduce the influence of chessboard artifacts at cardiac tomography and other locations. The shape, image contrast and the course of the blood vessel section generated from the first patient of (A) and the second patient of (B), (C) and (D) are also closer to the reality, and pay great attention to the generation of details.</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>Comparison of test metrics results of three models</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Evaluation Criteria</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Pix2Pix</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">CycleGAN</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">NC2C-TransCycleGAN</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">PSNR(dB)</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">19.9046</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">19.9718</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">20.3130</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">SSIM</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.8378</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.8429</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.8546</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">MSE</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">734.9404</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">703.0354</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">643.0944</span></p></td></tr></tbody></table>
        </table-wrap>
      
      <p>In addition, the data related to the experiment were statistically analyzed using SPSS (https://www.ibm.com/cn-zh/analytics/spss-statistics-software) statistical software (Windows version 22.0) in this paper. As appropriate, chi-square test or univariate analysis of variance was used to compare the age, sex, smoking, cancer type, metastasis, lesion location, and image thickness of lung cancer patients between the training set and the test set. No significant difference was found in patient feature between the training and test datasets (p=0.081-0.598, p &gt;0.05). Detailed results are given in <xref ref-type="table" rid="table_3">Table 3</xref>. The Kruskal-Wallis test was used to compare the evaluation indexes of the three groups model CVT, CycleGAN, Pix2Pix, and the independent samples Ttest was used to compare the results of model indexes between each two groups. The statistical results in <xref ref-type="table" rid="table_4">Table 4</xref> show that the synthetic CECT results generated by the three models are very significantly and statistically different (&lt;0.001).</p>
      
        <table-wrap id="table_3">
          <label>Table 3</label>
          <caption>
            <title>Patient characteristics and statistics</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Variable</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">χ</span><sup><span style="font-family: Times New Roman, serif">2</span></sup></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Male Number</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Age(years)</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">/</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.551</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Sex</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.356</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.213</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Type</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">1.386</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.500</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Metastatic</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.495</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.482</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Position</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.278</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.598</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Thickness</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">3.054</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">0.081</span></p></td></tr></tbody></table>
        </table-wrap>
      
      
        <table-wrap id="table_4">
          <label>Table 4</label>
          <caption>
            <title>Statistical differences about TransCycleGAN,Cyclegan and Pix2Pix method</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">Model</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">PSNR </span><italic><span style="font-family: Times New Roman, serif">p</span></italic><span style="font-family: Times New Roman, serif">-value</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">MSE </span><italic><span style="font-family: Times New Roman, serif">p</span></italic><span style="font-family: Times New Roman, serif">-value</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">SSIM </span><italic><span style="font-family: Times New Roman, serif">p</span></italic><span style="font-family: Times New Roman, serif">-value</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">NC2C-TransCycleGAN</span></p><p style="text-align: center"><span style="font-family: Times New Roman, serif">CycleGAN</span></p><p style="text-align: center"><span style="font-family: Times New Roman, serif">Pix2Pix</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">2.46E - 43</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">2.46E - 43</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">8.84E - 48</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">NC2C-TransCycleGAN</span></p><p style="text-align: center"><span style="font-family: Times New Roman, serif">CycleGAN</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">5.31E - 16</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">9.57E - 14</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">5.64E - 26</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">NC2C-TransCycleGAN</span></p><p style="text-align: center"><span style="font-family: Times New Roman, serif">Pix2Pix</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">1.34E - 32</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">6.10E - 10</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="font-family: Times New Roman, serif">3.84E - 11</span></p></td></tr></tbody></table>
        </table-wrap>
      
      
        <fig id="fig_7">
          <label>Figure 7</label>
          <caption>
            <title>The synthetic CT results at four different positions produced by the three models</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_NoqxHFy1uhMoe47o.jpeg"/>
        </fig>
      
    </sec>
    <sec sec-type="discussion">
      <title>4. Discussion</title>
      <p>The complexity of the blood supply around the tumor requires the generation of models with strong learning ability. To explore the capability of NC2C-TransCycleGAN in generating image details, we used the difference images as a reference in <xref ref-type="fig" rid="fig_8">Figure 8</xref> and selected a more diverse and complex CECT layer of lung cancer patients. In <xref ref-type="fig" rid="fig_8">Figure 8</xref>, the first row of (A) shows the results of CECT, Pix2Pix-generated images, CycleGAN, and NC2C-TransCycleGAN, respectively. The first column of the second row of (A)shows the NCCT images, and the remaining columns show the synthetic CECT generated by Pix2Pix, CycleGAN, and NC2C-TransCycleGAN, respectively and the difference images between the real CECT. From the visualization of the generated results and the results marked by the red magnified area, it can be seen that for the same locations of the CT images, our network control generates details and judgments about the presence or absence of the enhanced area are more accurate than Pix2Pix and CycleGAN. As in difference image of (A), our network accurately generates the burr signs of the lesions, which shows that our network has a strong ability to extract the features of the lesions during training. In difference image of (B) and (C), even the liquefied face, our network can generate an result closer to that of a true image liquefied surface and more similar contrast, and is more adept at generating details of the image. In difference image of (D), the shape of the bronchial bifurcation and its surroundings, the generation results of our network are closer to the real images than those of other networks. In summary, NC2C-TransCycleGAN can produce higher quality mediastinum, blood vessels, lesions, liquefaction surfaces and other areas, which shows that our method has greater potential to help doctors diagnose patients or medical dataset augmentation.</p>
      
        <fig id="fig_8">
          <label>Figure 8</label>
          <caption>
            <title>The synthetic CT results at four different positions produced by the three models</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_5l9jHhf78grMwond.jpeg"/>
        </fig>
      
      <p style="text-align: justify">In conclusion, although the resulting enhanced CT images show differences in subtle details from the ground truth enhanced CT images, the comparison results show that by improving CycleGAN, our proposed NC2C-TransCycleGAN network has better image quality and potential to accomplish this task. The results of the models in this study can assist in the training of deep learning models, such as pretraining or data enhancement techniques. In the future, our method will expand to generate enhanced 3D CT images and be used as an extended synthetic reality training dataset for lung cancer detection to compensate for the lack of data in the real image distribution. The conversion of low-dose CT to normal-dose flat-scan CT can also be accomplished with the help of deep learning methods, thus helping to reduce the radiation dose ingested into the patient's body.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>All authors have completed the ICMJE uniform disclosure form. The authors have no other conflicts of interest to declare.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <page-range>arXiv:2006.03374</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Agrawal</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Kori</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Anand</surname>
              <given-names>V. K.</given-names>
            </name>
            <name>
              <surname>Krishnamurthi</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2006.03374</pub-id>
          <article-title>Structurally aware bidirectional unpaired image to image translation between CT and MR</article-title>
          <source>arXiv preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>53</volume>
          <page-range>3381-3397</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Beji</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Blaiech</surname>
              <given-names>A. G.</given-names>
            </name>
            <name>
              <surname>Said</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Abdallah</surname>
              <given-names>A. B.</given-names>
            </name>
            <name>
              <surname>Bedoui</surname>
              <given-names>M. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10489-022-03612-6</pub-id>
          <article-title>An innovative medical image synthesis based on dual GAN deep neural networks for improved segmentation quality</article-title>
          <source>Appl. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>82</volume>
          <page-range>37573-37592</page-range>
          <issue>24</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bushara</surname>
              <given-names>A. R.</given-names>
            </name>
            <name>
              <surname>Vinod Kumar</surname>
              <given-names>R. S.</given-names>
            </name>
            <name>
              <surname>Kumar</surname>
              <given-names>S. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-023-14893-1</pub-id>
          <article-title>LCD-capsule network for the detection and classification of lung cancer on computed tomography images</article-title>
          <source>Multimed. Tools Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>41</volume>
          <page-range>ehaa946-0156</page-range>
          <issue>Supplement_2</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chandrashekar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Shivakumar</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Lapolla</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Handa</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Grau</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1093/ehjci/ehaa946.0156</pub-id>
          <article-title>A deep learning approach to generate contrast-enhanced computerised tomography angiograms without the use of intravenous contrast agents</article-title>
          <source>Eur. Heart J.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>229</volume>
          <page-range>107200</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>X.  et al.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.cmpb.2023.107200</pub-id>
          <article-title>Multi-domain medical image translation generation for lung image classification based on generative adversarial networks</article-title>
          <source>Comput. Methods Programs Biomed.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>18330-18357</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Dash</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ye</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/access.2023.3346273</pub-id>
          <article-title>A review of Generative Adversarial Networks (GANs) and its applications in a wide variety of disciplines: From medical to remote sensing</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>17906-17921</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Esmaeili</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Toosi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Roshanpoor</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Changizi</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Ghazisaeedi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Rahmim</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sabokrou</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/access.2023.3244741</pub-id>
          <article-title>Generative adversarial networks for anomaly detection in biomedical imaging: A study on seven medical image datasets</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>27</volume>
          <page-range>5495-5505</page-range>
          <issue>11</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Fan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Lou</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zeng</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/jbhi.2023.3311021</pub-id>
          <article-title>Cross-parametric generative adversarial network-based magnetic resonance image feature synthesis for breast lesion classification</article-title>
          <source>IEEE Journal of Biomedical and Health Informatics</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>39</volume>
          <page-range>3070-3079</page-range>
          <issue>6Part1</issue>
          <year>2012</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hertanto</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Dzyubak</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Rimner</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Mageras</surname>
              <given-names>Gig S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1118/1.4711802</pub-id>
          <article-title>Reduction of irregular breathing artifacts in respiration‐correlated CT images using a respiratory motion model</article-title>
          <source>Medical Physics</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>42</volume>
          <page-range>1799-1808</page-range>
          <issue>6</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kelkar</surname>
              <given-names>V. A.</given-names>
            </name>
            <name>
              <surname>Gotsis</surname>
              <given-names>D. S.</given-names>
            </name>
            <name>
              <surname>Brooks</surname>
              <given-names>F. J.</given-names>
            </name>
            <name>
              <surname>Prabhat</surname>
              <given-names>K. C.</given-names>
            </name>
            <name>
              <surname>Myers</surname>
              <given-names>K. J.</given-names>
            </name>
            <name>
              <surname>Zeng</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Anastasio</surname>
              <given-names>M. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/tmi.2023.3241454</pub-id>
          <article-title>Assessing the ability of generative adversarial networks to learn canonical medical image statistics</article-title>
          <source>IEEE Transactions on Medical Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>245104</page-range>
          <issue>24</issue>
          <year>2010</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kojima</surname>
              <given-names>Chie</given-names>
            </name>
            <name>
              <surname>Umeda</surname>
              <given-names>Yasuhito</given-names>
            </name>
            <name>
              <surname>Ogawa</surname>
              <given-names>Mikako</given-names>
            </name>
            <name>
              <surname>Harada</surname>
              <given-names>Atsushi</given-names>
            </name>
            <name>
              <surname>Magata</surname>
              <given-names>Yasuhiro</given-names>
            </name>
            <name>
              <surname>Kono</surname>
              <given-names>Kenji</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1088/0957-4484/21/24/245104</pub-id>
          <article-title>X-ray computed tomography contrast agents prepared by seeded growth of gold nanoparticles in PEGylated dendrimer</article-title>
          <source>Nanotechnology</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <page-range>3725-3731</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>P.</given-names>
              <surname>Komorowski</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Baniecki</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Biecek</surname>
            </name>
          </person-group>
          <article-title>Towards evaluating explanations of vision transformers for medical imaging</article-title>
          <source>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Vancouver, BC, Canada</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>40</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lahiri</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Maji</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Potdar</surname>
              <given-names>Pravin D.</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Parikh</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Bisht</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Mukherjee</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Paul</surname>
              <given-names>Manash K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s12943-023-01740-y</pub-id>
          <article-title>Lung cancer immunotherapy: Progress, pitfalls, and promises</article-title>
          <source>Molecular Cancer</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>48</volume>
          <page-range>3559-3571</page-range>
          <issue>7</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Maier</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lebedev</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Erath</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Eulig</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Sawall</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Fournié</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Stierstorfer</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Lell</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kachelrieß</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1002/mp.14927</pub-id>
          <article-title>Deep learning‐based coronary artery motion estimation and compensation for short‐scan cardiac CT</article-title>
          <source>Medical Physics</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>5521</page-range>
          <issue>9</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Maurício</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Domingues</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Bernardino</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app13095521</pub-id>
          <article-title>Comparing vision transformers and convolutional neural networks for image classification: A literature review</article-title>
          <source>Applied Sciences</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>161-185</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Prodan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Vlăsceanu</surname>
              <given-names>G. V.</given-names>
            </name>
            <name>
              <surname>Boiangiu</surname>
              <given-names>C. A.</given-names>
            </name>
          </person-group>
          <article-title>Comprehensive evaluation of metrics for image resemblance</article-title>
          <source>J. Inf. Syst. Oper. Manag.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>69</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Skandarani</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Jodoin</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Lalande</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/jimaging9030069</pub-id>
          <article-title>Gans for medical image synthesis: An empirical study</article-title>
          <source>Journal of Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="conf-paper">
          <volume>37</volume>
          <page-range>2375-2383</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>B.</given-names>
              <surname>Sun</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Jiang</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Fu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1609/aaai.v37i2.25333</pub-id>
          <article-title>Hybrid pixel-unshuffled network for lightweight image super-resolution</article-title>
          <source>In Proceedings of the AAAI Conference on Artificial Intelligence, Washington, DC, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="conf-paper">
          <page-range>702-712</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>D.</given-names>
              <surname>Torbunov</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Huang</surname>
            </name>
            <name>
              <given-names>H.  et al.</given-names>
              <surname>Yu</surname>
            </name>
          </person-group>
          <article-title>Uvcgan: Unet vision transformer cycle-consistent gan for unpaired image-to-image translation</article-title>
          <source>In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, Waikoloa, Hawaii, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>52-58</page-range>
          <issue>4</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Vyas</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Rajendran</surname>
              <given-names>R. M</given-names>
            </name>
          </person-group>
          <article-title>Generative adversarial networks for anomaly detection in medical images</article-title>
          <source>Int. J. Multidiscipl. Innov. Res. Methodol.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>42</volume>
          <page-range>3362-3373</page-range>
          <issue>11</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Clinton J.</given-names>
            </name>
            <name>
              <surname>Rost</surname>
              <given-names>Natalia S.</given-names>
            </name>
            <name>
              <surname>Golland</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/tmi.2023.3283948</pub-id>
          <article-title>Spatial-intensity transforms for medical image-to-image translation</article-title>
          <source>IEEE Transactions on Medical Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>61</volume>
          <page-range>1-14</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Zhuang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Plaza</surname>
              <given-names>Antonio J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/tgrs.2023.3276175</pub-id>
          <article-title>PDBSNet: Pixel-shuffle downsampling blind-spot reconstruction network for hyperspectral anomaly detection</article-title>
          <source>IEEE Transactions on Geoscience and Remote Sensing</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>20</volume>
          <page-range>e138586</page-range>
          <issue>4</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.5812/ijradiol-138586</pub-id>
          <article-title>Study of low kV, low contrast agent dosage, and low contrast agent flow rate scan in computed tomography angiography of children's liver</article-title>
          <source>IJ Radiology</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>1113-1125</page-range>
          <issue>4</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhong</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Ding</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>Y. F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/tetci.2023.3243920</pub-id>
          <article-title>Multi-scale attention generative adversarial network for medical image enhancement</article-title>
          <source>IEEE Transactions on Emerging Topics in Computational Intelligence</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>