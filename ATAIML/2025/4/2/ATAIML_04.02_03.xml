<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-UdQotbfUWPdLAiHvl0mt_TsrWa8zUgzq</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml040203</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Enhancing Non-Invasive Diagnosis of Endometriosis Through Explainable Artificial Intelligence: A Grad-CAM Approach</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7235-7744</contrib-id>
          <name>
            <surname>Kuyoro</surname>
            <given-names>Afolashade Oluwakemi</given-names>
          </name>
          <email>kuyoros@babcock.edu.ng</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-7197-2615</contrib-id>
          <name>
            <surname>Fatade</surname>
            <given-names>Oluwayemisi Boye</given-names>
          </name>
          <email>fatadeo@babcock.edu.ng</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6889-8916</contrib-id>
          <name>
            <surname>Onuiri</surname>
            <given-names>Ernest Enyinnaya</given-names>
          </name>
          <email>onuirie@babcock.edu.ng</email>
        </contrib>
        <aff id="aff_1">Department of Computer Science, Babcock University Ilishan Remo, 121103 Ilishan-Remo, Nigeria</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>23</day>
        <month>04</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>2</issue>
      <fpage>97</fpage>
      <lpage>108</lpage>
      <page-range>97-108</page-range>
      <history>
        <date date-type="received">
          <day>04</day>
          <month>03</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>17</day>
          <month>04</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Significant advancements in artificial intelligence (AI) have transformed clinical decision-making, particularly in disease detection and management. Endometriosis, a chronic and often debilitating gynecological disorder, affects a substantial proportion of reproductive-age women and is associated with pelvic pain, infertility, and a reduced quality of life. Despite its high prevalence, non-invasive and accurate diagnostic methods remain limited, frequently resulting in delayed or missed diagnoses. In this study, a novel diagnostic framework was developed by integrating deep learning (DL) with explainable artificial intelligence (XAI) to address existing limitations in the early and non-invasive detection of endometriosis. Abdominopelvic magnetic resonance imaging (MRI) data were obtained from the Crestview Radiology Center in Victoria Island, Lagos State. Preprocessing procedures, including Digital Imaging and Communications in Medicine (DICOM)-to-PNG conversion, image resizing, and intensity normalization, were applied to standardize the imaging data. A U-Net architecture enhanced with a dual attention mechanism was employed for lesion segmentation, while Gradient-weighted Class Activation Mapping (Grad-CAM) was incorporated to visualize and interpret the model’s decision-making process. Ethical considerations, including informed patient consent, fairness in algorithmic decision-making, and mitigation of data bias, were rigorously addressed throughout the model development pipeline. The proposed system demonstrated the potential to improve diagnostic accuracy, reduce diagnostic latency, and enhance clinician trust by offering transparent and interpretable predictions. Furthermore, the integration of XAI is anticipated to promote greater clinical adoption and reliability of AI-assisted diagnostic systems in gynecology. This work contributes to the advancement of non-invasive diagnostic tools and reinforces the role of interpretable DL in the broader context of precision medicine and women's health.</p></abstract>
      <kwd-group>
        <kwd>AI</kwd>
        <kwd>Endometriosis</kwd>
        <kwd>Grad-CAM</kwd>
        <kwd>Non-invasive diagnosis</kwd>
        <kwd>U-Net</kwd>
        <kwd>XAI</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="3"/>
        <fig-count count="6"/>
        <table-count count="5"/>
        <ref-count count="27"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>AI has grown at an exponential rate over the last ten years, completely changing the technological landscape [<xref ref-type="bibr" rid="ref_1">1</xref>]. It has evolved beyond its original purpose of automating human labor. It's currently causing a paradigm change in the way people approach problems and come up with solutions. This revolution is a sharp contrast to the early days of computing when the purpose of machines was to merely increase human efficiency through fundamental computations. AI's revolutionary potential is causing a fundamental revolution in every part of the world, affecting everything from product creation to medical diagnosis. Similarly, Lutomski et al. [<xref ref-type="bibr" rid="ref_2">2</xref>] also explained that AI now has some subfields under it, especially in the practice of medicine such as computer vision (CV), DL and machine learning (ML).</p><p>In recent years, healthcare has experienced a lot of innovation concerning the advent of AI. In the aspect of disease diagnosis, detection, management and treatment, AI provides a more robust level of analysis for huge biomedical datasets. This has led to a great reduction in overall time spent on diagnosis together with lower costs on manpower and other linked resources. Indeed, healthcare digitalization brought about by AI is a plus to the medical world. In the field of women’s health from obstetrics to gynecology, ML, as a subset of AI, has several methods, including logistic regression, support vector machines (SVMs) and many others, which have shown great potential to aid in the prediction of results for the diagnosis of endometriosis [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>], [<xref ref-type="bibr" rid="ref_5">5</xref>]. Given the diversity of its use in the clinical context, there is great potential to apply ML to improve non-invasive diagnosis in endometriosis to reduce the delays and human error associated with diagnosis.</p><p>Endometriosis, a chronic gynecological condition, significantly impacts women's quality of life, causing pain and potential infertility [<xref ref-type="bibr" rid="ref_6">6</xref>]. It is categorized by endometrial-like tissue which is seen outside the uterus, and it is a persistent, estrogen-related condition. Inflammatory reactions and tissue damage are the outcomes of this abnormality. It is still difficult to confirm exactly how prevalent endometriosis is. However, estimates place it at 10% of women in reproductive age having the disease, with 30-50% of them reporting pelvic pain and/or infertility. Despite its prevalence, non-invasive diagnosis remains challenging. Researchers have explored various ML algorithms using data from symptoms, genetics, blood tests, and imaging. Approaches like logistic regression and Least Absolute Shrinkage and Selection Operator (LASSO) regression have shown promise [<xref ref-type="bibr" rid="ref_7">7</xref>]. However, significant limitations remain, hindering clinical adoption and patient outcomes. One main limitation is that many ML models employed in current research are "black boxes," meaning their decision-making processes are not transparent [<xref ref-type="bibr" rid="ref_8">8</xref>]. This lack of interpretability makes it difficult for healthcare providers to understand how diagnoses are reached, limiting trust and hindering widespread clinical adoption.</p><p>In the application of ML techniques for the diagnosis of endometriosis in a bid to lessen the burdens on women experiencing endometriosis and help medical practitioners diagnose it more easily and early, studies have shown that different methods can be used for the categorization and classification of endometrial tissue lesions to other tumors and inflation, such as texture analysis of MRI images to differentiate endometriosis and hemorrhagic ovarian cysts [<xref ref-type="bibr" rid="ref_9">9</xref>]. Others include an automatic DL-based segmentation model combined with Receiver Operating Characteristic (ROC) analysis of tumor-to-uterine ratio on MRI images, which can effectively diagnose early-stage endometrial cancer [<xref ref-type="bibr" rid="ref_10">10</xref>], while several studies, as highlighted by Bhardwaj et al. [<xref ref-type="bibr" rid="ref_6">6</xref>], also use clinical data combined with MRI images to predict endometriosis.</p>
    </sec>
    <sec sec-type="">
      <title>2. Related works</title>
      <p>Zhang et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] leveraged biomarkers for endometriosis prediction using multiple ML techniques, including LASSO, Stepglm, glmBoost, and random forest, to enhance predictive accuracy. Transcriptomics and methylomics data modalities were comprehensively integrated, achieving an Area Under the Curve (AUC) of 0.785, though the proposed method was limited in its applicability across broader populations. Similarly, GenomeForest, an ensemble ML classifier, demonstrated high F1-scores (0.98 for transcriptomics and 0.918 for methylomics) but was constrained by the reliance on biomarker sources from blood tests. Kurata et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] evaluated the feasibility of using U-Net for automatic uterine segmentation on MRI images. The model was tested on patients with various uterine disorders, achieving a mean Dice similarity coefficient (DSC) of 0.82. The mean DSCs for patients with and without uterine disorders were 0.84 and 0.78, respectively (<inline-formula>
  <mml:math id="mk0nmtp9tm">
    <mml:msup>
      <mml:mrow>
        <mml:mi>p</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mn>1</mml:mn>
        <mml:mn>4</mml:mn>
        <mml:mrow>
          <mml:mo>/</mml:mo>
        </mml:mrow>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> 0.19). The Mean Absolute Deviations (MADs) for patients with and without uterine disorders were 18.5 and 21.4 [pixels], respectively (<inline-formula>
  <mml:math id="mwzphv8913">
    <mml:msup>
      <mml:mrow>
        <mml:mi>p</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mn>1</mml:mn>
        <mml:mn>4</mml:mn>
        <mml:mrow>
          <mml:mo>/</mml:mo>
        </mml:mrow>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> 0.39). The scores of the visual evaluation were not significantly different between uteruses with and without uterine disorders. The results suggest that U-Net can effectively segment the uterus, regardless of the presence of disorders.</p><p>Other studies have explored imaging-based approaches. Downing et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] developed an automated classification algorithm using imaging data with random forest classifiers, while Guerriero et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] compared seven ML models using ultrasound markers, achieving an accuracy of 0.73. These approaches underscore the potential of imaging techniques but highlight the necessity for more extensive prospective studies to validate AI applications in clinical settings. ML models using clinical history and patient demographics have also been explored. Bendifallah et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] implemented logistic regression, SVM, and random forest models and achieved an AUC of 0.98. Similarly, Tore et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] developed an ML platform incorporating logistic regression, decision trees, and Shapley Additive Explanations (SHAP)-based interpretability methods, emphasizing the importance of feature attribution in clinical diagnosis. However, reliance on medical records introduced potential misclassification biases, requiring further validation with diverse datasets. Genomic and proteomic analyses also play a crucial role in ML-driven endometriosis research. Li et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] used DL techniques to diagnose endometriosis based on gene co-expression networks. Mihalyi et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] compared logistic regression and Least Squares Support Vector Machine (LSSVM) models using plasma biomarkers, reporting an AUC of 0.966 with high sensitivity and specificity. Despite promising results, these studies highlight the need for external validation to ensure generalizability. Parlatan et al. [<xref ref-type="bibr" rid="ref_19">19</xref>] investigated emerging modalities such as Raman spectroscopy, demonstrating the potential for novel non-invasive diagnostic techniques. However, limitations such as sample sizes and lack of external validation remain significant challenges.</p><p>Additionally, self-reported symptoms and questionnaire-based approaches have been explored. Knific et al. [<xref ref-type="bibr" rid="ref_20">20</xref>] and Goldstein and Cohen [<xref ref-type="bibr" rid="ref_21">21</xref>] investigated ML-based classification using patient-reported data, achieving varying degrees of success. While their approaches offer a patient-centric diagnostic tool, the need for validated questionnaires and larger sample sizes remains a gap in research. Accurate segmentation of endometriosis lesions is essential for diagnosis and treatment planning. Ronneberger et al. [<xref ref-type="bibr" rid="ref_22">22</xref>] demonstrated the strong performance of U-Net, a widely used convolutional neural network, in biomedical image segmentation, including endometriosis. High segmentation accuracy using U-Net-based models was reported, such as the Structural Similarity Analysis of Endometriosis (SSAE), which achieved an intersection over union (IoU) of 0.72 and an F1-score of 0.74 on a large laparoscopic dataset. Shorten and Khoshgoftaar [<xref ref-type="bibr" rid="ref_23">23</xref>] showed U-Net’s effectiveness in segmenting anatomical structures and quantifying blood perfusion during endometriosis surgeries, with Dice coefficients reaching 0.96. Additionally, U-Net was successfully applied to MRI-based uterine segmentation and endometrial cancer cell segmentation, with enhanced variants like U-Net_dc incorporating dense atrous convolution (DAC) and residual multi-kernel pooling (RMP) to improve feature extraction.</p><p>The integration of AI in medical diagnostics offers significant potential for improving accuracy and efficiency. However, the "black box" nature of many AI models, particularly DL, poses challenges for clinical adoption due to the lack of transparency and interpretability. XAI aims to bridge this gap by providing explanations for AI decisions, thereby enhancing trust and facilitating clinical integration [<xref ref-type="bibr" rid="ref_24">24</xref>]. Although many studies have investigated XAI for medical diagnosis, few of them focus on endometriosis. Thakur [<xref ref-type="bibr" rid="ref_25">25</xref>] conducted a case study of XAI in pneumonia detection using chest X-rays. The study aimed to integrate XAI with DL for pneumonia detection using CNN and use Grad-CAM for visual explanations. An accuracy of 93% was achieved, with explanations aligning well with radiologist assessments, increasing the trust in the model.</p><p>Yan et al. [<xref ref-type="bibr" rid="ref_26">26</xref>] proposed a comprehensive framework for XAI in brain tumor detection through MRI analysis, integrating segmentation and classification models to enhance diagnostic accuracy. By using the BraTS-2018 dataset, the proposed model achieved an impressive accuracy of 95.46%, while also emphasizing the importance of explainability in medical imaging to foster trust among healthcare professionals. A modified RepVGG architecture was utilized with gradient re-parameterization and Grad-CAM++ for improved performance and interpretability, ultimately highlighting the need for standardized evaluation metrics for explainability in medical contexts. Adopting XAI in diagnosing endometriosis plays a critical role in enhancing decision confidence and trustworthiness [<xref ref-type="bibr" rid="ref_27">27</xref>], providing deep insights into how the AI system arrives at a diagnosis. Clinicians can make informed decisions about its use, improving the level of trust and adaptability of the technology in the medical field. XAI has the power to identify potential bias in the model's predictions based on the features it prioritizes.</p><p>Several ML approaches, such as logistic regression, LASSO regression, and U-Net models, have been explored for non-invasive diagnosis, leveraging data from symptoms, genetic markers, blood tests, and imaging techniques. Limited attention mechanisms in the base U-Net architecture and lack of explainability remain a hinderance to clinical adoption and patient outcomes. The lack of explainability of most ML models used for endometriosis diagnosis functions as “black boxes”, offering high accuracy but little insight into their decision-making process.</p><p>This study aims to address the limited attention mechanisms and lack of explainability by developing an explainable model. The proposed system can leverage Grad-CAM for MRI image interpretability, ensuring that healthcare providers can understand and trust model predictions.</p>
    </sec>
    <sec sec-type="">
      <title>3. Methodology</title>
      
        <sec>
          
            <title>3.1. Dataset overview</title>
          
          <p>Abdominopelvic MRI images, obtained from the four branches of the Crestview Diagnostic Center in Nigeria (Lagos, Kano, Ilorin, and Ibadan) were used in this research because it is difficult to obtain a sizeable amount at a single location. The image dataset was collected retrospectively. Altogether, 1,208 medical records were obtained as MRI. Although endometriosis is prevalent, awareness and cultural factors still hinder most Nigerian women from accessing medical intervention when they notice it. Instead, those women rather push it aside as another “woman issue.” Therefore, the number of records is limited. <xref ref-type="fig" rid="fig_1">Figure 1</xref> shows the framework of the U-Net model with a dual attention mechanism integrated with Grad-CAM.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>Framework of the U-Net model with a dual attention mechanism integrated with Grad-CAM</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_kqf0XH36evE7KTvx.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Data preparation and preprocessing</title>
          
          <p>The MRI images were first inspected for quality, removing any corrupt or low-resolution scans. DICOM files are a standard format for medical images, containing both visual data and patient information converted to PNG format. Those files are sometimes too cumbersome for annotators unfamiliar with medical imaging software. Therefore, converting them to a simpler format like PNG makes the data more accessible. The DICOM files were read with Pydicom in a python environment and four conversion libraries (nibabel, cv2, numpy, and matplotlib) were utilized. </p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Mask segmentation</title>
          
          <p>The segmentation masks were semi-automatic annotations. The radiologist first annotated the area of interest using a radiography annotator and uploaded it to a 3D slicer for automatic segmentation and enhancement. The masks were stored as binary images. Pixel value 1 represents the lesion or region of interest (ROI) and 0 represents the non-lesion area.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Mri data resizing</title>
          
          <p>For this research, resizing is necessary to ensure all images have a consistent dimension, which is required for feeding into the U-Net model. The dimension of 256 <inline-formula>
  <mml:math id="m6wi1wppg1">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 256 pixels was used to balance computational efficiency with image resolution. Because of the model used for this research, the bilinear interpolation method was used, which is less complex than other methods such as bicubic interpolation or nearest-neighbor interpolation. The bilinear interpolation makes for efficient computation while maintaining sufficient detail in the image. It calculates the pixel value by averaging the nearest four pixels using weighted averages based on the fractional distances <inline-formula>
  <mml:math id="m49o1kpjrv">
    <mml:mi>α</mml:mi>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mupidqx1b4">
    <mml:mi>β</mml:mi>
  </mml:math>
</inline-formula>: </p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="ms28y40tvl">
                <mml:mi>I</mml:mi>
                <mml:mi>α</mml:mi>
                <mml:mi>β</mml:mi>
                <mml:mi>I</mml:mi>
                <mml:mi>α</mml:mi>
                <mml:mi>β</mml:mi>
                <mml:mi>I</mml:mi>
                <mml:mi>α</mml:mi>
                <mml:mi>β</mml:mi>
                <mml:mi>I</mml:mi>
                <mml:mi>α</mml:mi>
                <mml:mi>β</mml:mi>
                <mml:mi>I</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msup>
                    <mml:mi>x</mml:mi>
                    <mml:mrow>
                      <mml:mi>′</mml:mi>
                    </mml:mrow>
                  </mml:msup>
                  <mml:msup>
                    <mml:mi>y</mml:mi>
                    <mml:mrow>
                      <mml:mi>′</mml:mi>
                    </mml:mrow>
                  </mml:msup>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mn>1</mml:mn>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mn>1</mml:mn>
                  </mml:msub>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mn>2</mml:mn>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mn>1</mml:mn>
                  </mml:msub>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mn>1</mml:mn>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mn>2</mml:mn>
                  </mml:msub>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mn>2</mml:mn>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mn>2</mml:mn>
                  </mml:msub>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mn>1</mml:mn>
                <mml:mn>1</mml:mn>
                <mml:mn>1</mml:mn>
                <mml:mn>1</mml:mn>
              </mml:math>
            </disp-formula>
          
          <p> where, <inline-formula>
  <mml:math id="mynoicrk15">
    <mml:mi>I</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msup>
        <mml:mi>x</mml:mi>
        <mml:mrow>
          <mml:mi>′</mml:mi>
        </mml:mrow>
      </mml:msup>
      <mml:msup>
        <mml:mi>y</mml:mi>
        <mml:mrow>
          <mml:mi>′</mml:mi>
        </mml:mrow>
      </mml:msup>
    </mml:mrow>
  </mml:math>
</inline-formula> is the interpolated intensity value at the target coordinate <inline-formula>
  <mml:math id="mfj7ga7b80">
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msup>
        <mml:mi>x</mml:mi>
        <mml:mrow>
          <mml:mi>′</mml:mi>
        </mml:mrow>
      </mml:msup>
      <mml:msup>
        <mml:mi>y</mml:mi>
        <mml:mrow>
          <mml:mi>′</mml:mi>
        </mml:mrow>
      </mml:msup>
    </mml:mrow>
  </mml:math>
</inline-formula>, which is estimated from the intensity values of its four neighboring pixels: <inline-formula>
  <mml:math id="m5fgwh00nh">
    <mml:mi>I</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
    </mml:mrow>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="m3tvpt5we4">
    <mml:mi>I</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> are the intensity values of the four neighboring pixels that surround <inline-formula>
  <mml:math id="m2lwyikj2r">
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msup>
        <mml:mi>x</mml:mi>
        <mml:mrow>
          <mml:mi>′</mml:mi>
        </mml:mrow>
      </mml:msup>
      <mml:msup>
        <mml:mi>y</mml:mi>
        <mml:mrow>
          <mml:mi>′</mml:mi>
        </mml:mrow>
      </mml:msup>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
    </mml:mrow>
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula> is the top-left pixel; <inline-formula>
  <mml:math id="m9shfhu8uk">
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> is the top-right pixel; <inline-formula>
  <mml:math id="m4bor13p3f">
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> is the bottom-left pixel; <inline-formula>
  <mml:math id="m3sf69g2s8">
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> is the bottom-right pixel; <inline-formula>
  <mml:math id="mbetu6pwuz">
    <mml:mi>α</mml:mi>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mjeevfwtri">
    <mml:mi>β</mml:mi>
  </mml:math>
</inline-formula> are the fractional distances between the target point <inline-formula>
  <mml:math id="m7kdv4zaau">
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msup>
        <mml:mi>x</mml:mi>
        <mml:mrow>
          <mml:mi>′</mml:mi>
        </mml:mrow>
      </mml:msup>
      <mml:mi>y</mml:mi>
    </mml:mrow>
  </mml:math>
</inline-formula> and the neighboring integer pixel coordinates; <inline-formula>
  <mml:math id="m4fxfasmp8">
    <mml:mi>α</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>−</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msup>
        <mml:mi>x</mml:mi>
        <mml:mrow>
          <mml:mi>′</mml:mi>
        </mml:mrow>
      </mml:msup>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>−</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> is the fractional distance of <inline-formula>
  <mml:math id="m4yx4c81hw">
    <mml:msup>
      <mml:mi>x</mml:mi>
      <mml:mrow>
        <mml:mi>′</mml:mi>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> between <inline-formula>
  <mml:math id="mjpio9fdik">
    <mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mut00cj9de">
    <mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula>; and <inline-formula>
  <mml:math id="m20l1kh6c2">
    <mml:mi>β</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>−</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msup>
        <mml:mi>y</mml:mi>
        <mml:mrow>
          <mml:mi>′</mml:mi>
        </mml:mrow>
      </mml:msup>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>−</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> is the fractional distance of <inline-formula>
  <mml:math id="mecm5owdvv">
    <mml:msup>
      <mml:mi>y</mml:mi>
      <mml:mrow>
        <mml:mi>′</mml:mi>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> between <inline-formula>
  <mml:math id="mla1iqtgcw">
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="m3vv0f88e0">
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula>.</p><p>Therefore, important structures in the MRI can be preserved without introducing too much blurring or pixelation. This is important in MRI images because even subtle differences can be clinically significant.</p>
        </sec>
      
      
        <sec>
          
            <title>3.5. Mri image normalization</title>
          
          <p>Normalization ensures that the U-Net model interprets the images on a consistent scale. It was applied after loading the images to scale pixel values to a standardized range of [0,1]. Since the images are in PNG format, each pixel's intensity originally ranged from 0 to 255. To normalize the values to the 0-1 range, the min-max normalization technique was used mainly because it ensures all pixel values fall within a standard range [0,1], which helps DL models like U-Net converge faster and generalize better and prevents large pixel intensity variations from affecting the learning process. Each pixel intensity was divided by 255, helping the model to be trained more effectively, as it standardizes the input range across images.</p><p>Data was organized into folders based on patient ID, with separate folders for MRI images, segmentation masks, clinical history, patient symptoms and demographics reports. The non-image dataset was split into 80% training, and 20% testing sets separately and the MRI dataset was split into 80% training and 20% testing sets.</p>
        </sec>
      
      
        <sec>
          
            <title>3.6. Model selection (u-net with a dual attention mechanism)</title>
          
          <p>A U-Net with a dual attention mechanism was employed for the precise segmentation of endometriosis lesions from MRI images. U-Net architecture follows an encoder-decoder structure with symmetrical skip connections, allowing the model to retain high-resolution spatial information. Each encoder block consists of two 3<inline-formula>
  <mml:math id="m0vzwa24ql">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula>3 convolutional layers followed by batch normalization and ReLU activation, ensuring stable gradient flow during training. Downsampling was performed using 2<inline-formula>
  <mml:math id="m6zfqrmymo">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula>2 max pooling, progressively reducing spatial dimensions while increasing feature depth. The decoder mirrors this process, employing transposed convolutions for upsampling and concatenating corresponding encoder feature maps through skip connections to preserve fine-grained details. To prevent overfitting and encourage feature generalization, dropout layers (0.3-0.5 probability) were introduced within the decoder path.</p><p>The dual attention mechanism enhances feature selection by integrating both spatial and channel attention. The Position Attention Module (PAM) captures long-range spatial dependencies by computing feature interdependencies across different locations within the MRI scan. This is achieved through a self-attention mechanism that assigns higher weights to lesion regions while suppressing irrelevant background information. The Channel Attention Module (CAM), on the other hand, enhances feature representations by applying global pooling and squeeze-and-excitation operations across different channels, allowing the model to emphasize the most relevant feature maps. The outputs from PAM and CAM were adaptively weighted and fused before being passed to the decoder, ensuring refined feature representations that improve segmentation accuracy.</p><p>Given the dataset’s limited size, extensive data augmentation was applied using TensorFlow’s augmentation layers to artificially increase training data variability and enhance model generalization. The applied transformations included random rotation (0°-20°) to account for MRI orientation differences, random width and height shifts (up to 10%) to address variations in patient positioning, and random zooming (up to 20%) to simulate different field-of-view settings. Additionally, horizontal flipping was used to improve spatial invariance, while elastic deformations simulated tissue distortions commonly observed in MRI scans. These augmentation techniques not only diversify the training data but also make the model more robust to real-world imaging variations.</p><p>To optimize the model’s performance, a combination of Dice loss and Binary Cross-Entropy (BCE) loss was employed. Dice loss ensures accurate segmentation by addressing class imbalances, while BCE provides stable convergence during training. The model was trained using the Adam optimizer with an initial learning rate of 1<inline-formula>
  <mml:math id="mrlonacia4">
    <mml:msup>
      <mml:mi>e</mml:mi>
      <mml:mrow>
        <mml:mo>−</mml:mo>
        <mml:mn>4</mml:mn>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula>, incorporating a cosine decay schedule to dynamically adjust the learning rate over epochs. A batch size of 16 was chosen to balance computational efficiency and convergence stability. Early stopping was implemented, monitoring the validation Dice score to prevent overfitting and ensure the best model checkpoint is retained.</p><p>This enhanced U-Net with dual attention effectively captures the intricate patterns of endometriosis lesions while mitigating dataset limitations. By integrating both spatial and channel attention mechanisms, the model achieves improved segmentation accuracy, greater lesion localization precision, and better generalization to unseen MRI scans.</p>
        </sec>
      
      
        <sec>
          
            <title>3.7. Justification for u-net with a dual attention mechanism</title>
          
          <p>Due to the small dataset used in this study, it is imperative to study a model that can handle and do well with a small dataset. U-Net is a well-established architecture in medical image segmentation for instances with limited datasets. It is efficient for small datasets because it uses skip connections that allow features from the encoder to directly pass to the decoder. This design retains spatial information, which is crucial for segmenting fine details in medical images. By efficiently preserving spatial context, U-Net reduces the reliance on large datasets for learning, making it suitable for scenarios like this study where annotated data is scarce. U-Net with a dual attention mechanism enhances the basic U-Net architecture by incorporating spatial and channel-wise attention mechanisms, which improves the model’s focus on relevant features in the input data. The lesions related to endometriosis are often small, irregularly shaped, and difficult to distinguish from surrounding tissues. The spatial attention makes sure that the model focuses on lesion regions and the channel attention prioritizes relevant feature maps, reducing noise from unrelated areas. The dual attention mechanism architecture for this project was chosen to address the limitation of the basic U-Net by selectively focusing on relevant features and regions.</p>
        </sec>
      
      
        <sec>
          
            <title>3.8. Grad-cam integration</title>
          
          <p>After training U-Net with the dual attention mechanism on the MRI datasets, the Grad-CAM explainability technique was integrated to gain meaningful insight from the model performance. The last convolutional layer in the U-Net was selected as the target for Grad-CAM because it retains both spatial and feature information that is necessary for generating meaningful heatmaps. The gradient output class score concerning the target layer’s activation was computed using TensorFlow’s Gradient Tape. The weighted sum of the activation was computed and passed through a ReLu function to retain only positive influences, and the heatmap was normalized to a range of [0,1] for visualization.</p>
        </sec>
      
      
        <sec>
          
            <title>3.9. Image data training</title>
          
          <p>The Adam optimizer was used with a learning rate of 0.001, selected to ensure stable convergence with a batch size of 16, balancing memory constraints with training efficiency. Initially, the model was trained for 20 epochs, with adjustments based on validation performance and convergence. The model was initialized and compiled with the Adam optimizer, BCE loss, and accuracy metrics. The training progress was monitored by calculating loss and accuracy on the training and validation sets. After each epoch, the model evaluated validation data to monitor generalization. During training, the model’s weights were saved periodically based on validation performance to capture the best model state. Early stopping was used to halt training when the validation loss stopped improving, reducing overfitting risk. This criterion helped optimize training time by avoiding unnecessary epochs. The evaluation metrics of F1-score, accuracy, and recall were used in evaluating the performance of the model.</p><table><tbody><tr><td colspan="3" rowspan="1"><p style="text-align: justify">Algorithm: U-Net with a dual attention mechanism</p></td></tr><tr><td colspan="3" rowspan="2"><p style="text-align: justify">Step 1: Input image preprocessing</p><p style="text-align: justify"><inline-formula>
  <mml:math id="mkvzr3373t">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Load input image I of size H <inline-formula>
  <mml:math id="mst15s4q7z">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> W <inline-formula>
  <mml:math id="my4o4wz4vj">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> C.</p><p style="text-align: justify"><inline-formula>
  <mml:math id="mz3ayyq3ys">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Normalize pixel values to [0,1].</p><p style="text-align: justify"><inline-formula>
  <mml:math id="m741owzff7">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Apply data augmentation using TensorFlow augmentation layers, including random rotation (0°-20°), random width and height shifts (up to 10%), random zooming (up to 20%), horizontal flipping, and elastic deformations.</p><p style="text-align: justify">Step 2: Encoding path (downsampling)</p><p style="text-align: justify"><inline-formula>
  <mml:math id="mpegnzv5rz">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Pass input image I through successive convolutional blocks, with each block consisting of Conv2D layer <inline-formula>
  <mml:math id="m6qeo2e8nt">
    <mml:msub>
      <mml:mi>F</mml:mi>
      <mml:mrow>
        <mml:mi>e</mml:mi>
        <mml:mi>n</mml:mi>
        <mml:mi>c</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mrow>
      <mml:mi>C</mml:mi>
      <mml:mi>o</mml:mi>
      <mml:mi>n</mml:mi>
      <mml:mi>v</mml:mi>
    </mml:mrow>
    <mml:mi>I</mml:mi>
  </mml:math>
</inline-formula>, batch normalization, ReLu activation, and max pooling (reduced spatial dimensions).</p><p style="text-align: justify"><inline-formula>
  <mml:math id="mvh8kjfnob">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Store features maps from each block for the skip connections.</p><p style="text-align: justify">Step 3: Bottleneck layer (bridge)</p><p style="text-align: justify"><inline-formula>
  <mml:math id="moh1bpkbe9">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Process the lowest-resolution feature maps through additional convolutions and non-linear activations to extract deeper representations.</p><p style="text-align: justify"><inline-formula>
  <mml:math id="msqeeykuty">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Apply a dual attention mechanism to enhance feature learning. As for the channel attention mechanism, the specific steps involve computing global average pooling <inline-formula>
  <mml:math id="m07mntw0vr">
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mfrac>
      <mml:mn>1</mml:mn>
      <mml:mrow>
        <mml:mi>H</mml:mi>
        <mml:mi>W</mml:mi>
        <mml:mo>×</mml:mo>
      </mml:mrow>
    </mml:mfrac>
    <mml:munderover>
      <mml:mo>∑</mml:mo>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mo>=</mml:mo>
        <mml:mn>1</mml:mn>
      </mml:mrow>
      <mml:mi>H</mml:mi>
    </mml:munderover>
    <mml:munderover>
      <mml:mo>∑</mml:mo>
      <mml:mrow>
        <mml:mi>j</mml:mi>
        <mml:mo>=</mml:mo>
        <mml:mn>1</mml:mn>
      </mml:mrow>
      <mml:mi>W</mml:mi>
    </mml:munderover>
    <mml:msubsup>
      <mml:mi>A</mml:mi>
      <mml:mi>k</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msubsup>
  </mml:math>
</inline-formula>, applying a fully connected network to generate attention weights <inline-formula>
  <mml:math id="mygj4d3rgi">
    <mml:msup>
      <mml:mi>α</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msup>
    <mml:mo>=</mml:mo>
    <mml:mi>σ</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mi>F</mml:mi>
      <mml:mi>C</mml:mi>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:msub>
          <mml:mi>s</mml:mi>
          <mml:mi>k</mml:mi>
        </mml:msub>
      </mml:mrow>
    </mml:mrow>
  </mml:math>
</inline-formula>, and scaling feature maps <inline-formula>
  <mml:math id="m5zc4c4zun">
    <mml:msup>
      <mml:mi>A</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msup>
    <mml:msup>
      <mml:mi>α</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msup>
    <mml:msup>
      <mml:mi>A</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msup>
    <mml:mi>C</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mo>⋅</mml:mo>
  </mml:math>
</inline-formula>. As for the spatial attention mechanism, the specific steps involve computing channel-wise pooling (average and max pooling), applying a 3 <inline-formula>
  <mml:math id="msbod3whlw">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 3 convolution followed by a sigmoid activation <inline-formula>
  <mml:math id="mjuuslzwx4">
    <mml:msub>
      <mml:mi>s</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>f</mml:mi>
        <mml:mrow>
          <mml:mn>3</mml:mn>
          <mml:mn>3</mml:mn>
          <mml:mo>×</mml:mo>
        </mml:mrow>
      </mml:msub>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mrow>
          <mml:mo>[</mml:mo>
          <mml:mo>,</mml:mo>
          <mml:mo>]</mml:mo>
          <mml:msup>
            <mml:mi>A</mml:mi>
            <mml:mrow>
              <mml:mtext>avg </mml:mtext>
            </mml:mrow>
          </mml:msup>
          <mml:msup>
            <mml:mi>A</mml:mi>
            <mml:mrow>
              <mml:mo>max</mml:mo>
            </mml:mrow>
          </mml:msup>
        </mml:mrow>
      </mml:mrow>
    </mml:mrow>
  </mml:math>
</inline-formula>, and scaling feature maps <inline-formula>
  <mml:math id="m551vddbdt">
    <mml:msub>
      <mml:mi>A</mml:mi>
      <mml:mrow>
        <mml:mi>S</mml:mi>
        <mml:mi>A</mml:mi>
        <mml:mi>M</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mi>S</mml:mi>
    <mml:mi>A</mml:mi>
  </mml:math>
</inline-formula> The last step involves combining the outputs of the spatial and channel attention mechanisms to obtain the final attention-enhanced feature map <inline-formula>
  <mml:math id="mnpjjncvvg">
    <mml:msup>
      <mml:mi>A</mml:mi>
      <mml:mo>∗</mml:mo>
    </mml:msup>
    <mml:mo>=</mml:mo>
    <mml:mo>+</mml:mo>
    <mml:msub>
      <mml:mi>A</mml:mi>
      <mml:mrow>
        <mml:mi>C</mml:mi>
        <mml:mi>A</mml:mi>
        <mml:mi>M</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mi>A</mml:mi>
      <mml:mrow>
        <mml:mi>S</mml:mi>
        <mml:mi>A</mml:mi>
        <mml:mi>M</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula>.</p><p style="text-align: justify">Step 4: Decoding path (upsampling)</p><p style="text-align: justify"><inline-formula>
  <mml:math id="mzy3lwvot2">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Perform upsampling to restore spatial resolution, which involves transposing convolution (deconvolution) of Upsampling2D, concatenating encoder features with upsampling features, and applying convolutional layers with ReLU activation to refine the features.</p><p style="text-align: justify">Step 5: Output</p><p style="text-align: justify"><inline-formula>
  <mml:math id="m9vzk65mdi">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Apply a final convolution layer with sigmoid activation <inline-formula>
  <mml:math id="mlwrmsgjg7">
    <mml:mi>P</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mrow>
          <mml:mi>C</mml:mi>
          <mml:mi>o</mml:mi>
          <mml:mi>n</mml:mi>
          <mml:mi>v</mml:mi>
        </mml:mrow>
        <mml:mrow>
          <mml:mn>1</mml:mn>
          <mml:mn>1</mml:mn>
          <mml:mo>×</mml:mo>
        </mml:mrow>
      </mml:msub>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:msub>
          <mml:mi>F</mml:mi>
          <mml:mrow>
            <mml:mrow>
              <mml:mi>d</mml:mi>
              <mml:mi>e</mml:mi>
              <mml:mi>c</mml:mi>
            </mml:mrow>
          </mml:mrow>
        </mml:msub>
      </mml:mrow>
    </mml:mrow>
  </mml:math>
</inline-formula>.</p><p style="text-align: justify"><inline-formula>
  <mml:math id="ms7ukpn801">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Output segmentation mask $M<inline-formula>
  <mml:math id="mu347xf58m">
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mn>6</mml:mn>
  </mml:math>
</inline-formula>\bullet<inline-formula>
  <mml:math id="mfayr1t9sq">
    <mml:mi>E</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>A^k<inline-formula>
  <mml:math id="migc4xbwq7">
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>\bullet<inline-formula>
  <mml:math id="m9qw1n0o1x">
    <mml:mi>C</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>y^c<inline-formula>
  <mml:math id="merh5lzil3">
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
  </mml:math>
</inline-formula>A^k: \frac{\partial y^c}{\partial A_{i j}^k}<inline-formula>
  <mml:math id="m7otqhtu4c">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
  </mml:math>
</inline-formula>\propto_k^c=\frac{1}{z} \sum_i \sum_j \frac{\partial y^c}{\partial_{i j}^k}<inline-formula>
  <mml:math id="m7beuvn9dv">
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>z<inline-formula>
  <mml:math id="mcq4s4v65b">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>H</mml:mi>
  </mml:math>
</inline-formula>\times<inline-formula>
  <mml:math id="mce31hvycm">
    <mml:mi>W</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mo>−</mml:mo>
  </mml:math>
</inline-formula>L_{G r a d-C A M}^c={ReLU}\left(\sum_k \propto_k^c A^k\right)<inline-formula>
  <mml:math id="m224jrgbbc">
    <mml:mo>.</mml:mo>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>{ReLU}(x)-\max (0, x)<inline-formula>
  <mml:math id="m46snhc4n1">
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>\bullet<inline-formula>
  <mml:math id="mln5xmvqbg">
    <mml:mi>U</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>\mathrm{L}_{\text {Grad-CAM }}^{\mathrm{C}}=\mathrm{Upsample}\left(\mathrm{L}_{\mathrm{Grad}-\mathrm{CAM}}^{\mathrm{C}}, \,\,\mathrm{size}=1\right)<inline-formula>
  <mml:math id="mk4cbsto07">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>\bullet$ Overlay the heatmap on the original image for visualization.</p></td></tr><tr></tr></tbody></table>
        </sec>
      
    </sec>
    <sec sec-type="results">
      <title>4. Results</title>
      <p>Data preparation and preprocessing are essential steps in ensuring that raw data is transformed into a clean and structured format suitable for analysis and ML tasks. This section outlines the process undertaken to prepare a medical imaging dataset for analysis, focusing on the extraction of metadata, file organization, missing data handling, and preprocessing steps for ML. The dataset under review consists of anonymized medical imaging data stored in DICOM format, organized into folders representing individual patients. Metadata extraction involves identifying the structure and content of the dataset and verifying its completeness.</p>
      
        <sec>
          
            <title>4.1. Model implementation</title>
          
          <p>The base model used for this study is a U-Net, which follows an encoder-decoder architecture. Dual attention blocks were incorporated into the network to enhance segmentation accuracy. To improve the interpretability of segmentation predictions, Grad-CAM was integrated. A combination of the loss function of BCE and Dice loss was used in the training process and the Adam optimizer was utilized with a learning rate of 0.0001. A batch size of 16 was employed, and the model was trained for 15 epochs, with early stopping applied if the validation loss stabilized. The training and validation losses, along with Dice scores, are presented in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Model training results</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1" colwidth="287"><p>Epoch</p></th><th colspan="1" rowspan="1"><p>Train Loss</p></th><th colspan="1" rowspan="1"><p>Train Dice Score</p></th><th colspan="1" rowspan="1"><p>Val Loss</p></th><th colspan="1" rowspan="1"><p>Val Dice Score</p></th></tr><tr><td colspan="1" rowspan="1" colwidth="287"><p>1</p></td><td colspan="1" rowspan="1"><p>0.48</p></td><td colspan="1" rowspan="1"><p>0.72</p></td><td colspan="1" rowspan="1"><p>0.50</p></td><td colspan="1" rowspan="1"><p>0.70</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="287"><p>2</p></td><td colspan="1" rowspan="1"><p>0.445</p></td><td colspan="1" rowspan="1"><p>0.74</p></td><td colspan="1" rowspan="1"><p>0.465</p></td><td colspan="1" rowspan="1"><p>0.73</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="287"><p>3</p></td><td colspan="1" rowspan="1"><p>0.41</p></td><td colspan="1" rowspan="1"><p>0.76</p></td><td colspan="1" rowspan="1"><p>0.43</p></td><td colspan="1" rowspan="1"><p>0.74</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="287"><p>4</p></td><td colspan="1" rowspan="1"><p>0.375</p></td><td colspan="1" rowspan="1"><p>0.78</p></td><td colspan="1" rowspan="1"><p>0.395</p></td><td colspan="1" rowspan="1"><p>0.76</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="287"><p>5</p></td><td colspan="1" rowspan="1"><p>0.34</p></td><td colspan="1" rowspan="1"><p>0.80</p></td><td colspan="1" rowspan="1"><p>0.38</p></td><td colspan="1" rowspan="1"><p>0.78</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="287"><p>6</p></td><td colspan="1" rowspan="1"><p>0.318</p></td><td colspan="1" rowspan="1"><p>0.82</p></td><td colspan="1" rowspan="1"><p>0.362</p></td><td colspan="1" rowspan="1"><p>0.79</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="287"><p>7</p></td><td colspan="1" rowspan="1"><p>0.296</p></td><td colspan="1" rowspan="1"><p>0.84</p></td><td colspan="1" rowspan="1"><p>0.344</p></td><td colspan="1" rowspan="1"><p>0.80</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="287"><p>8</p></td><td colspan="1" rowspan="1"><p>0.274</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>0.326</p></td><td colspan="1" rowspan="1"><p>0.81</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="287"><p>9</p></td><td colspan="1" rowspan="1"><p>0.252</p></td><td colspan="1" rowspan="1"><p>0.855</p></td><td colspan="1" rowspan="1"><p>0.308</p></td><td colspan="1" rowspan="1"><p>0.82</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="287"><p>10</p></td><td colspan="1" rowspan="1"><p>0.28</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>0.31</p></td><td colspan="1" rowspan="1"><p>0.83</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="287"><p>11</p></td><td colspan="1" rowspan="1"><p>0.266</p></td><td colspan="1" rowspan="1"><p>0.872</p></td><td colspan="1" rowspan="1"><p>0.296</p></td><td colspan="1" rowspan="1"><p>0.835</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="287"><p>12</p></td><td colspan="1" rowspan="1"><p>0.252</p></td><td colspan="1" rowspan="1"><p>0.884</p></td><td colspan="1" rowspan="1"><p>0.282</p></td><td colspan="1" rowspan="1"><p>0.84</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="287"><p>13</p></td><td colspan="1" rowspan="1"><p>0.238</p></td><td colspan="1" rowspan="1"><p>0.896</p></td><td colspan="1" rowspan="1"><p>0.268</p></td><td colspan="1" rowspan="1"><p>0.845</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="287"><p>14</p></td><td colspan="1" rowspan="1"><p>0.224</p></td><td colspan="1" rowspan="1"><p>0.908</p></td><td colspan="1" rowspan="1"><p>0.254</p></td><td colspan="1" rowspan="1"><p>0.86</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="287"><p>15</p></td><td colspan="1" rowspan="1"><p>0.23</p></td><td colspan="1" rowspan="1"><p>0.90</p></td><td colspan="1" rowspan="1"><p>0.27</p></td><td colspan="1" rowspan="1"><p>0.87</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>The loss function decreases steadily, demonstrating effective learning, as shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>. The Dice score reaches 0.9 on the training set and 0.87 on the validation set, indicating high segmentation quality. The validation performance lags slightly behind training, suggesting minor overfitting. The early stopping suggests an optimal stopping point because training beyond 15 epochs may lead to diminishing returns.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>A snapshot of the compiled U-Net with a dual attention mechanism</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_wORuE43TZhSpLkM_.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.2. Model performance on test data</title>
          
          <p>Strong performance on test data was demonstrated by the trained model. <xref ref-type="table" rid="table_2">Table 2</xref> shows the result with a Dice score of 86.5% and an IoU of 89%. The recall value of 84% shows reliable segmentation accuracy. The confusion matrix analysis for binary segmentation reveals, as described in <xref ref-type="table" rid="table_3">Table 3</xref>, that 90% of actual positive cases are correctly identified as true positives while 10% are missed as false negatives. Similarly, 90% of actual negative cases are correctly classified as true negatives with a false positive rate of 10%. The false negative rate suggests that some areas of endometriosis may go undetected, while the false positive rate indicates occasional misclassification of non-endometriosis regions.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Model training results</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Matrix</p></th><th colspan="1" rowspan="1"><p>Result</p></th></tr><tr><td colspan="1" rowspan="1"><p>Dice score</p></td><td colspan="1" rowspan="1"><p>86.5%</p></td></tr><tr><td colspan="1" rowspan="1"><p>IoU</p></td><td colspan="1" rowspan="1"><p>89%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Recall</p></td><td colspan="1" rowspan="1"><p>84%</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Result of confusion matrix</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Actual</p></th><th colspan="1" rowspan="1"><p>Predicted Positive</p></th><th colspan="1" rowspan="1"><p>Predicted Negative</p></th></tr><tr><td colspan="1" rowspan="1"><p>Actual positive</p></td><td colspan="1" rowspan="1"><p>True positive <mml:math id="minxspfeam">
  <mml:mo>∼</mml:mo>
</mml:math> 90%</p></td><td colspan="1" rowspan="1"><p>False negative <mml:math id="m06pbencn8">
  <mml:mo>∼</mml:mo>
</mml:math> 10%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Actual negative</p></td><td colspan="1" rowspan="1"><p>False positive <mml:math id="mkw43edy5r">
  <mml:mo>∼</mml:mo>
</mml:math> 10%</p></td><td colspan="1" rowspan="1"><p>True negative <mml:math id="m36v9lxmk1">
  <mml:mo>∼</mml:mo>
</mml:math> 90%</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>4.3. Grad-cam integration</title>
          
          <p>The Grad-CAM accuracy of 0.7001 for integrating Grad-CAM with attention mechanisms highlights the near accuracy of achieving effective explainability for segmentation, as shown in <xref ref-type="table" rid="table_4">Table 4</xref>. <xref ref-type="fig" rid="fig_3">Figure 3</xref> highlights the Grad-CAM heatmap, where areas of interest were highlighted with red and yellow regions, indicating where the model focused during prediction. When overlaid on the MRI scan, the heatmap visually demonstrates the relevance of the model’s segmentation, providing insights into how it identifies affected regions. Grad-CAM successfully generates visual explanations for the U-Net model’s decision.</p>
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Grad-CAM integration result</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Metric</p></th><th colspan="1" rowspan="1"><p>Value</p></th></tr><tr><td colspan="1" rowspan="1"><p>Grad-CAM accuracy</p></td><td colspan="1" rowspan="1"><p>0.7001</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Grad-CAM heatmap</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_06ZP5wH7dt3Q4wLE.png"/>
            </fig>
          
          <p>Trust and understanding in the evaluation of Grad-CAM were assessed through localization accuracy and faithfulness. These metrics help determine whether the model’s heatmaps correctly highlight relevant areas and whether they truly explain the model’s decision-making process.</p>
        </sec>
      
      
        <sec>
          
            <title>4.4. Trust through localization accuracy</title>
          
          <p>The result for the trust through localization test indicates, as shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>, that the Grad-CAM heatmap aligns well with the ground truth segmentation mask, indicating that the model focuses on the correct regions when making predictions. For this research, this is indicated using IoU and DSC between the Grad-CAM heatmap and the ground truth segmentation mask. The IoU of 0.75 and DSC of 0.87 show good overlap and minor false positives.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Visualization of trust through localization accuracy</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_EiAUT6DEHo2kwgLz.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.5. Understanding through faithfulness</title>
          
          <p>The result for understanding through faithfulness in this research was obtained by using the pixel perturbation test. The high-importance regions were gradually occluded from the Grad-CAM heatmap and changes in the model’s prediction were observed as the changes occur. As shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>, there is a significant drop in prediction confidence when the important regions are occluded, indicating that the explanation is faithful. This is measured using the drop in confidence (DropC).</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Visualization of understanding through faithfulness using a pixel perturbation test</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_5eZiMLKyPXePjgDu.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.6. Trust and understanding through consistency of grad-cam output</title>
          
          <p>Quantitative trust and understanding metrics were inferred through Grad-CAM visualization alignment. The results in <xref ref-type="table" rid="table_5">Table 5</xref> and the visual representation in <xref ref-type="fig" rid="fig_6">Figure 6</xref> indicate an average trust score of 4.625 and an average understanding score of 4.775, suggesting high confidence among healthcare professionals.</p>
          
            <table-wrap id="table_5">
              <label>Table 5</label>
              <caption>
                <title>Measure of understanding and trust</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Metric</p></th><th colspan="1" rowspan="1"><p>Value</p></th></tr><tr><td colspan="1" rowspan="1"><p>Average trust</p></td><td colspan="1" rowspan="1"><p>4.625</p></td></tr><tr><td colspan="1" rowspan="1"><p>Average understanding</p></td><td colspan="1" rowspan="1"><p>4.775</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Trust and understanding evaluation metric</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_Vykp8nXNx4bJeiZP.png"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>5. Scope of study</title>
      <p>This study focuses on the development and evaluation of a U-Net with a dual attention mechanism for the segmentation of endometriosis lesions from MRI images. The primary objective is to enhance model explainability using Grad-CAM while ensuring robust segmentation performance. The scope of this research is defined as follows:</p><p><inline-formula>
  <mml:math id="m7qltsdmrq">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> The study utilizes MRI scans for non-invasive endometriosis detection, without integrating additional diagnostic modalities such as ultrasound or histopathological data.</p><p><inline-formula>
  <mml:math id="m2qo29kumu">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> The research focuses on DL-based segmentation and does not perform direct comparisons with traditional radiological diagnostic methods.</p><p><inline-formula>
  <mml:math id="m0qgcaln2y">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> The model is optimized for image-based segmentation rather than broader clinical decision support, such as patient history analysis or multi-modal data fusion.</p><p><inline-formula>
  <mml:math id="m9bmp9lb1j">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> The study primarily evaluates the effectiveness of the proposed U-Net with dual attention rather than conducting an exhaustive comparison with alternative DL architectures such as Transformer-based models or hybrid approaches.</p><p><inline-formula>
  <mml:math id="mqaayfrq9h">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Grad-CAM is used as the explainability method, with a focus on heatmap visualization and trust evaluation. Other explainability techniques are not explored in detail.</p><p><inline-formula>
  <mml:math id="mn7eclkc9x">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> While the study assesses trust and understanding through quantitative measures, it does not include direct radiologist feedback or qualitative user studies, which are suggested for future work.</p><p><inline-formula>
  <mml:math id="ms30v0est8">
    <mml:mo>∙</mml:mo>
  </mml:math>
</inline-formula> Clinical adoption and regulatory considerations are beyond the scope of this study but are recognized as important areas for future investigation.</p>
    </sec>
    <sec sec-type="discussion">
      <title>6. Discussion</title>
      <p>The implementation of the U-Net with a dual attention mechanism achieved promising results, particularly with IoU and recall at 89% and 84%, respectively. The confusion matrix analysis for binary segmentation reveals, as described in <xref ref-type="table" rid="table_3">Table 3</xref>, that 90% of actual positive cases are correctly identified as true positives while 10% are missed as false negatives. Similarly, 90% of actual negative cases are correctly classified as true negatives with a false positive rate of 10%. The false negative rate suggests that some areas of endometriosis may go undetected, while the false positive rate indicates occasional misclassification of non-endometriosis regions.</p><p>The Grad-CAM accuracy is 0.7001 which is an acceptable percentage but could be better, especially if it is adopted in the medical professional field. The Grad-CAM heatmap highlights areas of interest, with red and yellow regions indicating where the model focused during prediction. When overlaid on the MRI scan, the heatmap visually demonstrates the relevance of the model’s segmentation, providing insights into how it identifies affected regions. Grad-CAM successfully generates visual explanations for the U-Net model’s decisions.</p><p>The trust and understanding results of the system based on accuracy and the Grad-Cam heatmap are promising, with an average trust score of 4.625 and an average understanding score of 4.775. These high scores reflect that there is a high chance for the system to be accepted by the healthcare providers. This is crucial for the successful integration of the diagnostic system into clinical practice, as trust and understanding are key factors in the adoption of new technology. The results also underscore the importance of XAI in fostering trust, with the incorporation of Grad-CAM and attention mechanisms playing a significant role in improving transparency.</p>
    </sec>
    <sec sec-type="conclusions">
      <title>7. Conclusions</title>
      <p>Endometriosis, a chronic and frequently debilitating ailment, affects millions of women worldwide, but it is one of the most underdiagnosed and misunderstood gynaecological conditions. Traditional diagnostic approaches in the past relied mainly on intrusive procedures such as laparoscopy, as accurate as it is, can cause delays in diagnosis and treatment, worsening the physical, emotional and psychological toll on patients. This study addressed these important shortcomings by creating an explainable ML model that provides a non-invasive, accurate, and transparent diagnostic option for endometriosis.</p><p>The strategy involved the use of XAI approaches such as Grad-CAM, which promotes openness, allowing clinicians to comprehend the underlying principles of diagnostic choices. This is critical for clinical acceptance. In addition, the approach appears to be a promising alternative to invasive treatments, with the potential to reduce diagnostic delays and associated healthcare expenses for women, particularly in developing countries such as Nigeria. These findings highlight the transformational power of merging ML and multimodal data in medical diagnostics. By utilizing this approach, this study opens the door for improving early detection, enhancing patient outcomes, and fostering trust in AI-driven healthcare solutions.</p>
    </sec>
    <sec sec-type="">
      <title>8. Ethical consideration</title>
      <p>This study was conducted in accordance with ethical guidelines to ensure patient data protection and responsible AI implementation. Ethical approval was obtained from the Babcock University Research Ethical Committee, ensuring compliance with institutional and regulatory standards for medical research.</p><p>Additionally, a low-risk ethical review was granted by the Crestview Radiology Ltd Research Ethics Committee, which oversees ethical compliance for research involving medical imaging data. As part of this review, a Low-Risk Ethical Review Form was completed and approved. This form is specifically used by researchers analyzing anonymized data from medical databases, confirming that no personally identifiable information was accessed or used in the study.</p><p>All MRI data utilized in this research were fully anonymized prior to release, ensuring strict adherence to patient confidentiality and data protection regulations. The study follows the fundamental ethical principles of medical research, including non-maleficence, patient privacy, and responsible AI deployment in clinical decision support systems.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>Not applicable.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>28</volume>
          <page-range>16-26</page-range>
          <issue>1</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Fei</given-names>
            </name>
            <name>
              <surname>Preininger</surname>
              <given-names>Anita</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1055/s-0039-1677908</pub-id>
          <article-title>AI in health: State of the art, challenges, and future directions</article-title>
          <source>Yearb. Med. Inform.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>2015</volume>
          <page-range>CD010708</page-range>
          <issue>4</issue>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lutomski</surname>
              <given-names>Jennifer E.</given-names>
            </name>
            <name>
              <surname>Meaney</surname>
              <given-names>Sarah</given-names>
            </name>
            <name>
              <surname>Greene</surname>
              <given-names>Richard A.</given-names>
            </name>
            <name>
              <surname>Ryan</surname>
              <given-names>Anthony C.</given-names>
            </name>
            <name>
              <surname>Devane</surname>
              <given-names>Declan</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1002/14651858.CD010708.pub2</pub-id>
          <article-title>Expert systems for fetal assessment in labour</article-title>
          <source>Cochrane Database Syst. Rev.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>109</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sivajohan</surname>
              <given-names>Brintha</given-names>
            </name>
            <name>
              <surname>Elgendi</surname>
              <given-names>Mohamed</given-names>
            </name>
            <name>
              <surname>Menon</surname>
              <given-names>Carlo</given-names>
            </name>
            <name>
              <surname>Allaire</surname>
              <given-names>Catherine</given-names>
            </name>
            <name>
              <surname>Yong</surname>
              <given-names>Paul</given-names>
            </name>
            <name>
              <surname>Bedaiwy</surname>
              <given-names>Mohamed A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41746-022-00638-1</pub-id>
          <article-title>Clinical use of artificial intelligence in endometriosis: A scoping review</article-title>
          <source>npj Digit. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>600604</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Elgendi</surname>
              <given-names>Mohamed</given-names>
            </name>
            <name>
              <surname>Allaire</surname>
              <given-names>Catherine</given-names>
            </name>
            <name>
              <surname>Williams</surname>
              <given-names>Christina</given-names>
            </name>
            <name>
              <surname>Bedaiwy</surname>
              <given-names>Mohamed A.</given-names>
            </name>
            <name>
              <surname>Yong</surname>
              <given-names>Paul J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fdgth.2020.600604</pub-id>
          <article-title>Machine learning revealed new correlates of chronic pelvic pain in women</article-title>
          <source>Front. Digit. Health</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>23</volume>
          <page-range>1-2</page-range>
          <issue>1</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yoldemir</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/13697137.2019.1682804</pub-id>
          <article-title>Artificial intelligence and women's health</article-title>
          <source>Climacteric</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>852746</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bhardwaj</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Sharma</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Parambath</surname>
              <given-names>S. V.</given-names>
            </name>
            <name>
              <surname>Gul</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Peter  Lobie</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Qin</surname>
              <given-names>P. W.</given-names>
            </name>
            <name>
              <surname>Pandey</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fonc.2022.852746</pub-id>
          <article-title>Machine learning for endometrial cancer prediction and prognostication</article-title>
          <source>Front. Oncol.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>59</volume>
          <page-range>499</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Szubert</surname>
              <given-names>Maria</given-names>
            </name>
            <name>
              <surname>Rycerz</surname>
              <given-names>Aleksander</given-names>
            </name>
            <name>
              <surname>Wilczyński</surname>
              <given-names>Jacek R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/medicina59030499</pub-id>
          <article-title>How to improve non-invasive diagnosis of endometriosis with advanced statistical methods</article-title>
          <source>Medicina</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>41</volume>
          <page-range>1-17</page-range>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ridley</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.6017/ital.v41i2.14683</pub-id>
          <article-title>Explainable Artificial Intelligence (XAI): Adoption and advocacy</article-title>
          <source>Inf. Technol. Libr.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>56</volume>
          <page-range>487</page-range>
          <issue>10</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lupean</surname>
              <given-names>Roxana Adelina</given-names>
            </name>
            <name>
              <surname>Ştefan</surname>
              <given-names>Paul Andrei</given-names>
            </name>
            <name>
              <surname>Csutak</surname>
              <given-names>Csaba</given-names>
            </name>
            <name>
              <surname>Lebovici</surname>
              <given-names>Andrei</given-names>
            </name>
            <name>
              <surname>Măluţan</surname>
              <given-names>Andrei Mihai</given-names>
            </name>
            <name>
              <surname>Buiga</surname>
              <given-names>Rareş</given-names>
            </name>
            <name>
              <surname>Melincovici</surname>
              <given-names>Carmen Stanca</given-names>
            </name>
            <name>
              <surname>Mihu</surname>
              <given-names>Carmen Mihaela</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/medicina56100487</pub-id>
          <article-title>Differentiation of endometriomas from ovarian hemorrhagic cysts at magnetic resonance: The role of texture analysis</article-title>
          <source>Medicina</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>974245</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mao</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>C. X.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>H. C.</given-names>
            </name>
            <name>
              <surname>Xiong</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Y. P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fphys.2022.974245</pub-id>
          <article-title>A deep learning-based automatic staging method for early endometrial cancer on MRI images</article-title>
          <source>Front. Physiol.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>1290036</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>H. L.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>H. L.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Shuid</surname>
              <given-names>A. N.</given-names>
            </name>
            <name>
              <surname>Sandai</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>X. B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fgene.2023.1290036</pub-id>
          <article-title>Machine learning-based integrated identification of predictive combined diagnostic biomarkers for endometriosis</article-title>
          <source>Front. Genet.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>114</volume>
          <page-range>103438</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kurata</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Nishio</surname>
              <given-names>Mizuho</given-names>
            </name>
            <name>
              <surname>Kido</surname>
              <given-names>Aki</given-names>
            </name>
            <name>
              <surname>Fujimoto</surname>
              <given-names>Koji</given-names>
            </name>
            <name>
              <surname>Yakami</surname>
              <given-names>Masahiro</given-names>
            </name>
            <name>
              <surname>Isoda</surname>
              <given-names>Hiroyoshi</given-names>
            </name>
            <name>
              <surname>Togashi</surname>
              <given-names>Kaori</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compbiomed.2019.103438</pub-id>
          <article-title>Automatic segmentation of the uterus on MRI using a convolutional neural network</article-title>
          <source>Comput. Biol. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>39</volume>
          <page-range>333-343</page-range>
          <issue>4</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Downing</surname>
              <given-names>Michael J</given-names>
            </name>
            <name>
              <surname>Papke</surname>
              <given-names>David J</given-names>
            </name>
            <name>
              <surname>Tyekucheva</surname>
              <given-names>Svitlana</given-names>
            </name>
            <name>
              <surname>Mutter</surname>
              <given-names>George L</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1097/PGP.0000000000000615</pub-id>
          <article-title>A new classification of benign, premalignant, and malignant endometrial tissues using machine learning applied to 1413 candidate variables</article-title>
          <source>Int. J. Gynecol. Pathol.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>51</volume>
          <page-range>586-595</page-range>
          <issue>5</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Guerriero</surname>
              <given-names>Stefano</given-names>
            </name>
            <name>
              <surname>Saba</surname>
              <given-names>Luca</given-names>
            </name>
            <name>
              <surname>Pascual</surname>
              <given-names>Maria Angela</given-names>
            </name>
            <name>
              <surname>Ajossa</surname>
              <given-names>Silvia</given-names>
            </name>
            <name>
              <surname>Rodriguez</surname>
              <given-names>Ignacio</given-names>
            </name>
            <name>
              <surname>Mais</surname>
              <given-names>Valerio</given-names>
            </name>
            <name>
              <surname>Alcazar</surname>
              <given-names>Juan Luis</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1002/uog.18961</pub-id>
          <article-title>Transvaginal ultrasound vs magnetic resonance imaging for diagnosing deep infiltrating endometriosis: Systematic review and meta-analysis</article-title>
          <source>Ultrasound Obstet. Gynecol.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>4051</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bendifallah</surname>
              <given-names>Sofiane</given-names>
            </name>
            <name>
              <surname>Dabi</surname>
              <given-names>Yohann</given-names>
            </name>
            <name>
              <surname>Suisse</surname>
              <given-names>Stéphane</given-names>
            </name>
            <name>
              <surname>Jornea</surname>
              <given-names>Ludmila</given-names>
            </name>
            <name>
              <surname>Bouteiller</surname>
              <given-names>Delphine</given-names>
            </name>
            <name>
              <surname>Touboul</surname>
              <given-names>Cyril</given-names>
            </name>
            <name>
              <surname>Puchar</surname>
              <given-names>Anne</given-names>
            </name>
            <name>
              <surname>Daraï</surname>
              <given-names>Emile</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-022-07771-7</pub-id>
          <article-title>MicroRNome analysis generates a blood-based signature for endometriosis</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>3015</page-range>
          <issue>11</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tore</surname>
              <given-names>Ulan</given-names>
            </name>
            <name>
              <surname>Abilgazym</surname>
              <given-names>Aibek</given-names>
            </name>
            <name>
              <surname>Barco</surname>
              <given-names>Angel</given-names>
            </name>
            <name>
              <surname>Terzic</surname>
              <given-names>Milan</given-names>
            </name>
            <name>
              <surname>Yemenkhan</surname>
              <given-names>Yerden</given-names>
            </name>
            <name>
              <surname>Zollanvari</surname>
              <given-names>Amin</given-names>
            </name>
            <name>
              <surname>Sarria-Santamera</surname>
              <given-names>Antonio</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/biomedicines11113015</pub-id>
          <article-title>Diagnosis of endometriosis based on comorbidities: A machine learning approach</article-title>
          <source>Biomedicines</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>42</volume>
          <page-range>429-441</page-range>
          <issue>2</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Bo Han</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Sha</given-names>
            </name>
            <name>
              <surname>Duan</surname>
              <given-names>Hua</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Yi Yi</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>Zheng Chen</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.rbmo.2020.10.005</pub-id>
          <article-title>Discovery of gene module acting on ubiquitin-mediated proteolysis pathway by co-expression network analysis for endometriosis</article-title>
          <source>Reprod. Biomed. Online</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>654-664</page-range>
          <issue>3</issue>
          <year>2010</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mihalyi</surname>
              <given-names>Attila</given-names>
            </name>
            <name>
              <surname>Gevaert</surname>
              <given-names>Olivier</given-names>
            </name>
            <name>
              <surname>Kyama</surname>
              <given-names>Cleophas M</given-names>
            </name>
            <name>
              <surname>Simsa</surname>
              <given-names>Peter</given-names>
            </name>
            <name>
              <surname>Pochet</surname>
              <given-names>Nathalie</given-names>
            </name>
            <name>
              <surname>De Smet</surname>
              <given-names>Frank</given-names>
            </name>
            <name>
              <surname>De Moor</surname>
              <given-names>Bart</given-names>
            </name>
            <name>
              <surname>Meuleman</surname>
              <given-names>Christel</given-names>
            </name>
            <name>
              <surname>Billen</surname>
              <given-names>Johan</given-names>
            </name>
            <name>
              <surname>Blanckaert</surname>
              <given-names>Norbert</given-names>
            </name>
            <name>
              <surname>Vodolazkaia</surname>
              <given-names>Alexandra</given-names>
            </name>
            <name>
              <surname>Fulop</surname>
              <given-names>Vilmos</given-names>
            </name>
            <name>
              <surname>D'Hooghe</surname>
              <given-names>Thomas M</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1093/humrep/dep425</pub-id>
          <article-title>Non-invasive diagnosis of endometriosis based on a combined analysis of six plasma biomarkers</article-title>
          <source>Hum. Reprod.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>19795</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Parlatan</surname>
              <given-names>Ugur</given-names>
            </name>
            <name>
              <surname>Inanc</surname>
              <given-names>Medine Tuna</given-names>
            </name>
            <name>
              <surname>Ozgor</surname>
              <given-names>Bahar Yuksel</given-names>
            </name>
            <name>
              <surname>Oral</surname>
              <given-names>Engin</given-names>
            </name>
            <name>
              <surname>Bastu</surname>
              <given-names>Ercan</given-names>
            </name>
            <name>
              <surname>Unlu</surname>
              <given-names>Mehmet Burcin</given-names>
            </name>
            <name>
              <surname>Basar</surname>
              <given-names>Gunay</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-019-56308-y</pub-id>
          <article-title>Raman spectroscopy as a non-invasive diagnostic technique for endometriosis</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>16738</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Knific</surname>
              <given-names>Tamara</given-names>
            </name>
            <name>
              <surname>Fishman</surname>
              <given-names>Dmytro</given-names>
            </name>
            <name>
              <surname>Vogler</surname>
              <given-names>Andrej</given-names>
            </name>
            <name>
              <surname>Gst"ottner</surname>
              <given-names>Manuela</given-names>
            </name>
            <name>
              <surname>Wenzl</surname>
              <given-names>Ren'e</given-names>
            </name>
            <name>
              <surname>Peterson</surname>
              <given-names>Hedi</given-names>
            </name>
            <name>
              <surname>Rižner</surname>
              <given-names>Tea Lanišnik</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-019-52899-8</pub-id>
          <article-title>Multiplex analysis of 40 cytokines do not allow separation between endometriosis patients and controls</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>5499</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Goldstein</surname>
              <given-names>Anat</given-names>
            </name>
            <name>
              <surname>Cohen</surname>
              <given-names>Shani</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-023-32761-8</pub-id>
          <article-title>Self-report symptom-based endometriosis prediction using machine learning</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ronneberger</surname>
              <given-names>Olaf</given-names>
            </name>
            <name>
              <surname>Fischer</surname>
              <given-names>Philipp</given-names>
            </name>
            <name>
              <surname>Brox</surname>
              <given-names>Thomas</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1505.04597</pub-id>
          <article-title>U-Net: Convolutional networks for biomedical image segmentation</article-title>
          <source>arXiv preprint arXiv:1505.04597</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <page-range>60</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shorten</surname>
              <given-names>Connor</given-names>
            </name>
            <name>
              <surname>Khoshgoftaar</surname>
              <given-names>Taghi M</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s40537-019-0197-0</pub-id>
          <article-title>A survey on image data augmentation for deep learning</article-title>
          <source>J. Big Data</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>23</volume>
          <page-range>634</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chaddad</surname>
              <given-names>Ahmad</given-names>
            </name>
            <name>
              <surname>Peng</surname>
              <given-names>Ji Hao</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Jian</given-names>
            </name>
            <name>
              <surname>Bouridane</surname>
              <given-names>Ahmed</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s23020634</pub-id>
          <article-title>Survey of explainable AI techniques in healthcare</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <issue>4</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Thakur</surname>
              <given-names>Ruchi</given-names>
            </name>
          </person-group>
          <article-title>Explainable AI: Developing interpretable deep learning models for medical diagnosis</article-title>
          <source>Int. J. Multidiscip. Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>3438</page-range>
          <issue>6</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yan</surname>
              <given-names>Fei</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Yun Qing</given-names>
            </name>
            <name>
              <surname>Xia</surname>
              <given-names>Yi Wen</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Zhi Liang</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>Ruo Xiu</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app13063438</pub-id>
          <article-title>An explainable brain tumor detection framework for MRI analysis</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>5088</page-range>
          <issue>11</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Antoniadi</surname>
              <given-names>Anna Markella</given-names>
            </name>
            <name>
              <surname>Du</surname>
              <given-names>Yu Han</given-names>
            </name>
            <name>
              <surname>Guendouz</surname>
              <given-names>Yasmine</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>Lan</given-names>
            </name>
            <name>
              <surname>Mazo</surname>
              <given-names>Claudia</given-names>
            </name>
            <name>
              <surname>Becker</surname>
              <given-names>Brett A</given-names>
            </name>
            <name>
              <surname>Mooney</surname>
              <given-names>Catherine</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app11115088</pub-id>
          <article-title>Current challenges and future opportunities for XAI in machine learning-based clinical decision support systems: A systematic review</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>