<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">NSIA</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Nonlinear Science and Intelligent Applications</journal-title>
        <abbrev-journal-title abbrev-type="issn">Nonlinear Sci. Intell. Appl.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">NSIA</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">3105-7829</issn>
      <issn publication-format="print">3105-7837</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-R3bkv6E83bGW8puzzUO3zqA67X9Le1o4</article-id>
      <article-id pub-id-type="doi">10.56578/nsia010102</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Benchmarking Convolutional Neural Network Architectures for Potato Leaf Disease Identification</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0008-7227-9182</contrib-id>
          <name>
            <surname>Cakmak</surname>
            <given-names>Yigitcan</given-names>
          </name>
          <email>ygtcncakmak@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1569-8715</contrib-id>
          <name>
            <surname>Yazgan</surname>
            <given-names>Ramazan</given-names>
          </name>
          <email>ryazgan503@gmail.com</email>
        </contrib>
        <aff id="aff_1">Department of Computer Engineering, Faculty of Engineering, Igdir Universty, 76000 Igdir, Turkey</aff>
        <aff id="aff_2">Department of Mathematics, Faculty of Science, Yuzuncu Yil University, 65080 Van, Turkey</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>29</day>
        <month>09</month>
        <year>2025</year>
      </pub-date>
      <volume>1</volume>
      <issue>1</issue>
      <fpage>18</fpage>
      <lpage>26</lpage>
      <page-range>18-26</page-range>
      <history>
        <date date-type="received">
          <day>01</day>
          <month>07</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>19</day>
          <month>09</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Potato production is critically influenced by foliar diseases such as Early Blight and Late Blight, which continue to threaten global food security. Although visual inspection remains widely used, such assessments are subjective, time-consuming, and difficult to scale, creating a pressing need for automated and reliable diagnostic frameworks. In this study, the classification performance and computational efficiency of four state-of-the-art Convolutional Neural Network (CNN) architectures—the Residual Network with 50 layers (ResNet-50), Densely Connected Network with 169 layers (DenseNet-169), EfficientNetV2-B3, and InceptionV3—were systematically benchmarked for the identification of healthy potato leaves and those affected by Early Blight or Late Blight using the publicly available PlantVillage dataset. Accuracy, precision, recall, and F1 score were employed to characterize predictive performance, while parameter count and giga floating-point operations per second (GFLOPS) were used to assess computational efficiency. High-level classification capability was consistently achieved across all models, with overall accuracies ranging from 98% to 99%. DenseNet-169 achieved the highest classification accuracy at 99% with fewer than 13 million parameters, and EfficientNetV2-B3 attained 98% accuracy while exhibiting tsshe lowest GFLOPS requirement. The results indicate that architectures designed for parameter efficiency and feature reuse, such as DenseNet-169 and EfficientNetV2-B3, provide accuracy that is comparable to or surpasses that of less efficient baseline models while offering significant advantages in resource efficiency. These findings reinforce the strong potential of lightweight and high-performance CNN architectures to support scalable, real-time agricultural disease diagnostic systems, particularly in regions where computational resources and technical expertise may be limited.</p></abstract>
      <kwd-group>
        <kwd>Potato diseases</kwd>
        <kwd>DL</kwd>
        <kwd>Plant disease classification</kwd>
        <kwd>Computational efficiency</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="2"/>
        <fig-count count="2"/>
        <table-count count="2"/>
        <ref-count count="37"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>As the fourth most important food crop globally, potato (<italic>Solanum tuberosum</italic>) is a key component of global food security [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>]. Potatoes are a major source of nutrition for over one billion people, demonstrating the importance of producing and supplying potatoes reliably [<xref ref-type="bibr" rid="ref_3">3</xref>]. Nevertheless, potato production is constantly threatened by several phytopathological conditions, particularly fungal diseases such as Early Blight (<italic>Alternaria solani</italic>) and Late Blight (<italic>Phytophthora infestans</italic>) [<xref ref-type="bibr" rid="ref_4">4</xref>]. These are very damaging diseases that are capable of causing significant losses in yield and economic losses in production, thereby disrupting a major part of the global food supply network [<xref ref-type="bibr" rid="ref_5">5</xref>]. The detection and control of potato diseases has traditionally been dependent on visual inspection by agronomists and growers [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>]. The manual approach is inherently problematic because it is subjective, labor-intensive, and also highly prone to human error [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>]. The quality of diagnosis is highly dependent on the expertise of the individual, lacks repeatability, and is especially unreliable during the early stages of infection, when symptoms may be ambiguous or easily mistaken for those associated with nutritional deficiencies [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>]. Misdiagnoses often lead to delays or improper episodes of fungicides that can increase costs and create a real threat to the environment [<xref ref-type="bibr" rid="ref_12">12</xref>], [<xref ref-type="bibr" rid="ref_13">13</xref>].</p><p>The significant constraints of traditional diagnostics in agriculture have highlighted an urgent need for automated, accurate, and scalable systems [<xref ref-type="bibr" rid="ref_14">14</xref>], [<xref ref-type="bibr" rid="ref_15">15</xref>], [<xref ref-type="bibr" rid="ref_16">16</xref>]. Paralleling this challenge, deep learning (DL), particularly CNNs, has achieved transformative success in other domains demanding high-stakes pattern recognition. This validation is exceptionally strong in medical diagnostics, where CNNs have demonstrated a robust ability to discriminate subtle pathologies across diverse and complex modalities [<xref ref-type="bibr" rid="ref_17">17</xref>], from neurological tumors in brain scans to malignancies in breast [<xref ref-type="bibr" rid="ref_18">18</xref>], [<xref ref-type="bibr" rid="ref_19">19</xref>], [<xref ref-type="bibr" rid="ref_20">20</xref>] and lung imagery [<xref ref-type="bibr" rid="ref_21">21</xref>]. This proven capacity to autonomously learn and extract hierarchical, discriminative features from such varied visual data is now converging with computer vision to create a new paradigm in precision agriculture. These architectures are consequently becoming the foundation for automated plant disease classification, providing the robust tools needed for accurate diagnosis from complex agricultural imagery, such as in potato leaves [<xref ref-type="bibr" rid="ref_22">22</xref>], [<xref ref-type="bibr" rid="ref_23">23</xref>].</p><p>Informed by this technological basis, the current study builds and evaluates a DL system for the classification of three key potato leaf classes, namely healthy, Early Blight, and Late Blight, using the public PlantVillage dataset. This study undertakes a systematic comparative study of four leading and diverse Convolutional Neural Network (CNN) architectures: Residual Network with 50 layers (ResNet-50), Densely Connected Network with 169 layers (DenseNet-169), EfficientNetV2-B3, and InceptionV3. The performance of these models is analyzed using standard performance metrics, providing information regarding potential effectiveness for real-world use at scale in agriculture. The key contributions of this study are briefly summarized as follows:</p><p>• A robust DL framework is developed for the three-class classification of potato leaf diseases (healthy, Early Blight, and Late Blight) utilizing the public PlantVillage dataset.</p><p>• A comprehensive benchmark analysis is conducted, comparing the performance of four distinct and influential CNN architectures: ResNet-50, DenseNet-169, EfficientNetV2-B3, and InceptionV3.</p><p>• The models are rigorously evaluated using a suite of metrics, including accuracy, precision, recall, and F1 score, to identify the most effective architecture for this specific diagnostic task.</p><p>• This research provides a comparative assessment that aids in selecting computationally efficient and accurate models, thereby supporting the development of accessible diagnostic tools for sustainable agriculture.</p>
    </sec>
    <sec sec-type="">
      <title>2. Related work</title>
      <p>Wang and Su [<xref ref-type="bibr" rid="ref_24">24</xref>] presented a detailed review covering DL applications throughout the potato production chain and grouped these uses into key areas like crop health management, yield prediction, and resource management. Their work examined various models, including CNNs and Recurrent Neural Networks and detailed their roles in tasks from pest detection to price forecasting. It was concluded that while DL offers major benefits for improving efficiency and productivity, major challenges remain about the availability of diverse datasets and the practical deployment of these technologies in real agricultural settings. Selvi et al. [<xref ref-type="bibr" rid="ref_25">25</xref>] developed CropViT, a computationally very efficient Vision Transformer architecture designed specifically for high-throughput plant disease diagnosis. Their experimental work involved a direct comparison of CropViT against a conventional CNN model on the PlantVillage dataset, with a focus on nine different plant species. The results indicated that CropViT achieved an average accuracy of 98.64% and significantly outperformed the traditional CNN. This highlights the strong potential of transformer-based approaches in agricultural diagnostics.</p><p>Dutta et al. [<xref ref-type="bibr" rid="ref_26">26</xref>] developed a specialized CNN architecture for automatically detecting and classifying potato blight diseases in their early stages. The proposed model was compared against common architectures like ResNet-50, VGG16, and GoogLeNet using a dataset consisting of healthy, Early Blight, and Late Blight samples. Their findings showed that the custom CNN model reached an accuracy of 98%. This demonstrates the value of tailored DL solutions for specific tasks in agricultural phytopathology. Bajpai et al. [<xref ref-type="bibr" rid="ref_27">27</xref>] proposed an architectural augmentation to the Swin Transformer model to improve the detection accuracy of potato leaf diseases, specifically Early Blight and Late Blight. Their modification involved adding a custom sequential head module consisting of linear, ReLU, and dropout layers to the standard Swin Transformer to improve feature representation and reduce overfitting. When evaluated on a custom dataset, the improved model achieved 99.38% accuracy. This confirmed the effectiveness of architectural refinements in boosting generalization performance for agricultural computer vision applications.</p><p>Zhang et al. [<xref ref-type="bibr" rid="ref_28">28</xref>] introduced an optimized VGG16 architecture, called VGG16S, to address both computational efficiency and diagnostic accuracy in potato disease detection. The optimization strategy involved replacing dense layers with global average pooling, integrating the Convolutional Block Attention Module, and using the leaky ReLU activation function. This multi-part approach reduced the model's parameter complexity to just one-tenth of the original VGG16 while still reaching an accuracy of 97.87%. This shows that lightweight, optimized architectures can offer major benefits in both accuracy and efficiency. Sharma and Sharma [<xref ref-type="bibr" rid="ref_29">29</xref>] explored using Recurrent Neural Networks for classifying healthy and diseased potato leaves from the PlantVillage dataset, which breaks from common CNN-based methods. Their proposed architecture used Long Short-Term Memory units for feature extraction and was compared against CNN and Feedforward Neural Network models. The experimental results demonstrated that the Recurrent Neural Networ model reached an accuracy of 92.7%. This suggests that Recurrent Neural Network architectures, with their capacity for temporal sequence processing, can offer a competitive advantage in image-based classification tasks.</p><p>Zoralioğlu and Polat [<xref ref-type="bibr" rid="ref_30">30</xref>] conducted a comparative analysis to examine the important role of data augmentation and class balancing in potato disease detection. They evaluated three different architectures (i.e., a custom 5-layer CNN, EfficientNetB2, and ConvNeXtSmall) on both the original (imbalanced) and balanced (augmented) versions of the PlantVillage dataset. Their findings highlighted the important relationship between data distribution and model performance: The custom CNN performed best on imbalanced data, while EfficientNetB2 achieved 99.89% accuracy on the balanced data. This clearly indicates the necessity of data balancing strategies for reaching the full potential of advanced DL models.</p>
    </sec>
    <sec sec-type="">
      <title>3. Materials and methods</title>
      
        <sec>
          
            <title>3.1. Dataset and data preprocessing</title>
          
          <p>The PlantVillage dataset, a large public dataset created for classifying plant diseases via plant leaf images [<xref ref-type="bibr" rid="ref_31">31</xref>], was used for this study. For this study, only the potato leaf subset was selected, which consists of three classes: two disease classes (Early Blight and Late Blight) and one healthy leaf class. <xref ref-type="fig" rid="fig_1">Figure 1</xref> shows examples of the visual characteristics of leaves in each class which this study intended for the models to identify.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>Sample images of potato leaves for healthy, Early Blight, and late Blight Classes</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/11/img_iSBj5ixlig14JY5i.png"/>
            </fig>
          
          <p>To ensure a complete and unbiased assessment of the models, the data was carefully separated into three distinct subsets of data: 70% for training, 15% for validation, and 15% for the final evaluation of the models. The distribution of images in the different subsets of data (with the count of samples for each class listed for training, validation, and testing) can be seen in <xref ref-type="table" rid="table_1">Table 1</xref> . This means the models are trained on the majority of the data, optimized on a separate validation dataset, and then finally tested on completely separate data.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Distribution of classes across data splits</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Class</p></td><td colspan="1" rowspan="1"><p>Train (70%)</p></td><td colspan="1" rowspan="1"><p>Validation (15%)</p></td><td colspan="1" rowspan="1"><p>Test (15%)</p></td><td colspan="1" rowspan="1"><p>Total</p></td></tr><tr><td colspan="1" rowspan="1"><p>Early Blight</p></td><td colspan="1" rowspan="1"><p>700</p></td><td colspan="1" rowspan="1"><p>150</p></td><td colspan="1" rowspan="1"><p>150</p></td><td colspan="1" rowspan="1"><p>1000</p></td></tr><tr><td colspan="1" rowspan="1"><p>Healthy</p></td><td colspan="1" rowspan="1"><p>106</p></td><td colspan="1" rowspan="1"><p>22</p></td><td colspan="1" rowspan="1"><p>24</p></td><td colspan="1" rowspan="1"><p>152</p></td></tr><tr><td colspan="1" rowspan="1"><p>Late Blight</p></td><td colspan="1" rowspan="1"><p>700</p></td><td colspan="1" rowspan="1"><p>150</p></td><td colspan="1" rowspan="1"><p>150</p></td><td colspan="1" rowspan="1"><p>1000</p></td></tr><tr><td colspan="1" rowspan="1"><p>Total</p></td><td colspan="1" rowspan="1"><p>1506</p></td><td colspan="1" rowspan="1"><p>322</p></td><td colspan="1" rowspan="1"><p>324</p></td><td colspan="1" rowspan="1"><p>2152</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>As the first necessary step, a consistent standardized data preprocessing pipeline was set up. All images were resized to a consistent 224 × 224 pixel size, based on the input size dimensions of the pre-trained networks used in this work. Pixel values for images were standardized to floating-point numbers in the range of [0, 1], which is a common practice to stabilize and speed up convergence during training. To reduce the chances of overfitting and maximize model generalizability, a data augmentation approach was used only for the purpose of training the dataset. Data augmentation in this study entailed the random application of transformations, including horizontal flipping, rotations, and zooming, for training images dynamically during training to generate an artificial pictorial variety for the training dataset—without changing validation or testing datasets [<xref ref-type="bibr" rid="ref_32">32</xref>], [<xref ref-type="bibr" rid="ref_33">33</xref>].</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Model architecture</title>
          
          <p>This study comparatively evaluates four distinct CNN architectures, each representing significant advancements in DL for computer vision tasks. These models were selected for their diverse structural philosophies and proven performance across various image recognition benchmarks. The ResNet architecture, particularly ResNet-50, presented the concept of residual learning to solve the issue of degradation associated with training extremely deep networks. The core idea concerns the use of identity shortcut connections (i.e., skip connections) that allow gradients to skip one or multiple layers. This permits the network to learn a residual function with respect to the inputs from the earlier layers, which supports easier optimization and enables deeper networks to be built without losing performance to vanishing gradients. The architecture of ResNet-50 includes an initial convolution layer, a max-pooling layer, aggregated stacked residual blocks that contain 1 × 1, 3 × 3, and 1 × 1 convolutions, a global average pooling layer and lastly a fully connected classification layer [<xref ref-type="bibr" rid="ref_34">34</xref>].</p><p>The Densely Connected Convolutional Network (DenseNet) displays maximum information flow between layers with a unique connectivity model that connects every layer directly to every other layer in a deep feed-forward manner (represented here with 169 layers). DenseNet connects feature maps from all previous layers together at each layer, while ResNets take a weighted sum of features (the skipped connections), which allows feature reuse and utilizes deeper neural networks. Thus, dense connectivity results in models that are often smaller and contain fewer parameters than similarly deep ResNets, while also alleviating the vanishing gradient problem. Each layer outputs features which are concatenated to all previous layers at specific depths in the network, referred to as “dense blocks,” followed by layers that perform batch normalization and average pooling, with the intention of reducing the feature map size, referred to as “transition layers” [<xref ref-type="bibr" rid="ref_35">35</xref>].</p><p>EfficientNetV2 is the next version of EfficientNets designed to address not only accuracy and parameter efficiency but also improved training speed. It is based on the compound scaling mechanism in the original EfficientNet that uniformly scales the depth, width, and resolution of the network, and it has a few additions, such as Fused-MBConv blocks (fusing the depthwise and 1 × 1 convolutions into one regular convolution in the initial layers) and a progressive training scheme that adjusts image size and regularization during training. EfficientNetV2-B3 is a configuration of EfficientNetV2 that attempts to achieve a strong trade-off between cost and performance in a variety of applications where speed and accuracy matter [<xref ref-type="bibr" rid="ref_36">36</xref>].</p><p>The InceptionV3 structure is a member of the GoogLeNet family and is noted for its distinctive Inception module. The purpose of this module is to allow learning from multiple scales at once in a deliberate attempt to achieve parallelism within a single layer. Inception achieves this parallelism by using convolutional filters of different sizes (1 × 1, 3 × 3, and 5 × 5) operating in parallel, along with max pooling. The InceptionV3 model made improvements such as factorizing larger convolutions into smaller convolutions (e.g., replacing a 5 × 5 convolution with two 3 × 3 convolutions) and using asymmetric convolution configurations (e.g., using a 1 × n convolution followed by an n × 1 convolution) with the intent of reducing computational cost while preserving the representational capacity of the model. InceptionV3 also introduced the use of batch normalization and label smoothing regularization to enhance stability during training and maximize generalization [<xref ref-type="bibr" rid="ref_37">37</xref>].</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Transfer learning</title>
          
          <p>In order to improve model convergence and prediction ability, the transfer learning approach was used in this study. The selected CNN architectures (ResNet-50, DenseNet-169, EfficientNetV2-B3, and InceptionV3) were initialized with weights trained on the large-scale ImageNet dataset. This takes advantage of the rich, hierarchical features learned from millions of varied images, producing strong inductive bias for the target task. For each pre-trained model, the original final classification layer, typically for the 1000 classes in ImageNet, was removed and replaced with a new fully connected dense layer specific to the three-class taxonomy of the potato leaf disease classification problem (healthy, Early Blight, and Late Blight).</p><p>A fine-tuning protocol consisting of two stages was implemented in the training procedure. In the first stage, the parameters of the pre-trained feature extraction backbone were frozen, and only the newly added classification layer was trained. This allows the classifier to become accustomed to the features generated by the frozen backbone. In stage three, the whole network was trained end-to-end, frequently with a lower learning rate, which refined and specialized the pre-trained features for the specifics of the potato leaf dataset. This approach, combined with data augmentation applied to the training set, helped generalize the model and lessen the risk of overfitting.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Experimental design and training protocol</title>
          
          <p>To facilitate a fair and straightforward comparison of the DL architectures selected for this study, a structured experimental protocol was developed. All models were implemented in the Python programming environment using the TensorFlow framework. Both training and inference were completed on a high-performance computer workstation with an NVIDIA GeForce RTX 5090 GPU (32 GB VRAM). All experiments in this study were conducted using the same training procedure to create consistency. The Adam optimization algorithm was used to optimize the model’s parameters. Then, for the fine-tuning (i.e., the second part of the training where the entire network is retrained), the learning rate remained at 1 × 10<sup>-4</sup>. A mini-batch size of 16 was utilized in all training. It was also predetermined to run for a maximum of 100 epochs.</p><p>To counteract overfitting and encourage generalizability, the criterion of early stopping was incorporated. This criterion observed the validation loss at the end of each epoch to stop training if it had not improved after 10 epochs (patience = 10). When training was completed (whether at the maximum allowable epochs or because of early stopping), the model’s weights from the epoch with the lowest validation loss were saved for final evaluation on the held-out test dataset.</p>
        </sec>
      
      
        <sec>
          
            <title>3.5. Performance evaluation metrics</title>
          
          <p>The performance of each of the DL models was evaluated on the independent test dataset. A full complement of standard performance metrics was used for evaluation. Accuracy is the key performance measure and reflects the overall ratio of correctly classified examples compared to the total number of examples in the test set. It provides an overall measure of predictive accuracy. </p><p>To better understand the class-level performance of the model and potential issues arising from class imbalance, precision, recall, and F1 score were also included in the evaluation. Precision indicates the proportion of positive predictions that are truly positive, also referred to as the positive predictive value. Recall is sometimes called sensitivity, or the true positive rate, and indicates the overall ability of the model to capture all actual positive examples that belong to the class. The F1 score is a measure that indicates the harmonic mean of precision and recall and is valuable for class imbalance scenarios. The mathematical definitions of these metrics are provided below.</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="m4p76r3nh3">
                <mml:mtext>Accuracy</mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>T</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>T</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="mu4tixvje8">
                <mml:mtext>Precision</mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="mlcqlyovsn">
                <mml:mtext>Recall</mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="mynkfy61u6">
                <mml:mtext>F1 score</mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mo>×</mml:mo>
                <mml:mn>2</mml:mn>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>P</mml:mi>
                    <mml:mi>r</mml:mi>
                    <mml:mi>e</mml:mi>
                    <mml:mi>c</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mi>s</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mi>o</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mi>R</mml:mi>
                    <mml:mi>e</mml:mi>
                    <mml:mi>c</mml:mi>
                    <mml:mi>a</mml:mi>
                    <mml:mi>l</mml:mi>
                    <mml:mi>l</mml:mi>
                    <mml:mo>×</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>P</mml:mi>
                    <mml:mi>r</mml:mi>
                    <mml:mi>e</mml:mi>
                    <mml:mi>c</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mi>s</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mi>o</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mi>R</mml:mi>
                    <mml:mi>e</mml:mi>
                    <mml:mi>c</mml:mi>
                    <mml:mi>a</mml:mi>
                    <mml:mi>l</mml:mi>
                    <mml:mi>l</mml:mi>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>where, <italic>TP</italic>, <italic>TN</italic>, <italic>FP</italic> and <italic>FN </italic>denote true positive, true negative, false positive, and false negative, respectively. For the multi-class classification problem solved in this project, all metrics were calculated separately for each class (healthy, Early Blight, and Late Blight) and subsequently macro-averaged to generate an overall performance score for the model where all classes were weighted equally irrespective of their sample size.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Results and discussion</title>
      <p>The experimental stage aimed to facilitate a comprehensive assessment of the four learning models, in particular evaluating their performance on the unseen potato leaf disease test dataset. A comprehensive assessment was conducted involving the extraction of quantitative results that represented standard classification performance measures as well as complexity measures. The four primary evaluation measures of accuracy, precision, recall, and F1 score were put in place to ensure an unbiased and reasonable assessment of each model’s generalization ability. In conjunction, each of the model's computational complexities was documented by measuring its parameters (Params) and giga floating-point operations per second (GFLOPS). The summarized findings, which inform this report and analysis, are organized in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>Performance evaluation results of deep learning models</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Models</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>Precision</p></td><td colspan="1" rowspan="1"><p>Recall</p></td><td colspan="1" rowspan="1"><p>F1 score</p></td><td colspan="1" rowspan="1"><p>Params</p></td><td colspan="1" rowspan="1"><p>GFLOPS</p></td><td colspan="1" rowspan="1"><p>Inference Time (ms)</p></td></tr><tr><td colspan="1" rowspan="1"><p>ResNet-50</p></td><td colspan="1" rowspan="1"><p>0.98</p></td><td colspan="1" rowspan="1"><p>0.96</p></td><td colspan="1" rowspan="1"><p>0.96</p></td><td colspan="1" rowspan="1"><p>0.96</p></td><td colspan="1" rowspan="1"><p>23.51 M</p></td><td colspan="1" rowspan="1"><p>8.26</p></td><td colspan="1" rowspan="1"><p>6,5754</p></td></tr><tr><td colspan="1" rowspan="1"><p>DenseNet-169</p></td><td colspan="1" rowspan="1"><p>0.99</p></td><td colspan="1" rowspan="1"><p>0.99</p></td><td colspan="1" rowspan="1"><p>0.99</p></td><td colspan="1" rowspan="1"><p>0.99</p></td><td colspan="1" rowspan="1"><p>12.49 M</p></td><td colspan="1" rowspan="1"><p>6.72</p></td><td colspan="1" rowspan="1"><p>10,121</p></td></tr><tr><td colspan="1" rowspan="1"><p>EfficientNetV2-B3</p></td><td colspan="1" rowspan="1"><p>0.98</p></td><td colspan="1" rowspan="1"><p>0.95</p></td><td colspan="1" rowspan="1"><p>0.98</p></td><td colspan="1" rowspan="1"><p>0.96</p></td><td colspan="1" rowspan="1"><p>12.83 M</p></td><td colspan="1" rowspan="1"><p>3.04</p></td><td colspan="1" rowspan="1"><p>4,0404</p></td></tr><tr><td colspan="1" rowspan="1"><p>InceptionV3</p></td><td colspan="1" rowspan="1"><p>0.98</p></td><td colspan="1" rowspan="1"><p>0.9750</p></td><td colspan="1" rowspan="1"><p>0.9750</p></td><td colspan="1" rowspan="1"><p>0.9750</p></td><td colspan="1" rowspan="1"><p>21.79 M</p></td><td colspan="1" rowspan="1"><p>5.67</p></td><td colspan="1" rowspan="1"><p>6,0674</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>Of the architectures examined, ResNet-50 provided a strong baseline, reaching a baseline accuracy of 98%. The precision, recall, and F1 score were all measured consistently to be 96%, demonstrating that deep Residual Networks possess strong extractive properties. However, this model is the most demanding in terms of computational complexity, with 8.26 GFLOPS of computation and 23.51 million parameters, resulting in an inference time of 6.58 ms. InceptionV3 achieved a comparable accuracy of 98%, along with precision, recall, and F1 score of 97.50%, respectively. This model was situated midway in terms of complexity with 21.79 million parameters, 5.67 GFLOPS of computation, and an inference time of 6.07 ms.</p><p>DenseNet-169 was the highest-performing model on the basis of pure classification capability, with an impressive accuracy of 99% and consistent performance across all metrics: precision = 99%, recall = 99%, and F1 score = 99%. The breakdown of performance relative to the three categories in the classification task is expressed in the confusion matrix in <xref ref-type="fig" rid="fig_2">Figure 2</xref>, for example. Notably, this best-in-class performance came with very low compute requirements, utilizing only 12.49 million parameters and 6.72 GFLOPS of computation. However, despite its parameter efficiency, it recorded the highest latency with an inference time of 10.12 ms, likely due to the complex memory access patterns of dense connections compared to the ResNet-50 baseline.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Confusion matrix showing the classification results of the Densely Connected Network with 169 layers model on the test dataset</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/11/img_k-jYOAhZ0Q-YT_Xh.png"/>
        </fig>
      
      <p>EfficientNetV2-B3 exhibited high diagnostic performance with a 98% accuracy. It achieved a precision of 95%, a recall of 98%, and an F1 score of 96%. However, EfficientNetV2-B3’s most prominent feature is its overall computational efficiency; it required only 3.04 GFLOPS and achieved the fastest inference speed of 4.04 ms, making it the most computationally feasible model tested. In addition, EfficientNetV2-B3 achieved a low parameter count of 12.83 million, which is comparable to DenseNet-169. Overall, the accuracy combined with the efficiency of EfficientNetV2-B3 is certainly an attractive option for resource-scarce environments.</p><p>The four models were all compared and showed relatively high classification performance for this task with final accuracies between 98% and 99%. The most major difference was efficiency. DenseNet-169 demonstrated the highest overall accuracy while providing significantly fewer parameters and GFLOPS than the ResNet-50 and InceptionV3 models. While EfficientNetV2-B3 achieved similar performance in terms of a 98% accuracy, it displayed superior computational performance with a relatively low GFLOPS requirement and the lowest inference latency among all evaluated architectures. Results revealed that the DenseNet and EfficientNet family of architectures could classify potato disease images with state-of-the-art accuracy while minimizing computational resources in a way that exceeds the performance of the established baseline models. The relatively high classification accuracy and computational efficiency of DenseNet-169 and EfficientNetV2-B3 demonstrate that they could be well-suited for practical use in an agricultural context.</p><p>While the experimental results underscore the potential of these CNN architectures, particularly the efficiency-accuracy balance of EfficientNetV2-B3, several limitations warrant attention. A primary constraint is the reliance on the PlantVillage dataset, which comprises images captured in controlled settings with homogenous backgrounds. Consequently, the models’ robustness against the visual complexity of real-world agricultural environments characterized by variable lighting, shadowing, and cluttered backgrounds remains to be fully validated. Furthermore, although the inference times reported in this study highlight the relative efficiency of the models, these metrics were derived from a high-performance workstation equipped with an NVIDIA GeForce RTX 5090. In practical agricultural scenarios, deployment often targets low-power hardware. Therefore, future research will focus on bridging this gap by evaluating model latency and energy consumption on resource-constrained edge devices, such as the Raspberry Pi or Jetson Nano, to ensure viability for in-field deployment. Additionally, expanding the diagnostic scope beyond the current three classes to encompass a broader spectrum of potato pathologies will be a critical step toward developing a comprehensive decision support system for farmers. Furthermore, exploring Vision Transformers and hybrid CNN-Transformer architectures represents a significant avenue for future research. Investigating these advanced models could further enhance classification performance, particularly in handling the high variability and complex patterns inherent in field-acquired agricultural imagery.</p>
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>This study evaluated four CNN architectures, i.e., ResNet-50, DenseNet-169, EfficientNetV2-B3, and InceptionV3, to investigate the feasibility of automated classification of potato leaf diseases using images from the PlantVillage dataset. Through the experimental results, this study confirmed the diagnostic ability of all models, which yielded test accuracies between 98% and 99%. DenseNet-169 did produce the best-performing model at a 99% test accuracy, though all models performed well. Results show that recent architectures such as DenseNet-169 and especially EfficientNetV2-B3 performed diagnostically well using far fewer parameters and a lower computational load (GFLOPS) in comparison to ResNet-50 and InceptionV3. These results suggest that efficient yet highly accurate models could serve as beneficial and effective alternatives for automated plant disease diagnosis. Efficient models present opportunities suitable for field use in resource-constrained agriculture to facilitate diagnosis and support sustainable agriculture. Future work may involve field execution, and if successful, the potential for the development of a broadly available, accessible decision support tool for farmers.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>Conceptualization, Y.C. and R.Y.; methodology, Y.C.; software, Y.C.; validation, Y.C. and R.Y.; formal analysis, Y.C.; investigation, Y.C.; data curation, Y.C.; writing—original draft preparation, Y.C.; writing—review and editing, Y.C. and R.Y.; visualization, Y.C.; supervision, R.Y. All authors have read and agreed to the published version of the manuscript.</p>
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The dataset analyzed for this study is the public PlantVillage dataset, which is available on Kaggle: https://www.kaggle.com/datasets/emmarex/plantdisease.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>8660</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kalpana</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Anandan</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Hussien</surname>
              <given-names>A. G.</given-names>
            </name>
            <name>
              <surname>Migdady</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Abualigah</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-024-56393-8</pub-id>
          <article-title>Plant disease recognition using residual convolutional enlightened Swin transformer networks</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>193902-193922</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kaur</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Randhawa</surname>
              <given-names>G. S.</given-names>
            </name>
            <name>
              <surname>Abbas</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Esau</surname>
              <given-names>T. J.</given-names>
            </name>
            <name>
              <surname>Farooque</surname>
              <given-names>A. A.</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2024.3510456</pub-id>
          <article-title>Artificial intelligence driven smart farming for accurate detection of potato diseases: A systematic review</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <article-title>Effvit-potatonet: An efficientvit-based model for potato leaf disease classification</article-title>
          <source>2025 International Conference on Digital Analysis and Processing, Intelligent Computation (DAPIC)</source>
          <publisher-name>Incheon, Korea, Republic of</publisher-name>
          <year>2025</year>
          <page-range>84-88</page-range>
          <pub-id pub-id-type="doi">10.1109/DAPIC66097.2025.00022</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>81</volume>
          <page-range>280</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kamal</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Sharma</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Gupta</surname>
              <given-names>P. K.</given-names>
            </name>
            <name>
              <surname>Siddiqui</surname>
              <given-names>M. K.</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Dutt</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11227-024-06494-y</pub-id>
          <article-title>DVTXAI: A novel deep vision transformer with an explainable AI-based framework and its application in agriculture</article-title>
          <source>J. Supercomput.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>26</volume>
          <page-range>200490</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Isinkaye</surname>
              <given-names>F. O.</given-names>
            </name>
            <name>
              <surname>Olusanya</surname>
              <given-names>M. O.</given-names>
            </name>
            <name>
              <surname>Akinyelu</surname>
              <given-names>A. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.iswa.2025.200490</pub-id>
          <article-title>A multi-class hybrid variational autoencoder and vision transformer model for enhanced plant disease identification</article-title>
          <source>Intell. Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <page-range>169-185</page-range>
          <issue>2</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Adhikari</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.36548/jscp.2024.2.005</pub-id>
          <article-title>Advancements in agricultural technology: Vision transformer-based potato leaf disease classification</article-title>
          <source>J. Soft Comput. Paradigm</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <page-range>614</page-range>
          <issue>11</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Benkaihoul</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Khadar</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Özüpak</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Aslan</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Almalki</surname>
              <given-names>M. M.</given-names>
            </name>
            <name>
              <surname>Mossa</surname>
              <given-names>M. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/WEVJ16110614</pub-id>
          <article-title>Advanced fault classification in induction motors for electric vehicles using a stacking ensemble learning approach</article-title>
          <source>World Electr. Veh. J.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Austin</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Barua</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Haider</surname>
              <given-names>S. N.</given-names>
            </name>
            <name>
              <surname>Niha</surname>
              <given-names>F. L.</given-names>
            </name>
            <name>
              <surname>Faisal</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Shawon</surname>
              <given-names>S. M.</given-names>
            </name>
          </person-group>
          <article-title>Precision classification of potato diseases using transformer-enhanced CNNs</article-title>
          <source>2025 International Conference on Quantum Photonics, Artificial Intelligence, and Networking (QPAIN)</source>
          <publisher-name>Rangpur, Bangladesh</publisher-name>
          <year>2025</year>
          <page-range>1-6</page-range>
          <pub-id pub-id-type="doi">10.1109/QPAIN66474.2025.11172153</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>1-12</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Aslan</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Özüpak</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.59543/COMDEM.V1I.10039</pub-id>
          <article-title>Diagnosis and accurate classification of apple leaf diseases using vision transformers</article-title>
          <source>Comput. Decis. Mak.: An Int. J.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>647</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sinamenye</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Chatterjee</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Shrestha</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s12870-025-06679-4</pub-id>
          <article-title>Potato plant disease detection: Leveraging hybrid deep learning models</article-title>
          <source>BMC Plant Biol.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <page-range>1-23</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Özüpak</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Alpsalaz</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Aslan</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Uzel</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/01140671.2025.2519570</pub-id>
          <article-title>Hybrid deep learning model for maize leaf disease classification with explainable AI</article-title>
          <source>N. Z. J. Crop Hortic. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Bajpai</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sahu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Tiwari</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Srivastava</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Yadav</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>An efficient approach for potato leaf disease classification using cascaded CNN-transformers</article-title>
          <source>2024 IEEE 12th International Conference on Intelligent Systems (IS)</source>
          <publisher-name>Varna, Bulgaria</publisher-name>
          <year>2024</year>
          <page-range>1-6</page-range>
          <pub-id pub-id-type="doi">10.1109/IS61756.2024.10705224</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <page-range>105412</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Alpsalaz</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Özüpak</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Aslan</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Uzel</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/J.CHEMOLAB.2025.105412</pub-id>
          <article-title>Classification of maize leaf diseases with deep learning: Performance evaluation of the proposed model and use of explicable artificial intelligence</article-title>
          <source>Chemom. Intell. Lab. Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>5-17</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zeynalov</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Çakmak</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Paçal</surname>
              <given-names>İ.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.30546/UNECCSDT.2025.01.005</pub-id>
          <article-title>Automated apple leaf disease classification using deep convolutional neural networks: A comparative study on the Plant Village Dataset</article-title>
          <source>J. Comput. Sci. Digit. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>37</volume>
          <page-range>2479-2496</page-range>
          <issue>4</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pacal</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Işik</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00521-024-10769-z</pub-id>
          <article-title>Utilizing convolutional neural networks and vision transformers for precise corn leaf disease identification</article-title>
          <source>Neural Comput. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>131</volume>
          <page-range>1061-1080</page-range>
          <issue>3</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kunduracioglu</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Pacal</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s41348-024-00896-z</pub-id>
          <article-title>Advancements in deep learning for accurate classification of grape leaves and diagnosis of grape diseases</article-title>
          <source>J. Plant Dis. Prot.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>8-14</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Çakmak</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.69882/ADBA.CSAI.2025072</pub-id>
          <article-title>Machine learning approaches for enhanced diagnosis of hematological disorders</article-title>
          <source>Comput. Syst. Artif. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>175-196</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cakmak</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Pacal</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.31181/JOPI31202539</pub-id>
          <article-title>Enhancing breast cancer diagnosis: A comparative evaluation of machine learning algorithms using the Wisconsin Dataset</article-title>
          <source>J. Oper. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>28-34</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Çakmak</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zeynalov</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.69882/ADBA.AI.2025075</pub-id>
          <article-title>A comparative analysis of convolutional neural network architectures for breast cancer classification from mammograms</article-title>
          <source>Artif. Intell. Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>13-19</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Çakmak</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Pacal</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.69882/ADBA.AI.2025073</pub-id>
          <article-title>Deep learning for automated breast cancer detection in ultrasound: A comparative study of four CNN architectures</article-title>
          <source>Artif. Intell. Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>20-25</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Çakmak</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Maman</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.69882/ADBA.CSAI.2025074</pub-id>
          <article-title>Deep learning for early diagnosis of lung cancer</article-title>
          <source>Comput. Syst. Artif. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>57</volume>
          <page-range>304</page-range>
          <issue>11</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pacal</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Kunduracioglu</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Alma</surname>
              <given-names>M. H.</given-names>
            </name>
            <name>
              <surname>Deveci</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kadry</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Nedoma</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Slany</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Martinek</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/S10462-024-10944-7</pub-id>
          <article-title>A systematic review of deep learning techniques for plant diseases</article-title>
          <source>Artif. Intell. Rev.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>238</volume>
          <page-range>122099</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pacal</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/J.ESWA.2023.122099</pub-id>
          <article-title>Enhancing crop productivity and sustainability through disease identification in maize leaves: Exploiting a large dataset with an advanced vision transformer model</article-title>
          <source>Expert Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>1225</page-range>
          <issue>8</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>R. F.</given-names>
            </name>
            <name>
              <surname>Su</surname>
              <given-names>W. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/agriculture14081225</pub-id>
          <article-title>The application of deep learning in the whole potato production Chain: A comprehensive review</article-title>
          <source>Agriculture</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Selvi</surname>
              <given-names>G. C.</given-names>
            </name>
            <name>
              <surname>Charan</surname>
              <given-names>H. J.</given-names>
            </name>
            <name>
              <surname>Kumar</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>CropViT: A light-weight transformer model for crop disease detection</article-title>
          <source>2024 3rd International Conference on Artificial Intelligence For Internet of Things (AIIoT)</source>
          <publisher-name>Vellore, India</publisher-name>
          <year>2024</year>
          <page-range>1-6</page-range>
          <pub-id pub-id-type="doi">10.1109/AIIoT58432.2024.10574729</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Dutta</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Neogi</surname>
              <given-names>S. G.</given-names>
            </name>
            <name>
              <surname>Halder</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Automatic early detection of potato blight disease using deep neural networks</article-title>
          <source>2024 IEEE International Conference on Intelligent Signal Processing and Effective Communication Technologies (INSPECT)</source>
          <publisher-name>Gwalior, India</publisher-name>
          <year>2024</year>
          <page-range>1-7</page-range>
          <pub-id pub-id-type="doi">10.1109/INSPECT63485.2024.10896181</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Bajpai</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Tiwari</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Rajput</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Sahu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Enhanced potato leaf disease detection via modified swin transformer architecture</article-title>
          <source>2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT)</source>
          <publisher-name>Kamand, India</publisher-name>
          <year>2024</year>
          <page-range>1-7</page-range>
          <pub-id pub-id-type="doi">10.1109/ICCCNT61001.2024.10724512</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>424</page-range>
          <issue>4</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Du</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zong</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/agriculture15040424</pub-id>
          <article-title>Research on a potato leaf disease diagnosis system based on deep learning</article-title>
          <source>Agriculture</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Sharma</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sharma</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Recurrent neural network-based classification of potato leaves using RGB images</article-title>
          <source>2024 2nd International Conference on Advancement in Computation &amp; Computer Technologies (InCACCT)</source>
          <publisher-name>Gharuan, India</publisher-name>
          <year>2024</year>
          <page-range>486-491</page-range>
          <pub-id pub-id-type="doi">10.1109/InCACCT61598.2024.10551226</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Zoralioğlu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Polat</surname>
              <given-names>Ö.</given-names>
            </name>
          </person-group>
          <article-title>Detection of potato plant disease from leaf images using deep learning models</article-title>
          <source>2024 Innovations in Intelligent Systems and Applications Conference (ASYU)</source>
          <publisher-name>Ankara, Turkiye</publisher-name>
          <year>2024</year>
          <page-range>1-5</page-range>
          <pub-id pub-id-type="doi">10.1109/ASYU62119.2024.10756961</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="webpage">
          <source>, https://www.kaggle.com/datasets/emmarex/plantdisease</source>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <issue>arXiv: 2405.09591</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Z. T.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>P. F.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>K. P.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>P. Y.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>Y. J.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>C. T.</given-names>
            </name>
            <name>
              <surname>Aggarwal</surname>
              <given-names>C. C.</given-names>
            </name>
            <name>
              <surname>Pei</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Y. C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2405.09591</pub-id>
          <article-title>A comprehensive survey on data augmentation</article-title>
          <source>arXiv preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>831-869</page-range>
          <issue>5</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mumuni</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Mumuni</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Gerrar</surname>
              <given-names>N. K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/S11633-022-1411-7</pub-id>
          <article-title>A survey of synthetic data augmentation methods in machine vision</article-title>
          <source>Mach. Intell. Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>He</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Ren</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Deep residual learning for image recognition</article-title>
          <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
          <year>2016</year>
          <page-range>770-778</page-range>
        </element-citation>
      </ref>
      <ref id="ref_35">
        <label>35.</label>
        <element-citation publication-type="webpage">
          <article-title>Densely Connected Convolutional Networks (DenseNets)</article-title>
          <source>, https://github.com/liuzhuang13/DenseNet</source>
        </element-citation>
      </ref>
      <ref id="ref_36">
        <label>36.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Tan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Le</surname>
              <given-names>Q.</given-names>
            </name>
          </person-group>
          <article-title>EfficientNetV2: Smaller models and faster training</article-title>
          <source>International Conference on Machine Learning</source>
          <year>2021</year>
          <page-range>10096-10106</page-range>
        </element-citation>
      </ref>
      <ref id="ref_37">
        <label>37.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Szegedy</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Vanhoucke</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Ioffe</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Shlens</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wojna</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <article-title>Rethinking the Inception Architecture for Computer Vision</article-title>
          <source>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
          <publisher-name>Las Vegas, NV, USA</publisher-name>
          <year>2016</year>
          <page-range>2818-2826</page-range>
          <pub-id pub-id-type="doi">10.1109/cvpr.2016.308</pub-id>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>