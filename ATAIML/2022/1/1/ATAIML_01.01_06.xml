<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR--gf5vnUOFoMIPZbxO-Xn2cWIq59dxiM1</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml010106</article-id>
      <title-group>
        <article-title>Mask Wearing Detection Based on YOLOv5 Target Detection Algorithm under COVID-19</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Xie</surname>
            <given-names>Jiuchao</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3929-7481</contrib-id>
          <email>202230510066@stu.shmtu.edu.cn</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="2">2</xref>
          <name>
            <surname>Xi</surname>
            <given-names>Rui</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2694-1901</contrib-id>
          <email>xirui@sdau.edu.cn</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="3">3</xref>
          <name>
            <surname>Chang</surname>
            <given-names>Daofang</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-7163-7741</contrib-id>
          <email>dfchang@shmtu.edu.cn</email>
        </contrib>
        <aff id="1">Institute of Logistics Science and Engineering, Shanghai Maritime University, 201308 Shanghai, China</aff>
        <aff id="2">College of Mechanical and Electronic Engineering, Shandong Agricultural University, 271018 Taian, China</aff>
        <aff id="3">College of Logistics Engineering, Shanghai Maritime University, 201308 Shanghai, China</aff>
      </contrib-group>
      <year>2022</year>
      <volume>1</volume>
      <issue>1</issue>
      <fpage>40</fpage>
      <lpage>51</lpage>
      <page-range>40-51</page-range>
      <history>
        <date date-type="received">
          <month>07</month>
          <day>11</day>
          <year>2022</year>
        </date>
        <date date-type="accepted">
          <month>09</month>
          <day>01</day>
          <year>2022</year>
        </date>
        <date date-type="pub">
          <month>11</month>
          <day>19</day>
          <year>2022</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2022 by the authors</copyright-statement>
        <copyright-year>2022</copyright-year>
        <license>. Licensee Acadlore Publishing Services Limited, Hong Kong. This article can be downloaded for free, and reused and quoted with a citation of the original published version, under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 license</ext-link>.</license>
      </permissions>
      <abstract><p>Deep learning methods have been widely used in object detection in recent years as a result of advancements in artificial intelligence algorithms and hardware computing capacity. In light of the drawbacks of current manual testing mask wearing methods, this study offers a real-time detection method of mask wearing status based on the deep learning YOLOv5 algorithm to prevent COVID-19 and quicken the recovery of industrial production. The algorithm normalizes the original dataset, before connecting the data to the YOLOv5 network for iterative training, and saving the ideal weight data as a test set. The training and test results of the suggested approach are presented visually on a tensor board. With the help of cameras, this technique can collect faces, identify masked faces, and present prompts for mask use. According to experiment results, the suggested algorithm can match the requirements of real-world applications and has a high detection accuracy and good real-time performance.</p></abstract>
      <kwd-group>
        <kwd>Algorithm</kwd>
        <kwd>COVID-19</kwd>
        <kwd>Real-time</kwd>
        <kwd>Mask</kwd>
        <kwd>YOLOv5</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors">3</count>
        <fig-count>11</fig-count>
        <table-count>1</table-count>
        <ref-count>20</ref-count>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec disp-level="level1" sec-type="intro">
      <title>1. Introduction</title>
      <p style="text-indent: 10.2pt;">In recent years, face recognition has become crucial to computer vision and digital image processing. The use of artificial intelligence technology is widespread. The impact of computer vision and natural language processing algorithms is notable when compared to the real effects of existing products, and is superior to existing manual or machine learning approaches. Target identification algorithms are employed in a variety of fields, including aerospace detection, traffic safety, and industrial equipment product detection, to name but a few [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>]. These are the primary application of computer vision algorithms. Visual algorithms perform better in situations involving several targets, a vast area, and multiple overlaps than conventional target identification and recognition techniques [<xref ref-type="bibr" rid="ref_3">3</xref>].</p><p style="text-indent: 10.2pt;">The rapid outbreak of a new pneumonia epidemic has gravely disrupted people's daily lives. In contrast to typical influenza, this pandemic can spread through human saliva and has a potent infection rate. Currently, China requires people to wear masks when using public transportation (trains, subways, planes, etc.) and in places where people congregate (shopping malls, hospitals, farmers' markets, etc.) due to the global spread of COVID-19, which puts the lives and property of 7 billion people in danger [<xref ref-type="bibr" rid="ref_4">4</xref>], [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>]. By preventing the virus from spreading over the initial barrier and isolating human saliva and airflow, wearing a mask can successfully protect both you and others. Manual inspection is the primary means of determining whether the personnel is wearing masks [<xref ref-type="bibr" rid="ref_8">8</xref>]. The staff will manually measure the temperature and oversee the use of masks by the passengers. During the busiest times of the day, detection is frequently overloaded. Much labor is wasted, and is unappreciative and ineffective. Often, human resources are unable to handle the high load. Some inspectors are stationed at the fixed entrances and exits, but not 24 hours a day [<xref ref-type="bibr" rid="ref_9">9</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>]. In regions with a large people flow, the inspectors are very likely to overlook some passengers, which undermines the prevention and management of the epidemic.</p><p style="text-indent: 10.2pt;">This research decides to take the well-known YOLOv5 target detection algorithm as the fundamental algorithm, trying to determine whether the face is wearing a mask accurately and in real-time. Face data were randomly obtained from the Internet, re-labeled, and then proofread through labeling. The trained model may be quickly and readily deployed on mobile devices in addition to having great recall and accuracy. The camera can be used to gather data, detect data, and swiftly judge the mask wearing situation. The proposed method is crucial to the current effort to manage the epidemic, and worthy of investment. </p><p style="text-indent: 10.2pt;">A few domestic academics are now researching mask wear detection. To detect the wearing of masks, Huangxiao Deng proposed using transfer learning and the retina face network, and achieved an average precision (AP) of 86.5% on the validation set. Junjie Xiao employed the YCrCb and YOLOv3 techniques. The accuracy rate of wearing recognition was 82.5%, while the AP for mask detection was 89%. A mask identification approach was proposed by Zuodong Niu et al. to enhance the performance of the retina face network in natural scenes. The test results demonstrate the method's effectiveness in detection. The YOLOv4 approach, which has the best detection accuracy and speed and meets the detection needs of wearing masks in most situations, was employed by Junlin Guan et al. The lightweight CNN mask detection method was proposed by Wang et al. [<xref ref-type="bibr" rid="ref_12">12</xref>]. They also examined the pyramid box Lite model, the keras model based on the SSD algorithm, and the mask detection model based on the center face. After testing and analyzing these three models, they provided the benefits, drawbacks, and applicability of various approaches. The pyramid box algorithm of the Baidu company, the DFS algorithm of the Didi company, and the mask detection of Huawei's model arts platform were all studied by Zhengyu Xie et al., with the aim to offer a mask wearing detecting technology suitable for rail transit stations.</p><p style="text-indent: 10.2pt;">With the speedy advancement of technology, our cameras and computers are now able to combine, analyze, and process data to rapidly and effectively determine whether people are wearing masks. This study proposes a mask wearing detection algorithm based on YOLOv5, in light of the research above, with the goal of addressing the challenge of recognizing occluded targets and small targets. The obtained image is first normalized, after which the YOLOv5 model is trained on the training image to produce the best network weights. Then, the test image is identified, and thoroughly analyzed. Experimental results show that the algorithm is more successful and practical than the approach suggested by the aforementioned researchers in terms of detection speed and accuracy.</p><p style="text-indent: 10.2pt;">Users can utilize the proposed system for detection at any time, which offers 24-hour service under typical Internet conditions. The detection is as quick and precise as possible. The system is flawless in terms of safety, usability, and maintenance. The system can rapidly, correctly, and automatically determine whether people are wearing masks in images or videos, which has significant commercial potential and application possibilities.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>2. Yolov5</title>
      <p style="text-indent: 10.2pt;">YOLO is simple to implement from the perspective of its principle. Many other algorithms can currently find targets, but they consume up too much resources [<xref ref-type="bibr" rid="ref_13">13</xref>], [<xref ref-type="bibr" rid="ref_14">14</xref>], [<xref ref-type="bibr" rid="ref_15">15</xref>]. It is difficult for some embedded devices to keep up with the demand. YOLO is frequently employed in large, quickly completed projects. In addition to images, it can also recognize videos in real time [<xref ref-type="bibr" rid="ref_16">16</xref>], [<xref ref-type="bibr" rid="ref_17">17</xref>], [<xref ref-type="bibr" rid="ref_18">18</xref>]. The original one-stage target detection algorithm of YOLO overcomes the disadvantages of the traditional two-stage target detection algorithm. It can complete classification and positioning of the targets in one step.</p><p style="text-indent: 10.2pt;">A continuous iterative method is adopted by the YOLO series target detection algorithms. Modern technologies have contributed to the YOLO series' advancement into YOLOv5 [<xref ref-type="bibr" rid="ref_19">19</xref>], [<xref ref-type="bibr" rid="ref_20">20</xref>]. The two most well-known of YOLO algorithms are YOLOv3 and YOLOv5. In May 2020, Ultralytics LLC proposed YOLOv5, which can process 140 frames per second and reason about images in as little as 0.007 seconds. This is fast enough for real-time video image identification. The construction is more compact than previous YOLO algorithms. YOLOv5's weight data file, which is 27 MB in size, is 1/9 the size of YOLOv4's. YOLOv5's extremely little weight file, which can be transported on mobile devices with lesser setup, is one of its benefits.</p><p style="text-indent: 10.2pt;">YOLOv5 can quickly train its own data sets and is incredibly user-friendly using the PyTorch framework. This framework is simpler to implement than the Darknet framework used by YOLOv4. A lot of computer vision technology is incorporated into the code, making it simple to read and excellent for learning and referencing. The environment can be configured with ease, the model can be trained fast, and batch reasoning can deliver findings in real time. It is capable of accurately predicting the input of a single image, a batch of images, video, and even a webcam port. PyTorch weight files can be readily converted to the onxx format used by Android, where they can subsequently be converted to OpenCV format or, via Core ML, to iOS format where they may then be utilized directly in mobile applications. YOLOv5 has a remarkable target identification speed that can reach 140 frames per second.</p>
      
        <sec disp-level="level2">
          
            <title>2.1. Network structure</title>
          
          <p style="text-indent: 10.2pt;">Overall, YOLO series algorithms are composed of input, backbone, neck and prediction. </p><p style="text-indent: 7.2pt;">The four network structures of the YOLOv5 series are the YOLOv5s, YOLOv5m, YOLOv5l, and YOLOv5x. The YOLOv5s network is the smallest of the four. While it has slightly lesser accuracy than the other three, it offers the fastest detection speed. <xref ref-type="fig" rid="fig_1">Figure 1</xref> compares the performance of YOLOv5 networks. Compared with YOLOv5s, the other three networks continue to deepen and extend, which improves accuracy while marginally slowing down detection speed. This study chooses the YOLOv5s target detection model as the fundamental algorithm, which should be deployed to mobile or embedded terminals and should have a high detection speed.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>Performance comparison of YOLOv5 networks</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2022/11/img_T6rT7mOjuwtWAHu4.png"/>
            </fig>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>2.2. Input terminal</title>
          
          <p style="text-indent: 10.2pt;">The input terminal adopts the mosaic data enhancement technique, adaptive anchor box and adaptive image scaling. The effectiveness of small target detection is considerably enhanced by mosaic data enhancement, through the operations of random scaling, random clipping, and random arranging.</p><p style="text-indent: 10.2pt;">The main principle is to randomly cut a selected image and three random images, and splice them into a training image. This can enhance the image background and combine the four images to increase batch size. Four images will also be calculated during the batch normalization, enabling YOLOv5 to batch itself.</p><p style="text-indent: 10.2pt;">YOLOv5 features anchor boxes with initial length and width for various datasets. There are a number of preset borders. The construction of the training samples—that is, the labels we apply—according to the offset of the actual border location from the preset boundary occurs during the training process. This is analogous to the preset boundary "framing" the target in its potential position before adjusting in accordance with those preset borders.</p><p style="text-indent: 10.2pt;">The anchor box is defined as follows: Each box is described by the height and width of the border. One may initially think that this anchor box is not fixed. Numerous points can be created using the box on the image. The initial anchor box does not need to indicate the central location because we already have a central point, which is the point of the feature map produced by the subsequent network. The model's final effect is directly adjusted by choosing the right anchor.</p><p style="text-indent: 10.2pt;">This method is integrated in the code, hence V5 is not fixed for V3 and V4. The best anchor box value from several training sets will be determined adaptively for each training.</p><p style="text-indent: 10.2pt;">The image size is typically varied, but when we want to start the network training, we need to make sure that it is constant. But if we solely utilize resizing, our results will be impacted by image distortion.</p><p style="text-indent: 10.2pt;">Consequently, a more effective technique—letterbox adaptive image scaling—is used. The size of the image is consistently maintained by the train. It creates a single, huge image the same size by combining the parts of the four images.</p><p style="text-indent: 10.2pt;">The missing image is filled with gray edges to create a fixed size via letterbox adaptive image, which tries to maintain the aspect ratio.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>2.3. Backbone network</title>
          
          <p style="text-indent: 10.2pt;">The backbone network employs the focus structure and CSP structure.</p><p style="text-indent: 10.2pt;">The focus module's input channel has been increased by four times in YOLOv5 to increase computational power without compromising data. Block slicing is initially applied to the feature map, after which the results are concatenated before being sent to the subsequent modules. YOLOv5's most recent version switches out the focus module for a 6*6 convolution layer. Although the two requiring the same amount of computation, some GPU devices will benefit more from the 6*6 convolution effect. YOLOv5s converts 608*608*3 images into 304*304*12 feature maps after focus processing, which somewhat enhances the functionality of the feature map. The feature map will then go through 32 convolution kernels once again to create a feature map with 304*304*32 nodes. The focus module in YOLOv5s eventually makes use of a convolution kernel with a size of 32.</p><p style="text-indent: 10.2pt;">In V3 and V4, there is no focus structure. The V5 mode's innovation lies in this structure. Slicing is the key to the focus module (<xref ref-type="fig" rid="fig_2">Figure 2</xref>).</p><p style="text-indent: 10.2pt;">In YOLOv5s, the CSP structure splits the original input into two branches, performs convolution operations on each branch, reduces the number of channels, applies bottleneck * N on one branch, and then concatenates the two branches to equalize the input and output sizes of the Bottlenneck CSP, so that the model can learn more features.</p><p style="text-indent: 10.2pt;">In YOLOv4, the CSPnet design concept is used as a reference, and the CSP structure is designed in the backbone network. Only the backbone network in V4 uses CSP structure, which is how V5 and V4 differ from one another. Two CSP structures are designed in V5. Take YOLOv5s network as an example, the backbone network adopts CSP1_ X structure, neck adopts CSP2_ X structure.</p><p style="text-indent: 10.2pt;">Compared with the residual structure of YOLOv3. The structure of CSP net is not complex, so it can be considered that there are large residual edges in CSP. CSP net can also be easily applied to ResNet and ResNeXt, and its architecture is shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>. Since only half of the characteristic channels pass through Res (x) Blocks, it is no longer necessary to introduce bottleneck. This makes it possible to theoretically lower the memory access cost (MAC) when performing fixed floating-point operations (FLOP).</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>Slicing operation</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2022/11/img_xRjAXVtw4WuGy0kg.png"/>
            </fig>
          
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>CSPnet structure</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2022/11/img_Z_ApDcbC3HuWDHoN.png"/>
            </fig>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>2.4. Neck</title>
          
          <p style="text-indent: 10.2pt;">As much information as possible is gathered by the "neck" structure, which is located between the head and the backbone, before the information obtained by the backbone is fed back to the head. By limiting the loss of small target information, this structure is crucial in the transmission of small target information. This is accomplished by increasing the feature map's resolution once more so that features from various backbone layers can be pooled to enhance overall detection performance.</p><p style="text-indent: 10.2pt;">Comparatively speaking, the neck components—basically consisting of CBS, Upsample, Concat, and CSP (C3) without shortcut—are quite straightforward.</p><p style="text-indent: 10.2pt;">The FPN+PAN structure is also utilized in the design of Neck structure. FPN builds feature pyramids with the traditional structure in subgraph (a) of <xref ref-type="fig" rid="fig_4">Figure 4</xref>, and uses top-down side connections to build high-level semantic feature maps on all sizes; The pan's construction is not unusual. The underlying target information is highly fuzzy after traversing the multilayer network in the midst of FPN. As a result, PAN strengthens and adds a bottom-up route to compensate for and balance out the positional information in <xref ref-type="fig" rid="fig_4">Figure 4</xref>.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>PANnet structure</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2022/11/img_lOUcK_hdSsRCoOaA.png"/>
            </fig>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>2.5. Activation function, optimization function and loss function</title>
          
          <p style="text-indent: 10.2pt;">The activation function non-linearizes the neural network, and determines whether the perceptron is excited. This nonlinearity of the activation function enables the deep network to learn complex functions. YOLOv5 uses two activation functions, namely Leaky ReLU and Sigmoid.</p><p style="text-indent: 10.2pt;">The YOLOv5 model is optimized by Adam, whose performance facing sparse gradients is optimized through deviation correction. During model training and optimization, Adam achieves better optimization quality and speed by making each parameter to adapt to the learning rate.</p><p style="text-indent: 10.2pt;">In YOLOv5, the loss function of the bounding box is GIoU loss:</p>
          
            <disp-formula>
              <label>(1)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mrow data-mjx-texclass="ORD">
                  <mi data-mjx-auto-op="false">GIoU</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi data-mjx-auto-op="false">IoU</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">|</mo>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi data-mjx-auto-op="false">Ac</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">U</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">|</mo>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo>/</mo>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">|</mo>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi data-mjx-auto-op="false">Ac</mi>
                </mrow>
                <mo>=</mo>
                <mo>−</mo>
                <mo>−</mo>
                <mo stretchy="false">|</mo>
              </math>
            </disp-formula>
          
          <p>As an upgraded version of IoU, GIoU not only inherits the merits, i.e., effective comparison of the similarity between two arbitrary shapes and scale invariance, and makes up for the disadvantage that IoU cannot measure the distance between non-overlapping frames.</p>
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>3. Methodology</title>
      <p>YOLOv5’s recognition model can detect some objects more effectively on its own and has a quick recognition speed and good effect. To achieve the exclusive detection of small targets like masks and enhance the precision of mask wearing recognition, the mask wearing detection system developed in this research merely requires further network structure optimization and YOLOv5 model parameter adjustments.</p>
      
        <sec disp-level="level2">
          
            <title>3.1. Training algorithm</title>
          
          <p>The inputs of the training algorithm are the face mask dataset image and tag file. The following parameters are initialized: the number of training iterations, learning rate, batch size, size of input image, network configuration yaml file, IoU threshold of tag and anchor, loss coefficient, data enhancement coefficient. Each input image is preprocessed through brightness adjustment, as well as contrast, saturation and mosaic processing. Finally, the algorithm outputs the detection model that performs best in this training.</p><p>The model is trained in the following steps: Firstly, prepare the original data, and divide them into a training set, a test set, and a verification set. Next, import data configurations and initial parameters, and preprocess the input data. After that, load the network model, extract the features of the input image, and locate and classify the objects. With the increase of iteration times, SGD is used to update and optimize each group of parameters in the network. If the current iteration is not the last round, produce the map of the current model on the validation set; If the calculated model has better performance, update the stored best model. After a set number of iterations, output the model with the optimal performance, and the model being trained the latest.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.2. Overall implementation</title>
          
          <p style="text-indent: 10.2pt;">The data gathered by the camera must first be preprocessed using the interface function of the free OpenCV package; the data is then prepared, the face mask images are screened and labeled, and the data set is randomly divided into the training, test, and verification sets; the model is trained by the proposed algorithm, and the most effective model for detecting facial masks is obtained; the obtained model is tested on the test data, and the final recognition results are drawn on the test images, namely, the position of the face and the mask's wearing status.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.3. Mask wearing identification</title>
          
          <p style="text-indent: 10.2pt;">The procedure of mask wearing recognition is depicted in <xref ref-type="fig" rid="fig_5">Figure 5</xref>. The target to be detected first enters the face detection module in the form of an image to carry out the matching facial area recognition and marking operations at the beginning of the mask wearing recognition process; then, the mask wearing recognition module is activated to frame the face, and create an image; After classifying the images into groups based on whether or not the mask is worn, the module outputs the image recognition result about whether or not the mask is worn.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>Flow chart of mask wearing identification</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2022/11/img_jRhBUfV-NWkbxS6z.png"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>4. System design</title>
      
        <sec disp-level="level2">
          
            <title>4.1. Dataset production</title>
          
          <p style="text-indent: 10.2pt;">The mask wearing statuses are predefined into two classes: wearing masks and not wearing masks. The two classes are labeled as face and mask, respectively. It is important to prevent mistaking mouth-covering actions (e.g., covering your mouth with hands or clothes) as mask wearing, the data about the mouth being covered by hands or clothes are added specifically to the dataset. The visualization results are obtained by analyzing the dataset: the objects in the dataset are relatively uniform in distribution, and most of them are small and medium-sized. There is occlusion between objects, which is normal in daily scenes. However, the class distribution is a bit unbalanced, and should be alleviated in data preprocessing.</p><p style="text-indent: 10.2pt;">The dataset folder has two subfolders: The image subfolder, and the label subfolder. The two subfolders store image and label txt files, respectively. The directory of images corresponds to that of labels. This is very important, for YOLO first reads the image path, and then directly replaces the image with label to find the label file.</p><p style="text-indent: 10.2pt;">Data labeling can frame, track, and transcribe images, text, voice, video, and other data, making the data suitable for AI and machine learning. LabelImg, written in Python and designed with QT, is the most common visual image calibration tool for image annotation, which is crucial for deep learning-based object detection. Fast-R-CNN, YOLO, SSD and other datasets required by the target detection network can be handled by this tool to calibrate the targets in the image. The generated XML file follows the Pascal VOC format. </p><p style="text-indent: 10.2pt;">Here, LabelImg is employed to train YOLOv5 model on our datasets. In the virtual environment, LabelImg is installed through PIP instructions, and the software is directly executed in the command line to start the data annotation process. Because YOLOv5 is applied in this design, the annotation format is directly changed to YOLO for annotation. The specific annotation process is as follows: open the image directory, set the directory for saving annotation files, and select automatic saving. Start labeling, framing, target labeling, save, and then switch to the next to continue labeling.</p><p style="text-indent: 10.2pt;">The labeling produces a series of txt files, which are a comment file for target detection. The names of the txt files and image files correspond one by one. Next, we need to open the specific annotation file, which contains the following contents: Each line in the txt file represents a target, which is distinguished by spaces, representing the its class ID, the X coordinate and Y coordinate of the normalized center point, and the W and h of the target box. The subsequent steps include: modify the dataset configuration file, assign the completed data into training set, test set and verification set by 7:2:1, and place the images and label txt files in the right subfolders.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>4.2. Model training</title>
          
          <p style="text-indent: 10.2pt;">The YOLO model is originally able to recognize simple objects. After training, the model will get the ability to frame the face and judge whether the subject wears a mask. The model is trained in the following steps:</p><p style="text-indent: 10.2pt;">First, create a mask under the data directory. Next, configure the yaml’s file, i.e., the dataset configuration file, with four contents, namely, the path of the validation image of the training set, the path of the validation image, the number of classes of the dataset, and the class alias of the dataset.</p><p style="text-indent: 10.2pt;">Hence, create a mask under YOLOv5s models. Next, configure the yaml’s model configuration file, which configures the datasets required for training, with two classes: face and mask.</p><p style="text-indent: 10.2pt;">Afterwards, select the pretraining weight file. The purpose is to shorten the training time, and achieve better accuracy. The 5.0 version of YOLOv5 provides several pretraining weights, which suit different needs. The greater the weight, the better the training accuracy, and the slower the detection. In this paper, the most suitable pretraining weight is YOLOv5s.pt.</p><p style="text-indent: 10.2pt;">Finally, run the program for 100 epochs on the dataset, and set the image data as a batch of 4. The program will scan the data before training the model.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>4.3. Ui interface</title>
          
          <p style="text-indent: 10.2pt;">The UI image interface is developed by configuring PyQt in the PyCharm compilation environment and encapsulating it with PyQt5, aiming to realize image mask detection, video mask detection and camera real-time mask detection. When the interface starts, load the model, and set the directory of TMP to output the intermediate processing results. In the output, face and mask identify the target without and with mask, respectively.</p>
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>5. Experiments and results analysis</title>
      
        <sec disp-level="level2">
          
            <title>5.1. Main parameters</title>
          
          <p style="text-indent: 10.2pt;">Run the "NVIDIA SMI" command to see the model, memory, driver version of the current graphics card, the process using the card, and CUDA version (<xref ref-type="fig" rid="fig_6">Figure 6</xref>).</p>
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>Equipment parameters</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2022/11/img_DrEn62tV6ZJkQvvI.png"/>
            </fig>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>5.2. Performance metrics</title>
          
          <p style="text-indent: 10.2pt;">Our model was evaluated by precision, recall, mean Average Precision (mAP) and frames per second (FPS). Accuracy is a measure of accuracy, while recall is a measure of coverage. The higher the accuracy and recall, the better the recognition effect of mask wearing status. The mAP measures the recognition accuracy. The larger the mAP, the better the recognition effect. FPS measures the number of frames of images transmitted per second. The higher the value, the faster the recognition.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>5.3. Results</title>
          
          <p style="text-indent: 10.2pt;">The loss value of YOLOv5 includes position loss, confidence loss and class loss. <xref ref-type="fig" rid="fig_7">Figure 7</xref> shows the convergence curve of loss value after 100 iterations of training. The results show that our model achieved a good fitting effect.</p><p style="text-indent: 10.2pt;">The confusion matrix indicates the classification accuracy. It can be seen that the detection accuracy of face and mask classes were 0.90 and 0.98, respectively.</p><p style="text-indent: 10.2pt;"><xref ref-type="fig" rid="fig_8">Figure 8</xref> shows the performance metrics of YOLOv5 model for mask wearing recognition. After 100 iterations, the model achieved convergence. During model training, accuracy and recall rate improved stably, and the mAP remained at a high level. The maximum precision, recall, and mAP of the trained model reached 0.987, 0.996 and 0.981, respectively.</p><p style="text-indent: 10.2pt;">On a test set of 349 random images, the P-R curve was drawn for our model by calculating the highest accuracy under different recall rates (<xref ref-type="fig" rid="fig_9">Figure 9</xref>). The test results show that our model recognized objects quickly in practical application, and basically met the speed requirements of real-time detection.</p><p style="text-indent: 10.2pt;">The training results demonstrate the high detection accuracy of our model. No object was missed by the model, despite the high number of objects in the images. The model also effectively filtered interferences like mouth covering by hands or clothes.</p>
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>Training loss curves</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2022/11/img_SS40eVWry1xIsNqk.png"/>
            </fig>
          
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>Metrics of YOLOv5 model</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2022/11/img_ouidyp8vpkauwyuC.png"/>
            </fig>
          
          <p style="text-indent: 10.2pt;">The proposed YOLOv5 algorithm, a real-time detection method, reached a speed of 130 f/s and an accuracy rate of about 95%. Meanwhile, it lowered the performance requirements of the hardware, and controlled the amount of calculation, thus meeting the needs of real-time detection.</p><p style="text-indent: 10.2pt;"><xref ref-type="fig" rid="fig_10">Figure 10</xref> and <xref ref-type="fig" rid="fig_11">Figure 11</xref> show the results of the UI interface, which supports the functions of image mask wearing detection, video mask wearing detection and camera real-time mask wearing detection. The results of image mask wearing detection show that our model can accurately identify the mask wearers and non-mask wearers, in the face of dense crowds. Hence, YOLOv5 algorithm is superior in mask wearing detection.</p>
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>P-R curve</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2022/11/img_WGncuj--kSoGBkSh.png"/>
            </fig>
          
          
            <fig id="fig_10">
              <label>Figure 10</label>
              <caption>Image mask wearing test</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2022/11/img_kBF14s1Kw3xyvT30.png"/>
            </fig>
          
          
            <fig id="fig_11">
              <label>Figure 11</label>
              <caption>Camera real-time mask wearing detection</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2022/11/img_t7G5GjFWgV4xKYIs.png"/>
            </fig>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>5.4. Algorithm comparison</title>
          
          <p style="text-indent: 10.2pt;">To further validates its superiority, our algorithm was compared with SSD, fast-R-CNN and YOLOv3, three common target detectors, using the control variate technique. The results are displayed in <xref ref-type="table" rid="table_1">Table 1</xref>. The comparison shows that our algorithm significantly outshines the other algorithms in mAP and recognition speed. The excellent performance is realized with a small size, low cost, and high efficiency.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>Comparison between algorithms</caption>
              <abstract/>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Detection algorithm</p></td><td colspan="1" rowspan="1"><p>mAP/%</p></td><td colspan="1" rowspan="1"><p>FPS</p></td><td colspan="1" rowspan="1"><p>Mask/%</p></td><td colspan="1" rowspan="1"><p>Face/%</p></td></tr><tr><td colspan="1" rowspan="1"><p>SSD</p></td><td colspan="1" rowspan="1"><p>74.12</p></td><td colspan="1" rowspan="1"><p>16.8</p></td><td colspan="1" rowspan="1"><p>73.20</p></td><td colspan="1" rowspan="1"><p>70.56</p></td></tr><tr><td colspan="1" rowspan="1"><p>Fast-R-CNN</p></td><td colspan="1" rowspan="1"><p>77.55</p></td><td colspan="1" rowspan="1"><p>8.7</p></td><td colspan="1" rowspan="1"><p>74.09</p></td><td colspan="1" rowspan="1"><p>76.77</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv3</p></td><td colspan="1" rowspan="1"><p>80.66</p></td><td colspan="1" rowspan="1"><p>50.54</p></td><td colspan="1" rowspan="1"><p>76.75</p></td><td colspan="1" rowspan="1"><p>79.36</p></td></tr><tr><td colspan="1" rowspan="1"><p>Our algorithm</p></td><td colspan="1" rowspan="1"><p>98.10</p></td><td colspan="1" rowspan="1"><p>70.33</p></td><td colspan="1" rowspan="1"><p>96.64</p></td><td colspan="1" rowspan="1"><p>97.23</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="conclusions">
      <title>6. Conclusions</title>
      <p style="text-indent: 10.2pt;">This study proposes a YOLOv5-based mask wearing identification approach, and develops and applies a lightweight YOLOv5 enhanced network to enhance target detection speed and accuracy while minimizing model parameters. K-means++ clustering is applied to customize the datasets, and the anchor box size of the target samples is optimized. Experiments demonstrate that even in complicated situations, the model maintains good accuracy, detection speed, and robustness. The algorithm performs well in terms of multiple targets, quick detection times, and small model parameters.</p><p style="text-indent: 10.2pt;">The model may considerably reduce dependence on the hardware environment, compress and accelerate reasoning speed, maximize model accuracy, and best suit the needs of actual applications. It can be used in conjunction with the mask wearing identification system to provide efficient public health protection monitoring, advance the intellectualization, scientific level, and humanization of the mask wearing detection process, and is crucial for fostering a secure environment in public.</p><p style="text-indent: 10.2pt;">There are still a few missed detections and false detections in the real detection because of uneven illumination, target occlusion, dense crowd, and other issues. We will continue to investigate this issue and contribute appropriately to the prevention and management of epidemics in the future. In addition to figuring out how to effectively deploy the mobile terminal model, the follow-up work will also verify and enhance the suggested model in real-world applications. At the same time, the integration of mask wearing and other related functions will result in a more functional safety and health supervision system, better suited to the current societal demands.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p></p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p style="text-indent: 10.2pt;">The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p style="text-indent: 10.2pt;">The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>H.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>A. Z.</given-names>
              <surname>Wu</surname>
            </name>
            <name>
              <given-names>Q. Q.</given-names>
              <surname>Fang</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>Q.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>Q. W.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Chen</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Lightweight mask R-CNN for long-range wireless power transfer systems</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2019 11th International Conference on Wireless Communications and Signal Processing</conf-name>
          <conf-acronym>WCSP 2019</conf-acronym>
          <conf-loc>Xi'an, China</conf-loc>
          <conf-date>October 23-25, 2019</conf-date>
          <year>2019</year>
          <volume/>
          <issue/>
          <page-range>1-6</page-range>
          <fpage>1</fpage>
          <lpage>6</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/WCSP.2019.8927856.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>R.</given-names>
              <surname>Gavrilescu</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Zet</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Foșalău</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Skoczylas</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Cotovanu</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Faster R-CNN: An approach to real-time object detection</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2018 International Conference and Exposition on Electrical and Power Engineering</conf-name>
          <conf-acronym>EPE 2018</conf-acronym>
          <conf-loc>Iasi, Romania</conf-loc>
          <conf-date>October 18-19, 2018</conf-date>
          <year>2018</year>
          <volume/>
          <issue/>
          <page-range>165-168</page-range>
          <fpage>165</fpage>
          <lpage>168</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICEPE.2018.8559776.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>M. D.</given-names>
              <surname>Pramita</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Kurniawan</surname>
            </name>
            <name>
              <given-names>N. P.</given-names>
              <surname>Utama</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Mask wearing classification using CNN</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2020 7th International Conference on Advance Informatics: Concepts, Theory and Applications</conf-name>
          <conf-acronym>ICAICTA 2020</conf-acronym>
          <conf-loc>Tokoname, Japan</conf-loc>
          <conf-date>September 8-9, 2020</conf-date>
          <year>2020</year>
          <volume/>
          <issue/>
          <page-range>1-4</page-range>
          <fpage>1</fpage>
          <lpage>4</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICAICTA49861.2020.9429029.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>J.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Han</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Chun</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Chen</surname>
            </name>
          </person-group>
          <article-title>A novel detection framework about conditions of wearing face mask for helping control the spread of COVID-19</article-title>
          <source>IEEE Access</source>
          <year>2021</year>
          <volume>9</volume>
          <issue/>
          <page-range>42975-42984</page-range>
          <fpage>42975</fpage>
          <lpage>42984</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ACCESS.2021.3066538</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>B.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Zhao</surname>
            </name>
            <name>
              <given-names>C. L. P.</given-names>
              <surname>Chen</surname>
            </name>
          </person-group>
          <article-title>Hybrid transfer learning and broad learning system for wearing mask detection in the COVID-19 era</article-title>
          <source>IEEE T. Instrum. Meas.</source>
          <year>2021</year>
          <volume>70</volume>
          <issue/>
          <page-range>1-12</page-range>
          <fpage>1</fpage>
          <lpage>12</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/TIM.2021.3069844</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Yu</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Zou</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Jia</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Cheng</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Recognition of the standardization of wearing masks during the epidemic of COVID-19</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2021 IEEE Asia-Pacific conference on image processing, electronics and computers</conf-name>
          <conf-acronym>IPEC 2021</conf-acronym>
          <conf-loc>Dalian, China</conf-loc>
          <conf-date>April 14-16, 2021</conf-date>
          <year>2021</year>
          <volume/>
          <issue/>
          <page-range>728-732</page-range>
          <fpage>728</fpage>
          <lpage>732</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/IPEC51340.2021.9421236.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>J.</given-names>
              <surname>Vadlapati</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Senthil Velan</surname>
            </name>
            <name>
              <given-names>E.</given-names>
              <surname>Varghese</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Facial recognition using the OpenCV Libraries of Python for the pictures of human faces wearing face masks during the COVID-19 pandemic</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2021 12th International Conference on Computing Communication and Networking Technologies</conf-name>
          <conf-acronym>ICCCNT 2021</conf-acronym>
          <conf-loc>Kharagpur, India</conf-loc>
          <conf-date>July 06-08, 2021</conf-date>
          <year>2021</year>
          <volume/>
          <issue/>
          <page-range>1-5</page-range>
          <fpage>1</fpage>
          <lpage>5</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICCCNT51525.2021.9579712.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Ratanaubol</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Wannapiroon</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Nilsook</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Video-based facial recognition develop for accurately identify people wearing surgical masks</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2021 3rd International Conference on Computer Communication and the Internet</conf-name>
          <conf-acronym>ICCCI 2021</conf-acronym>
          <conf-loc>Nagoya, Japan</conf-loc>
          <conf-date>June 25-27, 2021</conf-date>
          <year>2021</year>
          <volume/>
          <issue/>
          <page-range>19-22</page-range>
          <fpage>19</fpage>
          <lpage>22</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICCCI51764.2021.9486818.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Ratanaubol</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Wannapiroon</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Nilsook</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Video-based facial recognition develop for accurately identify people wearing surgical masks</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2021 3rd International Conference on Computer Communication and the Internet</conf-name>
          <conf-acronym>ICCCI 2021</conf-acronym>
          <conf-loc>Nagoya, Japan</conf-loc>
          <conf-date>June 25-27, 2021</conf-date>
          <year>2021</year>
          <volume/>
          <issue/>
          <page-range>19-22</page-range>
          <fpage>19</fpage>
          <lpage>22</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICCCI51764.2021.9486818.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>K.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Jia</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Cui</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Detection system of wearing face masks normatively based on deep learning</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2021 International Conference on Control Science and Electric Power Systems</conf-name>
          <conf-acronym>CSEPS 2021</conf-acronym>
          <conf-loc>Shanghai, China</conf-loc>
          <conf-date>May 28-30 2021</conf-date>
          <year>2021</year>
          <volume/>
          <issue/>
          <page-range>35-39</page-range>
          <fpage>35</fpage>
          <lpage>39</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/CSEPS53726.2021.00014.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>R. K.</given-names>
              <surname>Shukla</surname>
            </name>
            <name>
              <given-names>A. K.</given-names>
              <surname>Tiwari</surname>
            </name>
            <name>
              <given-names>V.</given-names>
              <surname>Verma</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Identification of with face mask and without face mask using face recognition model</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2021 10th International Conference on System Modeling &amp; Advancement in Research Trends</conf-name>
          <conf-acronym>SMART 2021</conf-acronym>
          <conf-loc>MORADABAD, India</conf-loc>
          <conf-date>December 10-11, 2021</conf-date>
          <year>2021</year>
          <volume/>
          <issue/>
          <page-range>462-467</page-range>
          <fpage>462</fpage>
          <lpage>467</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/SMART52563.2021.9676204.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>L.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Lin</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Sun</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Wu</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Improved faster-RCNN algorithm for mask wearing detection</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference</conf-name>
          <conf-acronym>IMCEC 2021</conf-acronym>
          <conf-loc>Chongqing, China</conf-loc>
          <conf-date>June 18-20, 2021</conf-date>
          <year>2021</year>
          <volume/>
          <issue/>
          <page-range>1119-1124</page-range>
          <fpage>1119</fpage>
          <lpage>1124</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/IMCEC51613.2021.9482098.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>T. H.</given-names>
              <surname>Wu</surname>
            </name>
            <name>
              <given-names>T. W.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>Y. Q.</given-names>
              <surname>Liu</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Real-time vehicle and distance detection based on improved YOLOv5 network</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2021 3rd World Symposium on Artificial Intelligence</conf-name>
          <conf-acronym>WSAI 2021</conf-acronym>
          <conf-loc>Guangzhou, China</conf-loc>
          <conf-date>June 18-20, 2021</conf-date>
          <year>2021</year>
          <volume/>
          <issue/>
          <page-range>24-28</page-range>
          <fpage>24</fpage>
          <lpage>28</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/WSAI51899.2021.9486316.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Yu</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Gui</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Yang</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Lv</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Research on mask wearing detection algorithm based on YOLOv5</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2021 IEEE 2nd International Conference on Information Technology, Big Data and Artificial Intelligence</conf-name>
          <conf-acronym>ICIBA 2021</conf-acronym>
          <conf-loc>Chongqing, China</conf-loc>
          <conf-date>December 17-19, 2021</conf-date>
          <year>2021</year>
          <volume/>
          <issue/>
          <page-range>625-630</page-range>
          <fpage>625</fpage>
          <lpage>630</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICIBA52610.2021.9688011.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>W.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Yan</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Huang</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Mask wearing detection based on improved YOLOv3</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2021 International Conference on Computer Information Science and Artificial Intelligence</conf-name>
          <conf-acronym>CISAI 2021</conf-acronym>
          <conf-loc>Kunming, China</conf-loc>
          <conf-date>September 17-19, 2021</conf-date>
          <year>2021</year>
          <volume/>
          <issue/>
          <page-range>194-197</page-range>
          <fpage>194</fpage>
          <lpage>197</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/CISAI54367.2021.00044.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Lin</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Embedded system real-time vehicle detection based on improved YOLO network</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2019 IEEE 3rd Advanced Information Management, Communicates, Electronic and Automation Control Conference</conf-name>
          <conf-acronym>IMCEC 2019</conf-acronym>
          <conf-loc>Chongqing, China</conf-loc>
          <conf-date>October 11-13, 2019</conf-date>
          <year>2019</year>
          <volume/>
          <issue/>
          <page-range>1400-1403</page-range>
          <fpage>1400</fpage>
          <lpage>1403</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/IMCEC46724.2019.8984055.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Jiang</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Yue</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Hu</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>AM-YOLO: Improved YOLOV4 based on attention mechanism and multi-feature fusion</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2022 IEEE 6th Information Technology and Mechatronics Engineering Conference</conf-name>
          <conf-acronym>ITOEC 2022</conf-acronym>
          <conf-loc>Chongqing, China</conf-loc>
          <conf-date>March 4-6, 2022</conf-date>
          <year>2022</year>
          <volume/>
          <issue/>
          <page-range>1403-1407</page-range>
          <fpage>1403</fpage>
          <lpage>1407</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ITOEC53115.2022.9734536.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>R.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Ren</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Application of YOLO on mask detection task</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2021 IEEE 13th International Conference on Computer Research and Development</conf-name>
          <conf-acronym>ICCRD 2021</conf-acronym>
          <conf-loc>Beijing, China</conf-loc>
          <conf-date>January 5-7, 2021</conf-date>
          <year>2021</year>
          <volume/>
          <issue/>
          <page-range>130-136</page-range>
          <fpage>130</fpage>
          <lpage>136</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICCRD51685.2021.9386366.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>M. H.</given-names>
              <surname>Nugraha</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Chahyati</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Tourism object detection around monumen nasional (monas) using YOLO and RetinaNet</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2020 International Conference on Advanced Computer Science and Information Systems</conf-name>
          <conf-acronym>ICACSIS 2020</conf-acronym>
          <conf-loc>Depok, Indonesia</conf-loc>
          <conf-date>October 17-18, 2020</conf-date>
          <year>2020</year>
          <volume/>
          <issue/>
          <page-range>317-322</page-range>
          <fpage>317</fpage>
          <lpage>322</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICACSIS51025.2020.9263240.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>B.</given-names>
              <surname>Strbac</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Gostovic</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Lukac</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Samardzija</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>YOLO multi-camera object detection and distance estimation</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2020 Zooming Innovation in Consumer Technologies Conference</conf-name>
          <conf-acronym>ZINC 2020</conf-acronym>
          <conf-loc>Novi Sad, Serbia</conf-loc>
          <conf-date>May 26-27, 2020</conf-date>
          <year>2020</year>
          <volume/>
          <issue/>
          <page-range>26-30</page-range>
          <fpage>26</fpage>
          <lpage>30</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ZINC50678.2020.9161805.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>