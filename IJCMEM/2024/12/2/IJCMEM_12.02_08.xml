<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IJCMEM</journal-id>
      <journal-id journal-id-type="doi">10.18280</journal-id>
      <journal-title-group>
        <journal-title>International Journal of Computational Methods and Experimental Measurements</journal-title>
        <abbrev-journal-title abbrev-type="issn">Int. J. Comput. Methods Exp. Meas.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IJCMEM</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2046-0554</issn>
      <issn publication-format="print">2046-0546</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-ri1Z9z-Wyb7c0QgMgYhGhElEfCmasgYC</article-id>
      <article-id pub-id-type="doi">10.18280/ijcmem.120208</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Enhanced Unsupervised Feature Selection Method Using Crow Search Algorithm and Calinski-Harabasz</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1828-5350</contrib-id>
          <name>
            <surname>Hasan</surname>
            <given-names>Fatima M.</given-names>
          </name>
          <email/>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-6276-9493</contrib-id>
          <name>
            <surname>Hussein</surname>
            <given-names>Talal F.</given-names>
          </name>
          <email>talal.math@uomosul.edu.iq</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1156-1044</contrib-id>
          <name>
            <surname>Saleem</surname>
            <given-names>Hanadi D.</given-names>
          </name>
          <email/>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3301-6271</contrib-id>
          <name>
            <surname>Qasim</surname>
            <given-names>Omar S.</given-names>
          </name>
          <email/>
        </contrib>
        <aff id="aff_1">Department of Mathematics, University of Mosul, Mosul 41002, Iraq</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>29</day>
        <month>06</month>
        <year>2024</year>
      </pub-date>
      <volume>12</volume>
      <issue>2</issue>
      <fpage>185</fpage>
      <lpage>190</lpage>
      <page-range>185-190</page-range>
      <history>
        <date date-type="received">
          <day>28</day>
          <month>05</month>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <day>29</day>
          <month>11</month>
          <year>2023</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>This paper proposes enhancing the K-means clustering method by incorporating the Crow Search Algorithm (CSA) and Calinski-Harabasz (CH) index to address the issue of determining the optimal number of clusters and attribute selection. The proposed approach, called Crow Search Algorithm K-mean clustering (CSAK_means), aims to explore the search space more effectively to find the best solutions. The efficiency of the CSAK_means algorithm is evaluated using a comparative experimental study for five datasets from the UCI repositories: Wine, Bodega, Cmc, Zoo, and Abalone. The results confirm that the proposed method outperforms the default algorithms in terms of average feature selection performance and silhouette value.</p></abstract>
      <kwd-group>
        <kwd>crow search algorithm</kwd>
        <kwd>Calinski-Harabasz index</kwd>
        <kwd>K-mean clustering</kwd>
        <kwd>feature selection</kwd>
        <kwd>data mining</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="4"/>
        <fig-count count="5"/>
        <table-count count="3"/>
        <ref-count count="24"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Feature selection is an important process in machine learning and data mining that involves choosing suitable features from many options to create a model that accurately represents the output variables, this step is necessary to improve the performance of the model by reducing overfitting, improving model interpretation, and reducing computational time [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>]. When it comes to selecting model features, there are basically three methods that can be used: wrapper-based, filter-based, combined methods [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>]. Wrapper-based methods use a classifier to evaluate the performance of an object, while filter-based methods use statistical measures [<xref ref-type="bibr" rid="ref_5">5</xref>]. Features are selected in the modeling process using embedded methods such as decision trees or neural networks. When there are many possibilities in the optimization problem, the feature selection method can be effective [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>].</p><p>Metaheuristic algorithms such as particle swarm optimization (PSO), artificial bee colony (ABC), differential evolution (DE), and gray wolf algorithm (GWA) have been developed to solve many problems. Feature selection is one of the applications that have some drawbacks despite the advantages of there is on, such as how the factors chosen for all situations or data sets may not be optimal for each, and if selection procedures are not well designed, they can introduce bias, so researchers need to be careful [<xref ref-type="bibr" rid="ref_8">8</xref>].</p><p>When observing crow food storage and retrieval behaviors developed an optimization method called Crow Search Algorithm (CSA) [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>]. It is a powerful technique that can modify hyperparameters and it is very interesting that be used in various industrial applications thanks to a population- based approach and only two parameters configurable: travel length and awareness probability. Crow swarm intelligence is the basis for CSA technology, which mimics the subtle behavior of swarms of crows. This method was published by Hussien et al. [<xref ref-type="bibr" rid="ref_10">10</xref>] in 2016. This method has been widely used by researchers due to its ease of use, efficiency, and low number of parameters so many modifications of the basic CSA algorithm are not used to overcome this problem. For example, some researchers introduced an adaptive inertial weighting factor and a roulette wheel selection system to enhance detection and control capabilities [<xref ref-type="bibr" rid="ref_11">11</xref>]. Prior work has employed Lévy flight motion to improve the search capabilities of the CSA algorithm, as well as dynamic modifications to the fixed awareness probability value based on the fitness value of individual candidate solutions. To further its efficiency, it has also been coupled with 10 chaotic maps and, under some situations, the CSA algorithm. Others include Differential Evolution (DE) and the BAT algorithm (BA).</p><p>Filter-based techniques use statistical measures to ascertain the value of each feature, while wrapper-based approaches use a classifier to evaluate feature performance [<xref ref-type="bibr" rid="ref_12">12</xref>]. Built-in techniques such as decision trees and neural networks use feature selection methods and classifiers together at the same time.</p><p>The Crow Search Method (CSA) was created after research was done on how crows store and recover food [<xref ref-type="bibr" rid="ref_13">13</xref>]. Its two adjustable parameters, flight length and awareness probability, along with its processing-based methodology make it a powerful technology that can optimize hyperparameters and is highly appealing for application in a wide range of technological domains. The foundation of CSA is swarm intelligence, which emulates the clever behavior of crow colonies. Since Askarzadeh first presented this approach, many scholars have used it with just a few modifications [<xref ref-type="bibr" rid="ref_14">14</xref>]. Therefore, researchers have proposed several modifications to the original CSA treatment to overcome this problem. For example, some researchers have included a roulette wheel selection mechanism and an adjustable inertia weight factor to enhance exploration and exploitation capabilities. On the fitness score for the individual possible answer.</p><p>The CSA technique outperforms several optimization algorithms in six engineering design tasks through the strategies in its design and structure, and with only a few criteria required, this strategy can be effective and easy to implement, leading to positive results in the search for possible solutions through solve the problem [<xref ref-type="bibr" rid="ref_15">15</xref>], [<xref ref-type="bibr" rid="ref_16">16</xref>].</p><p>The Calinski-Harabasz (CH) index, also known as the variance criteria ratio, is often used to evaluate the splitting quality using the K-means clustering technique for a specified number of clusters by calculating the ratio of the total between-cluster and inter-cluster dispersion for all classes [<xref ref-type="bibr" rid="ref_10">10</xref>]. Model performance can be evaluated using the CHI score, which is associated with greater clustering performance, in scenarios where ground truth labels are unknown. In addition to being used with other clustering techniques.</p><p>This study aims to improve the K-means clustering technique by combining the Crow Search Algorithm (CSA) and the Calinski-Harabasz (CH) to obtain the best selection of the number of clusters or centers as well as to select the best features, where we studied the nuances of the elements Our study has been meticulously designed to give readers a comprehensive understanding and enjoyment of these subjects via in-depth analysis.</p><p>The paper is structured as follows: Section 2 provides a thorough synopsis of CSA. In Section 3 we move on to the CH index, which is a measure to evaluate how well classified data is generated using the K-means clustering algorithm. Section 4 focuses on the K-means clustering method. We outline our implementation framework in Section 5, which also presents a proposed action. Research findings for several datasets are presented in Section 6. In Section 7, we conclude the research presented in this article.</p>
    </sec>
    <sec sec-type="">
      <title>2. Crow search algorithm (csa)</title>
      <p>The Crow Search Algorithm (CSA), developed by Seyadali Mirjalili, is inspired by the instinctive behavior of crows. This new method developed in 2016 has proven to be a useful tool for solving complex optimization problems [<xref ref-type="bibr" rid="ref_17">17</xref>]. A popular optimization strategy in the swarm intelligence family is called CSA. "A swarm intelligence" is a combination of algorithms that are triggered by biological factors or animal emotional reactions. Crows are a unique source of inspiration for CSA as new solutions are sought [<xref ref-type="bibr" rid="ref_18">18</xref>].</p><p>Swarm intelligence methods such as ant colony optimization (ACO), firefly algorithm (FA), and particle swarm optimization (PSO) are used for stakeholders to collectively generate intelligence to find answers to optimization problems of crows hiding, food, other birds CSA technology, using food and habit stealing, using the same concept [<xref ref-type="bibr" rid="ref_19">19</xref>].</p><p>Crows follow each other when they steal food, they live in groups, they can remember where they concealed their food, and they have the perceptual capacity to change where they are hiding when they sense danger. These are some of the four main principles of the CSA. To use this idea, picture a group of crows in a d-dimensional search space. The group is made up of N crows, each of which represents a possible solution to the problem, and d indicates the number of option variables.</p><p>Let <inline-formula>
  <mml:math id="m7yix946r3">
    <mml:msup>
      <mml:mi>x</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mo>,</mml:mo>
        <mml:mtext> iter </mml:mtext>
      </mml:mrow>
    </mml:msup>
    <mml:mo>=</mml:mo>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msubsup>
        <mml:mi>x</mml:mi>
        <mml:mn>1</mml:mn>
        <mml:mrow>
          <mml:mi>i</mml:mi>
          <mml:mo>,</mml:mo>
          <mml:mtext> iter </mml:mtext>
        </mml:mrow>
      </mml:msubsup>
      <mml:msubsup>
        <mml:mi>x</mml:mi>
        <mml:mn>2</mml:mn>
        <mml:mrow>
          <mml:mi>i</mml:mi>
          <mml:mo>,</mml:mo>
          <mml:mtext> iter </mml:mtext>
        </mml:mrow>
      </mml:msubsup>
      <mml:msubsup>
        <mml:mi>x</mml:mi>
        <mml:mi>d</mml:mi>
        <mml:mrow>
          <mml:mi>i</mml:mi>
          <mml:mo>,</mml:mo>
          <mml:mtext> iter </mml:mtext>
        </mml:mrow>
      </mml:msubsup>
    </mml:mrow>
  </mml:math>
</inline-formula> using the matching iteration number, indicate the position of the crow I that was determined during the iteration. The crow position may be updated using the following formula:</p>
      
        <disp-formula>
          <label>(1)</label>
          <mml:math id="m88ndn0nv3">
            <mml:msup>
              <mml:mi>x</mml:mi>
              <mml:mrow>
                <mml:mi>i</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mo>,</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mn>1</mml:mn>
              </mml:mrow>
            </mml:msup>
            <mml:mo>=</mml:mo>
            <mml:mrow>
              <mml:mo>{</mml:mo>
              <mml:mo>}</mml:mo>
              <mml:mtable columnspacing="1em" rowspacing="4pt">
                <mml:mtr>
                  <mml:mtd>
                    <mml:msup>
                      <mml:mi>x</mml:mi>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mo>,</mml:mo>
                      </mml:mrow>
                    </mml:msup>
                    <mml:msup>
                      <mml:mi>l</mml:mi>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mo>,</mml:mo>
                      </mml:mrow>
                    </mml:msup>
                    <mml:msup>
                      <mml:mi>P</mml:mi>
                      <mml:mrow>
                        <mml:mi>j</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mo>,</mml:mo>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mo>+</mml:mo>
                    <mml:mo>∗</mml:mo>
                    <mml:mo>∗</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>≥</mml:mo>
                    <mml:msub>
                      <mml:mi>r</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>r</mml:mi>
                      <mml:mi>j</mml:mi>
                    </mml:msub>
                    <mml:mi>f</mml:mi>
                    <mml:mi>A</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msup>
                        <mml:mi>m</mml:mi>
                        <mml:mrow>
                          <mml:mi>j</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>e</mml:mi>
                          <mml:mi>r</mml:mi>
                          <mml:mo>,</mml:mo>
                        </mml:mrow>
                      </mml:msup>
                      <mml:msup>
                        <mml:mi>x</mml:mi>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mi>t</mml:mi>
                          <mml:mi>e</mml:mi>
                          <mml:mi>r</mml:mi>
                          <mml:mo>,</mml:mo>
                        </mml:mrow>
                      </mml:msup>
                    </mml:mrow>
                  </mml:mtd>
                </mml:mtr>
                <mml:mtr>
                  <mml:mtd>
                    <mml:mtext> random position, otherwise </mml:mtext>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:mrow>
          </mml:math>
        </disp-formula>
      
      <p>where, <inline-formula>
  <mml:math id="mputhqw4bd">
    <mml:msup>
      <mml:mi>m</mml:mi>
      <mml:mrow>
        <mml:mi>j</mml:mi>
        <mml:mo>,</mml:mo>
        <mml:mtext> iter </mml:mtext>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> is the best place for the crow $j<inline-formula>
  <mml:math id="mrwjw1hyt1">
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>r_i<inline-formula>
  <mml:math id="m641l45sfy">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>[0,1]<inline-formula>
  <mml:math id="mv8q835fpl">
    <mml:mo>,</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>f l^{i, i t e r}<inline-formula>
  <mml:math id="mo9ft013sl">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>A P^{j, \text { iter }}<inline-formula>
  <mml:math id="m018jwcgv1">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
  </mml:math>
</inline-formula>j<inline-formula>
  <mml:math id="mhorc4s8y3">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>j<inline-formula>
  <mml:math id="m07x3h16cr">
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula>A P^{j, i t e r}<inline-formula>
  <mml:math id="mwao7tyas7">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula>r_j \geq A P^{j, i t e r}<inline-formula>
  <mml:math id="mz2rv97e2k">
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
  </mml:math>
</inline-formula>j<inline-formula>
  <mml:math id="mly1jxg5bd">
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>W</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>−</mml:mo>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>r_j \lt A P^{j, \text { iter }}<inline-formula>
  <mml:math id="muab42oh64">
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>W</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>j$ is position loses its tracking value, a new position for the crow is created at random.</p>
      
        <disp-formula>
          <label>(2)</label>
          <mml:math id="m1yyql30j8">
            <mml:msup>
              <mml:mi>m</mml:mi>
              <mml:mrow>
                <mml:mi>i</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>t</mml:mi>
                <mml:mi>e</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mo>,</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mn>1</mml:mn>
              </mml:mrow>
            </mml:msup>
            <mml:mo>=</mml:mo>
            <mml:mrow>
              <mml:mo>{</mml:mo>
              <mml:mo>}</mml:mo>
              <mml:mtable columnalign="center left" columnspacing="1em" rowspacing="4pt">
                <mml:mtr>
                  <mml:mtd>
                    <mml:msup>
                      <mml:mi>x</mml:mi>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mo>+</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mo>,</mml:mo>
                    <mml:mi>f</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msup>
                        <mml:mi>x</mml:mi>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mo>+</mml:mo>
                          <mml:mtext> iter </mml:mtext>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                      </mml:msup>
                    </mml:mrow>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:mtext> is better than </mml:mtext>
                    <mml:mi>f</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msup>
                        <mml:mi>m</mml:mi>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>,</mml:mo>
                          <mml:mtext> iter </mml:mtext>
                        </mml:mrow>
                      </mml:msup>
                    </mml:mrow>
                  </mml:mtd>
                </mml:mtr>
                <mml:mtr>
                  <mml:mtd>
                    <mml:msup>
                      <mml:mi>m</mml:mi>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mtext> iter </mml:mtext>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mo>,</mml:mo>
                  </mml:mtd>
                  <mml:mtd>
                    <mml:mtext> otherwise </mml:mtext>
                  </mml:mtd>
                </mml:mtr>
              </mml:mtable>
            </mml:mrow>
          </mml:math>
        </disp-formula>
      
      <p>where, f(.) is the primary objective,</p><p style="text-align: center"><inline-formula>
  <mml:math id="mdrzaxli41">
    <mml:msup>
      <mml:mi>m</mml:mi>
      <mml:mrow>
        <mml:mrow>
          <mml:mi>i</mml:mi>
        </mml:mrow>
        <mml:mo>,</mml:mo>
        <mml:mtext> iter </mml:mtext>
      </mml:mrow>
    </mml:msup>
    <mml:mo>=</mml:mo>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msubsup>
        <mml:mi>m</mml:mi>
        <mml:mn>1</mml:mn>
        <mml:mrow>
          <mml:mi>i</mml:mi>
          <mml:mo>,</mml:mo>
          <mml:mtext> iter </mml:mtext>
        </mml:mrow>
      </mml:msubsup>
      <mml:msubsup>
        <mml:mi>m</mml:mi>
        <mml:mn>2</mml:mn>
        <mml:mrow>
          <mml:mrow>
            <mml:mi>i</mml:mi>
          </mml:mrow>
          <mml:mo>,</mml:mo>
          <mml:mtext> iter </mml:mtext>
        </mml:mrow>
      </mml:msubsup>
      <mml:msubsup>
        <mml:mi>m</mml:mi>
        <mml:mi>d</mml:mi>
        <mml:mrow>
          <mml:mi>i</mml:mi>
          <mml:mo>,</mml:mo>
          <mml:mtext> iter </mml:mtext>
        </mml:mrow>
      </mml:msubsup>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p>Following the numerical iterations, stores the crow i at the best possible storage place. The exploration and exploitation phase of the CSA algorithm can also be learned from the <xref ref-type="fig" rid="fig_1">Figure 1</xref>.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>The crow position update diagram in CSA case i shows the effect of fl in searching process [<xref ref-type="bibr" rid="ref_17">17</xref>]</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_kv3rnx8JQMQu0-14.jpeg"/>
        </fig>
      
      <p>The main mechanism on which the CSA algorithm relies in finding solutions while searching the solution space in order to obtain the optimal solution can be summarized through the following sequential steps:</p><table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: justify"><span style="font-family: Times New Roman, serif">Algorithm:<span style="font-family: Times New Roman, serif"> <span style="font-family: Times New Roman, serif">Crow Search Algorithm</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Step 1. Begin by initializing a population of crows <span style="font-family: MathJax_Math-italic">xi<span style="font-family: MathJax_Main">,<span style="font-family: MathJax_Math-italic">iter with random positions.</p><p style="text-align: justify">Step 2. Evaluate the fitness of each crow using a fitness function. This helps to determine how well the crow is performing in solving the optimization problem.</p><p style="text-align: justify">Step 3. Generate new positions for the crows <inline-formula>
  <mml:math id="m9q4xfsc51">
    <mml:msup>
      <mml:mi>x</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mo>,</mml:mo>
        <mml:mo>+</mml:mo>
        <mml:mtext> iter </mml:mtext>
        <mml:mn>1</mml:mn>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> using a specified algorithm.</p><p style="text-align: justify">Step 4. Check the feasibility of the new positions. This ensures that the crows are not moving to illegal or impossible locations.</p><p style="text-align: justify">Step 5. Analyze the new positions' fitness function. This aids to evaluate the crows' performance following their creation of their new places.</p><p style="text-align: justify">Step 6. Update the memory of the algorithm <inline-formula>
  <mml:math id="m33p9beyqz">
    <mml:msup>
      <mml:mi>m</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>i</mml:mi>
        <mml:mi>t</mml:mi>
        <mml:mi>e</mml:mi>
        <mml:mi>r</mml:mi>
        <mml:mo>,</mml:mo>
        <mml:mo>+</mml:mo>
        <mml:mn>1</mml:mn>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula>. This involves keeping track of the best position found so far.</p><p style="text-align: justify">Step 7. Check the criteria for dismissal. Steps 3-6 are performed until the maximum number of iterations (itermax) has been accomplished. When the termination requirement is met, an ideal memory place in respect to the objective function's value is given as the optimization problem's solution.</p></td></tr></tbody></table>
    </sec>
    <sec sec-type="">
      <title>3. The calinski-harabasz index (ch)</title>
      <p>Focusing on integration, with the K-means clustering technique, the CH index is a commonly used statistic for evaluating clustering schemes [<xref ref-type="bibr" rid="ref_20">20</xref>], [<xref ref-type="bibr" rid="ref_21">21</xref>]. This index, which shows how effectively data is separated into a specified number of clusters by algorithms, is essential for evaluating the effectiveness of clustering algorithms.</p><p>This effectiveness is statistically evaluated using contrast ratio criteria, sometimes referred to as the CH index. Its calculation involves dividing the total dispersal within and between groups by the total dispersal, where “dispersal” is defined as the sum of the squared distances. In simple terms, it contrasts the degree of dispersion of data points between groups with the amount of distribution within each group [<xref ref-type="bibr" rid="ref_22">22</xref>]:</p>
      
        <disp-formula>
          <label>(3)</label>
          <mml:math id="md7hn8ys4n">
            <mml:mi>C</mml:mi>
            <mml:mi>H</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:mo>∗</mml:mo>
            <mml:mfrac>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mi>B</mml:mi>
                  <mml:mi>G</mml:mi>
                  <mml:mi>S</mml:mi>
                  <mml:mi>S</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>K</mml:mi>
                  <mml:mo>−</mml:mo>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
              </mml:mfrac>
              <mml:mfrac>
                <mml:mrow>
                  <mml:mi>W</mml:mi>
                  <mml:mi>G</mml:mi>
                  <mml:mi>S</mml:mi>
                  <mml:mi>S</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mi>N</mml:mi>
                  <mml:mi>K</mml:mi>
                  <mml:mo>−</mml:mo>
                </mml:mrow>
              </mml:mfrac>
            </mml:mfrac>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>B</mml:mi>
                <mml:mi>G</mml:mi>
                <mml:mi>S</mml:mi>
                <mml:mi>S</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>W</mml:mi>
                <mml:mi>G</mml:mi>
                <mml:mi>S</mml:mi>
                <mml:mi>S</mml:mi>
              </mml:mrow>
            </mml:mfrac>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>N</mml:mi>
                <mml:mi>K</mml:mi>
                <mml:mo>−</mml:mo>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>K</mml:mi>
                <mml:mo>−</mml:mo>
                <mml:mn>1</mml:mn>
              </mml:mrow>
            </mml:mfrac>
          </mml:math>
        </disp-formula>
      
      <p>where,</p><p style="text-align: justify"><italic>N</italic>: the total amount of observations.</p><p style="text-align: justify"><italic>K</italic>: the overall cluster count.</p><p style="text-align: justify">The formula for calculating the between-group sum of squares inter-cluster dispersion is as follows:</p>
      
        <disp-formula>
          <label>(4)</label>
          <mml:math id="maq8ndu67l">
            <mml:mi>B</mml:mi>
            <mml:mi>G</mml:mi>
            <mml:mi>S</mml:mi>
            <mml:mi>S</mml:mi>
            <mml:mo>=</mml:mo>
            <mml:mo>∗</mml:mo>
            <mml:munderover>
              <mml:mo>∑</mml:mo>
              <mml:mrow>
                <mml:mi>k</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mn>1</mml:mn>
              </mml:mrow>
              <mml:mi>K</mml:mi>
            </mml:munderover>
            <mml:msub>
              <mml:mi>n</mml:mi>
              <mml:mi>k</mml:mi>
            </mml:msub>
            <mml:msup>
              <mml:mrow>
                <mml:mo>‖</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>‖</mml:mo>
                <mml:msub>
                  <mml:mi>C</mml:mi>
                  <mml:mi>k</mml:mi>
                </mml:msub>
                <mml:mi>C</mml:mi>
              </mml:mrow>
              <mml:mn>2</mml:mn>
            </mml:msup>
          </mml:math>
        </disp-formula>
      
      <p>where,</p><p style="text-align: justify"><span style="font-family: MathJax_Math-italic"><inline-formula>
  <mml:math id="mwwzepy5i5">
    <mml:msub>
      <mml:mi>n</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>: how many observations there are in cluster <italic>k</italic>.</p><p style="text-align: justify"><span style="font-family: MathJax_Math-italic"><inline-formula>
  <mml:math id="mbff9kzy3z">
    <mml:msub>
      <mml:mi>C</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>: the cluster <italic>K</italic> centroid.</p><p style="text-align: justify"><italic>C:</italic> the barycenter, or centroid, of the dataset.</p><p style="text-align: justify"><italic>K:</italic> how many clusters there are enter an equation here.</p><p style="text-align: justify">The following equation is used to determine WGSS or within-group sum of squares intra-cluster dispersion.</p>
      
        <disp-formula>
          <label>(5)</label>
          <mml:math id="m6ipslwa5d">
            <mml:mi>W</mml:mi>
            <mml:mi>G</mml:mi>
            <mml:mi>S</mml:mi>
            <mml:msub>
              <mml:mi>S</mml:mi>
              <mml:mi>k</mml:mi>
            </mml:msub>
            <mml:mo>=</mml:mo>
            <mml:munderover>
              <mml:mo>∑</mml:mo>
              <mml:mrow>
                <mml:mi>i</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mn>1</mml:mn>
              </mml:mrow>
              <mml:mrow>
                <mml:msub>
                  <mml:mi>n</mml:mi>
                  <mml:mi>k</mml:mi>
                </mml:msub>
              </mml:mrow>
            </mml:munderover>
            <mml:msup>
              <mml:mrow>
                <mml:mo>‖</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>‖</mml:mo>
                <mml:msub>
                  <mml:mi>X</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mi>k</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>C</mml:mi>
                  <mml:mi>k</mml:mi>
                </mml:msub>
              </mml:mrow>
              <mml:mn>2</mml:mn>
            </mml:msup>
          </mml:math>
        </disp-formula>
      
      <p>where,</p><p style="text-align: justify"><span style="font-family: MathJax_Math-italic"><inline-formula>
  <mml:math id="midxdyfk2l">
    <mml:msub>
      <mml:mi>n</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>: the quantity of data in cluster <italic>k</italic>.</p><p style="text-align: justify"><span style="font-family: MathJax_Math-italic"><inline-formula>
  <mml:math id="me6nvt2rlw">
    <mml:msub>
      <mml:mi>C</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>: the cluster <italic>k</italic> centroid.</p><p style="text-align: justify"><span style="font-family: MathJax_Math-italic"><inline-formula>
  <mml:math id="mze18ijya8">
    <mml:msub>
      <mml:mi>X</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:mi>k</mml:mi>
  </mml:math>
</inline-formula>: the i-th observation of cluster <italic>k</italic>.</p><p style="text-align: justify">Then add up each individual square sum within a group:</p>
      
        <disp-formula>
          <label>(6)</label>
          <mml:math id="mlqxj8k577">
            <mml:mrow>
              <mml:mi data-mjx-auto-op="false">WGSS</mml:mi>
            </mml:mrow>
            <mml:mo>=</mml:mo>
            <mml:munderover>
              <mml:mo>∑</mml:mo>
              <mml:mrow>
                <mml:mi>k</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mn>1</mml:mn>
              </mml:mrow>
              <mml:mi>K</mml:mi>
            </mml:munderover>
            <mml:mi>W</mml:mi>
            <mml:mi>G</mml:mi>
            <mml:mi>S</mml:mi>
            <mml:msub>
              <mml:mi>S</mml:mi>
              <mml:mi>k</mml:mi>
            </mml:msub>
          </mml:math>
        </disp-formula>
      
      <p>where,</p><p style="text-align: justify"><span style="font-family: MathJax_Math-italic"><inline-formula>
  <mml:math id="myvg0my4g7">
    <mml:mi>W</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>: the within group sum of squares of cluster $k$. </p><p style="text-align: justify">The big values of the Calinski-Harabasz index, according to the above calculation, indicate superior clustering.</p>
    </sec>
    <sec sec-type="">
      <title>4. K-means clustering</title>
      <p>The sum of the squared distances between data points and the cluster centroids they correspond to is minimized using the K-means algorithm, a powerful unsupervised clustering tool. By dividing the data set into K distinct clusters, this method maximizes the similarity within each cluster while facilitating appropriate separation between clusters. The main steps of the CSA flowchart can be illustrated in <xref ref-type="fig" rid="fig_2">Figure 2</xref> [<xref ref-type="bibr" rid="ref_23">23</xref>]:</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Flowchart of K-means algorithm [<xref ref-type="bibr" rid="ref_24">24</xref>]</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_CsH_gMl5xPS2Njru.png"/>
        </fig>
      
      <p>The method randomly selects K data points as initial centroids before starting the iterative process of mapping each data point to its closest centroid. Once convergence is achieved, these centroids are later calculated again, often by averaging the data points within each group.</p><p>K-means is a popular choice in fields such as machine learning and image processing due to its known ability to rank large data sets according to underlying patterns and structures. To measure the similarity between data points and focal points [<xref ref-type="bibr" rid="ref_24">24</xref>].</p>
      
        <disp-formula>
          <label>(7)</label>
          <mml:math id="mv032yadaz">
            <mml:mi>D</mml:mi>
            <mml:mi>x</mml:mi>
            <mml:mi>y</mml:mi>
            <mml:mo>(</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:msqrt>
              <mml:msup>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>−</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mn>1</mml:mn>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mn>1</mml:mn>
                  </mml:msub>
                </mml:mrow>
                <mml:mn>2</mml:mn>
              </mml:msup>
              <mml:msup>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>−</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mn>2</mml:mn>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mn>2</mml:mn>
                  </mml:msub>
                </mml:mrow>
                <mml:mn>2</mml:mn>
              </mml:msup>
              <mml:msup>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>−</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mi>n</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mi>n</mml:mi>
                  </mml:msub>
                </mml:mrow>
                <mml:mi>n</mml:mi>
              </mml:msup>
              <mml:mo>+</mml:mo>
              <mml:mo>+</mml:mo>
              <mml:mo>⋯</mml:mo>
              <mml:mo>+</mml:mo>
            </mml:msqrt>
          </mml:math>
        </disp-formula>
      
    </sec>
    <sec sec-type="">
      <title>5. The proposed enhancement</title>
      <p>The proposed strategy aims to increase the performance of K-means clustering by simultaneously increasing the number of clusters and adding feature selection. The Crow Search Algorithm (CSA) is used as a population-based search strategy, with the K value and feature selection expressed as binary strings. Performance in K-means clustering is strongly influenced by the selection of K, where K stands for the number of clusters. While numerous methods, including some inspired by natural algorithms, have been employed to enhance K-means clustering performance through suitable K selection, none have attempted to choose multiple features at once. The suggested search technique incorporates feature selection using the CSA and tries to maximize the number of clusters in K-means clustering.</p><p>Input data set X of size <inline-formula>
  <mml:math id="mbdfss965h">
    <mml:mi>N</mml:mi>
    <mml:mi>D</mml:mi>
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula>, where D represents the number of features and N represents the number of instances. Set the number of clusters, Kmax, to be considered in the search.</p><p>Initialize the population of crows, $C<inline-formula>
  <mml:math id="mqoq6k6wu8">
    <mml:mo>,</mml:mo>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
  </mml:math>
</inline-formula>K<inline-formula>
  <mml:math id="mtir0p7d8w">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>D</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:msup>
      <mml:mi>w</mml:mi>
      <mml:mo>′</mml:mo>
    </mml:msup>
  </mml:math>
</inline-formula>C<inline-formula>
  <mml:math id="mxhurbzy1o">
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>K</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>K</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>C<inline-formula>
  <mml:math id="mmpk86dtem">
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>U</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>\mathcal{C}<inline-formula>
  <mml:math id="m65kwkhuoe">
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>C<inline-formula>
  <mml:math id="m82q73snjg">
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>K</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>K</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>C$ to form a new population Ć based on their fitness values.</p><p>Continue from step 6 through step 8 until a termination condition is met, such as when the maximum number of iterations is achieved or the best fitness value converges.</p><p>Output the solution with the highest fitness value from the final population as the selected K and feature subset for K-means clustering.</p><p>The suggested technique concurrently optimizes the number of clusters and feature selection for K-means clustering using a population-based search strategy with the CSA operators. The K-means clustering technique, which assesses the effectiveness of the clustering solution, serves as the foundation for the fitness evaluation. By changing and merging the already-existing solutions in the population, the CSA operators (such as levy flight and crossover) are employed to produce new solutions. When a stopping requirement is satisfied, such as a maximum number of iterations or convergence of the best fitness value.</p><p><xref ref-type="fig" rid="fig_3">Figure 3</xref> is an illustration of the solution representation, which shows how the K and feature selection are encoded in a binary string format. The first part of the string represents the K value, and the second part represents the feature selection, where a 1 indicates that the corresponding feature is selected and a 0 indicates that it is not selected.</p>
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>Representation of feature selection in the proposed algorithm [<xref ref-type="bibr" rid="ref_5">5</xref>]</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_dYnOm5UQK69OZ344.png"/>
        </fig>
      
    </sec>
    <sec sec-type="">
      <title>6. Results and discussion</title>
      <p>We used the proposed CSAK_means method on five distinct publicly accessible datasets to evaluate its effectiveness. We also examined how well it performed compared to the traditional K-means algorithm that uses CSA to determine the optimal number of clusters. Through this comparison, we can confirm that the proposed algorithm outperforms the traditional method in terms of results.</p><p><xref ref-type="table" rid="table_1">Table 1</xref> provides a brief overview of the datasets used in the experiments, which were obtained from the UCI Machine Learning Repository. The experiments used five real-world datasets that differed in dimensionality, number of observations, and clusters (represented by C). Each dataset is briefly described in the table to provide an overview of its characteristics.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Describe the characteristics of the dataset</title>
          </caption>
          <table><tbody><tr><td><p>The Datasets</p></td><td><p>Instances (N)</p></td><td><p>Dimensions (D)</p></td></tr><tr><td><p>Data1 (Wine)</p></td><td><p>178</p></td><td><p>13</p></td></tr><tr><td><p>Data2 (Biodeg)</p></td><td><p>1055</p></td><td><p>41</p></td></tr><tr><td><p>Data3 (Cmc)</p></td><td><p>1473</p></td><td><p>9</p></td></tr><tr><td><p>Data4 (Zoo)</p></td><td><p>101</p></td><td><p>17</p></td></tr><tr><td><p>Data5 (Abalone)</p></td><td><p>4177</p></td><td><p>8</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>The silhouette score is used as a statistic to evaluate the effectiveness of the algorithms put into practice. A cluster validation approach is used to obtain this result, to minimize the distance between points within each cluster and maximize the gap between clusters. It measures the quality of the clustering results. The average sampling distance from every other point in the same group is less than the average sampling distance from every other point in the closest group. High values indicate a strong match between the sample and its group, while low or negative values reflect the presence of too many or too few clustering. The final score ranges from -1 to +1 . The aggregation configuration is appropriate when most of the elements have high degrees of silhouettes, where the silhouette width <inline-formula>
  <mml:math id="mecbgernj2">
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is determined as follows:</p>
      
        <disp-formula>
          <label>(8)</label>
          <mml:math id="mcjpo33p6c">
            <mml:mi>s</mml:mi>
            <mml:mi>i</mml:mi>
            <mml:mo>(</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>b</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
              </mml:mrow>
              <mml:mrow>
                <mml:mo>max</mml:mo>
                <mml:mo fence="false">{</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo fence="false">}</mml:mo>
                <mml:mi>b</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>i</mml:mi>
              </mml:mrow>
            </mml:mfrac>
          </mml:math>
        </disp-formula>
      
      <p>The Silhouette width <inline-formula>
  <mml:math id="mgudpf7l7e">
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is a metric that pertains to a specific data point <inline-formula>
  <mml:math id="mkphhn9a4p">
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>i</mml:mi>
  </mml:math>
</inline-formula>. It is calculated using the average distance between <inline-formula>
  <mml:math id="maej8lpnom">
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>i</mml:mi>
  </mml:math>
</inline-formula> and all other data points in the same cluster <inline-formula>
  <mml:math id="m5jvajwfqx">
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
  </mml:math>
</inline-formula>, which is represented by <inline-formula>
  <mml:math id="msckzy26c0">
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula>, as well as the average distance between <inline-formula>
  <mml:math id="mrw5tfyksb">
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>i</mml:mi>
  </mml:math>
</inline-formula> and all other data points in a different cluster <inline-formula>
  <mml:math id="mmax0k64j5">
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
  </mml:math>
</inline-formula>, represented by <inline-formula>
  <mml:math id="m0hcd0tqb9">
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula>.</p>
      
        <disp-formula>
          <label>(9)</label>
          <mml:math id="mq1qtcurcr">
            <mml:mi>b</mml:mi>
            <mml:mi>i</mml:mi>
            <mml:mi>d</mml:mi>
            <mml:mi>i</mml:mi>
            <mml:mi>j</mml:mi>
            <mml:mo>(</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:mo>(</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:munder>
              <mml:mo>min</mml:mo>
              <mml:mrow>
                <mml:mi>I</mml:mi>
                <mml:mi>J</mml:mi>
                <mml:mo>≠</mml:mo>
              </mml:mrow>
            </mml:munder>
            <mml:munder>
              <mml:mo>∑</mml:mo>
              <mml:mrow>
                <mml:mi>j</mml:mi>
                <mml:mi>J</mml:mi>
                <mml:mo>∈</mml:mo>
              </mml:mrow>
            </mml:munder>
            <mml:mfrac>
              <mml:mn>1</mml:mn>
              <mml:mrow>
                <mml:mo>|</mml:mo>
                <mml:mo>|</mml:mo>
                <mml:msub>
                  <mml:mi>c</mml:mi>
                  <mml:mi>J</mml:mi>
                </mml:msub>
              </mml:mrow>
            </mml:mfrac>
          </mml:math>
        </disp-formula>
      
      <p>where, <italic>d</italic> stands for the separation between <italic>i</italic> and <italic>j</italic>.</p><p style="text-align: justify">Based on the results in <xref ref-type="table" rid="table_2">Table 2</xref>, it is clear, as evidenced by the silhouette value, that the CSAK-means method outperforms K-means for all datasets when clustering accuracy is taken into account. In contrast to K-means, CSAK-means is a better method for selecting and analyzing the number of clusters with a reasonable size. Furthermore, CSAK methods are exceptionally compatible with high-dimensional data and can easily handle complex data. However, data sets with multiple dimensions can cause problems with standard clustering techniques. As a result, CSAK-means provides a useful method for clustering high-dimensional data and outperforms k-means in terms of clustering accuracy.</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>Cumulative accuracy of CSAK-means and K-means algorithms based on silhouette value results</title>
          </caption>
          <table><tbody><tr><td><p>The Datasets</p></td><td><p>CSAK-Means</p></td><td><p>K-Means</p></td></tr><tr><td><p>Data1 (Wine)</p></td><td><p>0.4225</p></td><td><p>0.3051</p></td></tr><tr><td><p>Data2 (Biodeg)</p></td><td><p>0.3062</p></td><td><p>0.2344</p></td></tr><tr><td><p>Data3 (Cmc)</p></td><td><p>0.4915</p></td><td><p>0.3481</p></td></tr><tr><td><p>Data4 (Zoo)</p></td><td><p>0.5619</p></td><td><p>0.4819</p></td></tr><tr><td><p>Data5 (Abalone)</p></td><td><p>0.4957</p></td><td><p>0.4457</p></td></tr></tbody></table>
        </table-wrap>
      
      
        <table-wrap id="table_3">
          <label>Table 3</label>
          <caption>
            <title>Comparison between CSAK-means and K-means algorithms in terms of feature selection</title>
          </caption>
          <table><tbody><tr><td><p>The Datasets</p></td><td><p>CSAK-Means</p></td><td><p>K-Means</p></td></tr><tr><td><p>Data1 (Wine)</p></td><td><p>7.2</p></td><td><p>13</p></td></tr><tr><td><p>Data2 (Biodeg)</p></td><td><p>20.6</p></td><td><p>41</p></td></tr><tr><td><p>Data3 (Cmc)</p></td><td><p>3.2</p></td><td><p>9</p></td></tr><tr><td><p>Data4 (Zoo)</p></td><td><p>8.2</p></td><td><p>17</p></td></tr><tr><td><p>Data5 (Abalone)</p></td><td><p>4.8</p></td><td><p>8</p></td></tr></tbody></table>
        </table-wrap>
      
      
        <fig id="fig_4">
          <label>Figure 4</label>
          <caption>
            <title>Silhouette values compared between CSAK-means and K-means algorithms</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_7Vj43wREdDfankHx.png"/>
        </fig>
      
      
        <fig id="fig_5">
          <label>Figure 5</label>
          <caption>
            <title>A comparison of feature selection comparison between CSAK-means and K-means algorithms</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_pHmyMT-Et8MYWagZ.png"/>
        </fig>
      
      <p>Based on the data in <xref ref-type="table" rid="table_3">Table 3</xref>, it appears that the CSAK-means algorithm performs better than other methods in determining the optimal number of clusters and selecting features for each dataset, and this shows that the use of the CSAK-means algorithm is usually close to what is expected, compared to the K-means. In general, the CSAK-means algorithm has high statistical efficiency compared to the traditional method in all tests performed on the data, which is evident in <xref ref-type="table" rid="table_2">Table 2</xref> and <xref ref-type="table" rid="table_3">Table 3</xref>, and <xref ref-type="fig" rid="fig_4">Figure 4</xref> and <xref ref-type="fig" rid="fig_5">Figure 5</xref>.</p>
    </sec>
    <sec sec-type="conclusions">
      <title>7. Conclusions</title>
      <p>In this work, we presented a proposed algorithm that combines the crow search algorithm (CSA) and the Calinski-Harabasz (CH) index, where CSA is used to find the best feature selection, and CH is used to determine the optimal number of clusters. The proposed CSAK-means is used to improve and provide practical and efficient solutions to clustering problems. The proposed CSAK algorithm outperforms standard methods in terms of silhouette value index and average feature selection, and this is demonstrated by the tests conducted on five sets of data. The proposed algorithm can have many potential applications in analyzing complex and real-world data and can be used to analyze various optimization problems.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation/>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation/>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation/>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation/>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation/>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation/>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation/>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation/>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation/>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation/>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation/>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation/>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation/>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation/>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation/>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation/>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation/>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation/>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation/>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation/>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation/>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation/>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation/>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation/>
      </ref>
    </ref-list>
  </back>
</article>