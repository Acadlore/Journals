<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IDA</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Information Dynamics and Applications</journal-title>
        <abbrev-journal-title abbrev-type="issn">Inf. Dyn. Appl.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IDA</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-1494</issn>
      <issn publication-format="print">2958-1486</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-gw63OoMYF63Vk-6zUmC7p1jnWU1zTjmd</article-id>
      <article-id pub-id-type="doi">10.56578/ida030202</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Advancements in Image Recognition: A Siamese Network Approach</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1,2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-2448-1586</contrib-id>
          <name>
            <surname>Du</surname>
            <given-names>Jiaqi</given-names>
          </name>
          <email>djqo30923@cueb.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2,3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-7114-214X</contrib-id>
          <name>
            <surname>Fu</surname>
            <given-names>Wanshu</given-names>
          </name>
          <email>fwsdyx@cueb.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_4">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0165-903X</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Yi</given-names>
          </name>
          <email>yi.zhang-81@postgrad.manchester.ac.uk</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_5">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-6114-7960</contrib-id>
          <name>
            <surname>Wang</surname>
            <given-names>Ziqi</given-names>
          </name>
          <email>wangzw10@student.unimelb.edu.au</email>
        </contrib>
        <aff id="aff_1">Collaborative Innovation Center of Steel Technology, University of Science and Technology Beijing, 100083 Beijing, China</aff>
        <aff id="aff_2">School of Management and Engineering, Capital University of Economics and Business,100070 Beijing, China</aff>
        <aff id="aff_3">School of Information, Central University of Finance and Economics, 100081 Beijing, China</aff>
        <aff id="aff_4">School of Environment, Education and Development, The University of Manchester, M13 9PL Manchester, UK</aff>
        <aff id="aff_5">Faculty of Engineering and Information Technology, The University of Melbourne, 1446535 Melbourne, Australia</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>13</day>
        <month>06</month>
        <year>2024</year>
      </pub-date>
      <volume>3</volume>
      <issue>2</issue>
      <fpage>89</fpage>
      <lpage>103</lpage>
      <page-range>89-103</page-range>
      <history>
        <date date-type="received">
          <day>04</day>
          <month>04</month>
          <year>2024</year>
        </date>
        <date date-type="accepted">
          <day>30</day>
          <month>05</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p> In the realm of computer vision, image recognition serves as a pivotal task with extensive applications in intelligent security, autonomous driving, and robotics. Traditional methodologies for image recognition often grapple with computational inefficiencies and diminished accuracy in complex scenarios and extensive datasets. To address these challenges, an algorithm utilizing a siamese network architecture has been developed. This architecture leverages dual interconnected neural network submodules for the efficient extraction and comparison of image features. The effectiveness of this siamese network-based algorithm is demonstrated through its application to various benchmark datasets, where it consistently outperforms conventional approaches in terms of accuracy and processing speed. By employing weight-sharing techniques and optimizing neural network pathways, the proposed algorithm enhances the robustness and efficiency of image recognition tasks. The advancements presented in this study not only contribute to the theoretical understanding but also offer practical solutions, underscoring the significant potential and applicability of siamese networks in advancing image recognition technologies.</p></abstract>
      <kwd-group>
        <kwd>Image recognition</kwd>
        <kwd>Deep learning</kwd>
        <kwd>Siamese networks</kwd>
        <kwd>Weight sharing</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="4"/>
        <fig-count count="12"/>
        <table-count count="7"/>
        <ref-count count="21"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>As an important branch in the field of computer vision, image recognition has always been a hot and difficult research topic. With the vigorous development of deep learning technology, especially the wide application of Convolutional Neural Networks (CNNs) in image recognition tasks, traditional image recognition methods are gradually being replaced by deep learning algorithms. However, in the face of complex and changeable image scenes and large-scale datasets, traditional deep learning models still face problems such as large computational costs and poor generalization performance [<xref ref-type="bibr" rid="ref_1">1</xref>]. As a special neural network structure, the siamese networks can effectively extract the features of the input data and calculate the similarity by sharing weights, showing unique advantages in image recognition tasks.</p><p>This research aims to study the image recognition algorithm based on siamese networks, and improve the recognition performance and computational efficiency of the algorithm by optimizing the network structure and improving the training strategy. At the same time, this study also explores the applicability of image recognition algorithms based on siamese networks in different application scenarios, aiming to provide new ideas and methods for the development of the image recognition field.</p><p>The remaining structure of this study is as follows: Section 2 is a literature review in the field related to this study; Section 3 is research methods and data preprocessing; Section 4 is model construction; Section 5 is result analysis; and Section 6 is the conclusion and future research prospect of this study.</p>
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p>At first, researchers in the field of image recognition mainly relied on image recognition methods based on statistical features. For example, Zhou et al. [<xref ref-type="bibr" rid="ref_2">2</xref>] introduced the extraction method of invariant features at the local scale of images, and proposed a hybrid multi-scale representation method using pyramids and scale space to improve the real-time performance of image recognition. Shi and Zhang [<xref ref-type="bibr" rid="ref_3">3</xref>] proposed a method to realize moving target detection using a single Synthetic Aperture Radar (SAR) image, and proved the effectiveness of the proposed method with experiments. Galić et al. [<xref ref-type="bibr" rid="ref_4">4</xref>] introduced the use of different machine learning algorithms for image recognition, including feature extraction, feature selection, classifier design, etc. In general, the image recognition method based on statistical features involves statistical analysis of image rules, the extraction of features that reflect the essence of images, and the establishment of recognition models based on decision theory. However, this method has limitations, such as ignoring the spatial structure relationship of images, and the number of features surges, leading to difficulty in extraction and classification, especially for images with obvious structural features. The statistical recognition effect is not good.</p><p>With the rise of deep learning technology, the performance of image recognition has greatly improved. Chen [<xref ref-type="bibr" rid="ref_5">5</xref>] verified the effectiveness of CNNs in image recognition tasks through experiments. A variety of model optimization strategies were discussed to further improve the performance of CNNs. Qin et al. [<xref ref-type="bibr" rid="ref_6">6</xref>] used the CNN architecture to design a flower image classification and recognition model based on deep learning, and verified the effectiveness of the designed flower image classification and recognition model through experiments. The key to the image recognition method based on deep learning is to automatically extract the image features, and make the model recognize and classify the image through training and optimization. However, despite the great success of deep learning-based image recognition methods in the field of image recognition, there are still some challenges [<xref ref-type="bibr" rid="ref_7">7</xref>]. For example, deep learning methods often rely on large amounts of labeled data for model training. However, in practice, the sample size of some classes can be very small, making it difficult for deep learning models to fully learn their features. Therefore, the small-shot learning problem has become an important challenge in the field of deep learning image recognition. In addition, the generalization performance of deep learning models is also a challenge. Due to complex structures with a large number of parameters, deep learning models tend to be prone to overfitting, i.e., they perform well on training data but poorly on test or new data [<xref ref-type="bibr" rid="ref_8">8</xref>].</p><p>In order to solve the above problems, researchers have begun to explore new network structures and learning methods. Among them, the siamese networks have been widely used and explored because of their unique network structure. For example, Valero-Mas et al. [<xref ref-type="bibr" rid="ref_9">9</xref>] outlined the application of siamese networks in image classification tasks with a small number of samples, emphasized the importance of ensemble and feature learning in improving classification performance, and compared the advantages of siamese networks with other methods. He [<xref ref-type="bibr" rid="ref_10">10</xref>] compared the traditional Visual Geometry Group Network (VGGNet) series model with the siamese VGGNet model. The siamese VGGNet model made full use of the correlation and difference between image pairs, confirming its effectiveness in enhancing accuracy and robustness. Ren et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] combined the siamese CNN with Region Proposal Network (RPN) to solve the problem of untimely network model updates and insufficient training datasets during online tracking. Siamese networks learn similarity measures between image pairs by sharing weights, which gives them a significant advantage when dealing with tasks such as small-shot learning and fine-grained image recognition. In addition, the siamese networks took full advantage of the correlation and difference between image pairs, improving the accuracy and robustness of recognition.</p><p>However, this study of image recognition based on siamese networks is still in the development stage, and there are still many problems and challenges to be solved and explored. First of all, due to their special network structure, the siamese networks require more computing resources and time for training and optimization. Secondly, the siamese networks have high requirements for the quality and preprocessing of the input image pairs; otherwise, their performance may be affected. In summary, as a new type of network structure and learning method, siamese networks have a wide range of application prospects in the field of image recognition. However, they still have some disadvantages and shortcomings, which need to be further explored and optimized. Future research could focus on how to improve the training efficiency, generalization ability, and performance of the siamese networks in practical applications.</p>
    </sec>
    <sec sec-type="">
      <title>3. Research methods and data preprocessing</title>
      
        <sec>
          
            <title>3.1. Siamese networks</title>
          
          <p><xref ref-type="fig" rid="fig_1">Figure 1</xref> shows the composition of the siamese neural networks. Siamese networks have a special neural network structure, which mainly consists of two or more identical subnet modules, which share the same weights and parameters. Each subnetwork receives an input sample and generates a representation vector. These vectors are then used to calculate the similarity between the input samples. The weight-sharing mechanism between subnets is mainly reflected in the backpropagation phase. When the weights of one subnet are updated during the training process, these updates are immediately reflected in the other subnet. This process of synchronous updates ensures that both subnetworks use the same set of weights when processing different input data.</p><p>The siamese networks' comparison module, which comes after the subnet, receives the eigenvectors from the subnet as input and compares their similarity to determine the relationship between the input samples. The comparison module employs several methods to calculate the similarity between eigenvectors. Common methods include Euclidean distance, cosine similarity, inner product, etc. In addition, more complex neural network layers or functions can be used to calculate the similarity or difference between feature vectors, such as multilayer perceptrons (MLPs), CNNs, etc.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>Structure of the siamese networks</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_7QN_u2fjQ6QrjdNu.png"/>
            </fig>
          
          <p>Finally, the output module of the siamese networks is responsible for translating the output of the comparison module into the final decision-making result. The output module can also be customized to meet the needs of the specific task. For example, in a target tracking task, the output module may output the target location and size. In a face recognition task, the output module may output the identity information of the face, and so on.</p><p>Structurally, conventional neural networks typically consist of only one network that processes a single input sample and outputs the corresponding predictions. The siamese networks consist of two or more subnets that share weights and parameters, which are suitable for tasks that require the similarity comparison of two input samples. Functionally, the main role of the siamese networks is to calculate the similarity between two input samples, while conventional neural networks focus more on the prediction and classification of a single input sample. In addition, because of their special structure, the siamese networks are also suitable for small- or single-shot learning. That is, they can maintain good performance even when the training data is limited.</p><p>Siamese networks have a wide range of applications in the field of computer vision, especially in tasks involving comparing the similarity of two input samples. The following is a brief introduction to some of their application scenarios in computer vision:</p><p>(a) Face recognition [<xref ref-type="bibr" rid="ref_12">12</xref>]: In the face recognition task, by inputting a pair of face images, the siamese networks can learn to extract the image features, and judge whether the two images are the same person's face by calculating the similarity between the features. This method is particularly effective for dealing with face images with different angles, lighting conditions, and variations in expression [<xref ref-type="bibr" rid="ref_13">13</xref>].</p><p>(b) Image retrieval: In the image retrieval task, given a query image, the system can find images similar to it in a large image database. By learning the feature representation of the image, the siamese networks can accurately calculate the similarity between the query image and the image in the database, thereby selecting the image with the highest similarity to return.</p><p>(c) Target tracking [<xref ref-type="bibr" rid="ref_14">14</xref>]: In the target tracking task, the siamese networks can identify and continuously track the targets in the video sequence. By comparing the targets in the current and previous frames, the networks can learn how the object appears and predict its position in subsequent frames. This approach is very effective for dealing with challenging problems such as complex backgrounds, occlusions, and target deformations.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Data preprocessing</title>
          
          <p>In this study, three sets of public datasets were used: MNIST, Fashion-MNIST and CIFAR10.</p><p>Among them, MNIST and Fashion-MNIST have the same data format and scale. Both datasets contain 60,000 training samples and 10,000 test samples. Each sample is rendered in a 28×28 pixel grid with a single grayscale channel. As shown in <xref ref-type="table" rid="table_1">Table 1</xref>, the MNIST is a classic handwritten digital image dataset, with ten categories from 0 to 9 corresponding to handwritten digital images from 0 to 9. However, ten categories from 0 to 9 in the Fashion-MNIST dataset correspond to different kinds of clothing. This is the main difference between those two datasets. Compared with MNIST, the image content of Fashion-MNIST is more diverse, complex, and challenging. In this experiment, 60,000 original training samples were divided into 48,000 training samples and 12,000 validation samples.</p><p>The CIFAR10 dataset contains ten types of objects, with label values ranging from 0 to 9. The CIFAR10 dataset consists of 50,000 training samples and 10,000 test samples, each of which is a 32×32-pixel RGB image. In this experiment, 50,000 original training samples were divided into 40,000 training samples and 10,000 validation samples. During preprocessing, the image was normalized to the range of [0,1] and a channel dimension was added.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Dataset categories and object correspondence</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1" colwidth="165"><p>Categories \ Datasets</p></td><td colspan="1" rowspan="1" colwidth="134"><p>MNIST</p></td><td colspan="1" rowspan="1" colwidth="143"><p>Fashion-MNIST</p></td><td colspan="1" rowspan="1" colwidth="145"><p>CIFAR10</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="165"><p>0</p></td><td colspan="1" rowspan="1" colwidth="134"><p>0</p></td><td colspan="1" rowspan="1" colwidth="143"><p>T-shirt/top</p></td><td colspan="1" rowspan="1" colwidth="145"><p>Plane</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="165"><p>1</p></td><td colspan="1" rowspan="1" colwidth="134"><p>1</p></td><td colspan="1" rowspan="1" colwidth="143"><p>Trouser</p></td><td colspan="1" rowspan="1" colwidth="145"><p>Car</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="165"><p>2</p></td><td colspan="1" rowspan="1" colwidth="134"><p>2</p></td><td colspan="1" rowspan="1" colwidth="143"><p>Pullover</p></td><td colspan="1" rowspan="1" colwidth="145"><p>Bird</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="165"><p>3</p></td><td colspan="1" rowspan="1" colwidth="134"><p>3</p></td><td colspan="1" rowspan="1" colwidth="143"><p>Dress</p></td><td colspan="1" rowspan="1" colwidth="145"><p>Cat</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="165"><p>4</p></td><td colspan="1" rowspan="1" colwidth="134"><p>4</p></td><td colspan="1" rowspan="1" colwidth="143"><p>Coat</p></td><td colspan="1" rowspan="1" colwidth="145"><p>Deer</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="165"><p>5</p></td><td colspan="1" rowspan="1" colwidth="134"><p>5</p></td><td colspan="1" rowspan="1" colwidth="143"><p>Sandals</p></td><td colspan="1" rowspan="1" colwidth="145"><p>Dog</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="165"><p>6</p></td><td colspan="1" rowspan="1" colwidth="134"><p>6</p></td><td colspan="1" rowspan="1" colwidth="143"><p>Shirt</p></td><td colspan="1" rowspan="1" colwidth="145"><p>Frog</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="165"><p>7</p></td><td colspan="1" rowspan="1" colwidth="134"><p>7</p></td><td colspan="1" rowspan="1" colwidth="143"><p>Sneaker</p></td><td colspan="1" rowspan="1" colwidth="145"><p>Horse</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="165"><p>8</p></td><td colspan="1" rowspan="1" colwidth="134"><p>8</p></td><td colspan="1" rowspan="1" colwidth="143"><p>Bag</p></td><td colspan="1" rowspan="1" colwidth="145"><p>Boat</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="165"><p>9</p></td><td colspan="1" rowspan="1" colwidth="134"><p>9</p></td><td colspan="1" rowspan="1" colwidth="143"><p>Ankle boots</p></td><td colspan="1" rowspan="1" colwidth="145"><p>Truck</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Visualization results of (a) MNIST; (b) Fashion-MNIST; and (c) CIFAR10 image pairs</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_0xNg1Ysv2H2yXlX3.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_yzILQmL53LnSqIm0.png"/>
            </fig>
          
          <p>The siamese networks built in this experiment have two shared-weight subnets, providing inputs for two network models. This leads to the concept of positive and negative pairs. Positive pairs refer to images in the same category, while negative ones refer to images in different categories. In order to visually verify whether the image pair generation process in this experiment works normally, the generated image pair was randomly selected and visualized once. Subgraph (a), Subgraph (b) and Subgraph (c) of <xref ref-type="fig" rid="fig_2">Figure 2</xref> show the visualization. It can be observed that for each pair of images, the same class is labeled as POS and the different classes are labeled as NEG.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Modelling</title>
      
        <sec>
          
            <title>4.1. Algorithmic framework</title>
          
          <p>As shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>, the two subnets are referred to as subnets A and B. Both subnets share the same structure and weights. Each subnet starts with multiple convolutional layers that extract features from the input image. Convolutional layers filter images through convolutional operations to capture local spatial features. The convolution operation is usually followed by an activation function, such as the ReLU function, to increase the nonlinearity of the network. The pooling layer is used to reduce the dimensionality of the feature map while retaining important feature information. After multiple convolutional and pooling layers, a flattened layer is typically used to flatten the multidimensional feature map into a one-dimensional vector to input into a fully connected layer. The fully connected layer is used to map feature vectors to the sample space, generating a feature representation of each sample. After the feature vectors extracted by the subnet pass through the comparison and output modules, the similarity value between the image pairs is finally output.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Framework of the siamese networks </title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_hZe7CEk2XFbGettI.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.2. Subnet module</title>
          
          <p>In this study, the classical CNN structure LeNet-5 was used as the subnet of the siamese networks. LeNet-5 is an earlier CNN structure proposed by Yann LeCun et al. in 1998 for processing smaller images. <xref ref-type="fig" rid="fig_4">Figure 4</xref> shows the basic structure of LeNet-5 as siamese network subnets.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Siamese networks based on the LeNet-5 subnet</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_xFDSPuZDnbg1hTWY.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.3. Comparison module</title>
          
          <p>The similarity metric function of the comparison module is used to accurately calculate the similarity between the eigenvector outputs of the two subnetworks. This module measures the similarity or distance between vectors through a specific algorithm or measurement method. Common similarity measures include Euclidean distance, cosine similarity, etc. In this study, Euclidean distance serves as a comparison module, and the formula for calculation is as follows:</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="mu56p93ffg">
                <mml:mi>d</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:msqrt>
                  <mml:munderover>
                    <mml:mo>∑</mml:mo>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                    <mml:mi>n</mml:mi>
                  </mml:munderover>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>x</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>y</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:msup>
                </mml:msqrt>
              </mml:math>
            </disp-formula>
          
          <p> where, <inline-formula>
  <mml:math id="mpi6v7beur">
    <mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is the $i<inline-formula>
  <mml:math id="m707kxet98">
    <mml:mo>−</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula>x<inline-formula>
  <mml:math id="miky9fcex8">
    <mml:mo>,</mml:mo>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
  </mml:math>
</inline-formula>i=1,2 \ldots n ; y_i<inline-formula>
  <mml:math id="m6l9kxu4g6">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>i<inline-formula>
  <mml:math id="mizgw2ym2q">
    <mml:mo>−</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula>y<inline-formula>
  <mml:math id="m0kc2gkozd">
    <mml:mo>,</mml:mo>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
  </mml:math>
</inline-formula>i=1,2 \ldots n<inline-formula>
  <mml:math id="m7rd93n0dw">
    <mml:mo>;</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>d(x, y)<inline-formula>
  <mml:math id="mwsohlr2i1">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>x<inline-formula>
  <mml:math id="m6sa4giuyc">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>y$.</p>
        </sec>
      
      
        <sec>
          
            <title>4.4. Output module</title>
          
          <p>In most scenarios, the output module tends to be a simple classifier, such as a logistic regression or softmax layer. When the siamese networks are used for image recognition or verification tasks, the output module outputs the probability that two input images belong to the same category. In this study, the sigmoid function was used as an output module to output the similarity values of the two images.</p>
        </sec>
      
      
        <sec>
          
            <title>4.5. Training module</title>
          
          <p>In the training module of the siamese networks, the loss function and optimizer work together to drive the learning and optimization processes of the network. The loss function is responsible for quantifying the difference between the model prediction and the actual label, providing a clear optimization goal for network training. According to the loss function gradient, the optimizer updates the network parameters through a certain algorithm, thereby gradually reducing the loss value and improving the prediction performance of the model. This process ensures that the model can gradually approach the optimal state during the training process, and improves the ability to judge the similarity between samples.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Adam update rules</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1" colwidth="97"></td><td colspan="1" rowspan="1" colwidth="536"><p>Adam Update Rules</p></td><td colspan="1" rowspan="1" colwidth="698"></td></tr><tr><td colspan="1" rowspan="1" colwidth="97"><p>(a)</p></td><td colspan="1" rowspan="1" colwidth="536"><p>Calculation of the gradient for the <italic>t</italic> time step</p></td><td colspan="1" rowspan="1" colwidth="698"><p><mml:math id="m6k1xu9158">
  <mml:msub>
    <mml:mrow>
      <mml:mi>g</mml:mi>
    </mml:mrow>
    <mml:mrow>
      <mml:mrow>
        <mml:mi>t</mml:mi>
      </mml:mrow>
    </mml:mrow>
  </mml:msub>
  <mml:msub>
    <mml:mi>∇</mml:mi>
    <mml:mi>θ</mml:mi>
  </mml:msub>
  <mml:mo>=</mml:mo>
  <mml:mi>J</mml:mi>
  <mml:mrow>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:msub>
      <mml:mi>θ</mml:mi>
      <mml:mrow>
        <mml:mi>t</mml:mi>
        <mml:mo>−</mml:mo>
        <mml:mn>1</mml:mn>
      </mml:mrow>
    </mml:msub>
  </mml:mrow>
</mml:math></p></td></tr><tr><td colspan="1" rowspan="1" colwidth="97"><p>(b)</p></td><td colspan="1" rowspan="1" colwidth="536"><p>Calculation of the exponential moving average of the gradient</p></td><td colspan="1" rowspan="1" colwidth="698"><p><mml:math id="mu3oeedtso">
  <mml:msub>
    <mml:mrow>
      <mml:mi>m</mml:mi>
    </mml:mrow>
    <mml:mrow>
      <mml:mrow>
        <mml:mi>t</mml:mi>
      </mml:mrow>
    </mml:mrow>
  </mml:msub>
  <mml:msub>
    <mml:mi>β</mml:mi>
    <mml:mn>1</mml:mn>
  </mml:msub>
  <mml:msub>
    <mml:mi>m</mml:mi>
    <mml:mrow>
      <mml:mi>t</mml:mi>
      <mml:mo>−</mml:mo>
      <mml:mn>1</mml:mn>
    </mml:mrow>
  </mml:msub>
  <mml:msub>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
  </mml:msub>
  <mml:mo>=</mml:mo>
  <mml:mo>+</mml:mo>
  <mml:mrow>
    <mml:mo>(</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mn>1</mml:mn>
    <mml:msub>
      <mml:mi>β</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
  </mml:mrow>
</mml:math></p></td></tr><tr><td colspan="1" rowspan="1" colwidth="97"><p>(c)</p></td><td colspan="1" rowspan="1" colwidth="536"><p>Calculation of the exponential moving average of the gradient squared</p></td><td colspan="1" rowspan="1" colwidth="698"><p><mml:math id="mhom59ghi1">
  <mml:msub>
    <mml:mi>v</mml:mi>
    <mml:mrow>
      <mml:mrow>
        <mml:mi>t</mml:mi>
      </mml:mrow>
    </mml:mrow>
  </mml:msub>
  <mml:msub>
    <mml:mi>β</mml:mi>
    <mml:mn>2</mml:mn>
  </mml:msub>
  <mml:msub>
    <mml:mi>v</mml:mi>
    <mml:mrow>
      <mml:mi>t</mml:mi>
      <mml:mo>−</mml:mo>
      <mml:mn>1</mml:mn>
    </mml:mrow>
  </mml:msub>
  <mml:mo>=</mml:mo>
  <mml:mo>+</mml:mo>
  <mml:mrow>
    <mml:mo>(</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mn>1</mml:mn>
    <mml:msub>
      <mml:mi>β</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
  </mml:mrow>
  <mml:msubsup>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mn>2</mml:mn>
  </mml:msubsup>
</mml:math></p></td></tr><tr><td colspan="1" rowspan="1" colwidth="97"><p>(d)</p></td><td colspan="1" rowspan="1" colwidth="536"><p>A deviation correction is made for <mml:math id="m7nptjou8g">
  <mml:msub>
    <mml:mrow>
      <mml:mi>m</mml:mi>
    </mml:mrow>
    <mml:mrow>
      <mml:mrow>
        <mml:mi>t</mml:mi>
      </mml:mrow>
    </mml:mrow>
  </mml:msub>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="698"><p><mml:math id="mx32g567fi">
  <mml:msub>
    <mml:mrow>
      <mml:mover>
        <mml:mrow>
          <mml:mi>m</mml:mi>
        </mml:mrow>
        <mml:mo>^</mml:mo>
      </mml:mover>
    </mml:mrow>
    <mml:mrow>
      <mml:mrow>
        <mml:mi>t</mml:mi>
      </mml:mrow>
    </mml:mrow>
  </mml:msub>
  <mml:msub>
    <mml:mi>m</mml:mi>
    <mml:mi>t</mml:mi>
  </mml:msub>
  <mml:mo>=</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>(</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mn>1</mml:mn>
    <mml:msubsup>
      <mml:mi>β</mml:mi>
      <mml:mi>t</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msubsup>
  </mml:mrow>
</mml:math></p></td></tr><tr><td colspan="1" rowspan="1" colwidth="97"><p>(e)</p></td><td colspan="1" rowspan="1" colwidth="536"><p>A deviation correction is made for <mml:math id="m25sg2lz7c">
  <mml:msub>
    <mml:mi>v</mml:mi>
    <mml:mrow>
      <mml:mrow>
        <mml:mi>t</mml:mi>
      </mml:mrow>
    </mml:mrow>
  </mml:msub>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="698"><p><mml:math id="mupflowvry">
  <mml:msub>
    <mml:mrow>
      <mml:mover>
        <mml:mi>v</mml:mi>
        <mml:mo>^</mml:mo>
      </mml:mover>
    </mml:mrow>
    <mml:mrow>
      <mml:mrow>
        <mml:mi>t</mml:mi>
      </mml:mrow>
    </mml:mrow>
  </mml:msub>
  <mml:msub>
    <mml:mi>v</mml:mi>
    <mml:mi>t</mml:mi>
  </mml:msub>
  <mml:mo>=</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>(</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mn>1</mml:mn>
    <mml:msubsup>
      <mml:mi>β</mml:mi>
      <mml:mi>t</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msubsup>
  </mml:mrow>
</mml:math></p></td></tr><tr><td colspan="1" rowspan="1" colwidth="97"><p>(f)</p></td><td colspan="1" rowspan="1" colwidth="536"><p>Update of the parameters</p></td><td colspan="1" rowspan="1" colwidth="698"><p><mml:math id="m0h7g4akp1">
  <mml:msub>
    <mml:mi>θ</mml:mi>
    <mml:mrow>
      <mml:mrow>
        <mml:mi>t</mml:mi>
      </mml:mrow>
    </mml:mrow>
  </mml:msub>
  <mml:msub>
    <mml:mi>θ</mml:mi>
    <mml:mrow>
      <mml:mi>t</mml:mi>
      <mml:mo>−</mml:mo>
      <mml:mn>1</mml:mn>
    </mml:mrow>
  </mml:msub>
  <mml:msub>
    <mml:mrow>
      <mml:mover>
        <mml:mrow>
          <mml:mi>m</mml:mi>
        </mml:mrow>
        <mml:mo>^</mml:mo>
      </mml:mover>
    </mml:mrow>
    <mml:mrow>
      <mml:mrow>
        <mml:mi>t</mml:mi>
      </mml:mrow>
    </mml:mrow>
  </mml:msub>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>∗</mml:mo>
  <mml:mo>(</mml:mo>
  <mml:mo>+</mml:mo>
  <mml:mo>)</mml:mo>
  <mml:mi>α</mml:mi>
  <mml:mi>ε</mml:mi>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:msqrt>
    <mml:mrow>
      <mml:mover>
        <mml:mi>v</mml:mi>
        <mml:mo>^</mml:mo>
      </mml:mover>
    </mml:mrow>
  </mml:msqrt>
</mml:math></p></td></tr></tbody></table>
            </table-wrap>
          
          <p>(a) Loss function</p><p>The design of the loss function is usually related to the specific task. In the classification task of the siamese networks, binary cross-entropy or contrastive loss functions are usually used. In this study, a contrastive loss function was used to encourage the networks to produce similar and distant feature representations for similar and dissimilar samples, respectively. The formula for calculation is as follows:</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="marnd5kaoa">
                <mml:mi>L</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>s</mml:mi>
                <mml:mi>s</mml:mi>
                <mml:mi>m</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>r</mml:mi>
                <mml:mi>g</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mi>d</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mo>max</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mfrac>
                  <mml:mn>1</mml:mn>
                  <mml:mi>N</mml:mi>
                </mml:mfrac>
                <mml:munderover>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>N</mml:mi>
                </mml:munderover>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>−</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mn>1</mml:mn>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                </mml:mrow>
                <mml:msup>
                  <mml:mi>d</mml:mi>
                  <mml:mn>2</mml:mn>
                </mml:msup>
                <mml:msup>
                  <mml:mo>)</mml:mo>
                  <mml:mn>2</mml:mn>
                </mml:msup>
                <mml:msub>
                  <mml:mi>y</mml:mi>
                  <mml:mi>i</mml:mi>
                </mml:msub>
                <mml:mn>0</mml:mn>
              </mml:math>
            </disp-formula>
          
          <p>where, $N<inline-formula>
  <mml:math id="mw8xchpyzt">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>y_i<inline-formula>
  <mml:math id="mqp5zzah8t">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>i<inline-formula>
  <mml:math id="my774m06q8">
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>;</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mn>0</mml:mn>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula>d<inline-formula>
  <mml:math id="m5li2f8e33">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>;</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula>d<inline-formula>
  <mml:math id="me6aw9ez7i">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>W</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mn>1</mml:mn>
    <mml:mn>0.</mml:mn>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>y_i=1<inline-formula>
  <mml:math id="mxavudppj2">
    <mml:mo>(</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>\sum y_i \cdot \max (margin-d, 0)^2<inline-formula>
  <mml:math id="m21ke1etz9">
    <mml:mo>.</mml:mo>
    <mml:mi>W</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>d<inline-formula>
  <mml:math id="ma0pxejjoy">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>W</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>y_i=0<inline-formula>
  <mml:math id="m1ltbreph8">
    <mml:mo>(</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>\sum\left(1-y_i\right) \cdot d^2<inline-formula>
  <mml:math id="mo1w12qpbo">
    <mml:mo>.</mml:mo>
    <mml:mi>W</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>d<inline-formula>
  <mml:math id="mmzvexeno2">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>O</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>P</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>[</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mn>15</mml:mn>
    <mml:mn>2</mml:mn>
  </mml:math>
</inline-formula>\mathrm{g}_{\mathrm{t}}<inline-formula>
  <mml:math id="m1docewv3b">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>t ; \beta_1<inline-formula>
  <mml:math id="mqgsyk47bq">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\beta_2<inline-formula>
  <mml:math id="mre4rcmcdv">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\beta_1=0.9<inline-formula>
  <mml:math id="mmrctg7nmc">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\beta_2=0.999 ; \mathrm{m}_{\mathrm{t}}<inline-formula>
  <mml:math id="mvynqngtof">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\nu_{\mathrm{t}}<inline-formula>
  <mml:math id="m9nqqid57x">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\mathrm{m}_0=0<inline-formula>
  <mml:math id="m8vxze25n9">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>v_0=0<inline-formula>
  <mml:math id="m8x5lw9q5h">
    <mml:mo>,</mml:mo>
    <mml:mo>;</mml:mo>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>\hat{m}_{\mathrm{t}}<inline-formula>
  <mml:math id="m27yghk6rc">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\hat{v}_{\mathrm{t}}<inline-formula>
  <mml:math id="mzbu98smlv">
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
  </mml:math>
</inline-formula>\mathrm{m}_{\mathrm{t}}<inline-formula>
  <mml:math id="mfd3bsh48d">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>v_{\mathrm{t}} ; \theta_{\mathrm{t}}<inline-formula>
  <mml:math id="mci2g8ldgq">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>t<inline-formula>
  <mml:math id="mvp5l8409y">
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>\theta_{\mathrm{t}+1}<inline-formula>
  <mml:math id="mdlxtq7zij">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
  </mml:math>
</inline-formula>t+1 ; \alpha<inline-formula>
  <mml:math id="m93kbk79eu">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\varepsilon<inline-formula>
  <mml:math id="mvzs7cijtn">
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\alpha=0.001<inline-formula>
  <mml:math id="mlf7cjceib">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\varepsilon=10^{-8}$ to avoid the divisor becoming 0 .</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>5. Result analysis</title>
      <p>In this study, the experimental environment was configured on a system running the Windows operating system, equipped with an Intel Core i7 processor. The development environment utilized was PyCharm, operating under Python version 3.10. This setup was further enhanced by integrating the TensorFlow deep learning framework, which, in conjunction with CUDA and cuDNN, facilitated accelerated computational performance. These technological integrations were pivotal in establishing an efficient and stable platform for conducting image recognition experiments.</p>
      
        <sec>
          
            <title>5.1. Results of the comparative experiment</title>
          
          <p><xref ref-type="fig" rid="fig_5">Figure 5</xref> shows the results of 50 training sessions on the siamese networks based on LeNet-5 (SN-LeNet-5) of the MNIST dataset. After 50 training sessions, the accuracy of the training, validation and test sets reached 99.95%, 98.83% and 99.11%, respectively.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Iterative plot of MNIST on SN-LeNet-5</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_NdPsdEOZ4_8QCuT5.png"/>
            </fig>
          
          <p><xref ref-type="table" rid="table_3">Table 3</xref> shows the test performance comparison of the recognition methods based on the K-proximity method, Histogram of Oriented Gradients (HOG) and neural networks on the MNIST handwriting dataset in recent years. The applied SN-LeNet-5 exhibits superior recognition performance compared to other methods, surpassing the Lenet-5 model by 0.06%.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Comparison of the test accuracy</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Method</p></th><th colspan="1" rowspan="1"><p>Accuracy (%)</p></th></tr><tr><td colspan="1" rowspan="1"><p>Lenet-5 [<xref ref-type="bibr" rid="ref_16">16</xref>]</p></td><td colspan="1" rowspan="1"><p>99.05</p></td></tr><tr><td colspan="1" rowspan="1"><p>HOG_PCA [<xref ref-type="bibr" rid="ref_17">17</xref>]</p></td><td colspan="1" rowspan="1"><p>98.39</p></td></tr><tr><td colspan="1" rowspan="1"><p>HOG_SVM [<xref ref-type="bibr" rid="ref_18">18</xref>]</p></td><td colspan="1" rowspan="1"><p>97.25</p></td></tr><tr><td colspan="1" rowspan="1"><p>SNN_STPD [<xref ref-type="bibr" rid="ref_19">19</xref>]</p></td><td colspan="1" rowspan="1"><p>98.4</p></td></tr><tr><td colspan="1" rowspan="1"><p>CNN [<xref ref-type="bibr" rid="ref_20">20</xref>]</p></td><td colspan="1" rowspan="1"><p>98.99</p></td></tr><tr><td colspan="1" rowspan="1"><p>CNN_SVM [<xref ref-type="bibr" rid="ref_21">21</xref>]</p></td><td colspan="1" rowspan="1"><p>99.10</p></td></tr><tr><td colspan="1" rowspan="1"><p>SN-LeNet-5</p></td><td colspan="1" rowspan="1"><p>99.11</p></td></tr></tbody></table>
            </table-wrap>
          
          <p> <xref ref-type="table" rid="table_4">Table 4</xref> shows the time required for the identification of MNIST datasets by the SN-LeNet-5 and the traditional LeNet-5 networks. The time used by the two methods in the table was accumulated after performing a test of 10,000 images. The SN-LeNet-5 demonstrates a faster recognition speed.</p>
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Comparison of time performance</title>
              </caption>
              <table><tr><th >Method</th><th >Test Time (s)</th></tr><tr><td >Lenet-5</td><td >9.37</td></tr><tr><td >SN-LeNet-5</td><td >4.04</td></tr></table>
            </table-wrap>
          
          <p> <xref ref-type="fig" rid="fig_6">Figure 6</xref> shows the output results of the SN-LeNet-5 model on the handwritten digital image test set. In the figure, the label directly above each pair of images indicates their similarity values. The closer the similarity value is to 1, the greater the similarity between the pair of images, which may belong to the same category. The closer the similarity value is to 0, the greater the difference between the images, which may belong to different categories. It can be seen from the visualization results that the similarity prediction values of the siamese networks for image pairs in different types and in the same class are very close to 0 and 1, respectively, indicating that the model performs well on the test set.</p>
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Visualization of the test results of the MNIST dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_-wG6HCcISCqWmlE1.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>5.2. Analysis of model generalization</title>
          
          <p><xref ref-type="fig" rid="fig_7">Figure 7</xref> and <xref ref-type="fig" rid="fig_8">Figure 8</xref> show the iterative plots of the siamese network model based on LeNet-5 on the Fashion-MNIST and CIFAR10 datasets, respectively. The accuracy of the Fashion-MNIST on the test set reached 91.65% after 100 training times on the siamese networks. The CIFAR10 dataset did not reach complete convergence after 1,000 times of training on the networks. Therefore, it was trained 500 times on the basis of 1,000 times. The accuracy of the final model on the test set reached 81.64%.</p>
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>Iterative plot of Fashion-MNIST on SN-LeNet-5</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_t1ekDn7LUOZ2ao_j.png"/>
            </fig>
          
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>Iterative plots of CIFAR10 on SN-LeNet-5</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_pJnIHrPnj-CKnDzd.png"/>
            </fig>
          
          <p> <xref ref-type="table" rid="table_5">Table 5</xref> shows the results and running times of the Fashion-MNIST and CIFAR10 datasets on the siamese network model based on LeNet-5. It can be seen from the results that the siamese network model based on LeNet-5 constructed in this experiment has good generalization ability on these two datasets. This means that the model may also have some power to process new or unseen data.</p>
          
            <table-wrap id="table_5">
              <label>Table 5</label>
              <caption>
                <title>Algorithm performance of the siamese networks on Fashion-MNIST and CIFAR10</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Datasets</p></th><th colspan="1" rowspan="1"><p>Training Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Validation Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Test Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Training Time (s/epoch)</p></th><th colspan="1" rowspan="1"><p>Test Time (s)</p></th></tr><tr><td colspan="1" rowspan="1"><p>Fashion-MNIST</p></td><td colspan="1" rowspan="1"><p>98.56</p></td><td colspan="1" rowspan="1"><p>91.66</p></td><td colspan="1" rowspan="1"><p>91.65</p></td><td colspan="1" rowspan="1"><p>18.73</p></td><td colspan="1" rowspan="1"><p>3.73</p></td></tr><tr><td colspan="1" rowspan="1"><p>CIFAR10</p></td><td colspan="1" rowspan="1"><p>87.46</p></td><td colspan="1" rowspan="1"><p>82.53</p></td><td colspan="1" rowspan="1"><p>81.64</p></td><td colspan="1" rowspan="1"><p>27.32</p></td><td colspan="1" rowspan="1"><p>4.35</p></td></tr></tbody></table>
            </table-wrap>
          
          <p> <xref ref-type="fig" rid="fig_9">Figure 9</xref> and <xref ref-type="fig" rid="fig_10">Figure 10</xref> show the output results of the siamese network model based on LeNet-5 for the Fashion-MNIST and CIFAR10 test sets, respectively. According to the visualization results, based on the similarity value of the output image pairs, a high accuracy can be obtained to determine whether the images belong to the same class, showing that the constructed model has good generalization ability on these two datasets.</p>
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>
                <title>Visualization of the test results of the Fashion-MNIST dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_lkFC3ke7NWzbFQMN.png"/>
            </fig>
          
          
            <fig id="fig_10">
              <label>Figure 10</label>
              <caption>
                <title>Visualization of the test results of the CIFAR10 dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_U9QohALzNU3L3MLW.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>5.3. Analysis of parameter sensitivity</title>
          
          <p>Parameter sensitivity analysis is a key process to evaluate the sensitivity of model performance to different parameter settings. This chapter delves into the key parameters in the siamese networks and analyzes their impact on model performance, aiming to provide specific guidance for model optimization.</p>
          
            <sec>
              
                <title>5.3.1 Analysis of learning rates</title>
              
              <p>In this experiment, a siamese network model based on the LeNet-5 architecture was used to train the MNIST dataset. In order to optimize the training process of the model, different learning rate settings were tried, namely, 0.0001, 0.001, and 0.01. During the training, a batch size of 64 was set, meaning that 64 samples were processed each time the weights were updated. The number of iterations was set to 50 rounds to ensure the full convergence of the model. Through this series of settings, it is expected to find the most suitable learning rate to achieve the best model performance.</p>
              
                <table-wrap id="table_6">
                  <label>Table 6</label>
                  <caption>
                    <title>Training accuracy under different learning rates</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="1" colwidth="208"><p>Learning rates</p></td><td colspan="1" rowspan="1" colwidth="114"><p>0.0001</p></td><td colspan="1" rowspan="1" colwidth="150"><p>0.001</p></td><td colspan="1" rowspan="1" colwidth="165"><p>0.01</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="208"><p>Training accuracy (%)</p></td><td colspan="1" rowspan="1" colwidth="114"><p>99.75</p></td><td colspan="1" rowspan="1" colwidth="150"><p>99.95</p></td><td colspan="1" rowspan="1" colwidth="165"><p>98.10</p></td></tr></tbody></table>
                </table-wrap>
              
              <p>Subgraphs (a) and (b) of <xref ref-type="fig" rid="fig_11">Figure 11</xref> show the iteration plots with different learning rates. <xref ref-type="table" rid="table_6">Table 6</xref> shows the accuracy of the training set at each learning rate.</p><p>At a learning rate of 0.0001, the convergence speed of the model appears to be relatively slow. However, after a long period of training, it finally achieves a high accuracy rate of 99.7%. Although this result is already quite good, it may be necessary to find a better balance given the time and efficiency of training.</p><p>When the learning rate increases to 0.01, the convergence speed of the model significantly accelerates, and the training process becomes faster. However, this setting causes the model to oscillate significantly during the training process, leading to instability when updating the weights. Finally, although the model can converge, its accuracy is relatively low at 98.10%, indicating that it may be more likely to fall into the local optimal solution under the high learning rate, which affects its generalization ability.</p><p>After experimenting with various learning rates, it was found that when the learning rate was set to 0.001, the model reached a good balance between convergence speed and accuracy, as shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>. This setting not only keeps the model at a fast convergence speed, but also does not show obvious oscillation during training. In the end, the model achieved the highest accuracy rate of 99.95%, proving that it can achieve excellent performance and accuracy with an appropriate learning rate.</p>
              
                <fig id="fig_11">
                  <label>Figure 11</label>
                  <caption>
                    <title>Iterative plots with learning rates of (a) 0.0001; and (b) 0.01</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_ER_ER0HMcQQCjUNE.png"/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_mtgK_o5m70o6ihuP.png"/>
                </fig>
              
              
                <table-wrap id="table_7">
                  <label>Table 7</label>
                  <caption>
                    <title>The final training accuracy and time under different batch sizes</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="1" colwidth="188"><p>Batch size</p></td><td colspan="1" rowspan="1" colwidth="134"><p>32</p></td><td colspan="1" rowspan="1" colwidth="125"><p>64</p></td><td colspan="1" rowspan="1" colwidth="123"><p>96</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="188"><p>Training accuracy (%)</p></td><td colspan="1" rowspan="1" colwidth="134"><p>99.93</p></td><td colspan="1" rowspan="1" colwidth="125"><p>99.95</p></td><td colspan="1" rowspan="1" colwidth="123"><p>99.83</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="188"><p>Training time (s/epoch)</p></td><td colspan="1" rowspan="1" colwidth="134"><p>34.76</p></td><td colspan="1" rowspan="1" colwidth="125"><p>17.36</p></td><td colspan="1" rowspan="1" colwidth="123"><p>16.80</p></td></tr></tbody></table>
                </table-wrap>
              
            </sec>
          
          
            <sec>
              
                <title>5.3.2 Analysis of batch size</title>
              
              <p>In order to analyze the influence of batch size on the training process and model performance during the experiment, three different batch sizes of 32, 64 and 96 were selected and trained under the same experimental conditions. <xref ref-type="fig" rid="fig_12">Figure 12</xref> and <xref ref-type="table" rid="table_7">Table 7</xref> show the experimental results.</p><p>According to the experimental results, as the batch size increases, the training time per epoch gradually decreases from 34.76 seconds to 16.80 seconds. However, although the training speed further improves when the batch size is 96, the final accuracy of the model decreases slightly to 99.83%. When the batch size is 64, not only is the training speed faster, but the accuracy of the model reaches the highest (99.95%). Furthermore, a comparative analysis of <xref ref-type="fig" rid="fig_12">Figure 12</xref> and <xref ref-type="fig" rid="fig_5">Figure 5</xref> reveal that the loss values for batch sizes of 32 and 96 converge more slowly compared to a batch size of 64, and fail to reach the minimum loss value. These results demonstrate that the experimental setup can achieve high model accuracy and fast loss convergence while maintaining training speed when the batch size is 64.</p>
              
                <fig id="fig_12">
                  <label>Figure 12</label>
                  <caption>
                    <title>Iterative plots with batch sizes of (a) 32; and (b) 96</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_sos8RpWadLrqq4tn.png"/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_tjzwTM2oUGYyVgqx.png"/>
                </fig>
              
            </sec>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>6. Conclusions and prospects</title>
      <p>This study focuses on the image recognition of the siamese networks. After introducing the importance and challenges of image recognition, the relevant research progress was reviewed. Then, the structure and weight-sharing mechanism of the siamese neural networks were introduced in detail, and the data preprocessing method was described. On this basis, a siamese network model based on LeNet-5 was constructed, including a subnet, a comparison module, an output module and a training module. Finally, the result analysis shows that the constructed siamese network model realizes image recognition on the MNIST, Fashion-MNIST and CIFAR-10 datasets. Comparative experiments show that the siamese networks have higher accuracy and faster recognition speed than other traditional algorithms. In addition, the siamese networks have good recognition performance on different datasets, which fully proves their strong generalization ability. Finally, the parameter sensitivity analysis was carried out, and the influence of learning rate and batch size on model performance was discussed, providing an important basis for model optimization.</p><p>Although some progress has been made in the research on image recognition based on siamese networks, this study has some limitations. Future research could focus on semi-supervised and unsupervised learning to overcome the dependence on large amounts of annotated data, enabling the siamese network to better adapt to different data scenarios. Real-time performance and efficiency optimization could also become important areas of research. This would make it easier for siamese networks to handle large amounts of data by making the algorithms better and the hardware faster. Furthermore, the research on multimodal fusion could open up new development opportunities for siamese networks. It is expected that the recognition performance and generalization ability of the siamese networks could be further improved by fusing information from different modalities.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>1395</volume>
          <page-range>012008</page-range>
          <issue>1</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Luo</surname>
              <given-names>J. D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1088/1742-6596/1395/1/012008</pub-id>
          <article-title>Research on image recognition method based on deep learning</article-title>
          <source>J. Phys. Conf. Ser.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>34</volume>
          <page-range>742-745</page-range>
          <issue>5</issue>
          <year>2008</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhou</surname>
              <given-names>Z. Q.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lyu</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3321/j.issn:1002-1582.2008.05.017</pub-id>
          <article-title>Fast target recognition based on local scale invariant features</article-title>
          <source>Opt. Tech.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>43</volume>
          <page-range>431-439</page-range>
          <issue>3</issue>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shi</surname>
              <given-names>Hong Yin</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Nuo</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3969/j.issn.0372-2112.2015.03.003</pub-id>
          <article-title>Moving target detection method for single SAR image based on sparse representation and road assistance</article-title>
          <source>Acta Electron. Sin.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>31</volume>
          <page-range>316-323</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Galić</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Stojanović</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Čajić</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.17559/TV-20230621000751</pub-id>
          <article-title>Application of neural networks and machine learning in image recognition</article-title>
          <source>Tehn. Vjesn.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <year>2013</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>X. C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.7666/d.Y2531769</pub-id>
          <article-title>Research on deep learning algorithms and applications based on convolutional neural networks</article-title>
          <source>Zhejiang Gongshang University</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Qin</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Xi</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>A new improved convolutional neural network flower image recognition model</article-title>
          <source>2019 IEEE Symposium Series on Computational Intelligence (SSCI)</source>
          <publisher-name>Xiamen, China</publisher-name>
          <year>2019</year>
          <page-range>3110-3117</page-range>
          <pub-id pub-id-type="doi">10.1109/SSCI44817.2019.9003016</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>44</volume>
          <page-range>4223-4232</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jing</surname>
              <given-names>Wei</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3233/JIFS-220109</pub-id>
          <article-title>Classification and identification of garment images based on deep learning</article-title>
          <source>J. Intell. Fuzzy Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>1574</volume>
          <page-range>012161</page-range>
          <issue>1</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hu</surname>
              <given-names>H. Q.</given-names>
            </name>
            <name>
              <surname>Lyu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Yin</surname>
              <given-names>X. L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1088/1742-6596/1574/1/012161</pub-id>
          <article-title>Research and prospect of image recognition based on convolutional neural network</article-title>
          <source>J. Phys. Conf. Ser.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>83</volume>
          <page-range>19929-19952</page-range>
          <issue>7</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Valero-Mas</surname>
              <given-names>Jose J.</given-names>
            </name>
            <name>
              <surname>Gallego</surname>
              <given-names>Antonio Javier</given-names>
            </name>
            <name>
              <surname>Rico-Juan</surname>
              <given-names>Juan Ramón</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-023-15607-3</pub-id>
          <article-title>An overview of ensemble and feature learning in few-shot image classification using siamese networks</article-title>
          <source>Multimed. Tools Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>47</volume>
          <page-range>24-27</page-range>
          <issue>07</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>He</surname>
              <given-names>Y. T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.16280/j.videoe.2023.07.005</pub-id>
          <article-title>Lung image recognition based on siamese network</article-title>
          <source>Video Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>40</volume>
          <page-range>2686-2690</page-range>
          <issue>12</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ren</surname>
              <given-names>Jia Min</given-names>
            </name>
            <name>
              <surname>Gong</surname>
              <given-names>Ning Sheng</given-names>
            </name>
            <name>
              <surname>Han</surname>
              <given-names>Zhen Yang</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3969/j.issn.1000-1220.2019.12.038</pub-id>
          <article-title>An improved target tracking algorithm based on siamese convolutional neural network</article-title>
          <source>J. Chin. Comput. Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>135</volume>
          <page-range>109148</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pei</surname>
              <given-names>M. T.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Hao</surname>
              <given-names>H. L.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.patcog.2022.109148</pub-id>
          <article-title>Person-specific face spoofing detection based on a siamese network</article-title>
          <source>Pattern Recognition</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>27</volume>
          <page-range>100800</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kumar</surname>
              <given-names>C. Ranjeeth</given-names>
            </name>
            <name>
              <surname>Saranya</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Priyadharshini</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Derrick</surname>
              <given-names>Gilchrist E.</given-names>
            </name>
            <name>
              <surname>Kaleel</surname>
              <given-names>Rahman M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.measen.2023.100800</pub-id>
          <article-title>Face recognition using CNN and siamese network</article-title>
          <source>Measurement: Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>56</volume>
          <page-range>10-18</page-range>
          <issue>6</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>Y. F.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3778/j.issn.1002-8331.1911-0127</pub-id>
          <article-title>Survey of target tracking algorithms based on siamese network structure</article-title>
          <source>Comput. Eng. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>34</volume>
          <page-range>2051003</page-range>
          <issue>02</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Postalcıoğlu</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1142/s0218001420510039</pub-id>
          <article-title>Performance analysis of different optimizers for deep learning-based image recognition</article-title>
          <source>International Journal of Pattern Recognition and Artificial Intelligence</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>86</volume>
          <page-range>2278-2324</page-range>
          <issue>11</issue>
          <year>1998</year>
          <person-group person-group-type="author">
            <name>
              <surname>LeCun</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Bottou</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Bengio</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Haffner</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/5.726791</pub-id>
          <article-title>Gradient-based learning applied to document recognition</article-title>
          <source>Proc. IEEE</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Lu</surname>
              <given-names>W. S.</given-names>
            </name>
          </person-group>
          <article-title>Handwritten digits recognition using PCA of histogram of oriented gradient</article-title>
          <source>2017 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM)</source>
          <publisher-name>Victoria, BC, Canada</publisher-name>
          <year>2017</year>
          <page-range>1-5</page-range>
          <pub-id pub-id-type="doi">10.1109/PACRIM.2017.8121906</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>104</volume>
          <page-range>10-13</page-range>
          <issue>9</issue>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ebrahimzadeh</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Jampour</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Efficient handwritten digit recognition based on histogram of oriented gradients and SVM</article-title>
          <source>International Journal of Computer Applications</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>99</volume>
          <page-range>56-67</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kheradpisheh</surname>
              <given-names>S. R.</given-names>
            </name>
            <name>
              <surname>Ganjtabesh</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Thorpe</surname>
              <given-names>S. J.</given-names>
            </name>
            <name>
              <surname>Masquelier</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neunet.2017.12.005</pub-id>
          <article-title>STDP-based spiking deep convolutional neural networks for object recognition</article-title>
          <source>Neural Networks</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="thesis">
          <person-group person-group-type="author">
            <name>
              <surname>Nguyen</surname>
              <given-names>V. T.</given-names>
            </name>
          </person-group>
          <article-title>Research on neural network activation functions for handwritten character and image recognition</article-title>
          <year>2020</year>
          <publisher-name>Xidian University</publisher-name>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Maitra</surname>
              <given-names>D. S.</given-names>
            </name>
            <name>
              <surname>Bhattacharya</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Parui</surname>
              <given-names>S. K.</given-names>
            </name>
          </person-group>
          <article-title>CNN based common approach to handwritten character recognition of multiple scripts</article-title>
          <source>2015 13th International Conference on Document Analysis and Recognition (ICDAR)</source>
          <publisher-name>Tunis, Tunisia</publisher-name>
          <year>2015</year>
          <page-range>1021-1025</page-range>
          <pub-id pub-id-type="doi">10.1109/ICDAR.2015.7333916</pub-id>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>