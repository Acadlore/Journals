<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-P0izbZG8zD_1aqnrFs9GOrPW5XZQd5nU</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml010108</article-id>
      <title-group>
        <article-title>Liver Lesion Segmentation Using Deep Learning Models</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Rehman</surname>
            <given-names>Aasia</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2065-5699</contrib-id>
          <email>aasiarehman.scholar@kashmiruniversity.net</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Butt</surname>
            <given-names>Muheet Ahmed</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8059-0180</contrib-id>
          <email/>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="2">2</xref>
          <name>
            <surname>Zaman</surname>
            <given-names>Majid</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1070-8195</contrib-id>
          <email>zamanmajid@gmail.com</email>
        </contrib>
        <aff id="1">Department of Computer Science, University of Kashmir, 190006 Srinagar, India</aff>
        <aff id="2">Directorate of IT &amp; SS, University of Kashmir, 190006 J&amp;K, India</aff>
      </contrib-group>
      <year>2022</year>
      <volume>1</volume>
      <issue>1</issue>
      <fpage>61</fpage>
      <lpage>67</lpage>
      <page-range>61-67</page-range>
      <history>
        <date date-type="received">
          <month>07</month>
          <day>28</day>
          <year>2022</year>
        </date>
        <date date-type="accepted">
          <month>09</month>
          <day>19</day>
          <year>2022</year>
        </date>
        <date date-type="pub">
          <month>11</month>
          <day>19</day>
          <year>2022</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2022 by the authors</copyright-statement>
        <copyright-year>2022</copyright-year>
        <license>. Licensee Acadlore Publishing Services Limited, Hong Kong. This article can be downloaded for free, and reused and quoted with a citation of the original published version, under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 license</ext-link>.</license>
      </permissions>
      <abstract><p>An estimated 9.6 million deaths, or one in every six deaths, were attributed to cancer in 2018, making it the second highest cause of death worldwide. Men are more likely to develop lung, prostate, colorectal, stomach, and liver cancer than women, who are more likely to develop breast, colorectal, lung, cervical, and thyroid cancer. The primary goals of medical image segmentation include studying anatomical structure, identifying regions of interest (RoI), and measuring tissue volume to track tumor growth. It is crucial to diagnose and treat liver lesions quickly in order to stop the tumor from spreading further. Deep learning model-based liver segmentation has become very popular in the field of medical image analysis. This study explores various deep learning-based liver lesion segmentation algorithms and methodologies. Based on the developed models, the performance, and their limitations of these methodologies are contrasted. In the end, it was concluded that small size lesion segmentation, in particular, is still an open research subject for computer-aided systems of liver lesion segmentation, for there are still a number of technical issues that need to be resolved.</p></abstract>
      <kwd-group>
        <kwd>Lesion segmentation</kwd>
        <kwd>Computed Tomography (CT)</kwd>
        <kwd>Magnetic Resonance Imaging (MRI)</kwd>
        <kwd>Deep learning</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors">3</count>
        <fig-count>1</fig-count>
        <table-count>1</table-count>
        <ref-count>22</ref-count>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec disp-level="level1" sec-type="intro">
      <title>1. Introduction</title>
      <p>One of the most prevalent cancers in the world with a high mortality rate is liver cancer. The gold standard for diagnosing liver diseases such cirrhosis, liver cancer, and fulminant hepatic failure is a medical imaging modality like computed tomography (CT), magnetic resonance imaging (MRI), or positron emission tomography (PET) [<xref ref-type="bibr" rid="ref_1">1</xref>]. Among them, CT scans, which have a good signal-to-noise ratio and excellent resolution, are currently the modality most frequently employed for diagnosing and treating liver lesions or tumors. The accurate identification of liver cancer by doctors, together with knowledge of the shape, volume, and location of the lesion, can lead to more effective patient care. Clinicians must manually segment liver lesions on a slice-by-slice basis, which takes time and is error-prone. As a result, the accurate and automatic segmentation of the liver and hepatic lesions is required for computer assisted diagnosis of liver illness and for creating a plan for liver transplant surgery.</p><p>For volumetric or morphological analysis, segmentation is the technique of clearly defining an organ of interest on a multi-planar computed tomography (CT) or magnetic resonance imaging (MRI) image [<xref ref-type="bibr" rid="ref_2">2</xref>]. Although many deep learning-based models have been created, segmenting liver lesions is still a popular field of research. Several survey papers on the segmentation of the liver have been published [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>]. But to the best of our knowledge, there are not many survey publications on the segmentation of liver lesions.</p><p>This study carries out a critical analysis of some of the published works related to liver lesion segmentation using deep learning models. The authors compared various deep learning models based on the models proposed, datasets, performance, and disadvantages of each model, and presented some major challenges encountered while segmenting liver lesions. Finally, it was concluded that computer aided liver lesion segmentation is still an open research problem, especially facing small size lesions, for the available techniques have many limitations to be addressed.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>2. Liver lesion segmentation models</title>
      <p>This study summarizes the recent studies on medical image analysis for the segmentation of liver lesions or tumors. The majority of the research is built on supervised learning algorithms that train a model for a particular task, like liver or liver lesion segmentation, using labeled inputs. The deep learning techniques are an addition to these strategies [<xref ref-type="bibr" rid="ref_4">4</xref>]. The authors would discuss a variety of deep learning models that have been created for segmenting liver lesions or tumors.</p><p>A model for automatically segmenting the liver and lesions in abdominal CT images was created by Christ et al. [<xref ref-type="bibr" rid="ref_5">5</xref>] using cascaded fully convolutional neural networks (CFCNs) and dense 3D conditional random fields (CRFs). Cascaded FCNs were trained in two steps: first, an FCN was trained to segment the liver as a region of interest (ROI), which was then used as input by a second FCN. The second FCN is responsible for segmenting lesions from the anticipated liver ROI from the previous stage. The segmentation results generated by CFCN are then greatly improved in quality using a dense 3D conditional random field, as a step of post processing. Using the dataset 3DIRCAD, CFCN models were trained through two-fold cross-validations on CT scans of the abdomen. The results show that CFCN-based semantic liver and lesion segmentation achieved Dice scores of 94.3% for liver segmentation.</p><p>A fully convolutional neural network-based model, known as multi-channel FCN, was put forth by Sun et al. [<xref ref-type="bibr" rid="ref_6">6</xref>] for the segmentation of liver lesions from CT scans. The model is trained for each CT scan slice and coupled with its high-level features, for each slice of a contrast-enhanced CT image gives some kind of information. Two datasets based on CT images, 3Dircadb and JDRD, were used to evaluate the model. Compared to the earlier model [<xref ref-type="bibr" rid="ref_5">5</xref>], the suggested MC-FCN delivers improved values for VOE, RVD, ASD, and MSD, according to model performance data.</p><p>Given the restricted GPU capacity and training data, Xiao [<xref ref-type="bibr" rid="ref_7">7</xref>] suggested a 2.5D Deep CNN model that accepts a stack of neighboring slices as input and produces a 2D segmentation map corresponding to the center slice. They created a deep CNN model with 32 layers by utilizing both the long-distance skip connections of UNet [<xref ref-type="bibr" rid="ref_8">8</xref>] and the short-distance residual connections of ResNet [<xref ref-type="bibr" rid="ref_9">9</xref>]. The proposed Deep CNN model was trained and validated using the LiTS dataset. The intensity values of the input image were reduced to the range of [200, 200] HU in order to remove the unnecessary image features, but no further specific pre-processing was performed. To reduce the overall calculation time, two Deep CNN models were trained. The first was used to construct a rapid but coarse segmentation of the liver, and the second was applied to create a more detailed segmentation map of the liver and liver lesion. The suggested model's residual connections aid in information flow both forward and backward through the network and improve model performance. Using the LiTS dataset, two networks with the same design were trained. The Dice score for this approach was 0.67. Lesion segmentation accuracy might still be better, therefore more advancements are obviously needed.</p><p>Due to limitations on the addition of additional layers, fully convolutional networks with VGG-16 architecture find it challenging to learn more discriminative features associated with various classes. Convolutional filters' wide receptive fields in FCNs cause them to provide coarse outputs at lesion edges. ResNet is used by Bi et al. [<xref ref-type="bibr" rid="ref_10">10</xref>] to accomplish the liver lesion segmentation task, and to get over these limitations. Thanks to the ResNet's residual skip connections between convolutional layers, the issue of training accuracy decay in deeper networks was solved, enabling the insertion of additional layers to learn more discriminative features. In addition, the model was able to define boundaries more precisely by using a unique cascaded-ResNet architecture with multi-scale fusion to gradually learn and infer the borders of the liver as well as liver lesions. The suggested model is trained and tested on the LiTS dataset. To minimize the overall per-pixel loss, the network's weight parameters are iteratively updated via stochastic gradient descent (SGD). The segmentation results were greatest when cascaded-ResNet with multi-scale fusion was used. The Dice score for liver segmentation and the segmentation of liver lesions is improved by 3.94% and 20.13%, respectively, suggesting that the proposed model is more accurate than the VGG-Net-based FCN architecture.</p><p>For the ISBI 2017 Liver Tumor Segmentation Challenge, Chlebus et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] submitted a strategy that uses a 2D U-Net [<xref ref-type="bibr" rid="ref_8">8</xref>] network and a random forest classifier to automatically segment liver lesions (LiTS). Here, the liver segmentation task is first carried out to focus the network just on ROIs that may contain liver tumors. A trained neural network identifies the lesion candidates from within the liver ROIs, and then a random forest classifier is adopted to further refine them, producing the final lesion segmentation result. Typically, there are two steps: The liver mask is refined in the first step using an ensemble of three orthogonal 2D neural networks built on the U-Net architecture, and in the second step using a 3D U-Net. Using the LiTS dataset for evaluation and training, the suggested architecture obtains a Dice score of 0.65. To convert the numbers to Hounsfield units, DICOM rescale parameters are adopted. As a result, the convolutional networks' padding needed a fill value of -1000HU. After adding a threshold value of 0.5 to the soft-max output of the 3D U-net output, the researchers employed the biggest connected component to obtain the final liver mask.</p><p>Vorontsov et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] put forward a model for the joint segmentation of the liver and liver tumor in CT images. They created a model utilizing two cascaded FCNs and trained it in an end-to-end manner, with 2D axial slices as input. The FCNs in use have short- and long-range skip connections and a U-Net-like structure. Using the LiTS dataset, the suggested model was assessed. Rather than preprocess the dataset, the researchers only performed the minimal post-processing on the predicted results. The suggested model is a one stage model trained in an end-to-end manner. An axial slice serves as the first FCN's input, and the output is sent to a linear classifier to produce a probability map for each pixel containing the liver. Axial slices from the output of the first FCN are imported to the second FCN. The proposed model ended up with a DICE score of 0.661 for tumor segmentation and 0.951 for liver segmentation. However, the segmentation performance can be further enhanced by modifying the model to process the complete CT volume as opposed to only slices.</p><p>The majority of the models that have been created for the segmentation of liver tumors use FCNs (2D and 3D) as their main building blocks. But these models have the drawback of not being able to fully utilize spatial information along the third dimension. Besides that, 3D convolutions require high computational costs and GPU memory usage, but the high memory consumption limits the network depth as well as the filter's field of view. A hybrid densely connected UNet (H-DenseUNet) was created by Li et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] to circumvent the drawbacks of 2D and 3D convolutions. H-DenseUNet is made up of a 2D DenseUNet for precisely determining intra-slice features and a 3D part for incorporating volumetric context in a hierarchical manner for segmenting the liver and tumors. The learning process for H-DenseUNet was designed end to end by the researchers, who used a hybrid feature fusion layer to maximize both the inter- and intra-slice representations. The model achieved a global DICE score of 82.4% on the LiTS dataset and a Dice per case score of 72.2% when tested on the 3DIRCAD dataset.</p><p>To segment liver lesions, Chen et al. introduced a 2D segmentation architecture known as Feature Fusion Encoder-Decoder network [<xref ref-type="bibr" rid="ref_14">14</xref>]. The technique employs an attention procedure in which low-level features holding image details are coupled with high-level features conveying semantic information. Furthermore, to make up for the lost details during the up-sampling process, a dense up-sampling convolution is used instead of typical up-sampling procedures. Additionally, residual convolutional blocks are included to further improve the target boundary information. The Dice score for their strategy was 0.766 generally and 0.650 per case. In comparison to 2.5D and 3D models, the proposed architecture shows highly promising results. It is also lightweight and simple to install, with good generalization properties that make it easily transferable to other disciplines.</p><p>A pipeline made up of 2D U-Nets with dense connections and a Tversky loss function was suggested by Karsten et al. [<xref ref-type="bibr" rid="ref_15">15</xref>]. Prior to training a densely linked U-Net with a Tversky loss function to segment liver tumors, the researchers trained a regular U-Net model to segment the liver. They employed a cascaded pipeline to shorten the training period. The LiTS dataset was employed for data segmentation. In the LiTS competition, the proposed architecture outperformed the competitors in terms of relative volume difference (RVD), average symmetric surface distance (ASSD), maximum symmetric surface distance (MSSD), and volume overlap error (VOE). This suggests that when tumors are reliably diagnosed, the proposed architecture for segmenting liver lesions functions effectively. If some of the issues, such as high false positive predictions, are fixed, this architecture can be improved even further.</p><p>Jiang et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] created an attention-based hybrid network that incorporates long and short skip connections as well as soft and hard attention mechanisms. The researchers also suggested a cascaded network based on segmenting liver lesions, segmenting the liver, and localizing the liver. Additionally, they developed a focal binary cross entropy loss function to fine-tune the lesion segmentation network and a joint dice loss function to train the liver localization network, producing accurate 3D bounding boxes for the liver. The suggested architecture trained a network by removing 110 examples from the LiTS dataset before comparing it to 117 cases from the clinical dataset and the 3DIRCADb dataset. The results on the test dataset show that, the suggested network is able to segment liver lesions with a Dice score of 0.620.07 and faster convergence.</p><p>A cascaded Res-UNet that simultaneously segments the liver and liver lesion was created by Xi et al. [<xref ref-type="bibr" rid="ref_17">17</xref>]. With the proposed cascaded ResUNet, the researchers assessed five distinct loss functions: Weighted Cross Entropy (WCE), Dice Loss (DL), Weighted Dice Loss (WDL), Tversky Loss (TL), and Weighted Tversky Loss (WTL). To segment the liver and liver lesions simultaneously on the CT volume, they then ensembled all of the cascaded ResUNet models that had been trained with five different loss functions. The proposed ensembled model was trained and tested on the LiTS dataset. The proposed ensembled model outperforms the individual model for segmenting the liver and lesions, as per experimental results. For liver segmentation and liver lesion segmentation, it obtained Dice scores of 94.9% and 75.2%, respectively.</p><p>To address the problems with traditional UNet, Seo et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] modified it by introducing object dependent up sampling and altering the residual path and skip connections. The improved UNet [<xref ref-type="bibr" rid="ref_18">18</xref>] uses an optimal number of pooling operations to draw higher level global features for smaller objects, remove high level features of high-resolution edge information for larger objects, and cease the replication of low-resolution data of features. Using the LiTS dataset, the generated model's performance was evaluated. The model outperformed all others with a Dice score of 89.72%. As compared to traditional UNet, the proposed modified-UNet can operate on edge information and morphologic information of the objects more successfully.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>3. Major challenges</title>
      <p>There is a strong demand for precise and automatic liver and tumor segmentation to aid clinicians in diagnosis and treatment planning as liver cancer is one of the leading causes of cancer mortality today.</p><p>The majority of researchers noted the following difficulties in segmenting the liver and lesions: </p><p>a) Low intensity contrast between the liver and other nearby organs.</p><p>b) It is challenging to segment liver tumors since they might vary in size, shape, location, and quantity within a patient.</p><p>c) Some tumors lack distinct borders, which limits the effectiveness of segmentation approaches.</p><p>d) The majority of CT images have anisotropic dimensions with significant fluctuations along the z-axis direction, which makes segmentation approaches even more difficult.</p><p><xref ref-type="fig" rid="fig_1">Figure 1</xref> gives the examples of contrast-enhanced CT scans demonstrating vast dissimilarity of size, shape and position of liver lesion. Note that red regions represent liver and green regions represent liver lesions [<xref ref-type="bibr" rid="ref_13">13</xref>].</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>Examples of CT scans of liver</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2022/11/img_cFOJ70wenaRV5sAs.png"/>
        </fig>
      
      <p>In medical imaging, a major challenge to lesion segmentation is the imbalance between lesion class and non-lesion class, and the imbalance in tumor size, i.e., bigger size tumors dominate smaller ones when multiple tumors appear in a single modality. The class imbalance problem has been addressed by a number of strategies, but the tumor size imbalance problem has not received as much attention. As a result, many of these techniques either fail to partition smaller size tumors or produce less than ideal results. To overcome the challenge of minor lesion segmentation, Li et al. [<xref ref-type="bibr" rid="ref_19">19</xref>] devised a three-layer curriculum learning technique for deep neural networks. To effectively segment hepatic lesions, Dey and Hong [<xref ref-type="bibr" rid="ref_20">20</xref>] suggested a cascaded network that incorporates both 2D and 3D CNNs. In this network, a 3D network detects minor lesions that are frequently missed by a 2D segmentation model, while a 2D network segments the liver on a slice-by-slice basis and detects larger lesions. The model receives a Dice score of 68.1% on the LiTS dataset. A loss reweighting strategy was presented by Shirokikh et al. [<xref ref-type="bibr" rid="ref_21">21</xref>] to improve the capacity of the deep learning network to detect tiny size lesions. Further research is required concerning the segmentation of tiny liver lesions using the proposed networks. It is necessary to build a network that will concentrate on both small and large lesions at the same time and enhance the model's overall performance.</p>
    </sec>
    <sec disp-level="level1" sec-type="conclusions">
      <title>4. Conclusions</title>
      <p>In medical imaging modalities, the anatomy or region that is of interest typically only takes up a small amount of the modality. Given this, the learning process frequently becomes stuck in local minima of the cost function, resulting in a model with outcomes that are heavily biased towards the background rather than the foreground. Foreground zones are hence frequently overlooked or only partially noticed [<xref ref-type="bibr" rid="ref_22">22</xref>]. The efficacy of the segmentation models is impacted when there are several lesions per modality and large size lesions predominate smaller size lesions. As a result, smaller size lesions are frequently ignored.</p><p>For the purpose of early cancer stage diagnosis, clinical disease progression tracking, and treatment response evaluation, segmentation of smaller size lesions is crucial. Medical professionals will not be able to correctly diagnose a disease or they may completely miss the diagnosis when the disease is in its early stages if smaller size lesions go untreated. The various liver lesion segmentation methods covered in preceding sections are summarized in <xref ref-type="table" rid="table_1">Table 1</xref>, along with the models suggested, their performance, the datasets used, and any drawbacks.</p><p>This work thoroughly analyzes the various deep learning models for segmenting liver lesions, and notes some significant difficulties encountered when segmenting liver lesions. Future research will focus on finding ways to get around the drawbacks of segmenting liver lesions of different sizes.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>Deep learning-based liver lesion segmentation methods</caption>
          <abstract>VOE= Volumetric Overlap Error, RVD= Relative Volume Difference, ASD= Average Symmetric Surface Distance, MSD= Maximum Surface Distance, RMSD= Root Mean Square Symmetric Surface Distance.</abstract>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Paper</p></td><td colspan="1" rowspan="1"><p>Dataset</p></td><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="1" rowspan="1"><p>Performance</p></td><td colspan="1" rowspan="1"><p>Limitation/s</p></td></tr><tr><td colspan="1" rowspan="1"><p>Christ et al. [<xref ref-type="bibr" rid="ref_5">5</xref>]</p></td><td colspan="1" rowspan="1"><p>3DIRCAD</p></td><td colspan="1" rowspan="1"><p>Cascaded CFNs with dense 3D conditional random field (CRFs)</p></td><td colspan="1" rowspan="1"><p>DICE (%) for liver=94.3; VOE (%)=10.7; RVD (%) =-1.4; ASD (mm) =1.5; MSD (mm) =24.0</p></td><td colspan="1" rowspan="1"><p>The method is complex and time-consuming, as it requires an additional post processing step using CRFs, and adopts two CFNs.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Sun et al. [<xref ref-type="bibr" rid="ref_6">6</xref>]</p></td><td colspan="1" rowspan="1"><p>3DIRCAD and JDRD</p></td><td colspan="1" rowspan="1"><p>Multi-channel FCN to segment liver lesions from multiphase contrast-enhanced CT scans</p></td><td colspan="1" rowspan="1"><p>3DIRCAD:</p><p>VOE (%)=15.6 ± 4.3; RVD (%)=5.8±3.5; ASD= 2.0 ± 0.9%</p><p>JDRD:</p><p>VOE=8.1±4.5%; RVD=1.7 ± 1.0%;</p><p>ASD=1.5 ± 0.7%</p></td><td colspan="1" rowspan="1"><p> </p></td></tr><tr><td colspan="1" rowspan="1"><p>Xiao [<xref ref-type="bibr" rid="ref_7">7</xref>]</p></td><td colspan="1" rowspan="1"><p>LiTS</p></td><td colspan="1" rowspan="1"><p>Deep CNN model with 32 layers that operates in 2.5D</p></td><td colspan="1" rowspan="1"><p>Dice=0.67; VOE=0.45; RVD=0.040; ASSD=6.660; MSSD=57.93</p></td><td colspan="1" rowspan="1"><p>The accuracy of lesion segmentation needs further improvement, and the training time is too long.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Bi et al. [<xref ref-type="bibr" rid="ref_10">10</xref>]</p></td><td colspan="1" rowspan="1"><p>LiTS</p></td><td colspan="1" rowspan="1"><p>Cascaded deep residual network with ability to add more layers</p></td><td colspan="1" rowspan="1"><p>Liver Dice=95.90; Lesion Dice=50.01; Liver Jaccard=92.19; Lesion Jaccard=38.79</p></td><td colspan="1" rowspan="1"><p>The method excels in liver segmentation but does not achieve the ideal effect in segmenting liver lesions.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Chlebus et al. [<xref ref-type="bibr" rid="ref_11">11</xref>]</p></td><td colspan="1" rowspan="1"><p>LiTS</p></td><td colspan="1" rowspan="1"><p>2D U-Net network with random forest classifier</p></td><td colspan="1" rowspan="1"><p>Accuracy=90%; Dice score =0.65</p></td><td colspan="1" rowspan="1"><p> </p></td></tr><tr><td colspan="1" rowspan="1"><p>Vorontsov et al. [<xref ref-type="bibr" rid="ref_12">12</xref>]</p></td><td colspan="1" rowspan="1"><p>LiTS</p></td><td colspan="1" rowspan="1"><p>Two cascaded FCNs with U-Net-like structure, having short- and long-range skip connections</p></td><td colspan="1" rowspan="1"><p>Dice (liver segmentation) =0.951; Dice (liver lesion segmentation) =0.661</p></td><td colspan="1" rowspan="1"><p>The model must process input slice by slice, which degrades the performance.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Li et al. [<xref ref-type="bibr" rid="ref_13">13</xref>]</p></td><td colspan="1" rowspan="1"><p>LiTS and 3DIRCAD</p></td><td colspan="1" rowspan="1"><p>Hybrid densely connected UNet</p></td><td colspan="1" rowspan="1"><p>LiTS:</p><p>Dice Score=82.4</p><p>3DIRCAD:</p><p>Dice Score=0.937 ± 0.02; VOE (%) =11.68 ± 4.33; RVD (%) =-0.01 ± 0.05; ASD (mm) =0.58 ± 0.46; RMSD (mm) =1.87 ± 2.33</p></td><td colspan="1" rowspan="1"><p>The model cannot segment small size liver tumors effectively.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Chen et al. [<xref ref-type="bibr" rid="ref_14">14</xref>]</p></td><td colspan="1" rowspan="1"><p>LiTS</p></td><td colspan="1" rowspan="1"><p>Feature fusion encoder decoder network with residual blocks</p></td><td colspan="1" rowspan="1"><p>Dice per case=0.650;</p><p>Dice global=0.766</p></td><td colspan="1" rowspan="1"><p> </p></td></tr><tr><td colspan="1" rowspan="1"><p>Karsten et al. [<xref ref-type="bibr" rid="ref_15">15</xref>]</p></td><td colspan="1" rowspan="1"><p>LiTS</p></td><td colspan="1" rowspan="1"><p>2D Tiramisu network with Tversky loss function</p></td><td colspan="1" rowspan="1"><p>Dice Avg=0.57; Dice global=0.66;</p><p>VOE=0.34; RVD=0.02;</p><p>ASSD=0.95; MSSD=6.81;</p><p>RMSD=1.60</p></td><td colspan="1" rowspan="1"><p>The false prediction rate is very high.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Jiang et al. [<xref ref-type="bibr" rid="ref_16">16</xref>]</p></td><td colspan="1" rowspan="1"><p>LiTS and 3DIRCAD</p></td><td colspan="1" rowspan="1"><p>Attention-based hybrid network</p></td><td colspan="1" rowspan="1"><p>3DIRCADb Dataset:</p><p>Dice Score=0.62±0.07; VOE (%) =1.354; RVD (%) =0.129; ASSD (mm) =1.074; MSSD (mm) =6.271; RMSD (mm) =1.412</p></td><td colspan="1" rowspan="1"><p>The liver lesion segmentation is inefficient.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Xi et al. [<xref ref-type="bibr" rid="ref_17">17</xref>]</p><p> </p></td><td colspan="1" rowspan="1"><p>LiTS</p></td><td colspan="1" rowspan="1"><p>Ensembled model with cascaded ResUNet</p></td><td colspan="1" rowspan="1"><p>Liver Segmentation:</p><p>VOE=0.095; RVD=0.021;</p><p>Dice=94.9%</p><p>Liver Lesion Segmentation:</p><p>VOE=0.379; RVD=-0.159;</p><p>Dice=75.2%</p></td><td colspan="1" rowspan="1"><p> </p></td></tr><tr><td colspan="1" rowspan="1"><p>Seo et al. [<xref ref-type="bibr" rid="ref_18">18</xref>]</p></td><td colspan="1" rowspan="1"><p>LiTS and 3DIRCAD</p></td><td colspan="1" rowspan="1"><p>Modified UNet with residual path and object dependent up-sampling</p></td><td colspan="1" rowspan="1"><p>LiTS:</p><p>Dice Score=89.72%; VOE=21.93%; RVD=-0.49%</p><p>3DIRCAD:</p><p>Dice Score=68.14%</p></td><td colspan="1" rowspan="1"><p>The loss function, MSE, does not adequately capture structure similarity.</p></td></tr></tbody></table>
        </table-wrap>
      
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p></p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>P.</given-names>
              <surname>Campadelli</surname>
            </name>
            <name>
              <given-names>E.</given-names>
              <surname>Casiraghi</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Liver segmentation from CT scans: A survey</article-title>
          <source/>
          <publisher-loc>Berlin</publisher-loc>
          <publisher-name>Springer</publisher-name>
          <conf-name>Applications of Fuzzy Sets Theory, WILF 2007, Lecture Notes in Computer Science, Camogli</conf-name>
          <conf-acronym/>
          <conf-loc>Italy</conf-loc>
          <conf-date>July 7-10, 2007</conf-date>
          <year>2007</year>
          <volume/>
          <issue/>
          <page-range>520-528</page-range>
          <fpage>520</fpage>
          <lpage>528</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/978-3-540-73400-0_66.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>A.</given-names>
              <surname>Gotra</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Sivakumaran</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Chartrand</surname>
            </name>
            <name>
              <given-names>K. N.</given-names>
              <surname>Vu</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Vandenbroucke-Menu</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Kauffmann</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Kadoury</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Gallix</surname>
            </name>
            <name>
              <given-names>J. A.</given-names>
              <surname>de Guise</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Tang</surname>
            </name>
          </person-group>
          <article-title>Liver segmentation: Indications, techniques and future directions</article-title>
          <source>Insights Imaging</source>
          <year>2017</year>
          <volume>8</volume>
          <issue/>
          <page-range>377-392</page-range>
          <fpage>377</fpage>
          <lpage>392</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/s13244-017-0558-1</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>F. A.</given-names>
              <surname>Mohammed</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Viriri</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Liver segmentation: A survey of the state-of-the-art</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>Springer, Cham</publisher-name>
          <conf-name>2017 Sudan Conference on Computer Science and Information Technology</conf-name>
          <conf-acronym>SCCSIT 2017</conf-acronym>
          <conf-loc>Elnihood, Sudan</conf-loc>
          <conf-date>November 17-19, 2017</conf-date>
          <year>2017</year>
          <volume/>
          <issue/>
          <page-range>1-6</page-range>
          <fpage>1</fpage>
          <lpage>6</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/SCCSIT.2017.8293049.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Almotairi</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Kareem</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Aouf</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Almutairi</surname>
            </name>
            <name>
              <given-names>M. A. M.</given-names>
              <surname>Salem</surname>
            </name>
          </person-group>
          <article-title>Liver tumor segmentation in CT scans using modified SegNet</article-title>
          <source>Sensors</source>
          <year>2020</year>
          <volume>20</volume>
          <issue>5</issue>
          <page-range/>
          <fpage/>
          <lpage/>
          <pub-id pub-id-type="doi">https://doi.org/10.3390/s20051516</pub-id>
          <pub-id pub-id-type="publisher-id">1516</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>P. F.</given-names>
              <surname>Christ</surname>
            </name>
            <name>
              <given-names>M. E. A.</given-names>
              <surname>Elshaer</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Ettlinger</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Tatavarty</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Bickel</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Bilic</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Rempfler</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Armbruster</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Hofmann</surname>
            </name>
            <name>
              <given-names>M. D.</given-names>
              <surname>Anastasi</surname>
            </name>
            <name>
              <given-names>W. H.</given-names>
              <surname>Sommer</surname>
            </name>
            <name>
              <given-names>S. A.</given-names>
              <surname>Ahmadi</surname>
            </name>
            <name>
              <given-names>B. H.</given-names>
              <surname>Menze</surname>
            </name>
          </person-group>
          <article-title>Automatic liver and lesion segmentation in CT using cascaded fully convolutional neural networks and 3D conditional random fields</article-title>
          <source>Med. Image Compt. Comput. Assist. Interv.</source>
          <year>2016</year>
          <volume>9901</volume>
          <issue/>
          <page-range>415-423</page-range>
          <fpage>415</fpage>
          <lpage>423</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/978-3-319-46723-8_48</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>H. J.</given-names>
              <surname>Sun</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Guo</surname>
            </name>
            <name>
              <given-names>H. M.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>M. M.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>S. Z.</given-names>
              <surname>Ma</surname>
            </name>
            <name>
              <given-names>L. Y.</given-names>
              <surname>Jin</surname>
            </name>
            <name>
              <given-names>X. M.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>X. Y.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>X. H.</given-names>
              <surname>Qian</surname>
            </name>
          </person-group>
          <article-title>Automatic segmentation of liver tumors from multiphase contrast-enhanced CT images based on FCNs</article-title>
          <source>Artif Intell Med.</source>
          <year>2017</year>
          <volume>83</volume>
          <issue/>
          <page-range>58-66</page-range>
          <fpage>58</fpage>
          <lpage>66</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1016/j.artmed.2017.03.008</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>H.</given-names>
              <surname>Xiao</surname>
            </name>
          </person-group>
          <article-title>Automatic liver lesion segmentation using a deep convolutional neural network method</article-title>
          <source>ArXiv</source>
          <year>2017</year>
          <volume>9</volume>
          <issue/>
          <page-range>1-2</page-range>
          <fpage>1</fpage>
          <lpage>2</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.48550/arXiv.1704.07239</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>O.</given-names>
              <surname>Ronneberger</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Fischer</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Brox</surname>
            </name>
          </person-group>
          <article-title>U-Net: Convolutional networks for biomedical image segmentation</article-title>
          <source>Med. Image Compt. Comput. Assist. Interv.</source>
          <year>2015</year>
          <volume>9351</volume>
          <issue/>
          <page-range>234-241</page-range>
          <fpage>234</fpage>
          <lpage>241</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/978-3-319-24574-4_28</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>K. M.</given-names>
              <surname>He</surname>
            </name>
            <name>
              <given-names>X. Y.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>S. Q.</given-names>
              <surname>Ren</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Sun</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Deep residual learning for image recognition</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition</conf-name>
          <conf-acronym>CVPR 2016</conf-acronym>
          <conf-loc>Vegas, NV, USA</conf-loc>
          <conf-date>June 27-30, 2016</conf-date>
          <year>2016</year>
          <volume/>
          <issue/>
          <page-range>770-778</page-range>
          <fpage>770</fpage>
          <lpage>778</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/CVPR.2016.90.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>L.</given-names>
              <surname>Bi</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Kim</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Kumar</surname>
            </name>
            <name>
              <given-names>D. G.</given-names>
              <surname>Feng</surname>
            </name>
          </person-group>
          <article-title>Automatic liver lesion detection using cascaded deep residual networks</article-title>
          <source>ArXiv</source>
          <year>2017</year>
          <volume>17</volume>
          <issue/>
          <page-range/>
          <fpage/>
          <lpage/>
          <pub-id pub-id-type="doi">https://doi.org/10.48550/arXiv.1704.02703</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>G.</given-names>
              <surname>Chlebus</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Meine</surname>
            </name>
            <name>
              <given-names>J. H.</given-names>
              <surname>Moltz</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Schenk</surname>
            </name>
          </person-group>
          <article-title>Neural network-based automatic liver tumor segmentation with random forest-based candidate filtering</article-title>
          <source>ArXiv</source>
          <year>2017</year>
          <volume>17</volume>
          <issue/>
          <page-range/>
          <fpage/>
          <lpage/>
          <pub-id pub-id-type="doi">https://doi.org/10.48550/arXiv.1706.00842</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>E.</given-names>
              <surname>Vorontsov</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Tang</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Pal</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Kadoury</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Liver lesion segmentation informed by joint liver segmentation</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2018 IEEE 15th International Symposium on Biomedical Imaging</conf-name>
          <conf-acronym>ISBI 2018</conf-acronym>
          <conf-loc>Washington, DC, USA</conf-loc>
          <conf-date>April 04-07, 2018</conf-date>
          <year>2018</year>
          <volume/>
          <issue/>
          <page-range>1332-1335</page-range>
          <fpage>1332</fpage>
          <lpage>1335</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ISBI.2018.8363817.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>X.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Qi</surname>
            </name>
            <name>
              <given-names>Q.</given-names>
              <surname>Dou</surname>
            </name>
            <name>
              <given-names>C. W.</given-names>
              <surname>Fu</surname>
            </name>
            <name>
              <given-names>P. A.</given-names>
              <surname>Heng</surname>
            </name>
          </person-group>
          <article-title>H-DenseUNet: Hybrid densely connected UNet for liver and tumor segmentation from CT volumes</article-title>
          <source>IEEE Trans. Med Imaging</source>
          <year>2018</year>
          <volume>37</volume>
          <issue>2</issue>
          <page-range>2663-2674</page-range>
          <fpage>2663</fpage>
          <lpage>2674</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/tmi.2018.2845918</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>X. R.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>P. K.</given-names>
              <surname>Yan</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Feature fusion encoder decoder network for automatic liver lesion segmentation</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2019 IEEE 16th International Symposium on Biomedical Imaging</conf-name>
          <conf-acronym>ISBI 2019</conf-acronym>
          <conf-loc>Venice, Italy</conf-loc>
          <conf-date>April 08-11, 2019</conf-date>
          <year>2019</year>
          <volume/>
          <issue/>
          <page-range>430-433</page-range>
          <fpage>430</fpage>
          <lpage>433</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ISBI.2019.8759555.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>R.</given-names>
              <surname>Karsten</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Konopczyński</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Hesser</surname>
            </name>
          </person-group>
          <article-title>Liver lesion segmentation with slice-wise 2D Tiramisu and Tversky loss function</article-title>
          <source>ArXiv</source>
          <year>2019</year>
          <volume>19</volume>
          <issue/>
          <page-range/>
          <fpage/>
          <lpage/>
          <pub-id pub-id-type="doi">https://doi.org/10.48550/arXiv.1905.03639</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>H.</given-names>
              <surname>Jiang</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Shi</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Bai</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Huang</surname>
            </name>
          </person-group>
          <article-title>AHCNet: An application of attention mechanism and hybrid connection for liver tumor segmentation in CT volumes</article-title>
          <source>IEEE Access</source>
          <year>2019</year>
          <volume>7</volume>
          <issue/>
          <page-range>24898-24909</page-range>
          <fpage>24898</fpage>
          <lpage>24909</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ACCESS.2019.2899608</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>X.</given-names>
              <surname>Xi</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>V. S.</given-names>
              <surname>Sheng</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Cui</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Fu</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Hu</surname>
            </name>
          </person-group>
          <article-title>Cascade U-ResNets for simultaneous liver and lesion segmentation</article-title>
          <source>IEEE Access</source>
          <year>2020</year>
          <volume>8</volume>
          <issue/>
          <page-range>68944-68952</page-range>
          <fpage>68944</fpage>
          <lpage>68952</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ACCESS.2020.2985671</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>H.</given-names>
              <surname>Seo</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Huang</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Bassenne</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Xiao</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Xing</surname>
            </name>
          </person-group>
          <article-title>Modified U-Net (mU-Net) with incorporation of object-dependent high level features for improved liver and liver-tumor segmentation in CT images</article-title>
          <source>IEEE Trans. Med Imaging</source>
          <year>2020</year>
          <volume>39</volume>
          <issue>5</issue>
          <page-range>1316-1325</page-range>
          <fpage>1316</fpage>
          <lpage>1325</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/TMI.2019.2948320</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>H.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Boumaraf</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Gong</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Ma</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>A new three-stage curriculum learning approach for deep network based liver tumor segmentation</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2020 International Joint Conference on Neural Networks</conf-name>
          <conf-acronym>IJCNN 2020</conf-acronym>
          <conf-loc>Glasgow, United Kingdom</conf-loc>
          <conf-date>July 19-24, 2020</conf-date>
          <year>2020</year>
          <volume/>
          <issue/>
          <page-range>1-6</page-range>
          <fpage>1</fpage>
          <lpage>6</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/IJCNN48605.2020.9206789.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>R.</given-names>
              <surname>Dey</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Hong</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>Hybrid cascaded neural network for liver lesion segmentation</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2020 IEEE 17th International Symposium on Biomedical Imaging</conf-name>
          <conf-acronym>ISBI 2020</conf-acronym>
          <conf-loc>Iowa City, IA, USA</conf-loc>
          <conf-date>April 03-07, 2020</conf-date>
          <year>2020</year>
          <volume/>
          <issue/>
          <page-range>1173-1177</page-range>
          <fpage>1173</fpage>
          <lpage>1177</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ISBI45749.2020.9098656.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <given-names>B.</given-names>
              <surname>Shirokikh</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Shevtsov</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Kurmukov</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Dalechina</surname>
            </name>
            <name>
              <given-names>E.</given-names>
              <surname>Krivov</surname>
            </name>
            <name>
              <given-names>V.</given-names>
              <surname>Kostjuchenko</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Golanov</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Belyaev</surname>
            </name>
          </person-group>
          <article-title>Universal loss reweighting to balance lesion size inequality in 3D medical image segmentation</article-title>
          <source>Med. Image Compt. Comput. Assist. Interv.</source>
          <year>2020</year>
          <volume>1226</volume>
          <issue/>
          <page-range>523-532</page-range>
          <fpage>523</fpage>
          <lpage>532</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/978-3-030-59719-1_51</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>F.</given-names>
              <surname>Milletari</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Navab</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Ahmadi</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <article-title>V-Net: Fully convolutional neural networks for volumetric medical image segmentation</article-title>
          <source/>
          <publisher-loc/>
          <publisher-name>arXiv</publisher-name>
          <conf-name>2016 Fourth International Conference on 3D Vision</conf-name>
          <conf-acronym>3DV 2016</conf-acronym>
          <conf-loc>Stanford, CA, USA</conf-loc>
          <conf-date>October 25-28, 2016</conf-date>
          <year>2016</year>
          <volume/>
          <issue/>
          <page-range>565-571</page-range>
          <fpage>565</fpage>
          <lpage>571</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/3DV.2016.79.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>