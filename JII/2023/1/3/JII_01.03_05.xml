<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">JII</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Journal of Industrial Intelligence</journal-title>
        <abbrev-journal-title abbrev-type="issn">J. Ind Intell.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">JII</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-2695</issn>
      <issn publication-format="print">2958-2687</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-WZpxvk8OKUpXxoqNXiV4TWSJmWTiggOY</article-id>
      <article-id pub-id-type="doi">10.56578/jii010305</article-id>
      <title-group>
        <article-title>Text Readability Evaluation in Higher Education Using CNNs</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Zulqarnain</surname>
            <given-names>Muhammad</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0008-0494-6078</contrib-id>
          <email>mzg03@hotmail.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="2">2</xref>
          <name>
            <surname>Saqlain</surname>
            <given-names>Muhammad</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3617-6043</contrib-id>
          <email>Muhammad.saql@kmutt.ac.th</email>
        </contrib>
        <aff id="1">Department of Computer Science, Virtual University (VU), 54000 Lahore, Pakistan</aff>
        <aff id="2">Department of Mathematics, Faculty of Science, King Mongkut’s University of Technology Thonburi (KMUTT), 10140 Bangkok, Thailand</aff>
      </contrib-group>
      <year>2023</year>
      <volume>1</volume>
      <issue>3</issue>
      <fpage>184</fpage>
      <lpage>193</lpage>
      <page-range>184-193</page-range>
      <history>
        <date date-type="received">
          <month>08</month>
          <day>16</day>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <month>09</month>
          <day>19</day>
          <year>2023</year>
        </date>
        <date date-type="pub">
          <month>09</month>
          <day>29</day>
          <year>2023</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2023 by the authors</copyright-statement>
        <copyright-year>2023</copyright-year>
        <license>. Licensee Acadlore Publishing Services Limited, Hong Kong. This article can be downloaded for free, and reused and quoted with a citation of the original published version, under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 license</ext-link>.</license>
      </permissions>
      <abstract><p>The paramountcy of English in the contemporary global landscape necessitates the enhancement of English language proficiency, especially in academic settings. This study addresses the disparate levels of English proficiency among college students by proposing a novel approach to evaluate English text readability, tailored for the higher education context. Employing a deep learning (DL) framework, the research focuses on developing a model based on convolutional neural networks (CNNs) to assess the readability of English texts. This model diverges from traditional methods by evaluating the difficulty of individual sentences and extending its capability to ascertain the readability of entire texts through adaptive weight learning. The methodology's effectiveness is underscored by an impressive 72% accuracy rate in readability assessment, demonstrating its potential as a transformative tool in English language education. The application of this DL-based text readability evaluation model in college English training is explored, highlighting its potential to facilitate a more nuanced understanding of text complexity. Furthermore, the study contributes to the broader discourse on enhancing English language instruction in higher education, proposing a method that not only evaluates text comprehensibility but also aligns with diverse educational needs. The findings suggest that this approach could significantly support the enhancement of English teaching methodologies, thereby promoting a deeper, more accessible learning experience for students with varying levels of proficiency.</p></abstract>
      <kwd-group>
        <kwd>Text readability</kwd>
        <kwd>Convolutional neural networks (CNNs)</kwd>
        <kwd>English language</kwd>
        <kwd>Higher education</kwd>
        <kwd>Assessment</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors">2</count>
        <fig-count>8</fig-count>
        <table-count>3</table-count>
        <ref-count>40</ref-count>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec disp-level="level1" sec-type="intro">
      <title>1. Introduction</title>
      <p>English, often regarded as the universal language, exerts a significant influence on higher education, particularly in countries where English is not the native language. Its role transcends national borders, providing students from diverse linguistic backgrounds with access to a plethora of global academic resources. The widespread use of English in scholarly journals, textbooks, and academic conferences necessitates its mastery by students and researchers for engaging with contemporary research and academic discourse. Moreover, numerous universities in non-native English-speaking countries offer English-taught programs, attracting international students and fostering intercultural collaboration. This not only enhances educational experiences and aids in developing a global outlook but also improves professional prospects in an increasingly interconnected world [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>].</p><p>In the realm of text readability, traditional methods such as the Flesch Reading Ease, Gunning FOG, and Automated Readability Index were employed. However, these formulae provided less precise assessments as they merely skimmed the surface of word features, lacking in-depth understanding of word meanings. The advent of DL models has revolutionized text readability assessment by automating the process and obviating the need for manual word feature configuration. The efficiency and accuracy of automatic word feature extraction via DL models represent a significant advancement, reducing the reliance on human intervention. Notably, the development of the Ranked Sentence Readability Score method, a deep neural network-based approach capable of processing multilingual texts, has been a notable contribution by international researchers [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>]. Despite these advancements, there remains a gap in addressing the challenges of English instruction in college settings. The dual pressures of employment and further study faced by students in higher education exacerbate the competition for job opportunities. Additionally, the ongoing reforms in the higher education system, still in their nascent stages, require a more robust alignment with secondary education. Consequently, English education in higher education contexts necessitates an efficient and effective approach.</p><p>This paper contributes to the field by evaluating English text readability through a DL model tailored for college English education. A text CNN evaluation model is developed, with parameters fine-tuned for optimal performance. The methodology includes the creation of a text readability dataset from college English textbooks. The results demonstrate that the model effectively predicts text readability with an accuracy rate of 82%, evaluating sentence complexity and text length through weight learning. This approach provides substantial support for English instruction in colleges, aiding in bridging the gap in students’ English proficiency.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>2. Experimental techniques and design</title>
      <p>Deep learning algorithms have revolutionized the assessment of text readability, offering more precise and adaptable methodologies compared to traditional measures. Utilizing large-scale text data, these algorithms, which include recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and CNNs, are employed to predict textual complexity. RNNs and LSTMs, in particular, have been recognized for their ability to analyze the context and structure of text [<xref ref-type="bibr" rid="ref_5">5</xref>], significantly surpassing traditional readability formulas such as Flesch Reading Ease and Gunning FOG Index in accuracy. These models, through training on extensive datasets of human-rated text readability, can meticulously evaluate text complexity by considering factors such as sentence length, word difficulty, and syntactic complexity. The proficiency of RNNs and LSTMs lies in their capacity to capture sequential relationships in language, thus enabling a comprehensive assessment of text coherence and flow [<xref ref-type="bibr" rid="ref_6">6</xref>]. Furthermore, CNNs, traditionally used in image processing, have been adapted for text analysis, demonstrating their versatility. They can extract salient features from text data, such as n-grams, to determine readability. By training CNNs on large text corpora annotated with readability labels, these models can discern patterns indicative of text difficulty, rendering them effective for a broad spectrum of readability evaluation contexts.</p><p>Moreover, the introduction of Transformers, a groundbreaking deep learning architecture, has markedly enhanced the assessment of text readability. Renowned for their efficacy in natural language processing, models like Bidirectional Encoder Representations from Transformers (BERT) utilize a bidirectional approach to gather context, allowing for a more nuanced understanding of text complexity [<xref ref-type="bibr" rid="ref_7">7</xref>]. Transfer learning has been instrumental in adapting these pre-trained models to readability datasets across various domains, thereby broadening their application scope.</p><p>These deep learning methods, encompassing sophisticated architectures like Transformers and neural networks like RNNs, LSTMs, and CNNs, have been pivotal in evaluating text complexity. By training on large, labeled datasets, they consider aspects such as syntax, semantics, and context, offering more refined and nuanced readability assessments. This advancement has profound implications for content creation, educational applications, and other sectors where text readability analysis is crucial [<xref ref-type="bibr" rid="ref_8">8</xref>]. Additionally, self-training and domain adaptation techniques have been explored to enhance these systems' proficiency in processing input from second language learners.</p>
      
        <sec disp-level="level2">
          
            <title>2.1. Transformers</title>
          
          <p>In the context of text readability evaluation, Transformers have significantly outperformed conventional sequence models like RNNs or LSTMs. Their design, incorporating a self-attention mechanism, enables the assessment of the relative importance of each word in a sequence. Models like BERT, pre-trained on extensive text corpora, have been fine-tuned for readability evaluation, excelling in interpreting text complexity and adapting to domain-specific linguistic nuances [<xref ref-type="bibr" rid="ref_9">9</xref>]. Their ability to handle long-range dependencies in text positions Transformers as highly capable in considering context, semantics, and syntax in readability assessment.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>2.2. Rnns and lstms</title>
          
          <p>RNNs and LSTMs have been applied to various natural language processing applications, including text readability evaluation. RNNs process data sequences sequentially while maintaining a hidden state, and LSTMs, an advanced version of RNNs, effectively manage the vanishing gradient problem, making them suitable for longer sequences. These models excel in analyzing sentence structures and word dependencies, thus accurately estimating text complexity. While Transformers have largely superseded RNNs and LSTMs in NLP tasks, these models still hold relevance in certain contexts.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>2.3. Cnns</title>
          
          <p>CNNs, though primarily associated with image processing, have proven effective in text analysis tasks, including readability evaluation. Employed to extract textual features such as n-grams, CNNs undergo training on extensive datasets with readability labels, enabling them to identify patterns correlating with text complexity. While their popularity in NLP may not match that of Transformers, CNNs continue to offer valuable insights in text readability analysis and remain computationally efficient for specific tasks [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>].</p>
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>3. Encoding text</title>
      <p>The initial step in utilizing deep learning for text readability assessment is text encoding, a process in which textual words are converted into numerical representations. Commonly, this is achieved through one-hot encoding, where each word is denoted by a large vector. The length of this vector corresponds to the total number of words in the corpus, with the position representing the specific word marked as one, and all other values set to zero [<xref ref-type="bibr" rid="ref_12">12</xref>]. This method, however, presents two substantial limitations: its inability to capture relationships between words and the problem of high dimensionality in large vocabularies [<xref ref-type="bibr" rid="ref_13">13</xref>].</p><p>An alternative to one-hot encoding is distribution-based text representation, which addresses these drawbacks effectively. In this approach, words are represented as low-dimensional real variables, significantly reducing the dimensionality issue and allowing for the representation of word-to-word relationships. Nevertheless, this method encounters a challenge in differentiating words with similar meanings, potentially leading to an overlap in word properties [<xref ref-type="bibr" rid="ref_14">14</xref>].</p><p>Prominent models utilized in distribution-based text representation include the Continuous Bag-of-Words (CBOW) model and the Skip-Gram model. The CBOW model focuses on predicting a word based on its context, estimating the likelihood of a word's occurrence within a specific contextual framework. Conversely, the Skip-Gram model infers the contextual words from a given word, essentially reversing the process employed by the CBOW model. <xref ref-type="fig" rid="fig_1">Figure 1</xref> and <xref ref-type="fig" rid="fig_2">Figure 2</xref> provide schematic illustrations of the CBOW and Skip-Gram models, respectively.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>CBOW model</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_RS4prNgqv2vZGLCG.png"/>
        </fig>
      
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>Skip-Gram model</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_Z1RLjlZIa7zrjI11.png"/>
        </fig>
      
      <p>In enhancing the efficacy of word training, both the CBOW and Skip-Gram models employ hierarchical SoftMax in the fully connected layer, coupled with a negative sampling strategy [<xref ref-type="bibr" rid="ref_15">15</xref>], [<xref ref-type="bibr" rid="ref_16">16</xref>]. However, these models share a common limitation in their design: they are trained to recognize the contextual continuity of words but do not distinguish between words that may appear frequently throughout the text. The models do not differentiate between global and local word associations, which could be a drawback. Despite this, both the CBOW and Skip-Gram models have demonstrated competent performance in readability assessments, largely unaffected by these limitations.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>4. Dl networks</title>
      <p>DL networks, such as CNNs, RNNs, and LSTMs, play a pivotal role in the process of text readability assessment. Each network type possesses distinct characteristics and functions, as elaborated below.</p><p>•CNNs, comprising multiple deep layers, primarily utilize convolution computations. A standard CNN architecture includes an input layer, convolutional layers, and pooling layers. Renowned for their efficacy in image processing, CNNs have also demonstrated considerable capability in text categorization and semantic analysis, especially in the realm of text readability assessment, a form of text classification (<xref ref-type="fig" rid="fig_3">Figure 3</xref>).</p>
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>CNN structure diagram</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_JpLAPq3S6er_nQpU.png"/>
        </fig>
      
      <p>•As depicted in <xref ref-type="fig" rid="fig_3">Figure 3</xref>, CNNs use various convolution kernels in their convolutional layers to extract word properties essential for text classification [<xref ref-type="bibr" rid="ref_17">17</xref>]. The activation function layer often employs the ReLU function for efficiency, and the output from the fully connected layer is flattened, enhancing its suitability for subsequent processing stages.</p><p>•RNNs, designed to handle sequential input, connect nodes in accordance with the sequence order, making them particularly effective in sequence processing. They exhibit a unique capability to retain accurate memory of previous datasets due to their complete and progressive parameters [<xref ref-type="bibr" rid="ref_18">18</xref>]. <xref ref-type="fig" rid="fig_4">Figure 4</xref> illustrates an RNN's structure, comprising an input layer, hidden layer, and output layer, underscoring its utility in both text processing and image recognition.</p>
      
        <fig id="fig_4">
          <label>Figure 4</label>
          <caption>RNN model</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_LlD1yLgBgni5wxUI.png"/>
        </fig>
      
      <p>•LSTMs, a specialized form of sequence model, consist of three primary components: the forget gate, input gate, and output gate. These networks overcome the gradient vanishing problem commonly encountered in traditional RNN models. LSTMs' innovative cellular structure allows for intelligent decision-making regarding data retention or disposal. The model sequentially analyzes data, making crucial decisions about which information to preserve and which to discard [<xref ref-type="bibr" rid="ref_19">19</xref>], [<xref ref-type="bibr" rid="ref_20">20</xref>]. Eq. (1) outlines the control function governing the process.</p>
      
        <disp-formula>
          <label>(1)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <msup>
              <mi>f</mi>
              <mrow data-mjx-texclass="ORD">
                <mo stretchy="false">(</mo>
                <mo stretchy="false">)</mo>
                <mi>t</mi>
              </mrow>
            </msup>
            <mo>=</mo>
            <mi>σ</mi>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">(</mo>
              <mo>+</mo>
              <mo data-mjx-texclass="CLOSE">)</mo>
              <msub>
                <mi>w</mi>
                <mi>f</mi>
              </msub>
              <msub>
                <mi>b</mi>
                <mi>f</mi>
              </msub>
              <mrow data-mjx-texclass="INNER">
                <mo data-mjx-texclass="OPEN">[</mo>
                <mo>,</mo>
                <mo data-mjx-texclass="CLOSE">]</mo>
                <msup>
                  <mi>h</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mo stretchy="false">(</mo>
                    <mo>−</mo>
                    <mo stretchy="false">)</mo>
                    <mi>t</mi>
                    <mn>1</mn>
                  </mrow>
                </msup>
                <msup>
                  <mi>x</mi>
                  <mi>t</mi>
                </msup>
              </mrow>
            </mrow>
          </math>
        </disp-formula>
      
      <p> where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>w</mi>
      <mi>f</mi>
    </msub>
  </math>
</inline-formula> and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>b</mi>
      <mi>f</mi>
    </msub>
  </math>
</inline-formula> represent the weight and bias of the forget gate, respectively. The determination of the necessity and extent of alterations to historical data is a crucial step. Such data is selectively fed into the input gate, as delineated in Eqs. (2) to (4). </p>
      
        <disp-formula>
          <label>(2)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <msup>
              <mi>i</mi>
              <mi>t</mi>
            </msup>
            <mo>=</mo>
            <mi>σ</mi>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">(</mo>
              <mo>+</mo>
              <mo data-mjx-texclass="CLOSE">)</mo>
              <msub>
                <mi>w</mi>
                <mi>i</mi>
              </msub>
              <msub>
                <mi>b</mi>
                <mi>i</mi>
              </msub>
              <mrow data-mjx-texclass="INNER">
                <mo data-mjx-texclass="OPEN">[</mo>
                <mo>,</mo>
                <mo data-mjx-texclass="CLOSE">]</mo>
                <msup>
                  <mi>h</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mo stretchy="false">(</mo>
                    <mo>−</mo>
                    <mo stretchy="false">)</mo>
                    <mi>t</mi>
                    <mn>1</mn>
                  </mrow>
                </msup>
                <msup>
                  <mi>x</mi>
                  <mi>t</mi>
                </msup>
              </mrow>
            </mrow>
          </math>
        </disp-formula>
      
      
        <disp-formula>
          <label>(3)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <msup>
              <mi>c</mi>
              <mi>t</mi>
            </msup>
            <mo>=</mo>
            <mi>σ</mi>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">(</mo>
              <mo>+</mo>
              <mo data-mjx-texclass="CLOSE">)</mo>
              <msub>
                <mi>w</mi>
                <mi>c</mi>
              </msub>
              <msub>
                <mi>b</mi>
                <mi>c</mi>
              </msub>
              <mrow data-mjx-texclass="INNER">
                <mo data-mjx-texclass="OPEN">[</mo>
                <mo>,</mo>
                <mo data-mjx-texclass="CLOSE">]</mo>
                <msup>
                  <mi>h</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mo stretchy="false">(</mo>
                    <mo>−</mo>
                    <mo stretchy="false">)</mo>
                    <mi>t</mi>
                    <mn>1</mn>
                  </mrow>
                </msup>
                <msup>
                  <mi>x</mi>
                  <mi>t</mi>
                </msup>
              </mrow>
            </mrow>
          </math>
        </disp-formula>
      
      
        <disp-formula>
          <label>(4)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <msup>
              <mi>C</mi>
              <mi>t</mi>
            </msup>
            <msup>
              <mi>i</mi>
              <mi>t</mi>
            </msup>
            <msup>
              <mi>c</mi>
              <mi>t</mi>
            </msup>
            <msup>
              <mi>f</mi>
              <mrow data-mjx-texclass="ORD">
                <mo stretchy="false">(</mo>
                <mo stretchy="false">)</mo>
                <mi>t</mi>
              </mrow>
            </msup>
            <msup>
              <mi>C</mi>
              <mrow data-mjx-texclass="ORD">
                <mi>t</mi>
                <mo>−</mo>
                <mn>1</mn>
              </mrow>
            </msup>
            <mo>=</mo>
            <mo>∗</mo>
            <mo>+</mo>
            <mo>∗</mo>
          </math>
        </disp-formula>
      
      <p> where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>w</mi>
      <mi>i</mi>
    </msub>
  </math>
</inline-formula> and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>w</mi>
      <mi>c</mi>
    </msub>
  </math>
</inline-formula>, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>b</mi>
      <mi>i</mi>
    </msub>
  </math>
</inline-formula> and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>b</mi>
      <mi>c</mi>
    </msub>
  </math>
</inline-formula> denote the respective weights and biases integral to the process. The value of the current cell state is represented by <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msup>
      <mi>C</mi>
      <mi>t</mi>
    </msup>
  </math>
</inline-formula>. The role of the output gate is then to adjudicate whether the information, post-initial filtration by the preceding gates, is suitable for output. This gate encompasses internal switches that regulate the output mechanism, as shown in Eq. (5).</p>
      
        <disp-formula>
          <label>(5)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <msup>
              <mi>σ</mi>
              <mi>t</mi>
            </msup>
            <mo>=</mo>
            <mi>σ</mi>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">(</mo>
              <mo>+</mo>
              <mo data-mjx-texclass="CLOSE">)</mo>
              <msub>
                <mi>w</mi>
                <mi>o</mi>
              </msub>
              <msub>
                <mi>b</mi>
                <mi>o</mi>
              </msub>
              <mrow data-mjx-texclass="INNER">
                <mo data-mjx-texclass="OPEN">[</mo>
                <mo>,</mo>
                <mo data-mjx-texclass="CLOSE">]</mo>
                <msup>
                  <mi>h</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mo stretchy="false">(</mo>
                    <mo>−</mo>
                    <mo stretchy="false">)</mo>
                    <mi>t</mi>
                    <mn>1</mn>
                  </mrow>
                </msup>
                <msup>
                  <mi>x</mi>
                  <mi>t</mi>
                </msup>
              </mrow>
            </mrow>
          </math>
        </disp-formula>
      
      <p> where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>w</mi>
      <mi>o</mi>
    </msub>
  </math>
</inline-formula> and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>b</mi>
      <mi>o</mi>
    </msub>
  </math>
</inline-formula> are the parameters representing the weights and biases of the output gates, respectively.</p><p>In natural language processing applications, the Bidirectional LSTMs (BiLSTMs), which contain both forward and backward LSTM units, demonstrates the importance of contextual content in the representation of words [<xref ref-type="bibr" rid="ref_21">21</xref>]. Unlike standard LSTMs that solely utilize forward processing, BiLSTMs account for word context, highlighting the significance of bidirectional analysis.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>5. Text readability evaluation model using text cnn</title>
      <p>In the construction of the text readability evaluation model, datasets comprising textual information are employed. These datasets serve as the foundation for training the model, after which new textual data is introduced for the purpose of evaluating the model's efficacy [<xref ref-type="bibr" rid="ref_22">22</xref>], [<xref ref-type="bibr" rid="ref_23">23</xref>]. The model is initially trained with a portion of the text, which has been pre-labeled for this purpose. The text data utilized for labeling is denoted as <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>D</mi>
    <mi>d</mi>
    <mi>l</mi>
    <mi>d</mi>
    <mi>l</mi>
    <mi>d</mi>
    <mi>n</mi>
    <mi>l</mi>
    <mi>n</mi>
    <mo>=</mo>
    <mo fence="false" stretchy="false">{</mo>
    <mo stretchy="false">(</mo>
    <mo>,</mo>
    <mo stretchy="false">)</mo>
    <mo>,</mo>
    <mo stretchy="false">(</mo>
    <mo>,</mo>
    <mo stretchy="false">)</mo>
    <mo>…</mo>
    <mo>,</mo>
    <mo stretchy="false">(</mo>
    <mo>,</mo>
    <mo stretchy="false">)</mo>
    <mo fence="false" stretchy="false">}</mo>
    <mn>1</mn>
    <mn>1</mn>
    <mn>2</mn>
    <mn>2</mn>
  </math>
</inline-formula>, representing the text dataset, with $n<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>i</mi>
    <mi>n</mi>
    <mi>d</mi>
    <mi>i</mi>
    <mi>c</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>g</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>u</mi>
    <mi>m</mi>
    <mi>b</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>o</mi>
    <mi>f</mi>
    <mi>l</mi>
    <mi>a</mi>
    <mi>b</mi>
    <mi>e</mi>
    <mi>l</mi>
    <mi>e</mi>
    <mi>d</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>x</mi>
    <mi>t</mi>
    <mi>s</mi>
    <mo>.</mo>
  </math>
</inline-formula>l i \in\{G 1, G 2, \ldots, G m\}<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>s</mi>
    <mi>i</mi>
    <mi>g</mi>
    <mi>n</mi>
    <mi>i</mi>
    <mi>f</mi>
    <mi>i</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>x</mi>
    <mi>t</mi>
    <mi>r</mi>
    <mi>e</mi>
    <mi>a</mi>
    <mi>d</mi>
    <mi>a</mi>
    <mi>b</mi>
    <mi>i</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>t</mi>
    <mi>y</mi>
    <mi>g</mi>
    <mi>r</mi>
    <mi>a</mi>
    <mi>d</mi>
    <mi>e</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>d</mi>
    <mo>,</mo>
  </math>
</inline-formula>m$ represents the corresponding readability grade. The model's structure is bifurcated, aligning with its theoretical underpinnings. One segment encompasses the dataset used for model training, while the other pertains to the dataset under evaluation. The labeled data facilitates the extraction of text features, which are subsequently instrumental in assessing the text. <xref ref-type="fig" rid="fig_5">Figure 5</xref> offers a schematic depiction of the model's structure.</p>
      
        <fig id="fig_5">
          <label>Figure 5</label>
          <caption>Schematic diagram of the text evaluation model structure</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_cwwljgBwgDujF3gn.png"/>
        </fig>
      
      <p>As illustrated in <xref ref-type="fig" rid="fig_5">Figure 5</xref>, the model comprises three primary components: the text input layer, the text feature extraction layer, and the text classifier. These components collectively facilitate the transition from text dataset to readability evaluation. Additionally, references [<xref ref-type="bibr" rid="ref_24">24</xref>], [<xref ref-type="bibr" rid="ref_25">25</xref>], [<xref ref-type="bibr" rid="ref_26">26</xref>], [<xref ref-type="bibr" rid="ref_27">27</xref>], [<xref ref-type="bibr" rid="ref_28">28</xref>], [<xref ref-type="bibr" rid="ref_29">29</xref>], [<xref ref-type="bibr" rid="ref_30">30</xref>], [<xref ref-type="bibr" rid="ref_31">31</xref>], [<xref ref-type="bibr" rid="ref_32">32</xref>], [<xref ref-type="bibr" rid="ref_33">33</xref>] present further deep learning models pertinent to image detection, providing a broader context to the methodologies employed.</p><p>The process of text representation, crucial for text readability assessment, involves the generation of word vectors. These vectors, representing words numerically, are embedded within a vector space. The subsequent stages involve the extraction and training of features, with deep learning algorithms facilitating the training of words across multiple dimensions. This method allows for the representation of textual input, where the proximity of word vectors signifies similarity in meaning [<xref ref-type="bibr" rid="ref_34">34</xref>]. The model for text feature extraction, grounded in CNN, employs convolution kernels of varying sizes to train word data from the input layer, thus extracting word relationships.</p><p>In the text classifier component, words are categorized using a SoftMax layer, which applies logistic regression for classification. Logistic regression, under the assumption of a Bernoulli distribution for the data, utilizes gradient descent for optimizing parameters through maximum likelihood estimation [<xref ref-type="bibr" rid="ref_35">35</xref>], [<xref ref-type="bibr" rid="ref_36">36</xref>]. Post-classification, the categorized word vectors are then fed into the fully connected layer. Regarding dataset preparation, the required text readability dataset is derived from undergraduate English textbooks, following the procedure illustrated in <xref ref-type="fig" rid="fig_6">Figure 6</xref>.</p>
      
        <fig id="fig_6">
          <label>Figure 6</label>
          <caption>Data collection process</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_4tOTHA2tv9UNW5Q3.png"/>
        </fig>
      
      <p>The process begins with the segmentation of college textbooks into sections using a book cutter. High-speed scanning is then applied to each page. Subsequently, text recognition interfaces identify the textual content within the instructional materials. Under expert supervision, the scanned content undergoes rigorous examination. Post-evaluation, any repetitive scanned words are removed using Excel, followed by updating the text database [<xref ref-type="bibr" rid="ref_37">37</xref>].</p><p>For sample balance, three textbooks are selected, categorized into three levels based on difficulty: Level 1 (easiest), Level 2 (moderately difficult), and Level 3 (most difficult) [<xref ref-type="bibr" rid="ref_38">38</xref>], [<xref ref-type="bibr" rid="ref_39">39</xref>], [<xref ref-type="bibr" rid="ref_40">40</xref>]. To maintain consistency, the extraction of sentences is limited to 1,000 per level. <xref ref-type="table" rid="table_1">Table 1</xref> provides a breakdown of the sentence count across these levels.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>Number of sentences in full dataset</caption>
          <abstract/>
          <table><tr><td text-center>Level</td><td text-center>Number of Sentences</td></tr><tr><td text-center>Level 1</td><td text-center>999</td></tr><tr><td text-center>Level 2</td><td text-center>998</td></tr><tr><td text-center>Level 3</td><td text-center>999</td></tr></table>
        </table-wrap>
      
      <p>In the hyperparameter configuration for model training, 25% of the sample count is allocated for labeling, with the remaining 75% forming the training set. <xref ref-type="table" rid="table_2">Table 2</xref> outlines the hyperparameter settings for the text CNN model. A batch size of 64 is selected for each training iteration, incorporating a dropout rate of 0.5. Convolution kernel sizes are set at three, four, and five, with an established learning rate of 0.001.</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>Hyperparameter settings</caption>
          <abstract/>
          <table><tr><td text-center>Hyperparameter</td><td text-center>Value</td></tr><tr><td text-center>Batch size</td><td text-center>64</td></tr><tr><td text-center>Dropout</td><td text-center>0.5</td></tr><tr><td text-center>Kernel size</td><td text-center>3,4,5</td></tr><tr><td text-center>Learning rate</td><td text-center>0.001</td></tr></table>
        </table-wrap>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>6. Results and discussion</title>
      <p>The implemented deep learning-based model for text readability assessment has demonstrated an impressive accuracy rate of 82%. <xref ref-type="table" rid="table_3">Table 3</xref> illustrates the model's ability to classify sentences into distinct complexity levels following training.</p>
      
        <table-wrap id="table_3">
          <label>Table 3</label>
          <caption>Sentence readability evaluation results</caption>
          <abstract/>
          <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span>Text Type</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>Readability Levels</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>She consumes breakfast every single day.</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span>Level 1</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>Frank is currently working at a hotel in Yorkshire, which is regarded as one of the coldest places in the UK.</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span> </span></p><p style="text-align: center"><span>Level 2</span></p></td></tr><tr><td colspan="1" rowspan="1"><p><span>More than 3,000 native plant species, or more than 10% of the roughly 25,000 species in the United States, may go extinct in the next ten years, according to botanists.</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span> </span></p><p style="text-align: center"><span> </span></p><p style="text-align: center"><span>Level 3</span></p></td></tr></tbody></table>
        </table-wrap>
      
      <p>These results indicate that the model can accurately categorize textbook phrases into Level 1, Level 2, and Level 3. Previously, the focus was on identifying the difficulty level of sentences. In the evaluation of a complete text, Level 1, Level 2, and Level 3 sentences are assigned different weights, leading to Scores 1, 2, and 3, respectively. The lower the score, the simpler the text is to comprehend, with Score 1 indicating the easiest level. Conversely, a higher score, such as Score 3, suggests greater complexity. <xref ref-type="fig" rid="fig_7">Figure 7</xref> displays the outcomes of the model's evaluation of text readability.</p>
      
        <fig id="fig_7">
          <label>Figure 7</label>
          <caption>Text readability model evaluation results</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_hrwauGpoSctiRdTg.png"/>
        </fig>
      
      <p>The model categorizes Texts 1, 2, and 3 into Level 1, Level 2, and Level 3 complexities, respectively (<xref ref-type="fig" rid="fig_7">Figure 7</xref>). This classification underscores the feasibility and accuracy of the proposed approach. Such findings can be effectively employed in educational settings, allowing for tailored instruction that accounts for individual student capabilities. This approach is beneficial for student development. Moreover, as illustrated in <xref ref-type="fig" rid="fig_8">Figure 8</xref>, the model can classify students' English proficiency levels, with 12% fitting into Level 1, 31% into Level 2, and 57% into Level 3.</p>
      
        <fig id="fig_8">
          <label>Figure 8</label>
          <caption>Distribution of students' English proficiency levels</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_LnAJZsEAupmtCJWG.png"/>
        </fig>
      
      <p>Additionally, a control group experiment involving homework assignments was conducted to validate the model's impact on university teaching. The experimental group utilized the DL-based text readability evaluation method, while the control group did not. It was observed that texts classified as Level 1 and Level 2 received more correct responses compared to Level 3, which had the least. This correlation between reading test scores and text readability levels confirms the validity of the DL-based text readability evaluation method from a scientific perspective. The empirical evidence supports the effectiveness of this approach in enhancing college English instruction.</p>
    </sec>
    <sec disp-level="level1" sec-type="conclusions">
      <title>7. Conclusions</title>
      <p>This study has explored a deep learning-based methodology for evaluating text readability within the context of collegiate English education. A text CNN evaluation model was developed and demonstrated an accuracy rate of 72%. The findings suggest that further exploration into more sophisticated models, such as LSTM, BiLSTM, Transformer, and BERT, is warranted to enhance the accuracy of text readability evaluations.</p><p>A limitation of this study is identified in the small dataset size, which may not comprehensively represent the diversity of texts encountered in college English courses. This limitation potentially affects the model's ability to accurately assess a broader spectrum of educational materials. Additionally, the study's small sample size could impact the validity of the findings and may not reflect the full range of texts utilized in higher education settings. Moreover, the model's parameter settings are based on pre-existing knowledge, which introduces the risk of bias or suboptimal performance.</p><p>The research underscores the need for future investigations into advanced models like LSTM, BiLSTM, Transformer, and BERT. Such research endeavors would aim to address the current study's limitations and further refine the assessment of text readability, particularly in the context of college English instruction. The enhancement of text readability assessment methods could significantly contribute to the development of more effective teaching strategies, benefiting both educators and students in higher education.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p></p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <given-names>D.</given-names>
              <surname>Crystal</surname>
            </name>
          </person-group>
          <article-title/>
          <source>English as a Global Language</source>
          <publisher-loc/>
          <publisher-name>Cambridge University Press, Cambridge, UK</publisher-name>
          <year>2003</year>
          <volume/>
          <issue/>
          <page-range/>
          <fpage/>
          <lpage/>
          <pub-id/>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="report">
          <article-title>English as a medium of instruction-A growing global phenomenon</article-title>
          <source>, undefined</source>
          <year>2014</year>
          <publisher-name>British Council, UK</publisher-name>
          <publisher-loc/>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>18</volume>
          <page-range>6-22</page-range>
          <issue>1</issue>
          <year>2013</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Marginson</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1177/1028315313513036</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Student self-formation in international education</article-title>
          <source>J. Stud. Int. Educ.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>1-43</page-range>
          <issue>1</issue>
          <year>2008</year>
          <person-group person-group-type="author">
            <name>
              <given-names>R.</given-names>
              <surname>Phillipson</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/15427580701696886</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>The linguistic imperialism of neoliberal empire</article-title>
          <source>Crit. Inquiry Lang. Stud.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>404</volume>
          <page-range>132306</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A.</given-names>
              <surname>Sherstinsky</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.physd.2019.132306</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) network</article-title>
          <source>Physica D</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="thesis">
          <person-group person-group-type="author">
            <name>
              <given-names>F.</given-names>
              <surname>Gers</surname>
            </name>
          </person-group>
          <article-title>Long short-term memory in recurrent neural networks</article-title>
          <source>, undefined</source>
          <year>2001</year>
          <publisher-name>Universität Hannover, Hannover, Germany</publisher-name>
          <publisher-loc/>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range>arXiv:2106.07935</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J. M.</given-names>
              <surname>Imperial</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2106.07935</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>BERT embeddings for automatic readability assessment</article-title>
          <source>ArXiv</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>2022</volume>
          <page-range>1-9</page-range>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>L.</given-names>
              <surname>Jian</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Xiang</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Le</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2022/6984586</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>English text readability measurement based on convolutional neural network: A hybrid network model</article-title>
          <source>Comput. Intell. Neurosci.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>520-540</page-range>
          <issue>3</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>H.</given-names>
              <surname>Butt</surname>
            </name>
            <name>
              <given-names>M. R.</given-names>
              <surname>Raza</surname>
            </name>
            <name>
              <given-names>M. J.</given-names>
              <surname>Ramzan</surname>
            </name>
            <name>
              <given-names>M. J.</given-names>
              <surname>Ali</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Haris</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/forecast3030033</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Attention-based CNN-RNN Arabic text recognition from natural scene images</article-title>
          <source>Forecasting</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>483</page-range>
          <issue>3</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>N. C.</given-names>
              <surname>Dang</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>María Moreno-García</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Prieta</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/electronics9030483</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Sentiment analysis based on deep learning: A comparative study</article-title>
          <source>Electronics</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>40</volume>
          <page-range>7147-7158</page-range>
          <issue>4</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>L.</given-names>
              <surname>Diao</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Hu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3233/jifs-189543</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Deep learning and multimodal target recognition of complex and ambiguous words in automated English learning system</article-title>
          <source>J. Intell. Fuzzy Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>40</volume>
          <page-range>7097-7108</page-range>
          <issue>4</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Y.</given-names>
              <surname>Han</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3233/jifs-189539</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Evaluation of English online teaching based on remote supervision algorithms and deep learning</article-title>
          <source>J. Intell. Fuzzy Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>78</volume>
          <page-range>6296-6317</page-range>
          <issue>5</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Y.</given-names>
              <surname>Li</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11227-021-04130-7</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Construction of Internet of Things English terms model and analysis of language features via deep learning</article-title>
          <source>J. Supercomput.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>2021</volume>
          <page-range>440-446</page-range>
          <issue>6</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>R.</given-names>
              <surname>Zhang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>Construction method of network teaching resources based on deep learning</article-title>
          <source>Converter</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>118</volume>
          <page-range>115-119</page-range>
          <issue>8</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Kasamatsu</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Murakami</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Kengo</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>Examination of estimation accuracy of deep learning by data augmentation</article-title>
          <source>IEICE Tech. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>1-13</page-range>
          <issue>2</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.O.A.</given-names>
              <surname>Elhassan</surname>
            </name>
            <name>
              <given-names>A.S.</given-names>
              <surname>Muhamad</surname>
            </name>
            <name>
              <given-names>I.H.A.</given-names>
              <surname>Tharbe</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>The effects of surface and deep learning strategies on academic achievement in English among high school students: Do implicit beliefs of intelligence matter?</article-title>
          <source>J. Couns. Psychol.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>39-58</page-range>
          <issue>4</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>K.S.</given-names>
              <surname>Lee</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.16875/stem.2021.22.4.39</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A study of STEAM model development and assessment method for deep learning: Through the voice of mimesis and brontë</article-title>
          <source>J. Eng. Teach. Movie Media.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>880</page-range>
          <issue>7</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Z.</given-names>
              <surname>Xu</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Shi</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.17507/tpls.0807.21</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Application of constructivist theory in flipped classroom — Take college English teaching as a case study</article-title>
          <source>Theory Pract. Lang. Stud.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>1-74</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>L.</given-names>
              <surname>Alzubaidi</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Amjad Humaidi</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Al-Dujaili</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Duan</surname>
            </name>
            <name>
              <given-names>O.</given-names>
              <surname>Al-Shamma</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Santamaría</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Mohammed Fadhel</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Al-Amidie</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Farhan</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s40537-021-00444-8</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions</article-title>
          <source>J. Big Data</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>593-599</page-range>
          <issue>4</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Ram</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Gupta</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Agarwal</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/09720510.2018.1471264</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Devanagri character recognition model using deep convolution neural network</article-title>
          <source>J. Stat. Manage. Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>133-147</page-range>
          <issue>2</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M. P.</given-names>
              <surname>Sari</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.24071/ijiet.v5i2.3245</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>An evaluation of English program: A deep analysis of EFL learners’attitude towards English program</article-title>
          <source>Int. J. Inf. Educ.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range/>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>D.K.</given-names>
              <surname>Renuka</surname>
            </name>
            <name>
              <given-names>C.A.</given-names>
              <surname>Devi</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1701 0/ijcs/2020/v5/i4-5/l 54783</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Affective model based speech emotion recognition using deep learning techniques</article-title>
          <source>Indian J. Comput. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>73-78</page-range>
          <issue>5</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.S.</given-names>
              <surname>Ali</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>Problem based learning: A student-centered approach</article-title>
          <source>English Lang. Teach.</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>130-138</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Saqlain</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/jimd020303</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Sustainable hydrogen production: A decision-making approach using VIKOR and intuitionistic hypersoft sets</article-title>
          <source>J. Intell. Manag. Decis.</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>136-147</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>H. B. U.</given-names>
              <surname>Haq</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Saqlain</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/jii010301</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Iris detection for attendance monitoring in educational institutes amidst a pandemic: A machine learning approach</article-title>
          <source>J. Ind. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>32</volume>
          <page-range>317-329</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Saqlain</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Sana</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Jafar</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Saeed</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Said</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>Single and multi-valued neutrosophic hypersoft set and tangent similarity measure of single valued neutrosophic hypersoft sets</article-title>
          <source>Neutrosophic Sets Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>36</volume>
          <page-range>389-399</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Saqlain</surname>
            </name>
            <name>
              <given-names>X. L.</given-names>
              <surname>Xin</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.5281/ZENODO.4065475</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Interval valued, m-polar and m-polar interval valued neutrosophic hypersoft sets</article-title>
          <source>Neutrosophic Sets Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <volume>2021</volume>
          <page-range>449-470</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Saqlain</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Saeed</surname>
            </name>
            <name>
              <given-names>R. M.</given-names>
              <surname>Zulqarnain</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Moin</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-030-57197-9_21</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Neutrosophic hypersoft matrix theory: Its definition, operators, and application in decision-making of personnel selection problem</article-title>
          <source>J. Oper. Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>30803-30816</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Saqlain</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Riaz</surname>
            </name>
            <name>
              <given-names>M.A.</given-names>
              <surname>Saleem</surname>
            </name>
            <name>
              <given-names>M.S.</given-names>
              <surname>Yang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Distance and similarity measures for neutrosophic hypersoft set (NHSS) with construction of NHSS-TOPSIS and applications</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>1-14</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>H. B. U.</given-names>
              <surname>Haq</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Saqlain</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.31181/taci1120232</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>An implementation of effective machine learning approaches to perform Sybil Attack Detection (SAD) in IoT network</article-title>
          <source>Theor. Appl. Comput. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>73-82</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M. N.</given-names>
              <surname>Jafar</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Muniba</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Saqlain</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.31181/sems1120238u</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Enhancing diabetes diagnosis through an intuitionistic fuzzy soft matrices-based algorithm</article-title>
          <source>Spectr. Eng. Manage. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>1-12</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Abid</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Saqlain</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.31181/sems1120235a</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Decision-making for the bakery product transportation using linear programming</article-title>
          <source>Spec. Eng. Manage. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <volume>20</volume>
          <page-range>1-7</page-range>
          <issue>1</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>D. G.</given-names>
              <surname>Weldemariam</surname>
            </name>
            <name>
              <given-names>N.D.</given-names>
              <surname>Amaha</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Abdu</surname>
            </name>
            <name>
              <given-names>E. H.</given-names>
              <surname>Tesfamariam</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s12913-020-05418-9</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Assessment of completeness and legibility of handwritten prescriptions in six community chain pharmacies of Asmara, Eritrea: A cross-sectional study</article-title>
          <source>BMC Health Serv. Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>374-381</page-range>
          <issue>2</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>K.</given-names>
              <surname>Kusunose</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Abe</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Haga</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Fukuda</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Yamada</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Harada</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Sata</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.jcmg.2019.02.024</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A deep learning approach for assessment of regional wall motion abnormality from echocardiographic Images</article-title>
          <source>JACC Cardiovasc. Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_35">
        <label>35.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>33-46</page-range>
          <issue>1</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>W.</given-names>
              <surname>Souma</surname>
            </name>
            <name>
              <given-names>I.</given-names>
              <surname>Vodenska</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Aoyama</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s42001-019-00035-x</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Enhanced news sentiment analysis using deep learning methods</article-title>
          <source>J. Comput. Soc. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_36">
        <label>36.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>35-60</page-range>
          <issue>1</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Y.</given-names>
              <surname>Su</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Hu</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Carolyn Rosé</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11412-018-9269-y</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Exploring college English language learners’ self and social regulation of learning during wiki-supported collaborative reading activities</article-title>
          <source>Int. J. Comput. Support Collab. Learn.</source>
        </element-citation>
      </ref>
      <ref id="ref_37">
        <label>37.</label>
        <element-citation publication-type="journal">
          <volume>64</volume>
          <page-range>551-568</page-range>
          <issue>4</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>F.</given-names>
              <surname>Teng</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>Tertiary-level students’ English writing performance and metacognitive awareness: A group metacognitive support perspective</article-title>
          <source>Scand. J. Educ. Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_38">
        <label>38.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>3930-3935</page-range>
          <issue>10</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J.</given-names>
              <surname>Zhao</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Sun</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Zhu</surname>
            </name>
            <name>
              <given-names>J. E.</given-names>
              <surname>Antonio-Lopez</surname>
            </name>
            <name>
              <given-names>R. A.</given-names>
              <surname>Correa</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Pang</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Schülzgen</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1021/acsphotonics.8b00832</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Deep learning imaging through fully-flexible glass-air disordered fiber</article-title>
          <source>ACS Photonics</source>
        </element-citation>
      </ref>
      <ref id="ref_39">
        <label>39.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>61-78</page-range>
          <issue>7</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>B.</given-names>
              <surname>Mandasari</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Oktaviani</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>English language learning strategies: An exploratory study of management and engineering students</article-title>
          <source>Premise J. Eng. Linguist.</source>
        </element-citation>
      </ref>
      <ref id="ref_40">
        <label>40.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>6173-6177</page-range>
          <issue>11B</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>X.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>He</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Zhen</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Thompson</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>An empirical study of production-oriented approach in college English writing teaching</article-title>
          <source>Univers. J. Educ. Res.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>