<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IDA</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Information Dynamics and Applications</journal-title>
        <abbrev-journal-title abbrev-type="issn">Inf. Dyn. Appl.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IDA</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-1494</issn>
      <issn publication-format="print">2958-1486</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-TQdG_2Sl7VCrzFl92-J-iqDB5AolPQBN</article-id>
      <article-id pub-id-type="doi">10.56578/ida020105</article-id>
      <title-group>
        <article-title>A Deep Convolutional Neural Network Framework for Enhancing Brain Tumor Diagnosis on MRI Scans</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Bodapati</surname>
            <given-names>Jyostna Devi</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5185-882X</contrib-id>
          <email>jyostna.bodapati82@gmail.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Ahmed</surname>
            <given-names>Shaik Feroz</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0008-9921-4370</contrib-id>
          <email>ahmedshaik.0862@gmail.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Chowdary</surname>
            <given-names>Yarra Yashwant</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-0495-3135</contrib-id>
          <email>yarrayashwanth991@gmail.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Sekhar</surname>
            <given-names>Konda Raja</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0006-3337-2398</contrib-id>
          <email>rajasekhar.konda@gmail.com</email>
        </contrib>
        <aff id="1">Department of Advanced Computer Science and Engineering, Vignan’s Foundation for Science Technology and Research, 522213 Guntur, India</aff>
      </contrib-group>
      <year>2023</year>
      <volume>2</volume>
      <issue>1</issue>
      <fpage>42</fpage>
      <lpage>50</lpage>
      <page-range>42-50</page-range>
      <history>
        <date date-type="received">
          <month>02</month>
          <day>02</day>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <month>03</month>
          <day>08</day>
          <year>2023</year>
        </date>
        <date date-type="pub">
          <month>03</month>
          <day>30</day>
          <year>2023</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2023 by the authors</copyright-statement>
        <copyright-year>2023</copyright-year>
        <license>. Licensee Acadlore Publishing Services Limited, Hong Kong. This article can be downloaded for free, and reused and quoted with a citation of the original published version, under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 license</ext-link>.</license>
      </permissions>
      <abstract><p>Brain tumors are a critical public health concern, often resulting in limited life expectancy for patients. Accurate diagnosis of brain tumors is crucial to develop effective treatment strategies and improve patients' quality of life. Computer-aided diagnosis (CAD) systems that accurately classify tumor images have been challenging to develop. Deep convolutional neural network (DCNN) models have shown significant potential for tumor detection, and outperform traditional deep neural network models. In this study, a novel framework based on two pre-trained deep convolutional architectures (VGG16 and EfficientNetB0) is proposed for classifying different types of brain tumors, including meningioma, glioma, and pituitary tumors. Features are extracted from MR images using each architecture and merged before feeding them into machine learning algorithms for tumor classification. The proposed approach achieves a training accuracy of 98% and a test accuracy of 99% on the brain-tumor-classification-mri dataset available on Kaggle and btc_navoneel. The model shows promise to improve the accuracy and generalizability of medical image classification for better clinical decision support, ultimately leading to improved patient outcomes.</p></abstract>
      <kwd-group>
        <kwd>Brain tumors</kwd>
        <kwd>Medical image classification</kwd>
        <kwd>Deep convolutional neural network</kwd>
        <kwd>Computer-aided diagnosis</kwd>
        <kwd>MRI scans</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors">4</count>
        <fig-count>5</fig-count>
        <table-count>3</table-count>
        <ref-count>21</ref-count>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec disp-level="level1" sec-type="intro">
      <title>1. Introduction</title>
      <p>The human brain controls all the body's functions through billions of cells, making it one of the most vital organs. Brain tumors can develop when there is an abnormal growth of cells due to uncontrolled cell division, with glioma, meningioma, and pituitary tumors being the most common types [<xref ref-type="bibr" rid="ref_1">1</xref>]. Early detection of brain tumors is crucial to improve the survival chances of the affected individual. Magnetic Resonance Imaging (MRI) is a non-invasive imaging method that provides a comprehensive view of the brain's anatomy and tissue composition, making it the preferred method for detecting brain tumors over CT scans [<xref ref-type="bibr" rid="ref_2">2</xref>]. Accurate classification of brain tumors using MRI is essential for effective treatment planning.</p><p>Deep learning models have shown superior performance compared to other conventional machine learning techniques in various real-time tasks related to computer vision [<xref ref-type="bibr" rid="ref_3">3</xref>] and natural language processing [<xref ref-type="bibr" rid="ref_4">4</xref>]. For instance, deep neural network models have achieved state-of-the-art results in tasks such as facial emotion recognition in the vision domain and sentiment analysis in the natural language domain [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>]. These models have also been successful in disease prediction tasks such as diabetic retinopathy, pneumonia detection, and Covid-19 prediction in the healthcare domain [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>]. Deep Neural networks are designed to function similarly to neurons in the human brain, allowing for reliable classification results with appropriate weights and layers. As a result, deep learning-based approaches are preferred over other methods for classifying different types of brain tumors for effective treatment.</p><p>Upon reviewing existing literature on brain tumor classification (BTC) using MRI, it was found that neural network models have been widely employed by researchers for training MR image data [<xref ref-type="bibr" rid="ref_10">10</xref>]. Researchers have utilized various approaches based on the resources available, such as convolutional neural networks, back propagation neural networks, and combinations of different models. The studies reviewed reported varying levels of accuracy, with the highest reported accuracy being 95% using a generative adversarial network (GAN). Other reported accuracies included 91.28% for a convolutional model that considered the region of interest (ROI), 91.1% for a back-propagation network, 90.26% for a convolutional model with five-fold-cross-validation, and 93.68% for a kernel extreme learning machines (KELM) network. These results demonstrate the effectiveness of neural network models in BTC using MRI and highlight the potential for further research in this area.</p><p>Accurate brain tumor image classification is crucial for effective clinical decision support, which can improve patient survival chances. While traditional handcrafted techniques for medical image classification have limitations, deep learning-based approaches have shown promise in improving accuracy. However, challenges such as inter-class similarity and intra-class dissimilarity across different medical imaging modalities and limited labelled data remain obstacles to effective medical image classification. Therefore, there is a need for novel deep learning-based models that can address these challenges and improve the accuracy and generalizability of medical image classification for better clinical decision support.</p><p>In this study, deep neural network models, specifically VGG16 and EfficientNetB0, were employed to classify brain tumor images using MRI. The brain-tumor classification MRI dataset from the Kaggle repository was used to train and test the models. With the VGG16 model, block-by-block feature extraction was performed on the data, and the resulting features were then trained using machine learning algorithms such as Support Vector Machine (SVM) and Random Forest model. To further improve accuracy, the use of different Kernels with SVM models was investigated. For the EfficientNetB0 model, dense layers were added and trained on the input data. In testing, the models achieved accuracy rates of 85% and 98% for the test data, respectively. These results demonstrate the effectiveness of using deep neural network models for BTC using MRI images and highlight the potential for further research in this area. The proposed model addresses these challenges and demonstrates promising results for tumor prediction, suggesting its potential for improving medical image classification and ultimately, clinical decision support.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>Magnetic Resonance Imaging (MRI) images illustrating different types of tumor</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/3/img_h7tB9TJrCWhP7sv3.png"/>
        </fig>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>2. Literature survey</title>
      <p>Several related studies were reviewed prior to conducting our experiments in brain tumor classification (BTC) using Magnetic Resonance Imaging (MRI). <span style="color: black">Sample MR images showing Glioma, Meningioma and Pituitary tumors are shown</span> in <xref ref-type="fig" rid="fig_1">Figure 1</xref>.</p><p>A categorization approach for brain tumors based on the increased area was proposed by Cheng et al. [<xref ref-type="bibr" rid="ref_11">11</xref>]. They utilized enhanced areas as a region of interest (ROI) to determine the types of tumors, dividing the enhanced areas into sub-regions using an adaptive spatial division approach to address the issue of lost geographical information. Three feature extraction methods, including bag-of-words (BOW), intensity histogram, and grey level co-occurrence matrix (GLCM), were employed to evaluate the efficacy of their model. Experimental results showed that their new strategy achieved an accuracy of 91.28%, outperforming earlier methods and demonstrating the stability of the approach.</p><p>Ismael and Abdel-Qader [<xref ref-type="bibr" rid="ref_12">12</xref>] proposed a model that utilized segmentation methods or manual identification based on radiologist recommendations to create ROIs, or tumor segments, for classifying brain tumors. The model combined the Gabor filter and two-dimensional (2D) discrete wavelet transform (DWT) approaches to extract high-quality statistical features that would enhance classification performance. Back-propagation neural network was employed to evaluate the impact of the selected features. The experimental results showed a robustness and usefulness in their model achieving an accuracy of 91.9%.</p><p>Tahir et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] presented a model that improved the classification performance of brain tumors by employing a combined pre-processing pipeline. The model categorized different pre-processing techniques into three groups: edge detection, noise reduction, and contrast enhancement. They created several potential combinations based on these techniques using different image datasets, and the classification pipeline received these various combinations. The model achieved an accuracy of 86%, demonstrating that using multiple pre-processing techniques together significantly improves classification accuracy compared to using a single one.</p><p>Paul et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] proposed a deep learning-based model for accurate classification of brain tumors. They incorporated Convolutional Neural Networks (CNNs) into their model, which improved the precision of classification. The model achieved an accuracy of 90.26% for brain tumor imaging after five times of cross-validation. The authors suggested that reducing image size may further enhance training results and assist medical professionals in treating patients.</p><p>CapsNet model, a capsule network for effective categorization of brain tumors, was developed by Afshar et al. [<xref ref-type="bibr" rid="ref_15">15</xref>]. The proposed network overcomes the limitations of earlier CNN-based classification models by leveraging the spatial relationships between the tumor and its surrounding tissues. By accessing and incorporating the tissues surrounding the tumor as additional input, the model enhances the classification accuracy [<xref ref-type="bibr" rid="ref_16">16</xref>].</p><p>Zhou et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] proposed a comprehensive methodology to increase classification accuracy for brain tumors. They used a Dense Convolutional Neural Network (DenseNet) to extract characteristics from axial slices of the images and then employed a Recurrent Neural Network (RNN) to categorize the features into different types of tumors. Their system achieved high accuracy of 92.13% without requiring human or automatic segmentation of areas, demonstrating the effectiveness of their approach.</p><p>In another study [<xref ref-type="bibr" rid="ref_18">18</xref>], a CNN-based model was developed for the categorization of brain tumors. The model extracted features using CNN and then utilized a kernel extreme learning machine (KELM) network for classification. The experimental results of this joint-based mechanism of CNN and KELM outperformed other conventional machine learning classifiers such as radial basis function neural network (RBFNN), k-nearest neighbor (KNN), support vector machine (SVM), and others, achieving a promising accuracy of 93.68%.</p><p>These studies demonstrate the effectiveness and potential of deep learning-based approaches for BTC using MRI images. However, challenges such as inter-class similarity and intra-class dissimilarity across different medical imaging modalities, as well as limited labeled data, remain obstacles to effective medical image classification. Therefore, novel deep learning-based models are required to address these challenges and improve the accuracy and generalizability of medical image classification for better clinical decision support.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>3. Proposed model</title>
      <p>Convolutional Neural Networks (CNNs) are a type of deep learning algorithm used for image recognition based on pixel analysis. CNNs contain various models with different hidden layers, convolutional layers, dense layers, normalization, and dropout layers, allowing selection of the optimal model for a given image classification task. Each layer in a CNN exchanges information with the others, increasing the accuracy of classification by identifying the class of the image. To train a CNN model, feature extraction can be achieved by selecting layers of the model and extracting features from one of the blocks. These extracted features can be added, averaged, concatenated, or taken as a maximum, and the resultant features are then given to the model to predict the output and classify the given image into the most suitable class.</p><p>Our study employed deep neural network models for classifying brain tumor images using Magnetic Resonance Imaging (MRI). These deep convolutional networks automatically extract robust features from input MR images, making them more effective than standard deep neural network models. The proposed approach uses representations extracted from two pre-trained deep convolutional architectures, VGG16 and EfficientNetB0, for classifying different types of brain tumors, including meningioma, glioma, and pituitary tumors. Features were extracted from MR images using each architecture, and they were merged before being fed into machine learning algorithms for tumor classification in our study.</p><p><xref ref-type="fig" rid="fig_2">Figure 2</xref> shows the schematic diagram of the proposed approach, which begins by importing and extracting images along with labels from the dataset. The dataset is then split into training, validation, and test sets. Pre-processing and augmentation methods are applied to the images of the dataset, and the resulting data is used to train and test the proposed deep neural network model. The hyper-parameter setup, regularization tactics, and optimization technique used in the training process are explained in the framework of the recommended method. Finally, the figure displays computations for network performance and training.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>Proposed model</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/3/img_ITRvuz-ief8FnpBx.png"/>
        </fig>
      
      
        <sec disp-level="level2">
          
            <title>3.1. Pre-processing</title>
          
          <p>A pre-processing phase is essential to prepare the images before inputting them into the proposed structure. The first step involves resizing all the images to 255x255x1 pixels. This is followed by reducing the size of each image to 224x224x1 pixels, which helps to reduce dimensionality, computation time, and improve the network's efficiency with simpler calculations. The dataset has already been split into training, validation, and test sets, so further separation is not required. The y_train and y_test data are then converted into categorical data before providing them to the model for feature extraction and further processing.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.2. Vgg16</title>
          
          <p>The VGG16 is a convolutional neural network (CNN) model with 16 layers that is widely recognized for its high performance and efficiency. Unlike other models with a large number of parameters, the VGG16 architecture focuses on using convolutional layers with a 3x3 kernel size. This model is particularly notable for its simplicity and freely available values, which can be downloaded for use in other systems and applications. The VGG16 model requires a three-channel input image with a minimum size of 224x224 pixels. Optimization algorithms are employed in the neural network to decide whether or not a neuron needs to be activated, by computing the weighted sum of the input. The kernel function is used to make the output neuron non-linear. The neurons in a neural network work together with their weight, bias, and related training method.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>Layer wise details of VGG16</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/3/img_K39HhwCydinQDUI2.png"/>
            </fig>
          
          <p>The connection weights of the neurons are changed based on the output accuracy. The input layer and activation function give non-linearity to the artificial neural input, allowing it to learn and perform complex tasks. In machine learning model training, features can be extracted from the output features of a given block by taking them from the layers of the VGG16 model. In this project, blocks 3, 4, and 5 were used to extract features, which were then subjected to mathematical operations like Max, Merge, and Average. Taking the Max of all 3 blocks involved selecting the maximum value at each index position in the np array, and then training machine learning models such as SVM classifier and Random Forest Classifier. Similarly, the average of the features from the 3 blocks was also taken, and machine learning models were trained. Finally, all the features of the 3 blocks were combined to obtain a new nparray, and accuracies were recorded for each model with different inputs. <xref ref-type="fig" rid="fig_3">Figure 3</xref> would be helpful in visualizing the VGG16 network topology and better understanding its architecture.</p><p>Efficient Net B0: EfficientNet is a family of convolutional neural networks designed to achieve state-of-the-art accuracy on image classification tasks while being computationally efficient. The architecture is based on a novel scaling method that uniformly scales all dimensions of depth, width, and resolution, allowing for better performance and efficiency trade-offs.</p><p>EfficientNet-B0 is the smallest and baseline model in the EfficientNet family, with 5.3 million parameters. Despite its small size, it achieves state-of-the-art accuracy on the ImageNet dataset, which contains over 1.2 million images across 1,000 categories. EfficientNet-B0 achieves this accuracy while being 8.4 times smaller and 6 times faster than the previous state-of-the-art model.</p><p>EfficientNet-B0 has been pre-trained on the ImageNet dataset and can be fine-tuned for various image classification tasks. The EfficientNet family also includes larger models, such as B1, B2, B3, B4, B5, and B6, which achieve higher accuracy at the cost of increased computational complexity. Layer-wise details of the EfficientNetB0 is presented in <xref ref-type="fig" rid="fig_4">Figure 4</xref>.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>EfficientNetB0 architecture</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/3/img_9Rg0vId1wh7j_sqm.png"/>
            </fig>
          
          <p>In this study, EfficientNetB0 was utilized as the backbone of our CNN model, with a GlobalMaxPooling layer added to extract the most relevant features from the model's output. Additionally, a Batch Normalization layer was incorporated to normalize the features and improve the convergence of the model. To prevent overfitting, a Dropout layer with a drop rate of 0.5 was added.</p><p>An output layer with the softmax activation function was also included to classify input images into different categories. The model was compiled with a categorical_crossentropy loss function and an ADAM optimizer, which is known for its efficiency and good performance.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.3. Utilizing pre-trained models for tumor image representation</title>
          
          <p>The features extracted from the three blocks were combined to obtain a new numpy array, and the accuracies were recorded for each model with different inputs. In our study, the VGG16 model was fine-tuned using the brain-tumor-classification-mri dataset to classify different categories of brain tumors, such as glioma tumor, meningioma tumor, no tumor, and pituitary tumor. To preprocess the dataset, all images were resized to 224x224 pixels, which is the expected input size for VGG16, and the data was normalized by dividing the pixel values by 255. A subset of the data was used to train the model, while the remaining data was used for validation to prevent overfitting. After feature extraction from the VGG16 model, machine learning algorithms such as SVM classifier and Random Forest Classifier were trained on the extracted features. To improve the performance of the machine learning models, features from different blocks were combined using mathematical operations such as Max, Merge, and Average. The brain-tumor-classification-mri dataset from Kaggle, which contains 3265 files of all four categories, was used as the source of input data. The dataset was divided into training and testing sets, and feature extraction [<xref ref-type="bibr" rid="ref_19">19</xref>] was performed on the model's layers to extract features. Features from blocks 3, 4, and 5 were utilized, and mathematical operations like Max, Merge, and Average were applied to them. The Max operation involved selecting the maximum value at each index position in the np array, and then machine learning models such as SVM classifier [<xref ref-type="bibr" rid="ref_20">20</xref>] and Random Forest Classifier [<xref ref-type="bibr" rid="ref_21">21</xref>] were trained. Similarly, the Average operation was applied to the features from the three blocks, and the machine learning models were trained on the resulting features.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.4. Proposed model for tumor classification</title>
          
          <p>In image classification with CNN models, the number of hidden layers, convolutional layers, dense layers, normalization, and dropout layers are adjusted to achieve the best possible accuracy for the model. Each layer in a CNN exchanges information with one another to increase the accuracy of image classification. Feature extraction, a common method used in CNN, involves extracting features from the input data and giving the feature data in the form of an np array to the model to predict the classification and accuracy. In this study, feature extraction was performed by taking the layers of the model into different blocks of size and extracting features from one of the blocks. The extracted features were then added, averaged, concatenated, or taken max, and the resultant features were given to the model to predict the output and classify the given image into the respective class. After applying feature extraction from multiple blocks to extract relevant visual characteristics and patterns, classification and detection were performed on the target dataset. Two different methods were used for classification in this study: utilizing fine-tuned features from the pre-trained VGG network layers and a linear classifier (SVM), which yielded the highest accuracy of 87% with a non-linear kernel. Alternatively, the Random Forest Classifier achieved an accuracy of 85%. In the second case, an EfficientNetB0 model with more dense layers and an output layer containing four classes of neurons was employed to improve performance.</p>
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>4. Experiments and results</title>
      <p>Dataset: In this study, the researchers utilized the BTC_Navoneel dataset, which is a publicly available dataset for brain tumor classification tasks. The dataset consists of MRI images that have been pre-processed for training and testing purposes, and can be downloaded from the Kaggle repository at the URL: https://www.kaggle.com/datasets/sartajbhuvaji/brain-tumor-classification-mri. The dataset is divided into separate train and test folders, and includes a total of 3264 MRI scans of four different types of brain tumors. During the pre-processing stage, the images in the dataset were standardized to 224 x 224, although their sizes originally varied. The dataset was released in 2019 and <xref ref-type="table" rid="table_1">Table 1</xref> shows the distribution of tumor types across the different classes in the input dataset.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>Summary of the benchmark navoneel MR image dataset</caption>
          <abstract/>
          <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Tumour Category</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">No. of Files in Training</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">No. of Files in Testing</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">no_tumour</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">395</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">105</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">glioma_tumour</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">826</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">100</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">meningioma_tumour</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">822</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">115</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">pituitary_tumour</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">827</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">74</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Total</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">2130</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">394</span></p></td></tr></tbody></table>
        </table-wrap>
      
      <p>Hyper Parameters: During the training process, the dataset was split into two parts: a training set and a validation set, using a train_test_split of 0.2. This allowed for the calculation of performance metrics for the model. To optimize the performance of the network, the parameters were configured and tuned based on the MRI scan data. The batch size was set to 32, the learning rate was randomized, and the number of epochs was set to 100. The highest accuracy achieved was 97.709%, which occurred on epoch 23 of the EfficientNetB0 model with a learning rate of 9.005e-5. Checkpoints were used to save the highest accuracy data during training, and the validation accuracy was monitored every 2 epochs to ensure the model was not overfitting to the training data.</p><p>After changing the kernel of the SVM, a maximum accuracy of 87% was achieved, and upon changing the parameters of the Random Forest classifier, a maximum accuracy of 85% was achieved.</p><p>The EfficientNetB0 model was built and various parameters were adjusted to obtain the highest possible accuracy. A table was created to document the observations, as shown in <xref ref-type="table" rid="table_2">Table 2</xref>. Confusion Matrix for different hyper parameter values is as shown in <xref ref-type="table" rid="table_3">Table 3</xref>:</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>Performance measures with different hyper parameters</caption>
          <abstract/>
          <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Train-test split ratio</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">dropout value</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">optimizer</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Patience parameter</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Epochs</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Learning Rate</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Precision</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Recall</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">F1_Score</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Accuracy</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">80-20</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.5</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">ADAM</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">5</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">100</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">4.30E-12</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.97</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.93</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.97</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">95.98</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">80-20</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.3</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">SGD</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">2</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">100</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1.22E-23</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.92</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.94</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.93</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">95.029</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">80-20</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.5</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">SGD</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">5</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">100</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">4.78E-10</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.96</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.97</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.93</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">95.22</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">80-20</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.3</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">ADAM</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">2</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">100</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">9.00E-05</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.96</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.97</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.98</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">97.706</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">80-20</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.3</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">ADAM</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">5</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">100</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.0003</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.97</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.95</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.96</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">96.558</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">80-20</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.5</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">SGD</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">10</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">100</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">3.00E-03</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.94</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.93</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.96</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">95.793</span></p></td></tr></tbody></table>
        </table-wrap>
      
      
        <table-wrap id="table_3">
          <label>Table 3</label>
          <caption>Confusion matrices for various Hyper parameter values</caption>
          <abstract/>
          <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Train-test split Ratio</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">dropout</span></p><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">value</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">optimizer</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">patience</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Epochs</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Learning rate</span></p></td><td colspan="4" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Confusion Matrix</span></p></td></tr><tr><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">80-20</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.5</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">ADAM</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">5</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">100</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">4.30E-12</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">159</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">4</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">5</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">108</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">4</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">193</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">3</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">175</span></p></td></tr><tr><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">80-20</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.3</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">SGD</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">2</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">100</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1.22E-23</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">153</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">4</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">11</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">106</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">7</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">3</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">181</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">10</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">3</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">172</span></p></td></tr><tr><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">80-20</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.5</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">SGD</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">5</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">100</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">4.78E-10</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">158</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">4</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">6</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">106</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">5</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">187</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">8</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">4</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">172</span></p></td></tr><tr><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">80-20</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.3</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">ADAM</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">2</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">100</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">9.00E-05</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">156</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">5</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">6</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">108</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">2</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">199</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">176</span></p></td></tr><tr><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">80-20</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.3</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">ADAM</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">5</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">100</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.0003</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">159</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">3</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">6</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">108</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">3</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">195</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">2</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">175</span></p></td></tr><tr><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">80-20</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.5</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">SGD</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">10</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">100</span></p></td><td colspan="1" rowspan="4"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">3.00E-03</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">156</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">4</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">7</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">104</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">3</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">6</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">2</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">185</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">8</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">3</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">173</span></p></td></tr></tbody></table>
        </table-wrap>
      
      <p>The confusion matrix of the test data that gives the best result is presented in <xref ref-type="fig" rid="fig_5">Figure 5</xref>:</p>
      
        <fig id="fig_5">
          <label>Figure 5</label>
          <caption>Confusion matrix obtained with the best model on test data</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/3/img_7n-SD1eKMMuPj0Fd.png"/>
        </fig>
      
      <p>The confusion matrix shows that out of 168 predictions made for glioma, the model correctly predicted 156 as glioma, but misclassified 5 as none class, 6 as meningioma, and 1 as pituitary class. The model achieved 100% accuracy in predicting the none class, meningioma class, and pituitary class.</p>
    </sec>
    <sec disp-level="level1" sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>In conclusion, a significant advance has been made in the field of brain tumor categorization using transfer learning and deep convolutional neural network (CNN) architectures through our research. By utilizing actual MRI images from the Kaggle dataset, accurate classification of brain tumors as glioma, meningioma, pituitary, and no tumor was achieved through feature extraction. The study involved the application of two powerful deep CNN architectures, VGG16 and EfficientNetB0, on MRI scans to accurately identify the tumor type. The best accuracy of 98% across all tests was achieved by implementing thick layers into the EfficientNetB0 network, using 100 epochs and a random learning rate.</p><p>However, despite exploring three deep CNN and transfer learning architectures for brain tumors, further research is needed to discover different, less complex but powerful deep neural network topologies for better tumor classification. In summary, our research has demonstrated the potential of deep CNN architectures and transfer learning to accurately classify brain tumors, which could have significant implications for the diagnosis and treatment of brain cancer.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p></p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>753-760</page-range>
          <issue>4</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J. D.</given-names>
              <surname>Bodapati</surname>
            </name>
            <name>
              <given-names>N. S.</given-names>
              <surname>Shaik</surname>
            </name>
            <name>
              <given-names>V.</given-names>
              <surname>Naralasetti</surname>
            </name>
            <name>
              <given-names>N. B.</given-names>
              <surname>Mundukur</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11760-020-01793-2</pub-id>
          <article-title>Joint training of two-channel deep neural network for brain tumor classification</article-title>
          <source>Signal, Image and Video Processing</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>35</volume>
          <page-range>2157005</page-range>
          <issue>7</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J. D.</given-names>
              <surname>Bodapati</surname>
            </name>
            <name>
              <given-names>N. S.</given-names>
              <surname>Shaik</surname>
            </name>
            <name>
              <given-names>V.</given-names>
              <surname>Naralasetti</surname>
            </name>
            <name>
              <given-names>N. B.</given-names>
              <surname>Mundukur</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1142/S0218001421570056</pub-id>
          <article-title>MSENet: Multi-modal squeeze-and-excitation network for brain tumor severity prediction</article-title>
          <source>Inter. J. Pattern Recog. &amp; Artif. Int.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>103</volume>
          <page-range>1395-1405</page-range>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J. D.</given-names>
              <surname>Bodapati</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Sajja</surname>
            </name>
            <name>
              <given-names>V.</given-names>
              <surname>Naralasetti</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s40031-022-00746-2</pub-id>
          <article-title>A deep learning framework with cross pooled soft attention for facial expression recognition</article-title>
          <source>J. Institution Eng. (India): Series B</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>2023</volume>
          <page-range>1-10</page-range>
          <issue/>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J. D.</given-names>
              <surname>Bodapati</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Sajja</surname>
            </name>
            <name>
              <given-names>V.</given-names>
              <surname>Naralasetti</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s40031-023-00875-2</pub-id>
          <article-title>An efficient approach for semantic segmentation of salt domes in seismic images using improved UNET architecture</article-title>
          <source>J. Institution Eng. (India): Series B</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>103</volume>
          <page-range>439-448</page-range>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J. D.</given-names>
              <surname>Bodapati</surname>
            </name>
            <name>
              <given-names>U.</given-names>
              <surname>Srilakshmi</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Veeranjaneyulu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s40031-021-00681-8</pub-id>
          <article-title>FERNet: A deep CNN architecture for facial expression recognition in the wild</article-title>
          <source>J. Institution Eng. (India): Series B</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>24</volume>
          <page-range>125-129</page-range>
          <issue>1</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J. D.</given-names>
              <surname>Bodapati</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Veeranjaneyulu</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Shaik</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/isi.240119</pub-id>
          <article-title>Sentiment analysis from movie reviews using LSTMs</article-title>
          <source>Ingénierie des Systèmes d Inf.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>81</volume>
          <page-range>32033-32056</page-range>
          <issue>22</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J. D.</given-names>
              <surname>Bodapati</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-022-12811-5</pub-id>
          <article-title>Stacked convolutional auto-encoder representations with spatial attention for efficient diabetic retinopathy diagnosis</article-title>
          <source>Multimed. Tools &amp; Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>45</volume>
          <page-range>949-959</page-range>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J. D.</given-names>
              <surname>Bodapati</surname>
            </name>
            <name>
              <given-names>V. N.</given-names>
              <surname>Rohith</surname>
            </name>
            <name>
              <given-names>V.</given-names>
              <surname>Dondeti</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s13246-022-01169-5</pub-id>
          <article-title>Ensemble of deep capsule neural networks: an application to pediatric pneumonia prediction</article-title>
          <source>Phys. &amp; Eng. Sci. in Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>188</volume>
          <page-range>Article ID: 110491</page-range>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J. D.</given-names>
              <surname>Bodapati</surname>
            </name>
            <name>
              <given-names>V.N.</given-names>
              <surname>Rohith</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.measurement.2021.110491</pub-id>
          <article-title>ChxCapsNet: Deep capsule network with transfer learning for evaluating pneumonia in paediatric chest radiographs</article-title>
          <source>Meas.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>18</volume>
          <page-range>6203-6216</page-range>
          <issue>5</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>H. A.</given-names>
              <surname>Khan</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Jue</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Mushtaq</surname>
            </name>
            <name>
              <given-names>M. U.</given-names>
              <surname>Mushtaq</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3934/mbe.2020328</pub-id>
          <article-title>Brain tumor classification in MRI image using convolutional neural network</article-title>
          <source>Math. Biosci. &amp; Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>e0140381</page-range>
          <issue>10</issue>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Jun</given-names>
              <surname>Cheng</surname>
            </name>
            <name>
              <given-names>Wei</given-names>
              <surname>Huang</surname>
            </name>
            <name>
              <given-names>Shuangliang</given-names>
              <surname>Cao</surname>
            </name>
            <name>
              <given-names>Ru</given-names>
              <surname>Yang</surname>
            </name>
            <name>
              <given-names>Wei</given-names>
              <surname>Yang</surname>
            </name>
            <name>
              <given-names>Zhaoqiang</given-names>
              <surname>Yun</surname>
            </name>
            <name>
              <given-names>Zhijian</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>Qianjin</given-names>
              <surname>Feng</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0140381</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Enhanced Performance of Brain Tumor Classification via Tumor Region Augmentation and Partition</article-title>
          <source>PLOS ONE</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <volume>2018</volume>
          <page-range>252-257</page-range>
          <issue/>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M. R.</given-names>
              <surname>Ismael</surname>
            </name>
            <name>
              <given-names>I.</given-names>
              <surname>Abdel-Qader</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/eit.2018.8500308</pub-id>
          <article-title>Brain tumour classification via statistical features and backpropagation neural network</article-title>
          <source>in 2018 IEEE International Conference on Electro/Information Technology</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>82</volume>
          <page-range>803-811</page-range>
          <issue>6</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Bilal</given-names>
              <surname>Tahir</surname>
            </name>
            <name>
              <given-names>Sajid</given-names>
              <surname>Iqbal</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Usman Ghani Khan</surname>
            </name>
            <name>
              <given-names>Tanzila</given-names>
              <surname>Saba</surname>
            </name>
            <name>
              <given-names>Zahid</given-names>
              <surname>Mehmood</surname>
            </name>
            <name>
              <given-names>Adeel</given-names>
              <surname>Anjum</surname>
            </name>
            <name>
              <given-names>Toqeer</given-names>
              <surname>Mahmood</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1002/jemt.23224</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Feature enhancement framework for brain tumor segmentation and classification</article-title>
          <source>Microscopy Research and Technique</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="conf-paper">
          <volume>10137</volume>
          <page-range/>
          <issue/>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J. S.</given-names>
              <surname>Paul</surname>
            </name>
            <name>
              <given-names>A. J.</given-names>
              <surname>Plassard</surname>
            </name>
            <name>
              <given-names>B. A.</given-names>
              <surname>L</surname>
            </name>
            <name>
              <given-names/>
              <surname>man</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Fabbri</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1117/12.2254195</pub-id>
          <article-title>Deep learning for brain tumor classification</article-title>
          <source>Medical Imaging 2017: Biomedical Applications in Molecular, Structural, and Functional Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="conf-paper">
          <volume>2018</volume>
          <page-range>3129-3133</page-range>
          <issue/>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>P.</given-names>
              <surname>Afshar</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Mohammadi</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Konstantinos Plataniotis</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/icip.2018.8451379</pub-id>
          <article-title>Brain tumor type classification via capsule networks</article-title>
          <source>2018 25th IEEE International Conference on Image Processing (ICIP)</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>1368-1372</page-range>
          <issue/>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>P.</given-names>
              <surname>Afshar</surname>
            </name>
            <name>
              <given-names>K. N.</given-names>
              <surname>Plataniotis</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Mohammadi</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/ICASSP.2019.8683759</pub-id>
          <article-title>Capsule networks for brain tumour classification based on MRI images and coarse tumour boundaries</article-title>
          <source>in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing, Brighton, UK</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="book">
          <volume/>
          <page-range>208-217</page-range>
          <issue/>
          <year>2019</year>
          <publisher-name>Springer International Publishing</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>Yufan</given-names>
              <surname>Zhou</surname>
            </name>
            <name>
              <given-names>Zheshuo</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>Hong</given-names>
              <surname>Zhu</surname>
            </name>
            <name>
              <given-names>Changyou</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>Mingchen</given-names>
              <surname>Gao</surname>
            </name>
            <name>
              <given-names>Kai</given-names>
              <surname>Xu</surname>
            </name>
            <name>
              <given-names>Jinhui</given-names>
              <surname>Xu</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1007/978-3-030-11723-8_21</pub-id>
          <article-title>Holistic Brain Tumor Screening and Classification Based on DenseNet and Recurrent Neural Network</article-title>
          <source>Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>314-319</page-range>
          <issue/>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A.</given-names>
              <surname>Pashaei</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Sajedi</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Jazayeri</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/ICCKE.2018.8566571</pub-id>
          <article-title>Brain tumour classification via convolutional neural network and extreme learning machines</article-title>
          <source>in 2018 8th International Conference on Computer and Knowledge Engineering, Mashhad, Iran</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>259-265</page-range>
          <issue>2</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J. D.</given-names>
              <surname>Bodapati</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Vijay</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Veeranjaneyulu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/isi.250214</pub-id>
          <article-title>Brain tumor detection using deep features in the latent space</article-title>
          <source>Ingénierie des systèmes d information</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>391-395</page-range>
          <issue>3</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>N.</given-names>
              <surname>Veeranjaneyulu</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Bodapati</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Buradagunta</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/isi.250315</pub-id>
          <article-title>Classifying limited resource data using semi-supervised SVM</article-title>
          <source>Ingénierie des systèmes d information</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>26</volume>
          <page-range>217-222</page-range>
          <issue>1</issue>
          <year>2005</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Pal</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/01431160412331269698</pub-id>
          <article-title>Random forest classifier for remote sensing classification</article-title>
          <source>Inter. J. Remote Sensing</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>