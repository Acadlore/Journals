<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IJCMEM</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>International Journal of Computational Methods and Experimental Measurements</journal-title>
        <abbrev-journal-title abbrev-type="issn">Int. J. Comput. Methods Exp. Meas.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IJCMEM</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2046-0554</issn>
      <issn publication-format="print">2046-0546</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-H-2APguSMfxF5IFEbkTfPJ-SJPeQNS5u</article-id>
      <article-id pub-id-type="doi">10.56578/ijcmem130320</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>A Residual Temporal Convolutional with Attention Neural Network for Electromyogram-Based Hand Gesture Recognition</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0946-9636</contrib-id>
          <name>
            <surname>Namane</surname>
            <given-names>Rachid</given-names>
          </name>
          <email>rachid.namane@univ-boumerdes.dz</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-4140-6130</contrib-id>
          <name>
            <surname>Boutellaa</surname>
            <given-names>Elhocine</given-names>
          </name>
          <email>e.boutellaa@univ-boumerdes.dz</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-1487-8585</contrib-id>
          <name>
            <surname>Salem</surname>
            <given-names>Sif Eddine</given-names>
          </name>
          <email>salemsifeddine1@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-9692-5726</contrib-id>
          <name>
            <surname>Babaci</surname>
            <given-names>Yassine</given-names>
          </name>
          <email>yassinebabaci1@gmail.com</email>
        </contrib>
        <aff id="aff_1">Institute of Electrical Engineering and Electronics, University M’Hamed Bougara, 35000 Boumerdes, Algeria</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>29</day>
        <month>10</month>
        <year>2025</year>
      </pub-date>
      <volume>13</volume>
      <issue>3</issue>
      <fpage>739</fpage>
      <lpage>748</lpage>
      <page-range>739-748</page-range>
      <history>
        <date date-type="received">
          <day>29</day>
          <month>07</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>24</day>
          <month>09</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Electromyography (EMG)-based hand gesture classification is a developing core technology for designing intuitive and responsive human-computer interaction, notably for prosthetic control. EMG signals, which reflect muscle activity during contraction, offer a non-invasive and effective method for capturing user gestures. However, because of their natural variability, noise, and temporal richness pose significant hurdles to precise gesture recognition. In this paper, we investigate the use of causal convolutional layers, which are suitable for sequential data, to improve hand gesture recognition from raw EMG signals. We propose a deep neural network which bases on temporal convolutions and integrates residual connections and contextual attention in an end to end hand gesture recognition system. Furthermore, we apply multiple data augmentation techniques to mitigate intra-subject variability and enhance model generalization. Our approach is evaluated on the benchmark NinaProDB1 dataset. The proposed model show impressive classification performance with an average accuracy of 95.31% and where the majority of the gestures from various subjects were accurately recognized. These results demonstrate the effectiveness of causal convolutions and attention mechanisms for robust EMG-based gesture recognition.</p></abstract>
      <kwd-group>
        <kwd>Electromyogram</kwd>
        <kwd>Hand gesture recognition</kwd>
        <kwd>Deep learning</kwd>
        <kwd>Causal convolution</kwd>
        <kwd>Attention mechanism</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="4"/>
        <fig-count count="6"/>
        <table-count count="3"/>
        <ref-count count="20"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Electromyography (EMG)-based hand gesture recognition has become a pivotal technology in developing intuitive and responsive human-computer interaction (HMI) systems. In particular, prosthetic control represents a major application domain in biomedical engineering. Surface EMG signals, which reflect underlying muscle activity during contraction, offer a non-invasive and effective way to decode user gesture intent, particularly for amputees [<xref ref-type="bibr" rid="ref_1">1</xref>]. The process involves classifying muscle activation patterns into discrete hand or wrist movements, enabling intuitive control over assistive devices. However, the use of EMG signal classification presents several challenges due to the non-stationary nature of the signal, inter-subject variability, muscle fatigue, and noise introduced by sensors or motion artifacts. Therefore, accurate EMG signal classification requires models that can robustly extract temporal and spatial features from multi-channel time-series data especially in real-world scenarios where generalization across users and sessions is required.</p><p>In recent years, deep learning (DL) has exhibited impressive capability in biomedical signal processing. In the context of electromyography (EMG)-based hand gesture recognition, DL has demonstrated remarkable promise, offering the potential to build more accurate and adaptive human-machine interfaces (HMIs) [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>]. Unlike traditional machine learning algorithms that rely on handcrafted features, which often restrict their scalability and generalization to unseen data, deep neural networks can automatically extract hierarchical and temporal features directly from raw EMG data [<xref ref-type="bibr" rid="ref_4">4</xref>], [<xref ref-type="bibr" rid="ref_5">5</xref>]. CNNs have been widely applied for capturing spatial representations, while RNNs and LSTMs are used for sequential modeling. These capabilities make them well-suited for handling dynamic, high-dimensional time-series data such as EMG. More recently, hybrid approaches combining CNNs, LSTMs, or attention mechanisms have improved recognition accuracy, yet they often struggle with training efficiency, long-term temporal modeling, and generalization across subjects.</p><p>Temporal Convolutional Networks (TCNs) have emerged as a compelling alternative to recurrent architectures, thanks to their use of causal convolutions and dilation. These properties allow TCNs to model long-range temporal dependencies. Unlike standard CNNs, which are primarily designed for spatial feature extraction, TCNs utilize causal convolutions and dilation, enabling them to preserve sequence order while covering broad temporal contexts. This structure not only alleviates the vanishing gradient problem commonly associated with recurrent architectures but also allows for more efficient parallelization during training [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>]. Despite their promise, their application in EMG-based hand gesture recognition remains underexplored, particularity in combination with advanced techniques such as residual connections, attention mechanisms, and systematic data augmentation.</p><p>This study aims to address this gap by proposing a TCN-based model with residual connections and contextual attention for robust EMG-based hand gesture recognition. Unlike prior CNN or LSTM approaches, our model is designed to i) capture long-range temporal dependencies efficiently, ii) enhance discriminative focus through attention, and iii) mitigate overfitting via data augmentation. Our hypothesis is that this architecture can achieve superior accuracy and generalization compared to existing methods.</p><p>To validate our approach, the proposed model is trained and evaluated on a subset of the benchmark NinaProdataset, demonstrating competitive performance and promising generalization capabilities, which highlights its potential for real-time prosthetic control and human-machine interaction.</p><p>The remainder of this paper is organized as follows. Section 2 reviews related literature on deep learning for EMG-based gesture recognition. Section 3 presents the proposed methodology. Section 4 describes the experimental evaluation of the proposed approach, and Section 5 concludes the paper with a summary of contributions and directions for future improvement.</p>
    </sec>
    <sec sec-type="">
      <title>2. Related work</title>
      <p>A range of approaches have been explored for EMG-based hand gesture recognition, spanning classic machine learning, deep learning, and hybrid architectures. This section highlights key works, emphasizing temporal modeling methods and those tailored to the NinaPro dataset. Atzori et al. [<xref ref-type="bibr" rid="ref_8">8</xref>] leveraged a LeNet-based CNN on NinaPro DB2, achieving 78–85% accuracy across multiple gestures with feature-based SVM classifiers, a solid baseline for later DL models. Geng et al. [<xref ref-type="bibr" rid="ref_9">9</xref>] introduced the concept of instantaneous sEMG imaging and applied a CNN plus SVM on 8 gestures captured via high-density sEMG imaging, attaining ~89.3% per-frame accuracy (and 99.0% after majority voting), illustrating the potential of single-frame classification. Du et al. [<xref ref-type="bibr" rid="ref_4">4</xref>] designed a custom LSTM-based model on NinaProDB2/DB3, achieving 85–90% accuracy on sequence recognition tasks, outperforming classical approaches.</p><p>While earlier studies demonstrated strong foundational performance using CNNs and LSTMs, more recent archi- tectures have adopted TCNs and attention mechanisms to improve both accuracy and efficiency. Tsinganos et al. [<xref ref-type="bibr" rid="ref_10">10</xref>] employed TCN for EMG-based hand gesture recognition using the NinaPro DB1 dataset, achieving around 89.76% accuracy and demonstrating the strength of TCNs in capturing temporal dependencies in muscle activity signals. Zanghieri et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] proposed TEMPONetembedded TCN, A real-time embedded TCN model, achieving 93.7% accuracy on NinaPro enabled sessions with an ultra-low memory footprint.</p><p>Recent TCN and multi-stream architectures have further advanced the field. Rahimian et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] proposed a lightweight TCN-attention model evaluated on 17 gesture classes, achieving 81.65% (300 ms windows) and 80.72% (200 ms windows), with ~12 <inline-formula>
  <mml:math id="mg513d96ak">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> fewer parameters than benchmark models. A multi-stream deep architecture proposed by Shin et al. [<xref ref-type="bibr" rid="ref_13">13</xref>], integrating TCN, CNN, LSTM modules, and channel attention, was tested on NinaPro DB1 and DB9, attaining 94.3% and 98.96% accuracy, respectively, demonstrating the power of hybrid models for high-performance EMG classification.</p><p>Beyond these temporal models, recent research has introduced novel paradigms. Montazerin et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] applied a Vision Transformer (ViT-HGR) to HD-sEMG, reporting ~84.6% accuracy on 65 gestures with a remarkably compact parameter count, illustrating the promise of transformer-based methods. Zhong et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] developed a Spatio-Temporal Graph Convolutional Network (named STGCN-GR) for HD-sEMG-based human-machine interfaces, which explicitly models electrode topology and reached 91.07% accuracy on 65 gestures, outperforming CNN-based baselines. Xiang et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] introduced SE-DenseNet with channel attention, achieving 85.93% and 82.39% accuracy on NinaPro DB2 and DB4, respectively, highlighting the effectiveness of attention in EMG recognition.</p><p>To synthesize these advancements, <xref ref-type="table" rid="table_1">Table 1</xref> provides a comparative overview of representative studies. It summarizes the datasets used (including number of gestures), core architectures, and reported performance metrics, offering a consolidated view of how different methods perform across diverse experimental contexts.</p><p>Although our work employs temporal convolutions similar to those used in related studies, our proposed architecture distinguishes itself by incorporating multiple residual causal convolution blocks and a contextual attention mechanism placed at the top of the network to enhance feature discrimination. Moreover, unlike prior works that focus on reducing the number of layers to achieve lightweight models, our design leverages deeper residual structures to improve representational capacity. Additionally, we apply suitable data augmentation techniques to enhance learning and promote better model generalization.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Comparative overview of EMG-based hand gesture recognition models</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Study/Model</p></td><td colspan="1" rowspan="1"><p>Dataset</p></td><td colspan="1" rowspan="1"><p>Architecture</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td></tr><tr><td colspan="1" rowspan="1"><p>Atzori et al. [<xref ref-type="bibr" rid="ref_8">8</xref>]</p></td><td colspan="1" rowspan="1"><p>NinaProDB2</p></td><td colspan="1" rowspan="1"><p>CNN + SVM</p></td><td colspan="1" rowspan="1"><p>78-85%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Geng et al. [<xref ref-type="bibr" rid="ref_9">9</xref>]</p></td><td colspan="1" rowspan="1"><p>HD-sEMG</p></td><td colspan="1" rowspan="1"><p>CNN + SVM</p></td><td colspan="1" rowspan="1"><p>89.3% (frame), 99% (voting)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Du et al. [<xref ref-type="bibr" rid="ref_4">4</xref>]</p></td><td colspan="1" rowspan="1"><p>NinaProDB2/DB3</p></td><td colspan="1" rowspan="1"><p>Custom LSTM</p></td><td colspan="1" rowspan="1"><p>85-90%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Tainganos et al. [<xref ref-type="bibr" rid="ref_10">10</xref>]</p></td><td colspan="1" rowspan="1"><p>NinaProDB1</p></td><td colspan="1" rowspan="1"><p>TCN</p></td><td colspan="1" rowspan="1"><p>89.76%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Zanghieri et al. [<xref ref-type="bibr" rid="ref_11">11</xref>]</p></td><td colspan="1" rowspan="1"><p>NinaPro-enabled sessions</p></td><td colspan="1" rowspan="1"><p>TCN on GAP8 processor</p></td><td colspan="1" rowspan="1"><p>93.7% with ultra-low compute overhead</p></td></tr><tr><td colspan="1" rowspan="1"><p>Rahimian et al. [<xref ref-type="bibr" rid="ref_12">12</xref>]</p></td><td colspan="1" rowspan="1"><p>NinaProDB2</p></td><td colspan="1" rowspan="1"><p>TCN + Attention (lightweight)</p></td><td colspan="1" rowspan="1"><p>81% with few parameters</p></td></tr><tr><td colspan="1" rowspan="1"><p>Shin et al. [<xref ref-type="bibr" rid="ref_13">13</xref>]</p></td><td colspan="1" rowspan="1"><p>NinaProDB1/DB9</p></td><td colspan="1" rowspan="1"><p>TCN + CNN + LSTM + Attention</p></td><td colspan="1" rowspan="1"><p>94.3%/98.96%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Montazerin et al. [<xref ref-type="bibr" rid="ref_14">14</xref>]</p></td><td colspan="1" rowspan="1"><p>HD-sEMG</p></td><td colspan="1" rowspan="1"><p>VIT-HGR</p></td><td colspan="1" rowspan="1"><p>84.6%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Zhong et al. [<xref ref-type="bibr" rid="ref_15">15</xref>]</p></td><td colspan="1" rowspan="1"><p>HD-sEMG</p></td><td colspan="1" rowspan="1"><p>STGCN-GR</p></td><td colspan="1" rowspan="1"><p>91.07%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Xiang et al. [<xref ref-type="bibr" rid="ref_16">16</xref>]</p></td><td colspan="1" rowspan="1"><p>NinaProDB2/DB4</p></td><td colspan="1" rowspan="1"><p>SE-DenseNet</p></td><td colspan="1" rowspan="1"><p>85.93%/82.39%</p></td></tr></tbody></table>
        </table-wrap>
      
    </sec>
    <sec sec-type="">
      <title>3. Methodology</title>
      
        <sec>
          
            <title>3.1. Data and preprocessing</title>
          
          <p>There exists a number of publicly available sEMG hand gesture datasets. These data sets vary in terms of acquisition device, number of subjects, types of gestures, number of gesture repetitions, etc. In this context, NinaProdatabase [<xref ref-type="bibr" rid="ref_2">2</xref>] is considered to be one of the most comprehensive resources, today comprising 9 separate data sets collected from 204 healthy subjects and 15 amputee subjects. NinaProcontains a set of upper limb electromyographic, kinematic and dynamic data to allow the public to test machine learning algorithms for controlling hand prostheses and conducting other related researches. The 9 datasets include multiple repetitions of at least 50 hand movements. The latter were recorded with different acquisition protocols and configurations, to allow the recording of several multi-modal signals. Despite the fact that NinaPro only includes data from 15 hand amputees, it has been shown that data from healthy subjects can also be used as a proxy measure for amputees. This result justifies the use of healthy subjects in order to reduce the stress and pain that can be caused in amputees [<xref ref-type="bibr" rid="ref_17">17</xref>].</p><p>NinaPro DB1 contains sEMG data captured from 10 channels, obtained at a sampling frequency of 100 Hz. In our experiments, we use data of 10 subjects from NinaPro DB1. For classification, we consider a total of 22 gestures provided in <xref ref-type="fig" rid="fig_1">Figure 1</xref>. Each gesture was repeated 10 times. The dataset also includes a "Restimulus" label, which represents the same movements, but with improved time labels that better match when the movements really happened. The raw sEMG signals were preprocessed and structured into a consistent format for training and evaluating the model. The preprocessing includes segmenting the EMG signal according to the repetition and windowing. This process is performed for each subject separately.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>Hand gestures considered in our work</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_G_SMVbtUu5QHk46h.png"/>
            </fig>
          
          <p>Inspired by established methods in time-series literatures [<xref ref-type="bibr" rid="ref_18">18</xref>], [<xref ref-type="bibr" rid="ref_19">19</xref>], we apply the following multiple data augmentation techniques, which attempt to simulate real-world EMG signal variations during training:</p><p>• Jittering: adds adjustable Gaussian noise to match a target SNR, mimicking real-world EMG noise from sensors or ambient electrical interference.</p><p>• Channel Shuffling: circularly rotates the EMG channel order by a random offset (<inline-formula>
  <mml:math id="m6z3ya6w88">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula> 2 channels). It models slight misplacement of electrodes, especially relevant for wearable EMG systems.</p><p>• Scaling: multiplies each channel by a random factor sampled from a normal distribution around 1 (<inline-formula>
  <mml:math id="m9exn8c6pc">
    <mml:mi>σ</mml:mi>
  </mml:math>
</inline-formula> = 0.2). It simulates inter-session variability like sensor gain or changes in muscle contraction strength.</p><p>• Permutation: divides the signal into segments and randomly shuffles them. It encourages the model to focus on local temporal features rather than fixed global ordering.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Proposed model architecture</title>
          
          <p>As outlined in <xref ref-type="fig" rid="fig_2">Figure 2</xref>, our proposed model, named Residual Temporal Convolutional Attention Neural Network (RTCAN), attempts to capture both short-term and long-term patterns in EMG signals. To this end, we use i) causal convolutions which do depend only on past data combined with ii) residual connections that let the signal jump over some layers to help train deeper networks, and iii) an attention mechanism to focus on important parts of the signal. The input of the model is a preprocessed EMG sample of size n <inline-formula>
  <mml:math id="m9j253irsa">
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula> 10, corresponding to the number of time steps and sensor channels. The output of the model is a discrete value specifying the recognized gesture.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Proposed RTCAN model architecture</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_2Gc1-PrHrQhpN0G_.png"/>
            </fig>
          
          <p>In the following, we discuss the details of each block of the proposed architecture.</p>
          
            <sec>
              
                <title>3.2.1 Residual causal convolutional blocks</title>
              
              <p>The role of the Residual Causal Convolutional Blocks is to identify specific patterns over time for robust gesture classification. The basic building blocks are temporal convolution layers. In contrast to the usual convolution where the output at time t depends on past, present, and future inputs (<inline-formula>
  <mml:math id="mm4xalycjv">
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mn>2</mml:mn>
  </mml:math>
</inline-formula>) to <inline-formula>
  <mml:math id="mpozh5pvvj">
    <mml:mo>(</mml:mo>
    <mml:mo>+</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mn>2</mml:mn>
  </mml:math>
</inline-formula>), $k$ is the filter size), in the temporal convolution, the output depends only on the past time steps. Another advantage of the temporal convolution is the use of dilated convolutions to increase their receptive field without drastically increasing the number of parameters.</p>
              
                <disp-formula>
                  <label>(1)</label>
                  <mml:math id="mwhaykd999">
                    <mml:mi>y</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mi>f</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mi>d</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:munderover>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>0</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>k</mml:mi>
                        <mml:mo>−</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:munderover>
                  </mml:math>
                </disp-formula>
              
              <p> where, <inline-formula>
  <mml:math id="mi43uysmzw">
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is the output at time step <inline-formula>
  <mml:math id="md48zjp6bm">
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is the filter of size <inline-formula>
  <mml:math id="m7xu0l9y4k">
    <mml:mi>k</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is the input sequence, and $d$ is the dilation factor. This last controls the convolution receptive field.</p><p>Residual layers are added to maintain gradient flow, and the overall transformation can be described by:</p>
              
                <disp-formula>
                  <label>(2)</label>
                  <mml:math id="mlish4ptmm">
                    <mml:mi>H</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>+</mml:mo>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="m98i87jmzm">
    <mml:mi>H</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is the output, <inline-formula>
  <mml:math id="m22qh0dboc">
    <mml:mi>F</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is the function learned by the skipped convolution layers, and $x$ is the input that gets passed through via the residual connection.</p><p>Our model comprises four causal convolution blocks strengthened by two residual connections. The first is from the output of the first causal convolution block to the input of the third one, and the second connection is from the output of the third causal convolution block to the input of the contextual attention block. Each causal convolution block has specific parameters as follows: [ 2, 2, 2, 1] and [[32, 32], [32, 32], [64, 64], [128]] for the number convolution layers and their corresponding number of filters, respectively. The ReLU activation function is applied to all convolution layers. To prevent model overfitting L2 regularization is applied to the kernel weights and each convolution is followed by a dropout layer. A summary of the causal convolutional blocks’ parameters is provided in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
              
                <table-wrap id="table_2">
                  <label>Table 2</label>
                  <caption>
                    <title>Causal convolutional blocks parameters</title>
                  </caption>
                  <table><tbody><tr><th colspan="1" rowspan="1"><p>Block</p></th><th colspan="1" rowspan="1"><p># Layers</p></th><th colspan="1" rowspan="1"><p>Filter(s)</p></th><th colspan="1" rowspan="1"><p>Purpose</p></th></tr><tr><td colspan="1" rowspan="1"><p>Block 1</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>32, 32</p></td><td colspan="1" rowspan="1"><p>Initial temporal feature extraction</p></td></tr><tr><td colspan="1" rowspan="1"><p>Block 2</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>32, 32</p></td><td colspan="1" rowspan="1"><p>Reinforce base temporal patterns</p></td></tr><tr><td colspan="1" rowspan="1"><p>Block 3</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>64, 64</p></td><td colspan="1" rowspan="1"><p>Intermediate pattern abstraction</p></td></tr><tr><td colspan="1" rowspan="1"><p>Block 4</p></td><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>128</p></td><td colspan="1" rowspan="1"><p>High-level temporal representation</p></td></tr></tbody></table>
                </table-wrap>
              
              <p>The residual causal convolution blocks help the model learn how EMG signals change over time, while keeping the order of the signals, before being processed by the contextual attention.</p>
            </sec>
          
          
            <sec>
              
                <title>3.2.2 Contextual attention block</title>
              
              <p>The attention mechanism implemented in this work builds upon the hierarchical attention networks proposed by Yang et al. [<xref ref-type="bibr" rid="ref_20">20</xref>], adapting it for temporal EMG signal analysis. Unlike traditional sequence-to-sequence attention that operates between encoder and decoder states, this implementation processes a single temporal sequence to identify the most discriminative time steps for classification. Its mathematical formulation is as follows. Given an input sequence representation <inline-formula>
  <mml:math id="momm9dc3lb">
    <mml:mrow>
      <mml:mi>X</mml:mi>
    </mml:mrow>
    <mml:mo>∈</mml:mo>
    <mml:msup>
      <mml:mrow>
        <mml:mi>R</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mi>B</mml:mi>
        <mml:mi>T</mml:mi>
        <mml:mi>F</mml:mi>
        <mml:mo>×</mml:mo>
        <mml:mo>×</mml:mo>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula>, where $B<inline-formula>
  <mml:math id="m4td6imc5z">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>T<inline-formula>
  <mml:math id="mg5mrzlspt">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>F$ is features' dimension, the attention mechanism computes importance weights through three learned components:</p><p>Transformation Matrix (W): Projects the input features into the attention space:</p>
              
                <disp-formula>
                  <label>(3)</label>
                  <mml:math id="m9jhahmriu">
                    <mml:mi>U</mml:mi>
                    <mml:mi>tanh</mml:mi>
                    <mml:mi>X</mml:mi>
                    <mml:mi>W</mml:mi>
                    <mml:mi>b</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mo>⁡</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>)</mml:mo>
                  </mml:math>
                </disp-formula>
              
              <p> where, <inline-formula>
  <mml:math id="mvft8l6nhb">
    <mml:mi>W</mml:mi>
    <mml:mo>∈</mml:mo>
    <mml:msup>
      <mml:mrow>
        <mml:mi>R</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mi>F</mml:mi>
        <mml:mi>F</mml:mi>
        <mml:mo>×</mml:mo>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="motaqldvtd">
    <mml:mi>b</mml:mi>
    <mml:mo>∈</mml:mo>
    <mml:msup>
      <mml:mrow>
        <mml:mi>R</mml:mi>
      </mml:mrow>
      <mml:mi>F</mml:mi>
    </mml:msup>
  </mml:math>
</inline-formula> are learnable parameters.</p><p>Context Vector (u): Learns a fixed query vector that determines which time steps are relevant:</p>
              
                <disp-formula>
                  <label>(4)</label>
                  <mml:math id="m5nm4hh094">
                    <mml:msup>
                      <mml:mi>a</mml:mi>
                      <mml:mrow>
                        <mml:mi>′</mml:mi>
                      </mml:mrow>
                    </mml:msup>
                    <mml:mo>=</mml:mo>
                    <mml:mi>U</mml:mi>
                    <mml:mi>u</mml:mi>
                  </mml:math>
                </disp-formula>
              
              <p> where, <inline-formula>
  <mml:math id="mw337bnbqq">
    <mml:mi>u</mml:mi>
    <mml:mo>∈</mml:mo>
    <mml:msup>
      <mml:mrow>
        <mml:mi>R</mml:mi>
      </mml:mrow>
      <mml:mi>F</mml:mi>
    </mml:msup>
  </mml:math>
</inline-formula> is learned during training.</p><p>Normalized Attention Weights: Computed via softmax with masking support:</p>
              
                <disp-formula>
                  <label>(5)</label>
                  <mml:math id="mmoj41585l">
                    <mml:msub>
                      <mml:mi>α</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>m</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mi>exp</mml:mi>
                        <mml:mo>⁡</mml:mo>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:msubsup>
                            <mml:mi>a</mml:mi>
                            <mml:mi>t</mml:mi>
                            <mml:mrow>
                              <mml:mi>′</mml:mi>
                            </mml:mrow>
                          </mml:msubsup>
                        </mml:mrow>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:munderover>
                          <mml:mo>∑</mml:mo>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                            <mml:mo>=</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                          <mml:mi>T</mml:mi>
                        </mml:munderover>
                        <mml:mi>exp</mml:mi>
                        <mml:mo>⁡</mml:mo>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:msubsup>
                            <mml:mi>a</mml:mi>
                            <mml:mi>i</mml:mi>
                            <mml:mrow>
                              <mml:mi>′</mml:mi>
                            </mml:mrow>
                          </mml:msubsup>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:math>
                </disp-formula>
              
              <p> where, <inline-formula>
  <mml:math id="m4fp1pawfk">
    <mml:msub>
      <mml:mi>m</mml:mi>
      <mml:mrow>
        <mml:mrow>
          <mml:mi>t</mml:mi>
        </mml:mrow>
      </mml:mrow>
    </mml:msub>
    <mml:mo>∈</mml:mo>
    <mml:mo fence="false">{</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo fence="false">}</mml:mo>
    <mml:mn>0</mml:mn>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula> is the mask value for step $t$. </p><p>The final context vector is the weighted sum:</p>
              
                <disp-formula>
                  <label>(6)</label>
                  <mml:math id="m5pkhoowzo">
                    <mml:mi>c</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:munderover>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>T</mml:mi>
                    </mml:munderover>
                    <mml:msub>
                      <mml:mi>α</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>x</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:msub>
                  </mml:math>
                </disp-formula>
              
            </sec>
          
          
            <sec>
              
                <title>3.2.3 Classification layer</title>
              
              <p>The top of the model is the classification layer implemented as a fully connected layer with softmax activation and its size the number of classes (recognized gestures).</p>
            </sec>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Experimental evaluation</title>
      
        <sec>
          
            <title>4.1. Experimental setup</title>
          
          <p>The experimental setup defines the configuration encompassing the dataset, the model architecture, and the training strategy.</p><p>To ensure exclusive data samples between train and test, for each subject, data is portioned into disjoint sets based on repetitions. Specifically, for each subject signals from repetitions {1, 3, 4, 6, 8, 9, 10} are used for training and those from repetitions {2, 5, 7} are used for test. This setting allows fair model evaluation under unseen test data. For training, data augmentation is applied with a noise level of 25 dB SNR, along with magnitude warping and time warping, each set to 0.2. The batch size for training is 32, and samples are weighted based on their class frequency.</p><p>The training process uses the Adam optimizer with an initial learning rate of 0.001. A constant learning rate schedule is applied throughout the 100 training epochs. To promote model generalization on unseen data, we employ i) weight decay with a regularization strength of 0.0001; ii) a dropout rate of 0.4 after convolutions; iii) L2 regularization with a coefficient of 0.0005.</p>
        </sec>
      
      
        <sec>
          
            <title>4.2. Results</title>
          
          <p>Inspired by related works, we perform a subject-specific evaluation. Thus, for each subject, we train and evaluate the model with his own data ensuring that the test and train data subsets are disjoint.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Per subject train and test performance</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Subject #</p></th><th colspan="1" rowspan="1"><p>Training Time (s)</p></th><th colspan="1" rowspan="1"><p>Test Time (s)</p></th><th colspan="1" rowspan="1"><p>Train Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Test Accuracy (%)</p></th></tr><tr><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>543.57</p></td><td colspan="1" rowspan="1"><p>0.2177</p></td><td colspan="1" rowspan="1"><p>99.63</p></td><td colspan="1" rowspan="1"><p>92.19</p></td></tr><tr><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>560.14</p></td><td colspan="1" rowspan="1"><p>0.3490</p></td><td colspan="1" rowspan="1"><p>98.96</p></td><td colspan="1" rowspan="1"><p>93.75</p></td></tr><tr><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p>822.87</p></td><td colspan="1" rowspan="1"><p>0.4937</p></td><td colspan="1" rowspan="1"><p>99.48</p></td><td colspan="1" rowspan="1"><p>98.44</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>837.55</p></td><td colspan="1" rowspan="1"><p>0.5068</p></td><td colspan="1" rowspan="1"><p>99.27</p></td><td colspan="1" rowspan="1"><p>89.06</p></td></tr><tr><td colspan="1" rowspan="1"><p>5</p></td><td colspan="1" rowspan="1"><p>1087.82</p></td><td colspan="1" rowspan="1"><p>0.6797</p></td><td colspan="1" rowspan="1"><p>99.58</p></td><td colspan="1" rowspan="1"><p>100.00</p></td></tr><tr><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p>927.05</p></td><td colspan="1" rowspan="1"><p>0.3588</p></td><td colspan="1" rowspan="1"><p>99.58</p></td><td colspan="1" rowspan="1"><p>92.19</p></td></tr><tr><td colspan="1" rowspan="1"><p>7</p></td><td colspan="1" rowspan="1"><p>730.95</p></td><td colspan="1" rowspan="1"><p>0.3680</p></td><td colspan="1" rowspan="1"><p>99.22</p></td><td colspan="1" rowspan="1"><p>96.88</p></td></tr><tr><td colspan="1" rowspan="1"><p>8</p></td><td colspan="1" rowspan="1"><p>645.26</p></td><td colspan="1" rowspan="1"><p>0.4350</p></td><td colspan="1" rowspan="1"><p>99.48</p></td><td colspan="1" rowspan="1"><p>96.88</p></td></tr><tr><td colspan="1" rowspan="1"><p>9</p></td><td colspan="1" rowspan="1"><p>1515.46</p></td><td colspan="1" rowspan="1"><p>0.4797</p></td><td colspan="1" rowspan="1"><p>97.92</p></td><td colspan="1" rowspan="1"><p>95.31</p></td></tr><tr><td colspan="1" rowspan="1"><p>10</p></td><td colspan="1" rowspan="1"><p>676.32</p></td><td colspan="1" rowspan="1"><p>0.3479</p></td><td colspan="1" rowspan="1"><p>98.75</p></td><td colspan="1" rowspan="1"><p>98.44</p></td></tr><tr><td colspan="1" rowspan="1"><p>STD</p></td><td colspan="1" rowspan="1"><p>292.13</p></td><td colspan="1" rowspan="1"><p>0.13</p></td><td colspan="1" rowspan="1"><p>0.53</p></td><td colspan="1" rowspan="1"><p>3.46</p></td></tr><tr><td colspan="1" rowspan="1"><p>Average</p></td><td colspan="1" rowspan="1"><p>834.70</p></td><td colspan="1" rowspan="1"><p>0.4236</p></td><td colspan="1" rowspan="1"><p>99.19</p></td><td colspan="1" rowspan="1"><p>95.31</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Learning graphs for Subject #4</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_oLJOJ8FqvYbaqmD0.png"/>
            </fig>
          
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Learning graphs for Subject #5</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_zohk3nNvyARdhW8j.png"/>
            </fig>
          
          <p>In <xref ref-type="table" rid="table_3">Table 3</xref>, we report train and test times and classification accuracy for each subject. These results demonstrate that the model achieves consistent and robust accuracy, indicating its suitability for gesture recognition tasks using sEMG signals from NinaProDB1. Most of the accuracies are satisfactory, with a mean test accuracy over subjects of 95.31%. The worst model is that of Subject #4 with an accuracy of 89.06%. The best model is that of Subject #5 with a perfect test classification. Overall, the models generalize well to unseen gestures, as indicated by the small difference between train and test accuracies of each subject.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Confusion matrix for Subject #4</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_i5_6SJ0YfTuR7g4L.png"/>
            </fig>
          
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Confusion matrix for Subject #5</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_dXLbKEST5WT_Jt73.png"/>
            </fig>
          
          <p>In average a model takes a bit less than 15 minutes (834.70s) for training and requires almost 0.43 seconds for recognizing a hand gesture during test. Moreover, our developed model is lightweight, with a total of 84.758 parameters, all of which are trainable, occupying just 331.09KB storage memory. This compact design is well-suited for embedding, enabling real-life applications such as controlling myoelectric prosthesis. To this end, however, deeper investigation should be conducted in this context.</p><p>To gain a deeper insight into the experimental results, we depict the training graphs for the best and worst performing subjects in <xref ref-type="fig" rid="fig_3">Figure 3</xref> and <xref ref-type="fig" rid="fig_4">Figure 4</xref> as well as their corresponding confusion matrices in <xref ref-type="fig" rid="fig_5">Figure 5</xref> and <xref ref-type="fig" rid="fig_6">Figure 6</xref>. The learning graphs of the worst subject (<xref ref-type="fig" rid="fig_3">Figure 3</xref>) demonstrate an overfitting, as depicted in the divergence between the train and validation curves for both the loss and the accuracy. In contrast, the learning graphs of the best subject (<xref ref-type="fig" rid="fig_4">Figure 4</xref>) demonstrate the generalization of the model, as depicted by an almost perfect much between the train and validation curves.</p><p>Since the test accuracy of the best subject is 100%, a perfect confusion matrix is achieved as shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>. The classification of a number of gestures is mismatched in case of the worst subject as shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>, reducing the overall classification accuracy to 89.06%. Specifically, some EMG signals of five gestures (3, 7, 9, 16 and 17) were misclassified with different rates, while the remaining gestures are perfectly recognized.</p>
        </sec>
      
      
        <sec>
          
            <title>4.3. Discussion</title>
          
          <p>Directly comparing EMG-based hand gesture recognition (HGR) models remains challenging due to differences in experimental settings, including the number of subjects, gesture classes, recording sessions, sensor configurations, and preprocessing pipelines. These factors can significantly influence reported performance, making absolute comparisons less straightforward. Despite these challenges, when evaluated against the most relevant works reviewed in Section 2, our approach achieves an average classification accuracy of 95.31% on the NinaPro DB1 dataset, which is competitive with and in several cases surpasses recent baselines.</p><p>The combination of Temporal Convolutional Networks with residual connections, attention mechanisms, and extensive data augmentation appears to be particularly effective in enhancing temporal feature modeling and generalization. In addition to accuracy gains, our framework demonstrates greater robustness across different subjects, as reflected by reduced variance in inter-subject performance compared to baselines such as CNN and RNN-based models. This stability is largely attributable to residual connections and extensive data augmentation, which mitigate overfitting and improve generalization, an essential property for practical deployment in prosthetic control and human-computer interaction.</p>
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>In this paper, we explored the development of a deep learning framework tailored for surface EMG-based hand gesture recognition, with a particular focus on the use of Causal Convolutional, which present the advantage of its ability to model long-term dependencies, maintain temporal dependence over layers, and support efficient parallel training. We applied a set of preprocessing steps to structure the dataset for optimal learning and introduced our proposed architecture, RTCAN. RTCAN integrates residual causal blocks with contextual attention to enhance the extraction of meaningful EMG temporal features for robust hand gesture recognition. We evaluated our proposed approach on the NinaPro DB1 dataset, where the obtained results demonstrate strong classification performance, with most gestures correctly recognized across different subjects. In the future, we intend to conduct deeper analysis of the misclassified gestures and low performing subjects. Moreover, we will investigate subject independent evaluation of our proposed system. Extending the number of gestures and subjects will inform how our system scales. Another promising area is the implementation of the model as a real-time systems and testing its performance on embedded and low-power devices to determine its practicality for wearable prosthetic applications.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      <p>The authors would like to thank the NinaPro project team for creating and maintaining the NinaPro DB1 dataset and for making it publicly available for research purposes. The authors are also grateful to the contributors whose open-source code and tools facilitated the development and evaluation of this work.</p>
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>20</volume>
          <page-range>2467</page-range>
          <issue>9</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jaramillo-Yánez</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Benalcázar</surname>
              <given-names>M. E.</given-names>
            </name>
            <name>
              <surname>Mena-Maldonado</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s20092467</pub-id>
          <article-title>Real-time hand gesture recognition using surface electromyography and machine learning: A systematic literature review</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>140053</page-range>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Atzori</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Gijsberts</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Castellini</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Caputo</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Hager</surname>
              <given-names>A. G. M.</given-names>
            </name>
            <name>
              <surname>Elsig</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Giatsidis</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Bassetto</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Müller</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/sdata.2014.53</pub-id>
          <article-title>Electromyography data for noninvasive naturally controlled robotic hand prostheses</article-title>
          <source>Sci. Data</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>55</volume>
          <page-range>1956-1965</page-range>
          <issue>8</issue>
          <year>2008</year>
          <person-group person-group-type="author">
            <name>
              <surname>Oskoei</surname>
              <given-names>M. A.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TBME.2008.919734</pub-id>
          <article-title>Support vector machine-based classification scheme for myoelectric control applied to upper limb</article-title>
          <source>IEEE Trans. Biomed. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>458</page-range>
          <issue>3</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Du</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Jin</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Geng</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s17030458</pub-id>
          <article-title>Surface EMG-based inter-session gesture recognition enhanced by deep domain adaptation</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>58</volume>
          <page-range>101838</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Varrecchia</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>D'Anna</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Schmid</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Conforto</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.bspc.2019.101838</pub-id>
          <article-title>Generalization of a wavelet-based algorithm to adaptively detect activation intervals in weak and noisy myoelectric signals</article-title>
          <source>Biomed. Signal Process. Control</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bai</surname>
              <given-names>Shaojie</given-names>
            </name>
            <name>
              <surname>Kolter</surname>
              <given-names>J Zico</given-names>
            </name>
            <name>
              <surname>Koltun</surname>
              <given-names>Vladlen</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1803.01271</pub-id>
          <article-title>An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</article-title>
          <source>arXiv preprint arXiv:1803.01271</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>1003–1012</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lea</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Flynn</surname>
              <given-names>M. D.</given-names>
            </name>
            <name>
              <surname>Vidal</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Reiter</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>G. D. Hager</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CVPR.2017.113</pub-id>
          <article-title>Temporal convolutional networks for action segmentation and detection</article-title>
          <source>, undefined</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>9</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Atzori</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Cognolato</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Müller</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fnbot.2016.00009</pub-id>
          <article-title>Deep learning with convolutional neural networks applied to electromyography data: A resource for the classification of movements for prosthetic hands</article-title>
          <source>Front. Neurorobot.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <page-range>36571</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Geng</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Du</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Jin</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/srep36571</pub-id>
          <article-title>Gesture recognition by instantaneous surface EMG images</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1169–1173</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tsinganos</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Cornelis</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Cornelis</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Jansen</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Skodras</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICASSP.2019.8683239</pub-id>
          <article-title>Improved gesture recognition based on sEMG signals and TCN</article-title>
          <source>Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, U.K.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>244-256</page-range>
          <issue>2</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zanghieri</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Benatti</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Kartsch</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Burrello</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Conti</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Benini</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TBCAS.2019.2959160</pub-id>
          <article-title>Robust real-time embedded EMG recognition framework using temporal convolutional networks on a multicore IoT processor</article-title>
          <source>IEEE Trans. Biomed. Circuits Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1196-1200</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Rahimian</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Zabihi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Asif</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Farina</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Atashzar</surname>
              <given-names>S. F.</given-names>
            </name>
            <name>
              <surname>Mohammadi</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICASSP43922.2022.9746174</pub-id>
          <article-title>Hand gesture recognition using temporal convolutions and attention mechanism</article-title>
          <source>ICASSP 2022–IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>22061</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shin</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Miah</surname>
              <given-names>A. S. M.</given-names>
            </name>
            <name>
              <surname>Konnai</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>I. Takahashi</surname>
            </name>
            <name>
              <surname>Hirooka</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-024-72996-7</pub-id>
          <article-title>Hand gesture recognition using sEMG signals with a multi-stream time-varying feature enhancement approach</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="conf-paper">
          <page-range>5115–5119</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Montazerin</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Zabihi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Rahimian</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Mohammadi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Naderkhani</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/EMBC48229.2022.9871489</pub-id>
          <article-title>ViT-HGR: Vision transformerbased hand gesture recognition from high-density surface EMG signals</article-title>
          <source>2022 44th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Glasgow, U.K.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-6</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhong</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Xiong</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/M2VIP58386.2023.10413402</pub-id>
          <article-title>Aspatio-temporal graph convolutional network for gesture recognition from high-density electromyography</article-title>
          <source>2023 29th International Conference on Mechatronics and Machine Vision in Practice (M2VIP), Queenstown, New Zealand</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>70</volume>
          <page-range>207-216</page-range>
          <issue>3</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Xiang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Pang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1515/bmt-2024-0282</pub-id>
          <article-title>Gesture recognition from surface electromyography signals based on the SE-DenseNet network</article-title>
          <source>Biomed. Eng./Biomed. Tech.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1154–1159</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Palermo</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Rossi</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Marchis</surname>
              <given-names>G. D.</given-names>
            </name>
            <name>
              <surname>Artemi</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Giovacchini</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Vitiello</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICORR.2017.8009405</pub-id>
          <article-title>Repeatability of grasp recognition for robotic hand prosthesis control based on sEMG data</article-title>
          <source>2017 International Conference on Rehabilitation Robotics (ICORR), London, U.K.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>56</page-range>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Luwe</surname>
              <given-names>Y. J.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>C. P.</given-names>
            </name>
            <name>
              <surname>Lim</surname>
              <given-names>K. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/informatics9030056</pub-id>
          <article-title>Wearable sensor-based human activity recognition with hybrid deep learning model</article-title>
          <source>Informatics</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>1476</page-range>
          <issue>4</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Shahabi</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Xia</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Deng</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Alshurafa</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s22041476</pub-id>
          <article-title>Deep learning in human activity recognition with wearable sensors: A review on advances</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1480–1489</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Dyer</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Smola</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Hovy</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18653/v1/N16-1174</pub-id>
          <article-title>Hierarchical attention networks for document classification</article-title>
          <source>Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), San Diego, CA, USA</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>