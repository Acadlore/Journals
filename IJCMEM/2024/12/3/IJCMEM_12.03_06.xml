<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IJCMEM</journal-id>
      <journal-id journal-id-type="doi">10.18280</journal-id>
      <journal-title-group>
        <journal-title>International Journal of Computational Methods and Experimental Measurements</journal-title>
        <abbrev-journal-title abbrev-type="issn">Int. J. Comput. Methods Exp. Meas.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IJCMEM</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2046-0554</issn>
      <issn publication-format="print">2046-0546</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-VUv7FDZX9LGCKroSbC2eLdkuDUIIeCA0</article-id>
      <article-id pub-id-type="doi">10.18280/ijcmem.120306</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Identifying Suitable Deep Learning Approaches for Dental Caries Detection Using Smartphone Imaging</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0006-2284-5591</contrib-id>
          <name>
            <surname>Mehta</surname>
            <given-names>Leena Rohan</given-names>
          </name>
          <email>leena.mehta@cumminscollege.in</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1930-7788</contrib-id>
          <name>
            <surname>Borse</surname>
            <given-names>Megha S.</given-names>
          </name>
          <email/>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-8055-2837</contrib-id>
          <name>
            <surname>Tepan</surname>
            <given-names>Meenal</given-names>
          </name>
          <email/>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0006-1072-1831</contrib-id>
          <name>
            <surname>Shah</surname>
            <given-names>Janhavi</given-names>
          </name>
          <email/>
        </contrib>
        <aff id="aff_1">Department of Electronics and Telecommunication, MKSSS’s Cummins College of Engineering for Women, Pune 411052, India</aff>
        <aff id="aff_2">Department of Oral Medicine &amp; Radiology, Bharati Vidyapeeth’s Dental College, Pune 411052, India</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>29</day>
        <month>09</month>
        <year>2024</year>
      </pub-date>
      <volume>12</volume>
      <issue>3</issue>
      <fpage>251</fpage>
      <lpage>267</lpage>
      <page-range>251-267</page-range>
      <history>
        <date date-type="received">
          <day>07</day>
          <month>08</month>
          <year>2024</year>
        </date>
        <date date-type="accepted">
          <day>26</day>
          <month>09</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>This study aims to identify the most suitable deep learning model for early detection of dental caries in a new database of dental diseases. The study compares the performance of residual and dense networks using standard performance metrics. Dental caries is categorized into four classes based on dental practitioner recommendations. A novel database consisting of 1064 intraoral digital RGB images from 194 patients was collected in collaboration with Bharati Vidyapeeth’s Dental College, Pune. These images were cropped to obtain a total of 987 single-tooth images, which were divided into 888 training, 45 testing, and 54 validation images. In Phase I experimentation, ResNet50V2, ResNet101V2, ResNet152, DenseNet169, and DenseNet201 were utilized. Phase II focused on ResNet50V2, DenseNet169, and DenseNet201, while Phase III concentrated on DenseNet169 and DenseNet201. For Phase I experimentation, the overall accuracy of dental caries classification ranged from 0.55 to 0.84, with DenseNet exhibiting superior performance. In Phase II, the overall accuracy varied from 0.72 to 0.78, with DenseNet achieving the highest accuracy of 0.78. Similarly, in Phase III, DenseNet201 surpassed other models with an overall accuracy of 0.93. The DenseNet201 algorithm shows promise for detecting and classifying dental caries in digital RGB images. This finding is significant for the future development of automated mobile applications based on dental photographs, which could assist dental practitioners during examinations. Additionally, it could enhance patient understanding of dental caries severity, thereby promoting dental health awareness.</p></abstract>
      <kwd-group>
        <kwd>Deep learning</kwd>
        <kwd>Dental caries</kwd>
        <kwd>ResNet50V2</kwd>
        <kwd>ResNet101V2</kwd>
        <kwd>ResNet152</kwd>
        <kwd>DenseNet169</kwd>
        <kwd>DenseNet201</kwd>
        <kwd>Dental imaging</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="4"/>
        <fig-count count="37"/>
        <table-count count="5"/>
        <ref-count count="39"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>“Dental caries”, commonly referred as “tooth decay or cavities”, is recognized as one of the most prevalent chronic diseases globally. “Dental caries” is one such disease that affects individuals across all age groups. As per the reports by “World Health Organization (WHO)”, approximately 60-90% of young children and about all of adult population are affected by dental caries, making it a widespread public health issue. Early childhood caries (ECC) typically initiates at approximately 7 months of age and can progress to affect permanent dentition [<xref ref-type="bibr" rid="ref_1">1</xref>]. According to “Centers for Disease Control and Prevention (CDC)” over 52% of children between 6 and 8 years old have had decay in their primary dentition, indicating the scale of the problem.</p><p style="text-align: justify">ECC not only affects dental health but can have broader impacts on a child’s development, nutrition, and quality of life. As the condition advances, it causes pain, infection, and difficulties in eating, speaking, and sleeping, leading to poorer general health and delayed growth. ECC typically starts as “white-spot lesions” on the “gingival margin of the upper primary incisors” and, if untreated, can result in severe destruction of the tooth crown [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>]. The rapid progression of ECC is exacerbated by modern dietary habits, particularly a diet high in sugars, which accelerates bacterial activity in the mouth [<xref ref-type="bibr" rid="ref_3">3</xref>]. Studies show that sugary foods and drinks are the primary contributors to caries development, especially in young children, further emphasizing the need for preventive strategies and early intervention [<xref ref-type="bibr" rid="ref_4">4</xref>].</p><p style="text-align: justify">The global burden of ECC is alarmingly high, with reports of prevalence reaching up to 70% in preschool-aged children in some regions. Despite the severity of the problem, the disease often goes unnoticed in its early stages, and many parents fail to recognize its potential consequences, assuming that damage to temporary (primary) teeth is insignificant. However, untreated caries in primary teeth can lead to complications in permanent teeth, such as misalignment, enamel hypoplasia, and increased susceptibility to decay in the future [<xref ref-type="bibr" rid="ref_5">5</xref>]. Moreover, untreated ECC can progress to severe ECC (S-ECC), which affects the smooth surfaces of teeth and often requires more invasive treatments, including tooth extractions, under general anaesthesia in severe cases [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>]. Thus, ECC must be treated as a significant health issue rather than a minor inconvenience.</p><p style="text-align: justify">Beyond dental caries, other oral health problems, including gingivitis, periodontal disease, tooth sensitivity, and malocclusion (misaligned teeth), are also common in children. These conditions, if left untreated, can lead to more serious dental and systemic health issues. Suboptimal oral hygiene practices and a delayed identification of oral diseases further exacerbate these problems, contributing to the worldwide impact of oral health. According to surveys conducted in different regions, many children, especially in developing countries, lack access to routine dental care, and parents often neglect the importance of dental hygiene in the early stages of life [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>]. A survey in Riyadh highlighted that awareness and knowledge about dental care, particularly in early childhood, are often lacking, leading to higher incidences of oral health problems [<xref ref-type="bibr" rid="ref_10">10</xref>]. Implementing preventive dental care and education, starting even before teeth emerge, has proven to greatly lower the occurrence of dental caries and other oral health issues.</p><p style="text-align: justify">Early detection of oral health issues is crucial for preventing further damage and reducing the burden on healthcare systems. While larger lesions are often visible during routine dental examinations, the initial stages of caries, such as white-spot lesions, are not easily detectable through visual inspection alone, even by experienced dental practitioners. Tools such as dental mirrors, light sources, and X-rays are commonly used in dental clinics to aid in diagnosis, but these methods are not always accessible in rural or underprivileged areas. The lack of effective screening and detection tools outside of dental offices, particularly in non-dental environments like schools, homes, or community health centers, poses a significant challenge to ensuring early intervention.</p><p style="text-align: justify">This gap underscores the urgent need for developing automated, non-invasive systems for early detection of dental diseases. Advances in technology, particularly in the fields of “artificial intelligence (AI) and deep learning”, offer promising solutions. Automated systems can assist in screening for dental caries and other oral diseases using simple tools, such as smartphones or intraoral cameras, enabling timely identification of issues even in resource-limited environments. Such systems can be especially beneficial in rural or developing regions where access to professional dental care is limited. Moreover, these technologies have the potential to assist healthcare providers and non-professionals alike in identifying early signs of dental diseases, ultimately improving patient outcomes and reducing the time and costs associated with late-stage treatments.</p><p style="text-align: justify">Larger lesions in teeth are visible to the naked eye, but initial stages of dental caries or other dental diseases are not easily detected through visual examination alone, even by trained dental practitioners. They may use tools such as light sources, dental mirrors, or X-rays for a more thorough assessment.</p><p style="text-align: justify">There is a notable absence of effective screening and detection methods for dental diseases, particularly in non-dental environments like schools or homes, especially in rural areas of developing nations. Consequently, there is a pressing need for an automated system using simple tools to facilitate the timely identification of dental diseases. The outcomes of such a system could assist dentists and physicians in conducting thorough oral health examinations and save valuable time. Moreover, these automated systems could mitigate the limitations posed by the lack of training among non-professionals. The organization of the paper employs five major sections. Section 1 covers “Introduction” which offers general overview of the topic followed by bibliographic analysis covered in Section 2. Section 3 provides the details of “Materials and Methods” used for “dental caries” classification. Section 4 is dedicated to the “Results and Discussion”, while Section 5 presents the “Conclusion”.</p>
    </sec>
    <sec sec-type="">
      <title>2. Literature survey</title>
      <p>The existing literature predominantly emphasizes the utilization of 'deep learning algorithms' within the dental field. Numerous systems have been devised for diagnosing and prognosticating various dental diseases. Numerous studies have explored the application of various “machine learning models” for detecting different dental conditions, such as “caries, gingivitis, and other oral health issues”, using digital images. Most approaches have focused on leveraging deep learning models like “convolutional neural networks (CNNs)”, with significant progress reported in accuracy and diagnostic capabilities.</p><p style="text-align: justify">Patil et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] proposed an "Adaptive Dragonfly Algorithm (DA) &amp;amp; Neural Network (NN) classifier" for the classification of 120 “digital X-ray images”, achieving an accuracy of 93%. This enlightens the potential of hybrid models combining optimization algorithms with neural networks. While many of the studies focus on the potential of deep learning and machine learning in dental disease diagnosis, certain limitations persist. Sun et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] provided a comprehensive review on the application of “machine learning” in dentistry, encompassing areas such as “oral cancer, periodontitis, dental caries, diseases of dental pulp and periapical lesion, dental implants, and orthodontics”. For example, Stratigaki et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] evaluated the use of “near-infrared light transillumination (NILT)” alongside bitewing radiography (BWR) for diagnosing dental conditions. Their results indicated that while NILT could be useful for routine examinations, it was not reliable enough to replace BWR for critical treatment decisions. This points to the ongoing challenge of finding non-invasive diagnostic methods that can match the reliability of traditional imaging techniques.</p><p style="text-align: justify">Similarly, Divakaran et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] demonstrated that utilizing "GLCM features, SVM, KNN, and ANN classifiers" can effectively differentiate between decayed teeth and healthy ones. These studies underscore the versatility of machine learning models in handling different types of dental images.</p><p style="text-align: justify">Another area of research involves using machine learning for predictive analysis. Park and Choi [<xref ref-type="bibr" rid="ref_15">15</xref>] introduced the concept of "decayed occupied teeth" (DOT) to assess the relationship between feeding practices and cavity development in infants. Their study used logistic regression to find a significant correlation between feeding practices and early cavity development, revealing lower instances of cavities in children who consumed external foods compared to those exclusively breastfed. Similarly, Hung et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] analyzed a large dataset of 5135 samples using SVM, XGBoost, random forest, KNN, and logistic regression classifiers to predict root caries based on patient age. They achieved 95% accuracy with SVM, indicating that machine learning models can be highly effective for predictive analysis in dentistry.</p><p style="text-align: justify">A notable trend is the use of “CNNs” for diagnosing dental diseases. The studies [<xref ref-type="bibr" rid="ref_17">17</xref>], [<xref ref-type="bibr" rid="ref_18">18</xref>], [<xref ref-type="bibr" rid="ref_19">19</xref>] demonstrated the superiority of “CNNs” for dental applications, particularly in detecting caries. “CNNs” have proven to be highly effective in processing dental images. In terms of future research directions, Chen et al. [<xref ref-type="bibr" rid="ref_20">20</xref>] emphasized the importance of collaboration between clinicians, researchers, and engineers to advance AI integration into dentistry. They argued that interdisciplinary efforts are necessary to ensure that AI tools not only achieve high accuracy but are also practical and user-friendly in clinical settings. Javid et al. [<xref ref-type="bibr" rid="ref_21">21</xref>] employed ResNet50 to detect enamel decay from digital photographs with a 95% accuracy rate. However, while CNNs show great promise, their performance often depends on the quality and type of data used. For instance, Leo and Reddy [<xref ref-type="bibr" rid="ref_22">22</xref>] proposed a “hybrid neural network (HNN)” combining “artificial neural networks (ANN)” and “deep neural networks (DNN)” for dental caries classification on 480 digital radiographs. Their model outperformed traditional CNNs, suggesting that hybrid models may offer advantages over standalone deep learning methods in certain contexts. Additionally, Myint et al. [<xref ref-type="bibr" rid="ref_23">23</xref>] identified a gap in dental caries and gingivitis detection, stressing the need for more comprehensive models that consider bacterial levels and oral hygiene habits alongside image-based data. Uoshima et al. [<xref ref-type="bibr" rid="ref_24">24</xref>] emphasized the importance of a comprehensive skill set, including technical and non-technical skills, in dental education. They suggested integrating artificial intelligence (AI) to enhance the dental education system.</p><p style="text-align: justify">Beyond CNNs, other deep learning architectures have been explored. For example, Verma et al. [<xref ref-type="bibr" rid="ref_25">25</xref>] combined CNN with SVM for image classification, applied to 250 digital radiographs, achieving better performance than conventional CNN approaches. This suggests that hybrid deep learning models may provide more robust solutions for dental disease detection. Kumar et al. [<xref ref-type="bibr" rid="ref_26">26</xref>] conducted a comprehensive review of dental image fractionation and modalities utilized in dental image analysis. Similarly, Chen et al. [<xref ref-type="bibr" rid="ref_27">27</xref>] introduced a stage-wise detection approach for dental image analysis, utilizing a neural network to detect missing teeth and apply a numbering system, but their dataset was limited to 1250 digital X-rays. Tuzoff et al. [<xref ref-type="bibr" rid="ref_28">28</xref>] applied Faster R-CNN to 1594 panoramic dental radiographs, concluding that their proposed method could effectively update digital dental records in practice. Reyes et al. [<xref ref-type="bibr" rid="ref_29">29</xref>] identified the potential of machine learning in various dental subfields but noted the challenge of generalizing machine learning methods across different applications. Additionally, Musri et al. [<xref ref-type="bibr" rid="ref_30">30</xref>] reviewed the use of “deep learning convolutional neural networks (DLCNNs)” for identifying dental problems, concluding that DLCNNs have shown promising results, particularly in detecting dental caries.</p><p style="text-align: justify">A key limitation in many studies is the restricted scope of datasets used. For instance, Zhang et al. [<xref ref-type="bibr" rid="ref_31">31</xref>] developed a multistage “deep learning model” using SSD MobilenetV2 for cavity detection from RGB images, but their study was limited to front teeth, leaving out other areas of the mouth. While these models show high precision and recall, the restricted dataset scope limits their applicability to broader, real-world dental scenarios.</p><p style="text-align: justify">Some researchers have attempted to tackle the challenge of limited datasets by integrating different types of imaging techniques or by expanding the size of their datasets. For example, Rashid et al. [<xref ref-type="bibr" rid="ref_32">32</xref>] used a mixed dataset of 936 digital radiographs and 90 digital photographs to develop a hybrid Mask RCNN model for automated dental caries detection, achieving accuracy levels ranging from 0.78 to 0.92. This demonstrates the potential of combining different image types to improve the robustness of deep learning models in detecting dental diseases across varied conditions. In many other studies [<xref ref-type="bibr" rid="ref_33">33</xref>], [<xref ref-type="bibr" rid="ref_34">34</xref>], [<xref ref-type="bibr" rid="ref_35">35</xref>], [<xref ref-type="bibr" rid="ref_36">36</xref>], [<xref ref-type="bibr" rid="ref_37">37</xref>], [<xref ref-type="bibr" rid="ref_38">38</xref>] specialised models for dedicated dental caries detection were designed.</p><p style="text-align: justify">Additionally, real-time applications of deep learning in clinical settings are beginning to emerge. Hung et al. [<xref ref-type="bibr" rid="ref_39">39</xref>] suggested that a real-time online clinical tool could significantly enhance diagnostic precision for dentists. While tools like CNNs and hybrid models have shown success, the generalization of these methods in clinical practice remains an obstacle due to variability in dental conditions and imaging techniques.</p><p style="text-align: justify">Data from the Indian Dental Association's National Oral Health Programme survey indicates a concerning shortage of dentists relative to the rural population. Dental health in India is further hampered by socioeconomic factors such as limited education, awareness, and economic constraints, leading to severe oral health issues. Dental treatments are often financially prohibitive for disadvantaged families. Therefore, there is a critical need for easily accessible early diagnosis of dental problems. An automated, easily accessible, and cost-effective system for early detection and prognosis of dental issues is essential. Such a system would facilitate timely intervention and treatment, ultimately improving oral healthcare outcomes. Additionally, it would enhance precision in diagnosis and optimize time utilization for dental practitioners. This approach has the potential to significantly enhance oral healthcare accessibility and affordability for underserved communities.</p><p style="text-align: justify">In conclusion, while significant progress has been made in dental healthcare using machine learning models, several gaps remain. Many studies are limited by small datasets, specific imaging techniques, and challenges in generalizing models to clinical environments. More research is needed to validate models in real-world settings and develop AI systems for non-clinical environments, such as rural areas. Additionally, there is a lack of research on automation using digital RGB images, which this study addresses by evaluating deep learning models on intra-oral photographs and comparing residual and dense networks to find the best-performing algorithm.</p>
    </sec>
    <sec sec-type="">
      <title>3. Materials and method</title>
      <p>The literature review identified a notable challenge in dental research: the absence of labeled dental databases containing digital RGB images. This motivated us to create a new database specifically targeting dental diseases. For an overview of the methodology used in our proposed model, please refer to <xref ref-type="fig" rid="fig_1">Figure 1</xref>.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Figure 1. Sample images: (a) Anterior region, teeth in centric occlusion (b) Right posterior region, teeth in centric occlusion (c) Left posterior region, teeth in centric occlusion (d) Anterior region, teeth in edge-to-edge occlusion (e) Palatal/occlusal surface view of maxillary teeth (f) Palatal/ occlusal surface view of mandibular teeth</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_o0ahE8uChuZRCMut.png"/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_SBnVbZr_K1b6Y-Sk.jpeg"/>
        </fig>
      
      
        <sec>
          
            <title>3.1. Noninvasive data acquisition</title>
          
          <p>The collected database comprises 1164 digital photographs of 194 adult patients in the age group 14 to 60 years who came to OPD (out patient’s department) of ‘Bharati Vidyapeeth’s Dental College and Hospital, Pune’. The dentist will assess each patient clinically for the presence of disease or abnormal conditions. The general dental assessment commences from the moment the patient enters the room. The external appearance of the patient includes facial appearance, skin, mobility, smell, etc. General dental assessment is divided into extraoral and intraoral examinations. An extraoral examination is performed for head and neck posture, etc. It involves the symmetry of the face. The extraoral examination is followed by the intraoral examination.</p><p style="text-align: justify">For this procedure, each patient is asked to rinse their mouth with normal tap water. For opening of the mouth, winged cheek retractors are used. For the purpose of intraoral examination, diagnostic instruments including mirror, straight probe and explorer were used. With the help of One plus Nord 2T images were clicked. One plus Nord 2T is with 50-megapixel Sony IMX766 camera sensor with pixel size of 1.0 µm, lens quality of 6P, optical image stabilization, aperture of f/1.8, and ARM Mali-G77 MC9 GPU for post-processing of the image.</p><p style="text-align: justify">For examination and clicking of intraoral cavities images, the patient was asked to sit in a supine position on the dental chair with the light turned off. Camera flash was used when clicking pictures. The camera was adjusted and stabilized at a distance of 2 inches from the lips such that the camera is perpendicular to the plane of the teeth in focus for each image. The patient’s face was adjusted such that the head is parallel to the plane on which the camera lens was adjusted and stabilized. Then on the basis of clinical examination, a diagnosis was made by the dentist.</p><p style="text-align: justify">Six images per patient are taken to cover the entire oral cavity. Sample images are given below. Please refer to <xref ref-type="fig" rid="fig_1">Figure 1</xref>. For the first and second images, the central incisors are considered the object of focus and camera lens will be perpendicular to them. The patient faces directly upwards. The third and fourth images are taken with the first premolar as the focus on the respective sides. For these, the patient is asked to turn their head at 45° to the left side while taking image of the right side and the head will be turned to the right side while taking a picture of the left side. For the first 4 images, a winged cheek retractor will be used to ensure the required details are captured well and for stabilization. For the fifth and sixth images, an intraoral mirror will be placed for taking maxillary and mandibular occlusal surfaces, respectively. The focus will be adjusted to the mirror so that the occlusal surface of each arch is clearly visible.</p><p style="text-align: justify"><inline-formula>
  <mml:math id="moqw5deycg">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula>i. Inclusion criteria</p><p style="text-align: justify">·Patients willing to participate in the study and those who provide their consent.</p><p style="text-align: justify">·Patients with hard tissue diseases including caries, stains, erosion, attrition, abrasion, abfraction, periodontal diseases including gingivitis and malocclusions.</p><p style="text-align: justify">·Patients with healthy teeth, ideal occlusion, and ideal periodontal conditions.</p><p style="text-align: justify">·Images with appropriate resolution taken with predetermined standardized method</p><p style="text-align: justify"><inline-formula>
  <mml:math id="mgkpi3zfno">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula>ii. Exclusion Criteria</p><p style="text-align: justify">·Patients with incomplete clinical records, previous history of surgery or craniofacial anomalies, maxillofacial trauma</p><p style="text-align: justify">·Patients with a history of medical conditions and vulnerable patients</p>
          
            <sec>
              
                <title>3.1.1 Database labeling</title>
              
              <p>Image-wise labeling would have become tedious in this case and the model would also have become complicated, hence tooth-wise labeling is done. Labeling is done in consultation with the dentist. The standard FDI tooth notation system is used. This was also useful for detecting the location of the tooth. Dental caries is classified as Grade 0 (Healthy), Grade 1(Pit and Fissure / Start of cavitation), Grade 2 (Deep Cavities, Structural damage, Occlusal) and Grade 3 (Total loss of tooth structure, root stumps) as suggested by dental practitioner.</p><p style="text-align: justify">According to the FDI notation system, the mouth is divided into four quadrants, each with its specific numbering: “the upper right” quadrant is numbered 11 to 18, “the upper left” 21 to 28, “the lower right” 41 to 48, and “the lower left” 31 to 38. These numbers assist dentists in identifying individual teeth. In our study, we adopt a labeling system that aligns with this numbering. For example, patient 1 is recorded with class 0 caries in tooth 11, while patient 34 has class 3 caries in tooth 45. Dental practitioners document these labels in an Excel sheet, where the first column shows patient identifiers and the other columns list tooth numbers, with caries class numbers noted in the intersecting cells. Images were cropped tooth-wise. They were labeled class-wise as per the excel sheet prepared.</p>
            </sec>
          
          
            <sec>
              
                <title>3.1.2 Data augmentation</title>
              
              <p><span style="color: rgb(30, 30, 30); font-family: Lato, Helvetica Neue, Arial, sans-serif">Consider the database <italic>‘D’</italic><span style="color: rgb(30, 30, 30); font-family: Lato, Helvetica Neue, Arial, sans-serif"> as the collection of original RGB images of individual teeth. Initially, the database was organized into two primary folders namely “train and test”. Each of these folders was further divided into subfolders corresponding to different classes, with images manually sorted according to the type of “dental caries” with the assistance of a dental practitioner. Upon conducting a statistical analysis, it was discovered that the database is significantly imbalanced, as class 0 contains nearly 4,000 images, while classes 1 and 2 have considerably fewer images. To address this imbalance, “data augmentation techniques” such as image scaling, zooming, flipping, and shearing were applied. This process also involved filling in missing values and encoding the dataset to prepare it for further processing.</p><p>Let <inline-formula>
  <mml:math id="m3bfqquar0">
    <mml:msub>
      <mml:mi>l</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> represents the images where <inline-formula>
  <mml:math id="myscxbplq0">
    <mml:mi>i</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>1</mml:mn>
    <mml:mn>2</mml:mn>
    <mml:mn>3</mml:mn>
    <mml:mn>4</mml:mn>
  </mml:math>
</inline-formula>. Each image is located in the dataset ' $D<inline-formula>
  <mml:math id="mzp8laj86m">
    <mml:msup>
      <mml:mi/>
      <mml:mo>′</mml:mo>
    </mml:msup>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>I_j(x, y)<inline-formula>
  <mml:math id="mtlgozkdi5">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>(150 \times 150)$. After this rescaling of the image was carried out as shown in Eq. (1).</p><p style="text-align: justify">1. Rescaling</p>
              
                <disp-formula>
                  <label>(1)</label>
                  <mml:math id="mtxqhcpku0">
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mi>r</mml:mi>
                    </mml:msub>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msup>
                        <mml:mi>x</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:msup>
                        <mml:mi>y</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mfrac>
                        <mml:mi>x</mml:mi>
                        <mml:msub>
                          <mml:mi>s</mml:mi>
                          <mml:mi>x</mml:mi>
                        </mml:msub>
                      </mml:mfrac>
                      <mml:mfrac>
                        <mml:mi>y</mml:mi>
                        <mml:msub>
                          <mml:mi>s</mml:mi>
                          <mml:mi>y</mml:mi>
                        </mml:msub>
                      </mml:mfrac>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mfrac>
                        <mml:mi>x</mml:mi>
                        <mml:mn>255</mml:mn>
                      </mml:mfrac>
                      <mml:mfrac>
                        <mml:mi>y</mml:mi>
                        <mml:mn>255</mml:mn>
                      </mml:mfrac>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mi>I</mml:mi>
                    <mml:mi>I</mml:mi>
                  </mml:math>
                </disp-formula>
              
              <p>2. Shear transformation</p>
              
                <disp-formula>
                  <label>(2)</label>
                  <mml:math id="miqx1g6gz7">
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mi>s</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mi>s</mml:mi>
                    </mml:msub>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msup>
                        <mml:mi>x</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:msup>
                        <mml:mi>y</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>[</mml:mo>
                      <mml:mo>]</mml:mo>
                      <mml:msub>
                        <mml:mi>I</mml:mi>
                        <mml:mi>r</mml:mi>
                      </mml:msub>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msup>
                          <mml:mi>x</mml:mi>
                          <mml:mrow>
                            <mml:mi>′</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                        <mml:msup>
                          <mml:mi>y</mml:mi>
                          <mml:mrow>
                            <mml:mi>′</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msup>
                        <mml:mi>x</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:msup>
                        <mml:mi>y</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:msup>
                        <mml:mi>y</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mi>k</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msup>
                        <mml:mi>x</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:msup>
                        <mml:mi>y</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:msup>
                        <mml:mi>y</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mn>0.2</mml:mn>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mi>I</mml:mi>
                    <mml:mi>I</mml:mi>
                  </mml:math>
                </disp-formula>
              
              <p>3. Zoom transformation</p><p style="text-align: justify">Pixel values of the image <italic>x'</italic> and <italic>y' </italic>are updated in this phase of augmentation. It is scaled up by a factor of 20%. Nearest neighbour interpolation method is used.</p>
              
                <disp-formula>
                  <label>(3)</label>
                  <mml:math id="mdizuhsh61">
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mi>z</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mi>s</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mi>s</mml:mi>
                    </mml:msub>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msup>
                        <mml:mi>x</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:msup>
                        <mml:mi>y</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mfrac>
                        <mml:msup>
                          <mml:mi>x</mml:mi>
                          <mml:mrow>
                            <mml:mi>′</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                        <mml:msub>
                          <mml:mi>s</mml:mi>
                          <mml:mi>x</mml:mi>
                        </mml:msub>
                      </mml:mfrac>
                      <mml:mfrac>
                        <mml:msup>
                          <mml:mi>y</mml:mi>
                          <mml:mrow>
                            <mml:mi>′</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                        <mml:msub>
                          <mml:mi>s</mml:mi>
                          <mml:mi>y</mml:mi>
                        </mml:msub>
                      </mml:mfrac>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mfrac>
                        <mml:msup>
                          <mml:mi>x</mml:mi>
                          <mml:mrow>
                            <mml:mi>′</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                        <mml:mn>0.2</mml:mn>
                      </mml:mfrac>
                      <mml:mfrac>
                        <mml:msup>
                          <mml:mi>y</mml:mi>
                          <mml:mrow>
                            <mml:mi>′</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                        <mml:mn>0.2</mml:mn>
                      </mml:mfrac>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:mo>=</mml:mo>
                  </mml:math>
                </disp-formula>
              
              <p>4. Horizontal flip</p>
              
                <disp-formula>
                  <label>(4)</label>
                  <mml:math id="m6pnzlxdcv">
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mi>f</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mi>z</mml:mi>
                    </mml:msub>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msup>
                        <mml:mi>x</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:msup>
                        <mml:mi>y</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>−</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>W</mml:mi>
                        <mml:msup>
                          <mml:mi>x</mml:mi>
                          <mml:mrow>
                            <mml:mi>′</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                      <mml:msup>
                        <mml:mi>y</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="myt644yi0z">
    <mml:mi>W</mml:mi>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula> width of the image.</p><p style="text-align: justify">5. Sequential operation for augmented output image</p>
              
                <disp-formula>
                  <label>(5)</label>
                  <mml:math id="mvrpbmn4z1">
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mrow>
                        <mml:mtext>augmented </mml:mtext>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mi>r</mml:mi>
                    </mml:msub>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msup>
                        <mml:mi>x</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                      <mml:msup>
                        <mml:mi>y</mml:mi>
                        <mml:mrow>
                          <mml:mi>′</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>−</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>W</mml:mi>
                        <mml:mfrac>
                          <mml:mrow>
                            <mml:msup>
                              <mml:mi>x</mml:mi>
                              <mml:mrow>
                                <mml:mi>′</mml:mi>
                              </mml:mrow>
                            </mml:msup>
                            <mml:msup>
                              <mml:mi>y</mml:mi>
                              <mml:mrow>
                                <mml:mi>′</mml:mi>
                              </mml:mrow>
                            </mml:msup>
                            <mml:mo>−</mml:mo>
                            <mml:mi>k</mml:mi>
                          </mml:mrow>
                          <mml:msub>
                            <mml:mi>s</mml:mi>
                            <mml:mi>x</mml:mi>
                          </mml:msub>
                        </mml:mfrac>
                      </mml:mrow>
                      <mml:mfrac>
                        <mml:msup>
                          <mml:mi>y</mml:mi>
                          <mml:mrow>
                            <mml:mi>′</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                        <mml:msub>
                          <mml:mi>s</mml:mi>
                          <mml:mi>y</mml:mi>
                        </mml:msub>
                      </mml:mfrac>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                  </mml:math>
                </disp-formula>
              
              <p>This equation represents the final pixel value after applying the rescaling, shear, zoom, and horizontal flip operations sequentially. <xref ref-type="fig" rid="fig_2">Figure 2</xref> showcases sample augmented sample images.</p>
              
                <fig id="fig_2">
                  <label>Figure 2</label>
                  <caption>
                    <title>Figure 2. Augmentaged (a) Class 0 (b) Class 1 (c) Class 2 (d) Class 3</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img__BFD6w0kXJcW5r5G.jpeg"/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_gaDQZWWi6FHAsuAD.jpeg"/>
                </fig>
              
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Methodology</title>
          
          
            <sec>
              
                <title>3.2.1 Deep neural network models</title>
              
              <p>“ResNet” and “DenseNet” variants were chosen for their proven performance in complex image classification tasks, including medical imaging, due to their ability to handle deep network training and feature extraction effectively. ResNet's residual learning framework helps mitigate the vanishing gradient problem, enabling it to train very deep networks and extract features from intricate dental images with high accuracy. DenseNet’s feature reuse mechanism enhances gradient flow and feature propagation, which is advantageous for detecting subtle patterns in dental images. Both models are also known for their efficiency in training and inference, which is crucial given the computational resources available for dental datasets. Their previous success in medical and dental imaging tasks further validates their suitability for this study. The advanced feature extraction capabilities of ResNet and DenseNet, along with their scalability to various dataset sizes and image resolutions, align well with the diverse nature of dental images. Supported by existing literature, these models are confirmed to be effective for dental image analysis, ensuring they are well-suited to deliver high accuracy and reliable results for this research.</p><p style="text-align: justify">The study was conducted in three phases, each utilizing different numbers of images across all classes. Phase I involved 240 images, Phase II utilized 800 images, and Phase III used 967 images. During each phase, the performance of residual networks and dense networks was compared for dental caries classification on the novel dataset. Standard architectures of Residual networks and DenseNet were adapted to accommodate the customized dataset, with certain layers modified as needed. Hyperparameter tuning was conducted to enhance performance. In total, three models of residual networks and three models of dense networks were implemented. Fine-tuning and adjustments were made to these models to improve accuracy and training. The following architectures were employed. Python was used for implementation of the models. Libraries like “Numpy, Pandas, Matplotlib, Sklearn, Seaborn, Tensorflow, Keras” etc. were used.</p><p style="text-align: justify">A. Modified residual networks</p><p style="text-align: justify">The residual networks comprise skip connections, due to which the issue of vanishing gradients is resolved up to great extent in backward propagation. Three different layer models were implemented, namely “ResNet50, ResNet101 and ResNet152”. The number in the name of the model indicates the depth of the model. For example, ResNet50 is 50 layers deep and so on. In the model implemented, we changed the pooling function from Maxpooling to Average pooling. <xref ref-type="fig" rid="fig_3">Figure 3</xref> shows the implementation of “ResNet50V2” for the collected database.</p>
              
                <fig id="fig_3">
                  <label>Figure 3</label>
                  <caption>
                    <title>Modified ResNet50v2 architecture</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_8qVaa9gVerNWE2mC.png"/>
                </fig>
              
              <p>The implemented ResNet model comprises functional layers such as “average pooling, batch normalization, dropout, and dense layers”. The input image size used was (256×256). It incorporates skip connections, enabling direct connections from input to output to mitigate the issue of vanishing gradients. The network is pretrained on the large-scale "Imagenet" database with millions of images. Base model of the residual network is frozen and top layers were added to deal with novel database. The model employs the “categorical cross-entropy loss function” and the “Adam optimizer” with a standard learning rate, along with early stopping. A batch size of 32 was employed for training. Dropout layer with a factor of 0.2 was added to avoid overfitting of the model. The 'Relu' activation function was employed, while the “softmax” activation function was used in the last dense layer to reduce data dimensionality from 2048 to 256 in this study. Similar configurations and layers are utilized for ResNet101 and ResNet152 models.</p><p style="text-align: justify">B. Modified DenseNet</p><p style="text-align: justify">DenseNet is a parametrically efficient model that has been pre-trained on large datasets such as "ImageNet." In our approach, we utilized transfer learning by leveraging the pretrained DenseNet model. Unlike residual networks, DenseNet exhibits strong connectivity between all previous and future layers. This dense connectivity allows even smaller features from the initial layer to influence the final feature maps. This connectivity proves advantageous, especially in smaller object databases like dental images. Additionally, DenseNet is known to perform better in mitigating the vanishing gradient problem compared to other models. <xref ref-type="fig" rid="fig_4">Figure 4</xref> presents the proposed DenseNet201 model for the classification of dental diseases.</p>
              
                <fig id="fig_4">
                  <label>Figure 4</label>
                  <caption>
                    <title>Modified DenseNet201 model architecture</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_cJfJZEc2b7MpqgSW.png"/>
                </fig>
              
              <p style="text-align: justify">1. Base model (DenseNet201)</p><p style="text-align: justify">The primary base model employed in this study utilizes “DenseNet201”, which features four dense blocks. Each dense block incorporates a bottleneck layer with a (1×1) convolutional filter, preceding a (3×3) “convolutional layer”. This design is computationally efficient to reduce the size of the feature maps maintaining high feature extraction quality. A transfer learning approach was employed, leveraging the “DenseNet201 architecture” pre-trained on the “ImageNet dataset”, which includes millions of labelled images across thousands of classes. The pre-trained weights provide a robust foundation for feature extraction. During training, the weights of the “DenseNet201 base model” were frozen. Thus, only top layer weights were updated resulting adaptation of the model for the specific task of dental caries classification without altering the foundational features learned. This selective training approach contrasts with the standard DenseNet201 model, where all weights, including those in the base architecture, are typically trainable by default unless specified otherwise.</p><p style="text-align: justify">2. Top layers of the model</p><p style="text-align: justify">Following the DenseNet201 base model, the proposed algorithm integrates several additional layers designed to tailor the model's learned features to the specific task of dental caries classification across four classes. These top layers include “Global Average Pooling, Batch Normalization, Dropout, and Dense layers”. The “Global Average Pooling layer (GlobalAvgPool2D ())” replaces traditional flattening layers, reducing spatial dimensions by averaging each feature map, thereby capturing global spatial information while minimizing overfitting risks. Furthermore, to decrease the dimensionality of the feature matrix and reduce the number of trainable parameters, a (2×2) average pooling with a stride of 2 was applied.</p><p style="text-align: justify">"Batch Normalization" is implemented to stabilize the output by normalizing it to have a "zero mean and unit variance," which speeds up training and minimizes internal covariate shifts. The “Dropout layer” randomly set 20% of the input units to zero during training updates that prevent overfitting. The architecture included two “fully connected Dense layers” each with 128 units, utilizing “ReLU (Rectified Linear Unit) activation function” to introduce “non-linearity” and identify complex patterns within the data. The combination of these layers effectively transforms the 4D tensor output from DenseNet201 into a 1D tensor suitable for the final classification task.</p><p style="text-align: justify">3. Output Layer</p><p style="text-align: justify">The proposed model culminates in an output layer specifically designed for the classification of dental caries into four classes. It features a "dense layer" with a number of units equal to the four classes, using the "Softmax activation function" to produce a "probability distribution" over the classes, making it ideal for multi-class classification tasks. Unlike the standard DenseNet architecture, which typically concludes with classification layers following four dense blocks, this study modifies the architecture by incorporating a combination of global average 2D pooling and three BDD layers (“Batch Normalization, Dropout, and Dense layers”). Image batch size used was of 32 and dropout factor of 0.2 was utilized. The final “Dense layer” applies the “Softmax activation function” to output a flattened vector representing the model's confidence in each class. The model is trained using the “Adam optimizer with a standard learning rate”, ensuring efficient and effective convergence.</p><p style="text-align: justify">This proposed model’s architecture, with its customized top layers and output configuration, presents a novel approach for the task of dental caries detection, capitalizing on the strength of DenseNet201's feature extraction capabilities while adapting the model to the specific needs of this classification problem.</p><p style="text-align: justify">C. Modified DenseNet201’s architectural benefits for dental caries classification</p><p style="text-align: justify">1. Leveraging DenseNet201 for feature extraction</p><p style="text-align: justify"><inline-formula>
  <mml:math id="mgx50okhe2">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula>a. Dense connections: It allows reuse of all features reducing the need for redundant parameters resulting in more compact and efficient representations. “DenseNet” architecture inherently mitigates the “vanishing gradient problem”, maintaining high efficiency and reduced computational costs.</p><p style="text-align: justify"><inline-formula>
  <mml:math id="ma6w6go4u3">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula>b. Pre-trained Weights: Since the proposed model was already pre-trained on the “ImageNet dataset”, it is beneficial in scenarios with limited data availability, enabling the model to generalize more effectively on smaller, domain-specific datasets.</p><p style="text-align: justify"><inline-formula>
  <mml:math id="mus5askmtb">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula>c. Adaptability: In the proposed model, the weights of the DenseNet201 base layers are frozen, allowing the network to retain the features learned from "ImageNet" while concentrating on fine-tuning the newly added dense layers.</p><p style="text-align: justify">2. Efficient dimensionality reduction</p><p style="text-align: justify"><inline-formula>
  <mml:math id="mp6ysyr8ty">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula>a. Dimensionality Reduction: “Global Average Pooling (GAP)” reduces each feature map to a single value, transforming high-dimensional tensors into lower-dimensional vectors without losing spatial information. This technique not only reduces the risk of overfitting, particularly in complex models, but also maintains translational invariance, making the model robust to different spatial configurations.</p><p style="text-align: justify"><inline-formula>
  <mml:math id="msk9gd8tya">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula>b. Contrast with Flattening: Traditional flattening methods can lead to a high number of parameters, thereby increasing the risk of overfitting. GAP addresses this issue by minimizing the number of parameters while preserving crucial features.</p><p style="text-align: justify">3. Robust Regularization with Batch Normalization and Dropout</p><p style="text-align: justify"><inline-formula>
  <mml:math id="m7etpop3r4">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula>a. Combination of “Batch Normalization and Dropout: Integrating “Batch Normalization and Dropout” between layers is a novel approach that ensures model stability during training. Batch Normalization reduces internal covariate shifts by normalizing inputs, accelerating training and improving gradient flow through the network. Dropout, as a regularization technique, randomly drops neurons during training, preventing co-adaptation and improving the model's generalization.</p><p style="text-align: justify"><inline-formula>
  <mml:math id="ma6fnkl2ho">
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="1em"/>
    </mml:mstyle>
  </mml:math>
</inline-formula>b. Effect on Convergence: The sequential application of Batch Normalization and Dropout enhances convergence speed and stability, often leading to better accuracy and robustness against noise in the input data.</p><p style="text-align: justify">D. Mathematical model of modified DenseNet201</p><p style="text-align: justify">Let <span style="font-family: Times New Roman, serif">‘<italic><span style="font-family: Times New Roman, serif">D</italic><span style="font-family: Times New Roman, serif">’ be the image database containing the images of “class 0, class 1, class 2 and class 3”. As explained in Section 3.1, Data augmentation was carried out for increasing the number of images and balancing the class-wise dataset. Following mathematical operations were carried out on the images of the dataset.</p><p style="text-align: justify">1. Base Model Transformation</p><p style="text-align: justify">Each input image <inline-formula>
  <mml:math id="mdlniynm3s">
    <mml:msup>
      <mml:mi>X</mml:mi>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msup>
    <mml:mo>=</mml:mo>
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mrow>
        <mml:mtext>augmented </mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msup>
        <mml:mi>x</mml:mi>
        <mml:mrow>
          <mml:mi>′</mml:mi>
        </mml:mrow>
      </mml:msup>
      <mml:msup>
        <mml:mi>y</mml:mi>
        <mml:mrow>
          <mml:mi>′</mml:mi>
        </mml:mrow>
      </mml:msup>
    </mml:mrow>
  </mml:math>
</inline-formula> from the database is transformed into feature maps <inline-formula>
  <mml:math id="mnnqiiajpb">
    <mml:msubsup>
      <mml:mi>X</mml:mi>
      <mml:mrow>
        <mml:mtext>Base </mml:mtext>
      </mml:mrow>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msubsup>
  </mml:math>
</inline-formula> using the Modified DenseNet201 architecture.</p>
              
                <disp-formula>
                  <label>(6)</label>
                  <mml:math id="muwfhvwjti">
                    <mml:msubsup>
                      <mml:mi>X</mml:mi>
                      <mml:mrow>
                        <mml:mtext>Base </mml:mtext>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>=</mml:mo>
                    <mml:mi>D</mml:mi>
                    <mml:mi>e</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mi>s</mml:mi>
                    <mml:mi>e</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mi>e</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mn>201</mml:mn>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msup>
                        <mml:mi>X</mml:mi>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mi>i</mml:mi>
                        </mml:mrow>
                      </mml:msup>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="mry9xudt9n">
    <mml:msup>
      <mml:mrow>
        <mml:mi>X</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msup>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula> ith Image in the database ' D '; <inline-formula>
  <mml:math id="mw6ky4kop8">
    <mml:msubsup>
      <mml:mi>X</mml:mi>
      <mml:mrow>
        <mml:mtext>Base </mml:mtext>
      </mml:mrow>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msubsup>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula> feature map with shape <inline-formula>
  <mml:math id="m2y6r262wc">
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mn>4</mml:mn>
    <mml:mn>4</mml:mn>
    <mml:mn>1920</mml:mn>
  </mml:math>
</inline-formula>.</p><p>2. Global Average Pooling 2D</p><p style="text-align: justify">The feature maps <inline-formula>
  <mml:math id="mfv54r7trf">
    <mml:msubsup>
      <mml:mi>X</mml:mi>
      <mml:mrow>
        <mml:mtext>Base </mml:mtext>
      </mml:mrow>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msubsup>
  </mml:math>
</inline-formula> are reduced to a 1D vector <inline-formula>
  <mml:math id="mb6bjx1ewt">
    <mml:msubsup>
      <mml:mi>X</mml:mi>
      <mml:mrow>
        <mml:mtext>Pooled </mml:mtext>
      </mml:mrow>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msubsup>
  </mml:math>
</inline-formula> by applying global average pooling. Eq. (7) represents standard equation for global average pooling and Eq. (8) shows Global average pooling implemented in Modified DenseNet201 model.</p>
              
                <disp-formula>
                  <label>(7)</label>
                  <mml:math id="mrfkp5wbbo">
                    <mml:mi>G</mml:mi>
                    <mml:mi>A</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>f</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:mn>1</mml:mn>
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mi>H</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>W</mml:mi>
                        </mml:mrow>
                        <mml:mo>×</mml:mo>
                      </mml:mrow>
                    </mml:mfrac>
                    <mml:munderover>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mi>H</mml:mi>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:munderover>
                    <mml:munderover>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mi>j</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mi>w</mml:mi>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:munderover>
                    <mml:msub>
                      <mml:mi>f</mml:mi>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mi>j</mml:mi>
                        <mml:mo>,</mml:mo>
                      </mml:mrow>
                    </mml:msub>
                  </mml:math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(8)</label>
                  <mml:math id="mcylv7ry6a">
                    <mml:mtable displaystyle="true" columnspacing="1em" rowspacing="3pt">
                      <mml:mtr>
                        <mml:mtd>
                          <mml:msubsup>
                            <mml:mi>X</mml:mi>
                            <mml:mrow>
                              <mml:mtext>Pooled </mml:mtext>
                            </mml:mrow>
                            <mml:mrow>
                              <mml:mo>(</mml:mo>
                              <mml:mo>)</mml:mo>
                              <mml:mi>i</mml:mi>
                            </mml:mrow>
                          </mml:msubsup>
                          <mml:msubsup>
                            <mml:mi>X</mml:mi>
                            <mml:mi>i</mml:mi>
                            <mml:mrow>
                              <mml:mtext>Base </mml:mtext>
                            </mml:mrow>
                          </mml:msubsup>
                          <mml:mo>=</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mfrac>
                            <mml:mn>1</mml:mn>
                            <mml:mrow>
                              <mml:mn>4</mml:mn>
                              <mml:mn>4</mml:mn>
                              <mml:mo>×</mml:mo>
                            </mml:mrow>
                          </mml:mfrac>
                          <mml:munderover>
                            <mml:mo>∑</mml:mo>
                            <mml:mrow>
                              <mml:mi>x</mml:mi>
                              <mml:mo>=</mml:mo>
                              <mml:mn>1</mml:mn>
                            </mml:mrow>
                            <mml:mn>4</mml:mn>
                          </mml:munderover>
                          <mml:munderover>
                            <mml:mo>∑</mml:mo>
                            <mml:mrow>
                              <mml:mi>y</mml:mi>
                              <mml:mo>=</mml:mo>
                              <mml:mn>1</mml:mn>
                            </mml:mrow>
                            <mml:mn>4</mml:mn>
                          </mml:munderover>
                          <mml:mi>x</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mi>c</mml:mi>
                        </mml:mtd>
                      </mml:mtr>
                      <mml:mtr>
                        <mml:mtd>
                          <mml:mtext> For </mml:mtext>
                          <mml:mi>c</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>…</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mn>1</mml:mn>
                          <mml:mn>2</mml:mn>
                          <mml:mn>3</mml:mn>
                          <mml:mn>1920</mml:mn>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="m5wqcapnsf">
    <mml:msubsup>
      <mml:mi>X</mml:mi>
      <mml:mrow>
        <mml:mtext>Pooled </mml:mtext>
      </mml:mrow>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msubsup>
  </mml:math>
</inline-formula> has shape (1920); <inline-formula>
  <mml:math id="mhm7rnrjfj">
    <mml:mi>c</mml:mi>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula> Number of channels.</p><p>3. Batch normalization Layer 1</p><p style="text-align: justify">Batch normalization is applied to the pooled feature map <inline-formula>
  <mml:math id="moo98dt66y">
    <mml:msubsup>
      <mml:mi>X</mml:mi>
      <mml:mrow>
        <mml:mtext>Pooled </mml:mtext>
      </mml:mrow>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msubsup>
  </mml:math>
</inline-formula>. Eq. (9) shows standard form of Batch Normalization and Eq. (10), shows implemented one.</p>
              
                <disp-formula>
                  <label>(9)</label>
                  <mml:math id="mfnye3i9ib">
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">BN</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>x</mml:mi>
                          <mml:mi>μ</mml:mi>
                          <mml:mo>−</mml:mo>
                        </mml:mrow>
                        <mml:msqrt>
                          <mml:msup>
                            <mml:mi>σ</mml:mi>
                            <mml:mn>2</mml:mn>
                          </mml:msup>
                          <mml:mo>+</mml:mo>
                          <mml:mi>ϵ</mml:mi>
                        </mml:msqrt>
                      </mml:mfrac>
                    </mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mi>γ</mml:mi>
                    <mml:mi>β</mml:mi>
                  </mml:math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(10)</label>
                  <mml:math id="m9najy9l2k">
                    <mml:msubsup>
                      <mml:mi>X</mml:mi>
                      <mml:mrow>
                        <mml:mtext>Normalized </mml:mtext>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>=</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:msub>
                      <mml:mi>γ</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>β</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msub>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:msubsup>
                          <mml:mi>X</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mrow>
                            <mml:mtext>Pooled </mml:mtext>
                          </mml:mrow>
                        </mml:msubsup>
                        <mml:mo>−</mml:mo>
                        <mml:msub>
                          <mml:mi>μ</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                      </mml:mrow>
                      <mml:msqrt>
                        <mml:msubsup>
                          <mml:mi>σ</mml:mi>
                          <mml:mn>1</mml:mn>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                        <mml:mo>+</mml:mo>
                        <mml:mi>ϵ</mml:mi>
                      </mml:msqrt>
                    </mml:mfrac>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="meh5en1qbi">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mdrtnm1ss0">
    <mml:msubsup>
      <mml:mi>σ</mml:mi>
      <mml:mn>1</mml:mn>
      <mml:mn>2</mml:mn>
    </mml:msubsup>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula> Mean and variance of <inline-formula>
  <mml:math id="mmxwlksuoa">
    <mml:msubsup>
      <mml:mi>X</mml:mi>
      <mml:mi>i</mml:mi>
      <mml:mrow>
        <mml:mtext>Pooled </mml:mtext>
      </mml:mrow>
    </mml:msubsup>
  </mml:math>
</inline-formula> within the batch of 32 images respectively; <inline-formula>
  <mml:math id="mp2stxr394">
    <mml:msub>
      <mml:mi>γ</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mirin50liy">
    <mml:msub>
      <mml:mi>β</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula> learnable parameters</p><p>4. Dropout Layer 1</p><p style="text-align: justify">Dropout is applied to normalized feature map <inline-formula>
  <mml:math id="mv4ompvva4">
    <mml:msubsup>
      <mml:mi>X</mml:mi>
      <mml:mrow>
        <mml:mtext>Normalized </mml:mtext>
        <mml:mn>1</mml:mn>
      </mml:mrow>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msubsup>
  </mml:math>
</inline-formula>.</p>
              
                <disp-formula>
                  <label>(11)</label>
                  <mml:math id="mtk8s55vmw">
                    <mml:msubsup>
                      <mml:mi>X</mml:mi>
                      <mml:mrow>
                        <mml:mtext>Dropped </mml:mtext>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>=</mml:mo>
                    <mml:mi>Dropout</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msubsup>
                        <mml:mi>X</mml:mi>
                        <mml:mrow>
                          <mml:mtext>Normalized </mml:mtext>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mi>i</mml:mi>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:mrow>
                        <mml:mi>p</mml:mi>
                      </mml:mrow>
                      <mml:mn>0.2</mml:mn>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="m5ifzpb5np">
    <mml:mi>p</mml:mi>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula> Dropout rate <inline-formula>
  <mml:math id="my5quylf77">
    <mml:mo>=</mml:mo>
    <mml:mn>20</mml:mn>
    <mml:mi>%</mml:mi>
  </mml:math>
</inline-formula> of the neurons are randomly set to zero.</p><p>5. Dense Layer 1</p><p style="text-align: justify">A fully connected dense layer with ReLU activation was employed.</p>
              
                <disp-formula>
                  <label>(12)</label>
                  <mml:math id="mtlg072tiw">
                    <mml:msubsup>
                      <mml:mi>X</mml:mi>
                      <mml:mrow>
                        <mml:mtext>Dense1 </mml:mtext>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>=</mml:mo>
                    <mml:mi>ReLU</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>.</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mrow>
                        <mml:mi>W</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>B</mml:mi>
                      </mml:mrow>
                      <mml:mn>1</mml:mn>
                      <mml:mn>1</mml:mn>
                      <mml:msubsup>
                        <mml:mi>X</mml:mi>
                        <mml:mrow>
                          <mml:mtext>Dropped </mml:mtext>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mi>i</mml:mi>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where, W1=Weight Matrix of the dense layer; B1=Bias Vector.</p><p style="text-align: justify">6. Batch normalization Layer 2</p>
              
                <disp-formula>
                  <label>(13)</label>
                  <mml:math id="m713tl3irr">
                    <mml:msubsup>
                      <mml:mi>X</mml:mi>
                      <mml:mrow>
                        <mml:mtext>Normalized </mml:mtext>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>=</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:msub>
                      <mml:mi>γ</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>β</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msub>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:msubsup>
                          <mml:mi>X</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mrow>
                            <mml:mtext>Dense1 </mml:mtext>
                          </mml:mrow>
                        </mml:msubsup>
                        <mml:mo>−</mml:mo>
                        <mml:msub>
                          <mml:mi>μ</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msub>
                      </mml:mrow>
                      <mml:msqrt>
                        <mml:msubsup>
                          <mml:mi>σ</mml:mi>
                          <mml:mn>2</mml:mn>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                        <mml:mo>+</mml:mo>
                        <mml:mi>ϵ</mml:mi>
                      </mml:msqrt>
                    </mml:mfrac>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="m4kqxkhvyd">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="m8img8iae9">
    <mml:msubsup>
      <mml:mi>σ</mml:mi>
      <mml:mn>2</mml:mn>
      <mml:mn>2</mml:mn>
    </mml:msubsup>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula> Mean and variance of <inline-formula>
  <mml:math id="mixkp7551o">
    <mml:msubsup>
      <mml:mi>X</mml:mi>
      <mml:mrow>
        <mml:mtext>Dense1 </mml:mtext>
      </mml:mrow>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msubsup>
  </mml:math>
</inline-formula> within the batch of respectively; <inline-formula>
  <mml:math id="mqrzsatyjr">
    <mml:msub>
      <mml:mi>γ</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mqi5d7ks5g">
    <mml:msub>
      <mml:mi>β</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula> learnable parameters.</p><p>7. Dropout Layer 2</p><p style="text-align: justify">Dropout is applied to normalized feature map <inline-formula>
  <mml:math id="mxqh7mfgoq">
    <mml:msubsup>
      <mml:mi>X</mml:mi>
      <mml:mrow>
        <mml:mtext>Normalized2 </mml:mtext>
      </mml:mrow>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msubsup>
  </mml:math>
</inline-formula>.</p>
              
                <disp-formula>
                  <label>(14)</label>
                  <mml:math id="md0beri4m9">
                    <mml:msubsup>
                      <mml:mi>X</mml:mi>
                      <mml:mrow>
                        <mml:mtext>Dropped </mml:mtext>
                        <mml:mn>2</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>=</mml:mo>
                    <mml:mi>Dropout</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msubsup>
                        <mml:mi>X</mml:mi>
                        <mml:mrow>
                          <mml:mtext>Normalized </mml:mtext>
                          <mml:mn>2</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mi>i</mml:mi>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:mrow>
                        <mml:mi>p</mml:mi>
                      </mml:mrow>
                      <mml:mn>0.2</mml:mn>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where, p=Dropout rate = 20% of the neurons are randomly set to zero.</p><p style="text-align: justify">8. Dense Layer 2</p><p style="text-align: justify">A fully connected dense layer with ReLU activation was employed.</p>
              
                <disp-formula>
                  <label>(15)</label>
                  <mml:math id="m48b9dik9i">
                    <mml:msubsup>
                      <mml:mi>X</mml:mi>
                      <mml:mrow>
                        <mml:mtext>Dense2 </mml:mtext>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>=</mml:mo>
                    <mml:mi>ReLU</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>⋅</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>W</mml:mi>
                        </mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:msub>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mi>B</mml:mi>
                        </mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:msub>
                      <mml:msubsup>
                        <mml:mi>X</mml:mi>
                        <mml:mrow>
                          <mml:mtext>Dropped2 </mml:mtext>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mi>i</mml:mi>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p style="text-align: justify">where, W<sub>2</sub>= Weight Matrix of the dense layer; B<sub>2</sub>=Bias Vector.</p><p style="text-align: justify">9. Batch normalization Layer 3</p>
              
                <disp-formula>
                  <label>(16)</label>
                  <mml:math id="me8l6w9vwr">
                    <mml:msubsup>
                      <mml:mi>X</mml:mi>
                      <mml:mrow>
                        <mml:mtext>Normalized </mml:mtext>
                        <mml:mn>3</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>=</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:msub>
                      <mml:mi>γ</mml:mi>
                      <mml:mn>3</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>β</mml:mi>
                      <mml:mn>3</mml:mn>
                    </mml:msub>
                    <mml:mfrac>
                      <mml:msubsup>
                        <mml:mi>X</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mrow>
                          <mml:mtext>Dense22 </mml:mtext>
                          <mml:mo>−</mml:mo>
                          <mml:msub>
                            <mml:mi>μ</mml:mi>
                            <mml:mn>3</mml:mn>
                          </mml:msub>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:msqrt>
                        <mml:msubsup>
                          <mml:mi>σ</mml:mi>
                          <mml:mn>3</mml:mn>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                        <mml:mo>+</mml:mo>
                        <mml:mi>ϵ</mml:mi>
                      </mml:msqrt>
                    </mml:mfrac>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="myz0qg5sw9">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mn>3</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="m40cy45ztb">
    <mml:msubsup>
      <mml:mi>σ</mml:mi>
      <mml:mn>3</mml:mn>
      <mml:mn>2</mml:mn>
    </mml:msubsup>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula> Mean and variance of <inline-formula>
  <mml:math id="mc6neq68yk">
    <mml:msubsup>
      <mml:mi>X</mml:mi>
      <mml:mrow>
        <mml:mtext>Dense </mml:mtext>
        <mml:mn>2</mml:mn>
      </mml:mrow>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msubsup>
  </mml:math>
</inline-formula> within the batch of respectively; <inline-formula>
  <mml:math id="mv53igi2oi">
    <mml:msub>
      <mml:mi>γ</mml:mi>
      <mml:mn>3</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mycvtact8m">
    <mml:msub>
      <mml:mi>β</mml:mi>
      <mml:mn>3</mml:mn>
    </mml:msub>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula> learnable parameters.</p><p>10. Dropout Layer 3</p><p style="text-align: justify">Dropout is applied to normalized feature map <inline-formula>
  <mml:math id="mvsrlqluwr">
    <mml:msubsup>
      <mml:mi>X</mml:mi>
      <mml:mrow>
        <mml:mtext>Normalized </mml:mtext>
        <mml:mn>2</mml:mn>
      </mml:mrow>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msubsup>
  </mml:math>
</inline-formula>.</p>
              
                <disp-formula>
                  <label>(17)</label>
                  <mml:math id="m041qkp8ly">
                    <mml:msubsup>
                      <mml:mi>X</mml:mi>
                      <mml:mrow>
                        <mml:mtext>Dropped </mml:mtext>
                        <mml:mn>3</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>=</mml:mo>
                    <mml:mi>Dropout</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msubsup>
                        <mml:mi>X</mml:mi>
                        <mml:mrow>
                          <mml:mtext>Normalized </mml:mtext>
                          <mml:mn>3</mml:mn>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mi>i</mml:mi>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:mrow>
                        <mml:mi>p</mml:mi>
                      </mml:mrow>
                      <mml:mn>0.2</mml:mn>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where, p=Dropout rate=20% of the neurons are randomly set to zero.</p><p style="text-align: justify">11. Output Layer</p><p style="text-align: justify">Softmax function is given by the Eq. (18).</p>
              
                <disp-formula>
                  <label>(18)</label>
                  <mml:math id="m222qv3rnj">
                    <mml:mi>ϕ</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>x</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:msup>
                        <mml:mi>e</mml:mi>
                        <mml:mrow>
                          <mml:msub>
                            <mml:mi>x</mml:mi>
                            <mml:mi>i</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                      </mml:msup>
                      <mml:mrow>
                        <mml:munderover>
                          <mml:mo>∑</mml:mo>
                          <mml:mrow>
                            <mml:mi>j</mml:mi>
                            <mml:mo>=</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                          <mml:mi>k</mml:mi>
                        </mml:munderover>
                        <mml:msup>
                          <mml:mi>e</mml:mi>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mi>x</mml:mi>
                              <mml:mi>j</mml:mi>
                            </mml:msub>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(19)</label>
                  <mml:math id="mtb3znpzvr">
                    <mml:msubsup>
                      <mml:mi>Y</mml:mi>
                      <mml:mrow>
                        <mml:mtext>Output </mml:mtext>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>i</mml:mi>
                      </mml:mrow>
                    </mml:msubsup>
                    <mml:mo>=</mml:mo>
                    <mml:mi>Softmax</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>⋅</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>W</mml:mi>
                        <mml:mrow>
                          <mml:mtext>output </mml:mtext>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>B</mml:mi>
                        <mml:mrow>
                          <mml:mtext>output </mml:mtext>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msubsup>
                        <mml:mi>X</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mrow>
                          <mml:mtext>Dropped </mml:mtext>
                          <mml:mn>3</mml:mn>
                        </mml:mrow>
                      </mml:msubsup>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="mriysch7nc">
    <mml:msub>
      <mml:mi>W</mml:mi>
      <mml:mrow>
        <mml:mtext>Output </mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula> Weight Matrix of the output layer; <inline-formula>
  <mml:math id="mn0l3wm9yf">
    <mml:msub>
      <mml:mi>B</mml:mi>
      <mml:mrow>
        <mml:mtext>Output </mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mo>=</mml:mo>
  </mml:math>
</inline-formula> Bias Vector.</p><p>The aggregate count of parameters in the model is 18,593,604, with 267,268 trainable parameters and 18,326,336 non-trainable parameters.</p>
            </sec>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Results and discussion</title>
      <p><span style="font-family: Times New Roman, serif">A total of 1064 intraoral images from 194 patients were initially captured and cropped tooth by tooth. Among these, 200 images were specifically selected for a performance analysis study. The patients' ages ranged from 14 to 70 years, with an average age of 26 years. Both male and female samples were included in the dataset, and patient demographics such as age and sex were kept undisclosed. Various types of teeth (molars, premolars, canines, etc.) were considered, while teeth with root canal caps and implants were excluded from the study. The primary objective was to classify dental caries into four classes to aid in early detection.</p><p><span style="font-family: Times New Roman, serif">To enhance the model's robustness, data enrichment techniques were employed. The research primarily centers on early dental caries detection and was conducted in phases. Phase I involved training on 200 images and testing on 40 images. Phase II expanded the dataset to 640 training images and 160 testing images. Finally, in Phase III, the dataset further increased to 888 training images and 79 testing images, with each phase building upon the results of the previous one.</p><p><span style="font-family: Times New Roman, serif">In Phase I, five different models—ResNet50V2, ResNet101V2, ResNet152, DenseNet169, and DenseNet201—were implemented and meticulously evaluated for performance. Phase II focused on ResNet50V2, DenseNet169, and DenseNet201 models, while Phase III compared DenseNet201 and DenseNet169 models. The performance parameters considered for the study include precision, recall, F1 score, and overall accuracy. Additionally, a confusion matrix is presented in the results to justify the suitability of the chosen model.</p>
      
        <sec>
          
            <title>4.1. Phase i experimental results</title>
          
          <p><span style="font-family: Times New Roman, serif">The accuracy levels ranged from 0.66 to 0.86 when utilizing 50 images per class for training, calculated across random samples encompassing all classes collectively. For class 0, precision varied from 0.69 to 0.90 across different models. Notably, ResNet101 exhibited superior performance in identifying healthy teeth, whereas DenseNet169 demonstrated greater efficacy in early dental caries detection, achieving a precision of 0.86 for class 1. However, it showed lower precision (0.77) in detecting class 3 caries. DenseNet201 also yielded a precision of 0.83 for class 1 and achieved 100% precision for class 3. Overall, DenseNet201 emerged as a consistently reliable solution among the models under consideration.</p><p><span style="font-family: Times New Roman, serif">Upon conducting comparative performance analysis, DenseNet models exhibited superior performance. The detailed performance results are tabulated in <xref ref-type="table" rid="table_1">Table 1</xref>, while graphical representations of accuracy, class-wise precision, and Comparative F1 scores are depicted in <xref ref-type="fig" rid="fig_5">Figures 5</xref>(a), 5(b), and 5(c) respectively. <xref ref-type="fig" rid="fig_6">Figure 6-20</xref> depict confusion matrices, as well as plots illustrating "training and validation accuracy" and "training and validation loss," for all models employed in the study.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Performance matrix for Phase I experimentation</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Sr. No.</p></td><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="1" rowspan="1"><p>Number of Epochs</p></td><td colspan="1" rowspan="1"><p>Class 0</p></td><td colspan="1" rowspan="1"><p>Class 1</p></td><td colspan="1" rowspan="1"><p>Class 2</p></td><td colspan="1" rowspan="1"><p>Class 3</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>Recall</p></td><td colspan="1" rowspan="1"><p>F1 Score</p></td></tr><tr><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>ResNet50V2</p></td><td colspan="1" rowspan="1"><p>50</p></td><td colspan="1" rowspan="1"><p>0.69</p></td><td colspan="1" rowspan="1"><p>0.50</p></td><td colspan="1" rowspan="1"><p>0.83</p></td><td colspan="1" rowspan="1"><p>0.77</p></td><td colspan="1" rowspan="1"><p>0.70</p></td><td colspan="1" rowspan="1"><p>0.75</p></td><td colspan="1" rowspan="1"><p>0.68</p></td></tr><tr><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>ResNet101V2</p></td><td colspan="1" rowspan="1"><p>50</p></td><td colspan="1" rowspan="1"><p>0.90</p></td><td colspan="1" rowspan="1"><p>0.80</p></td><td colspan="1" rowspan="1"><p>0.71</p></td><td colspan="1" rowspan="1"><p>0.82</p></td><td colspan="1" rowspan="1"><p>0.81</p></td><td colspan="1" rowspan="1"><p>0.8</p></td><td colspan="1" rowspan="1"><p>0.78</p></td></tr><tr><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p>ResNet152V2</p></td><td colspan="1" rowspan="1"><p>50</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>0.74</p></td><td colspan="1" rowspan="1"><p>0.38</p></td><td colspan="1" rowspan="1"><p>0.50</p></td><td colspan="1" rowspan="1"><p>0.66</p></td><td colspan="1" rowspan="1"><p>0.57</p></td><td colspan="1" rowspan="1"><p>0.55</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>DenseNet169</p></td><td colspan="1" rowspan="1"><p>50</p></td><td colspan="1" rowspan="1"><p>0.80</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>0.75</p></td><td colspan="1" rowspan="1"><p>0.77</p></td><td colspan="1" rowspan="1"><p>0.81</p></td><td colspan="1" rowspan="1"><p>0.8</p></td><td colspan="1" rowspan="1"><p>0.79</p></td></tr><tr><td colspan="1" rowspan="1"><p>5</p></td><td colspan="1" rowspan="1"><p>DenseNet201</p></td><td colspan="1" rowspan="1"><p>50</p></td><td colspan="1" rowspan="1"><p>0.87</p></td><td colspan="1" rowspan="1"><p>0.83</p></td><td colspan="1" rowspan="1"><p>0.77</p></td><td colspan="1" rowspan="1"><p>1.00</p></td><td colspan="1" rowspan="1"><p>0.86</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>0.84</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>(a) Comparison of accuracy (Phase I) (b) Comparison of class-wise precision (Phase I) (c) Comparison of class-wise F1 score (Phase I)</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_pBV47W6lDrn1WEAq.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_9Fs1JQzRYzOCnlme.png"/>
            </fig>
          
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Confusion matrix for ResNet50V2</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img__5TaQvRL4AkZwodO.jpeg"/>
            </fig>
          
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>Confusion matrix for ResNet101V2</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_H4f6Qok1VSqSxNMK.jpeg"/>
            </fig>
          
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>Confusion matrix for ResNet152</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_2n0SE3-8ybvy-Yji.jpeg"/>
            </fig>
          
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>
                <title>Confusion matrix for DenseNet169</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_c6I3hGi0GIwVTEGg.jpeg"/>
            </fig>
          
          
            <fig id="fig_10">
              <label>Figure 10</label>
              <caption>
                <title>Confusion matrix for DenseNet201</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_4Te5MPOi9Qxvt9zi.jpeg"/>
            </fig>
          
          
            <fig id="fig_11">
              <label>Figure 11</label>
              <caption>
                <title>Training and validation accuracy for ResNet50V2</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_Xdt3vOwvDzc9PfG5.png"/>
            </fig>
          
          
            <fig id="fig_12">
              <label>Figure 12</label>
              <caption>
                <title>Training and validation loss for ResNet50V2</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_aVISFOCpm-c1UBOr.jpeg"/>
            </fig>
          
          
            <fig id="fig_13">
              <label>Figure 13</label>
              <caption>
                <title>Training and validation accuracy for ResNet101V2</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_B9A4k4aaMDvGwsEl.jpeg"/>
            </fig>
          
          
            <fig id="fig_14">
              <label>Figure 14</label>
              <caption>
                <title>Training and validation loss for ResNet101V2</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_fu6GWPm90_xA4m81.jpeg"/>
            </fig>
          
          
            <fig id="fig_15">
              <label>Figure 15</label>
              <caption>
                <title>Training and validation accuracy for ResNet152</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_tfvTKnQrl1zzVzvr.jpeg"/>
            </fig>
          
          
            <fig id="fig_16">
              <label>Figure 16</label>
              <caption>
                <title>Training and validation loss for ResNet152</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_MaAJc6TNasExiRBn.jpeg"/>
            </fig>
          
          
            <fig id="fig_17">
              <label>Figure 17</label>
              <caption>
                <title>Training and validation accuracy for DenseNet169</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_wFxtKDnDY4f8LOor.jpeg"/>
            </fig>
          
          
            <fig id="fig_18">
              <label>Figure 18</label>
              <caption>
                <title>Training and validation loss for DenseNet169</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_VPlYFkcPAJd0mEPO.jpeg"/>
            </fig>
          
          
            <fig id="fig_19">
              <label>Figure 19</label>
              <caption>
                <title>Training and validation accuracy for DenseNet201</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_JdFW34a1LM-nQvTy.jpeg"/>
            </fig>
          
          
            <fig id="fig_20">
              <label>Figure 20</label>
              <caption>
                <title>Training and validation loss for DenseNet201</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_5CQSs85pkrGVpBIa.jpeg"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.2. Phase ii experimental results</title>
          
          <p><span style="font-family: Times New Roman, serif">The accuracy ranged from 0.73 to 0.79 across the models. ResNet50v2 demonstrated a promising precision of 0.90 for class 0 (representing healthy teeth), but struggled to effectively identify and classify dental caries in other classes, particularly exhibiting poor performance with a precision of approximately 0.58 for class 2 caries. Overall, DenseNet models outperformed other models. However, DenseNet169 consistently struggled to accurately detect class 3 caries compared to DenseNet201. The performance metrics are summarized in <xref ref-type="table" rid="table_2">Table 2</xref>. Graphical representations of accuracy, class-wise precision, and Comparative F1 scores can be found in <xref ref-type="fig" rid="fig_21">Figure 21</xref>(a), 21(b), and 21(c) respectively. Confusion matrices, as well as plots illustrating training and validation accuracy, and training and validation loss for all models used in the study, are depicted in <xref ref-type="fig" rid="fig_22">Figure 22-30</xref>.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Performance matrix for Phase II experimentation</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Sr. No.</p></td><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="1" rowspan="1"><p>Number of Epochs</p></td><td colspan="1" rowspan="1"><p>Class 0</p></td><td colspan="1" rowspan="1"><p>Class 1</p></td><td colspan="1" rowspan="1"><p>Class 2</p></td><td colspan="1" rowspan="1"><p>Class 3</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>Recall</p></td><td colspan="1" rowspan="1"><p>F1 Score</p></td></tr><tr><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>ResNet50V2</p></td><td colspan="1" rowspan="1"><p>30</p></td><td colspan="1" rowspan="1"><p>0.90</p></td><td colspan="1" rowspan="1"><p>0.71</p></td><td colspan="1" rowspan="1"><p>0.58</p></td><td colspan="1" rowspan="1"><p>0.73</p></td><td colspan="1" rowspan="1"><p>0.73</p></td><td colspan="1" rowspan="1"><p>0.72</p></td><td colspan="1" rowspan="1"><p>0.72</p></td></tr><tr><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>DenseNet169</p></td><td colspan="1" rowspan="1"><p>30</p></td><td colspan="1" rowspan="1"><p>1.00</p></td><td colspan="1" rowspan="1"><p>0.78</p></td><td colspan="1" rowspan="1"><p>0.70</p></td><td colspan="1" rowspan="1"><p>0.69</p></td><td colspan="1" rowspan="1"><p>0.79</p></td><td colspan="1" rowspan="1"><p>0.78</p></td><td colspan="1" rowspan="1"><p>0.78</p></td></tr><tr><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p>DenseNet201</p></td><td colspan="1" rowspan="1"><p>30</p></td><td colspan="1" rowspan="1"><p>0.75</p></td><td colspan="1" rowspan="1"><p>0.78</p></td><td colspan="1" rowspan="1"><p>0.67</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>0.79</p></td><td colspan="1" rowspan="1"><p>0.75</p></td><td colspan="1" rowspan="1"><p>0.72</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <fig id="fig_21">
              <label>Figure 21</label>
              <caption>
                <title>(a) Comparison of accuracy (Phase II) (b) Comparison of class-wise precision (Phase II) (c) Comparison of class-wise F1 score (Phase II)</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_uJu3QFI8sLCaHnE9.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_ygd0eiYdnqZLEZ-c.png"/>
            </fig>
          
          
            <fig id="fig_22">
              <label>Figure 22</label>
              <caption>
                <title>Confusion matrix for ResNet50V2</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_1uNwdlReM-tVKZEk.jpeg"/>
            </fig>
          
          
            <fig id="fig_23">
              <label>Figure 23</label>
              <caption>
                <title>Confusion matrix for DenseNet169</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_yoWeI6k3FAfnnNFV.jpeg"/>
            </fig>
          
          
            <fig id="fig_24">
              <label>Figure 24</label>
              <caption>
                <title>Confusion matrix for DenseNet201</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_J_bhc1DmygjozPh7.jpeg"/>
            </fig>
          
          
            <fig id="fig_25">
              <label>Figure 25</label>
              <caption>
                <title>Training and validation accuracy for ResNet50V2</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_RGAWmSxoZsIpDeOg.jpeg"/>
            </fig>
          
          
            <fig id="fig_26">
              <label>Figure 26</label>
              <caption>
                <title>Training and validation loss for ResNet50V2</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_affwIUh4A24bPwB8.jpeg"/>
            </fig>
          
          
            <fig id="fig_27">
              <label>Figure 27</label>
              <caption>
                <title>Training and validation accuracy for DenseNet169</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_fhlNA3MqnC9CYBH7.jpeg"/>
            </fig>
          
          
            <fig id="fig_28">
              <label>Figure 28</label>
              <caption>
                <title>Training and validation loss for DenseNet169</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_Ui6udjuTMjVrEUZU.jpeg"/>
            </fig>
          
          
            <fig id="fig_29">
              <label>Figure 29</label>
              <caption>
                <title>Training and validation accuracy for DenseNet201</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_fPqZuorFVqb2Rizu.jpeg"/>
            </fig>
          
          
            <fig id="fig_30">
              <label>Figure 30</label>
              <caption>
                <title>Training and validation loss for DenseNet201</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_NoAU6A0mCORD_e7L.jpeg"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.3. Phase iii experimental results</title>
          
          <p><span style="font-family: Times New Roman, serif">The analysis of the initial two phases of experimentation indicated that DenseNet models exhibited superior performance in the early identification of dental caries. Consequently, only DenseNet models were utilized in this stage of the study. A greater number of images, including a higher volume of augmented images in classes 2 and 3, were incorporated to enhance the resilience of class 2 and class 3 detection and classification by the models. This augmentation of data proved to be effective, as DenseNet201 achieved an overall accuracy of 93%. Moreover, DenseNet201 demonstrated class-specific precision ranging from 0.87 to 1.00, marking the most favorable outcome attained across all phases of experimentation. On the other hand, DenseNet169 exhibited precision ranging from 0.82 to 0.91, with class 3 showing a lower accuracy of 0.82. Thus, based on the accumulated results, DenseNet201 emerged as the superior choice for early dental caries detection. The performance metrics are succinctly outlined in <xref ref-type="table" rid="table_3">Table 3</xref>, while graphical representations showcasing accuracy, class-specific precision, and Comparative F1 scores can be found in <xref ref-type="fig" rid="fig_31">Figure 31</xref>(a), 31(b), and 31(c) respectively. Additionally, confusion matrices and plots illustrating "training and validation accuracy, as well as training and validation loss," for all employed models, were presented in <xref ref-type="fig" rid="fig_32">Figure 32-37</xref>. <xref ref-type="table" rid="table_4">Table 4</xref> shows comparison of mean confidence and standard deviation across dental caries classes. “DenseNet201” exhibits more stable and consistent confidence levels across all caries grades, while DenseNet169 shows slightly higher mean confidences but with some variability. This stability in DenseNet201 could make it a preferable choice for applications requiring reliable and consistent predictions across different classes.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Performance matrix for Phase III experimentation</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Sr. No.</p></td><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="1" rowspan="1"><p>Number of Epochs</p></td><td colspan="1" rowspan="1"><p>Class 0</p></td><td colspan="1" rowspan="1"><p>Class 1</p></td><td colspan="1" rowspan="1"><p>Class 2</p></td><td colspan="1" rowspan="1"><p>Class 3</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>Recall</p></td><td colspan="1" rowspan="1"><p>F1 Score</p></td></tr><tr><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>DenseNet201</p></td><td colspan="1" rowspan="1"><p>80</p></td><td colspan="1" rowspan="1"><p>0.87</p></td><td colspan="1" rowspan="1"><p>0.94</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>1.00</p></td><td colspan="1" rowspan="1"><p>0.93</p></td><td colspan="1" rowspan="1"><p>0.93</p></td><td colspan="1" rowspan="1"><p>0.93</p></td></tr><tr><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>DenseNet169</p></td><td colspan="1" rowspan="1"><p>80</p></td><td colspan="1" rowspan="1"><p>0.93</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>1.00</p></td><td colspan="1" rowspan="1"><p>0.82</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>0.91</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <fig id="fig_31">
              <label>Figure 31</label>
              <caption>
                <title>(a) Class-wise accuracy (Phase III) (b) Comparison of class-wise precision (Phase III) (c) Comparison of class-wise F1 score (Phase III)</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_tLXWvMqcznWM8Ycn.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_JQ-a-ljmG0TDaHmj.png"/>
            </fig>
          
          
            <fig id="fig_32">
              <label>Figure 32</label>
              <caption>
                <title>Confusion matrix for DenseNet201</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_vf1anXp-nDTl3rf3.png"/>
            </fig>
          
          
            <fig id="fig_33">
              <label>Figure 33</label>
              <caption>
                <title>Confusion matrix for DenseNet169</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_qsFFj_S28l61J3Wm.png"/>
            </fig>
          
          
            <fig id="fig_34">
              <label>Figure 34</label>
              <caption>
                <title>Training and validation accuracy for DenseNet169</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_2KxtmlYJTXOU6o8z.jpeg"/>
            </fig>
          
          
            <fig id="fig_35">
              <label>Figure 35</label>
              <caption>
                <title>Training and validation loss for DenseNet169</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_eKbuGKkjBGsEo4I0.jpeg"/>
            </fig>
          
          
            <fig id="fig_36">
              <label>Figure 36</label>
              <caption>
                <title>Training and validation accuracy for DenseNet201</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_hWC3HDxqrfVBwWd6.jpeg"/>
            </fig>
          
          
            <fig id="fig_37">
              <label>Figure 37</label>
              <caption>
                <title>Training and validation loss for DenseNet201</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_0xMKERuIUANMUNLr.jpeg"/>
            </fig>
          
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Comparison of mean confidence and standard deviation</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Sr. No.</p></td><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="1" rowspan="1"><p>Metric</p></td><td colspan="1" rowspan="1"><p>Class 0</p></td><td colspan="1" rowspan="1"><p>Class 1</p></td><td colspan="1" rowspan="1"><p>Class 2</p></td><td colspan="1" rowspan="1"><p>Class 3</p></td></tr><tr><td colspan="1" rowspan="2"><p>1</p></td><td colspan="1" rowspan="2"><p>DenseNet201</p></td><td colspan="1" rowspan="1"><p>Mean Confidence</p></td><td colspan="1" rowspan="1"><p>0.99</p></td><td colspan="1" rowspan="1"><p>0.94</p></td><td colspan="1" rowspan="1"><p>0.99</p></td><td colspan="1" rowspan="1"><p>0.96</p></td></tr><tr><td colspan="1" rowspan="1"><p>Standard Deviation</p></td><td colspan="1" rowspan="1"><p>0.02</p></td><td colspan="1" rowspan="1"><p>0.12</p></td><td colspan="1" rowspan="1"><p>0.03</p></td><td colspan="1" rowspan="1"><p>0.09</p></td></tr><tr><td colspan="1" rowspan="2"><p>2</p></td><td colspan="1" rowspan="2"><p>DenseNet169</p></td><td colspan="1" rowspan="1"><p>Mean Confidence</p></td><td colspan="1" rowspan="1"><p>0.96</p></td><td colspan="1" rowspan="1"><p>0.92</p></td><td colspan="1" rowspan="1"><p>1.00</p></td><td colspan="1" rowspan="1"><p>0.94</p></td></tr><tr><td colspan="1" rowspan="1"><p>Standard Deviation</p></td><td colspan="1" rowspan="1"><p>0.09</p></td><td colspan="1" rowspan="1"><p>0.14</p></td><td colspan="1" rowspan="1"><p>0.00</p></td><td colspan="1" rowspan="1"><p>0.10</p></td></tr></tbody></table>
            </table-wrap>
          
          <p><span style="color: black; font-family: Times New Roman, serif">Potential advantages of the modified DenseNet201 model</p><p><span style="color: black; font-family: Times New Roman, serif">1. Efficient parameter utilization: <span style="color: black; font-family: Times New Roman, serif">DenseNet's dense connectivity ensures efficient parameter usage, reducing redundant calculations and improving computational efficiency. This compactness contributes to faster training and inference times, which is beneficial for applications requiring real-time predictions.</p><p><span style="color: black; font-family: Times New Roman, serif">2. Improved generalization: <span style="color: black; font-family: Times New Roman, serif">Utilizing pre-trained weights, along with robust “regularization techniques” such as “Dropout and Batch Normalization”, enhances the model's generalization capabilities, even when data is limited. These techniques collectively improve the model's resilience against overfitting, resulting in more reliable performance on unseen data.</p><p><span style="color: black; font-family: Times New Roman, serif">3. Transfer learning and domain adaptation: <span style="color: black; font-family: Times New Roman, serif">The model's architecture supports transfer learning, allowing adaptation to specific domains or datasets while leveraging the robust features learned from DenseNet201. Fine-tuning through dense layers ensures the capture of domain-specific features, thus boosting accuracy in niche applications.</p><p><span style="color: black; font-family: Times New Roman, serif">4. Scalability and adaptability: <span style="color: black; font-family: Times New Roman, serif">The modular design of the architecture allows for easy modifications, such as adding more dense layers or adjusting dropout rates, to suit different datasets and problem complexities. This adaptability makes the model a versatile choice for various image classification tasks, ranging from medical imaging to natural scene analysis.</p><p><span style="color: black; font-family: Times New Roman, serif">5. Resource efficiency: <span style="color: black; font-family: Times New Roman, serif">Global Average Pooling and Batch Normalization contribute to resource efficiency by minimizing memory usage and computational overhead, making the model suitable for deployment on devices with limited resources.</p><p><span style="color: black; font-family: Times New Roman, serif">6. Computational efficiency:<span style="color: black; font-family: Times New Roman, serif"> The model required 21min and 24.8 sec for training which is advantageous for clinical implementations where quick model updates or real-time training may be necessary.</p><p style="text-align: justify"><span style="color: black; font-family: Times New Roman, serif">Evaluation of the proposed model for optimum results</p><p><span style="color: black; font-family: Times New Roman, serif"><xref ref-type="table" rid="table_5">Table 5</xref> shows the comparison of Densenet201 model variants.</p><p><span style="color: black; font-family: Times New Roman, serif">1. Increasing the dropout rate from 0.2 to 0.5</p><p><span style="color: black; font-family: Times New Roman, serif">The model with a dropout rate of 0.2 exhibits a high overall accuracy of 0.93, indicating strong overall performance and effective classification across all classes. The F1 scores for each class are consistently high, reflecting balanced precision and recall. Notably, the model achieves perfect recall (1.00) for class 3, suggesting it is very effective in identifying this class. Lower dropout rates tend to retain more information during training, which can be beneficial in capturing detailed features from the dataset. However increased dropout rate shows trade-off between regularization and model performance. Thus, the model with a dropout rate of p=0.2 appears to be the optimum choice based on overall performance metrics, including accuracy, recall, and F1 score.</p><p><span style="color: black; font-family: Times New Roman, serif">2. Performance of 4BDD DenseNet201 variant</p><p><span style="color: black; font-family: Times New Roman, serif">This model performed almost similar to DenseNet201 with 3BDD layers for advanced classes detection however it lagged behind in early detection of the dental caries resulting in 0.77 as class 1 accuracy. Also, overall accuracy reduced from 0.93 to 0.85. The main disadvantage of additional BDD layer is that it makes proposed model computationally less effective resulting in requirement of 28 minutes and 35.5 seconds for training. In real time applications it may affect lot in critical detection.</p><p><span style="color: black; font-family: Times New Roman, serif">3. Modified DenseNet201 with learning rate scheduler</p><p><span style="color: black; font-family: Times New Roman, serif">Overall accuracy is bit lower than the proposed model. Since the lesser amount of dataset proposed model is overfitting hence learning rate scheduler was implemented but it is also showing same results of overfitting of the model as that of proposed model. so proposed model is suitable for critical detection of early detection of dental caries.</p><p><span style="color: black; font-family: Times New Roman, serif">4. DenseNet201 with SGD optimizer</p><p><span style="font-family: Times New Roman, serif">The obvious choice of optimizer other than Adam is SGD. But proposed model shows reduced effectiveness in class 2 (F1 score = 0.83). Even though, overall accuracy is similar to learning rate scheduler model, it does not show significant changes to the proposed model.</p>
          
            <table-wrap id="table_5">
              <label>Table 5</label>
              <caption>
                <title>Comparison of DenseNet201 model variants</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Sr. No.</p></td><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="1" rowspan="1"><p>Class 0</p></td><td colspan="1" rowspan="1"><p>Class 1</p></td><td colspan="1" rowspan="1"><p>Class 2</p></td><td colspan="1" rowspan="1"><p>Class 3</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>Recall</p></td><td colspan="1" rowspan="1"><p>F1 Score</p></td><td colspan="1" rowspan="1"><p>Computational Complexity</p></td><td colspan="1" rowspan="1"><p>Inference Time</p></td></tr><tr><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>DenseNet201 with 3BDD layer</p></td><td colspan="1" rowspan="1"><p>0.87</p></td><td colspan="1" rowspan="1"><p>0.94</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>1.00</p></td><td colspan="1" rowspan="1"><p>0.93</p></td><td colspan="1" rowspan="1"><p>0.93</p></td><td colspan="1" rowspan="1"><p>0.93</p></td><td colspan="1" rowspan="1"><p>Moderate</p></td><td colspan="1" rowspan="1"><p>21min 24.8sec</p></td></tr><tr><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>DenseNet201 with 4 BDD layer</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>0.77</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>1.00</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>0.85</p></td><td colspan="1" rowspan="1"><p>High</p></td><td colspan="1" rowspan="1"><p>28 min</p><p>35.5sec</p></td></tr><tr><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p>DenseNet201 with learning rate scheduler</p></td><td colspan="1" rowspan="1"><p>0.93</p></td><td colspan="1" rowspan="1"><p>0.94</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>0.83</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>Moderate</p></td><td colspan="1" rowspan="1"><p>22min 36.4sec</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>DenseNet201 with p=0.5</p></td><td colspan="1" rowspan="1"><p>0.81</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>1.00</p></td><td colspan="1" rowspan="1"><p>0.90</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>0.89</p></td><td colspan="1" rowspan="1"><p>Moderate</p></td><td colspan="1" rowspan="1"><p>23min 24.8sec</p></td></tr><tr><td colspan="1" rowspan="1"><p>5</p></td><td colspan="1" rowspan="1"><p>DenseNet201 with SGD</p></td><td colspan="1" rowspan="1"><p>0.87</p></td><td colspan="1" rowspan="1"><p>0.94</p></td><td colspan="1" rowspan="1"><p>0.83</p></td><td colspan="1" rowspan="1"><p>1.00</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>Moderate</p></td><td colspan="1" rowspan="1"><p>25min 25.6sec</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>This paper presents a comprehensive analysis of “deep learning architectures”, particularly residual and dense networks, applied to a novel dental dataset for the early detection of dental caries. Among the evaluated models, “DenseNet201” emerges as the optimal architecture, achieving an overall accuracy of 93% and excelling in both precision and sensitivity across all classes, especially in detecting advanced caries (class 3). The dense connections in “DenseNet” proved advantageous in handling small-scale objects like dental caries, offering superior performance compared to residual networks, which faced challenges with classifying early-stage caries as the number of training images increased.</p><p style="text-align: justify">Despite the promising results, the study is limited by the relatively small dataset size, which may hinder the model's ability to generalize to unseen data and introduce potential biases, such as class imbalance or demographic skew. Overfitting remains a concern, and the lack of validation across diverse populations limits the model's broader applicability. While computational complexity and inference time were considered, deploying the model in resource-constrained environments, such as mobile devices, remains a challenge.</p><p style="text-align: justify">Future research should focus on addressing these limitations by expanding the dataset to improve generalizability and reduce biases, exploring new architectures like transformers, and optimizing the model for mobile deployment to enable real-time, accessible dental caries detection in clinical and home settings. Furthermore, this framework could be extended to classify other dental diseases, making it a valuable tool for noninvasive, intelligent dental disease detection in preventive dentistry.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation/>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation/>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation/>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation/>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation/>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation/>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation/>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation/>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation/>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation/>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation/>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation/>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation/>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation/>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation/>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation/>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation/>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation/>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation/>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation/>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation/>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation/>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation/>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation/>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation/>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation/>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation/>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation/>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation/>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation/>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation/>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation/>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation/>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation/>
      </ref>
      <ref id="ref_35">
        <label>35.</label>
        <element-citation/>
      </ref>
      <ref id="ref_36">
        <label>36.</label>
        <element-citation/>
      </ref>
      <ref id="ref_37">
        <label>37.</label>
        <element-citation/>
      </ref>
      <ref id="ref_38">
        <label>38.</label>
        <element-citation/>
      </ref>
      <ref id="ref_39">
        <label>39.</label>
        <element-citation/>
      </ref>
    </ref-list>
  </back>
</article>