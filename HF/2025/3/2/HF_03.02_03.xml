<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">HF</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Healthcraft Frontiers</journal-title>
        <abbrev-journal-title abbrev-type="issn">Healthcraft. Front.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">HF</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">3005-799X</issn>
      <issn publication-format="print">3005-7981</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-jgQGqV914Iy7W19wNW1z_fM9PGJa2xnt</article-id>
      <article-id pub-id-type="doi">10.56578/hf030203</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Hybrid Deep Learning Architecture for Automated Chest X-ray Disease Detection with Explainable Artificial Intelligence</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5534-2142</contrib-id>
          <name>
            <surname>Vivekanandam</surname>
            <given-names>B.</given-names>
          </name>
          <email>vivekanandam@lincoln.edu.my</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9522-4482</contrib-id>
          <name>
            <surname>Kumar</surname>
            <given-names>Kambala Vijaya</given-names>
          </name>
          <email>kvijayakumar@kluniversity.in</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-2155-2050</contrib-id>
          <name>
            <surname>Annam</surname>
            <given-names>Jagadeeswara Rao</given-names>
          </name>
          <email>jagadeesha@gmail.com</email>
        </contrib>
        <aff id="aff_1">School of Artificial Intelligence, Computing and Multimedia, Lincoln University College, 47301 Selangor, Malaysia</aff>
        <aff id="aff_2">Department of Computer Science and Engineering, Koneru Lakshmaiah Education Foundation, 522502 Andhra Pradesh, India</aff>
        <aff id="aff_3">Department of Computer Science and Engineering (Artificial Intelligence and Machine Learning), CVR College of Engineering, 501510 Telangana, India</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>08</day>
        <month>05</month>
        <year>2025</year>
      </pub-date>
      <volume>3</volume>
      <issue>2</issue>
      <fpage>86</fpage>
      <lpage>96</lpage>
      <page-range>86-96</page-range>
      <history>
        <date date-type="received">
          <day>13</day>
          <month>03</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>04</day>
          <month>05</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Deep learning (DL) has increasingly been adopted to support automated medical diagnosis, particularly in radiological imaging where rapid and reliable interpretation is essential. In this study, a hybrid architecture integrating convolutional neural network (CNN), residual networks (ResNet), and densely connected networks (DenseNet) was developed to improve automated disease recognition in chest X-ray images. This unified framework was designed to capture shallow, residual, and densely connected representations simultaneously, thereby strengthening feature diversity and improving classification robustness relative to conventional single-model or dual-model approaches. The model was trained and evaluated using the ChestX-ray14 dataset, comprising more than 100,000 X-ray images representing 14 thoracic disease classes. Performance was assessed using established metrics, including accuracy, precision, recall, F1-score, and the area under the receiver operating characteristic curve (AUC-ROC). A classification accuracy of 92.5% was achieved, representing an improvement over widely used machine learning (ML) and contemporary DL baselines. To promote transparency and clinical interpretability, Gradient-weighted Class Activation Mapping (Grad-CAM) was incorporated, enhancing clinician confidence in model decisions. The findings demonstrate that DL-based diagnostic support systems can reduce diagnostic uncertainty, alleviate clinical workload, and facilitate timely decision-making in healthcare environments. The proposed hybrid model illustrates the potential of advanced feature-integration strategies to improve automated radiographic interpretation and underscores the importance of explainable artificial intelligence (XAI) in promoting trustworthy deployment of medical artificial intelligence (AI) technologies.</p></abstract>
      <kwd-group>
        <kwd>Deep learning</kwd>
        <kwd>Medical diagnosis</kwd>
        <kwd>AI-based image recognition</kwd>
        <kwd>Chest X-ray</kwd>
        <kwd>XAI</kwd>
        <kwd>Grad-CAM</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="3"/>
        <fig-count count="4"/>
        <table-count count="6"/>
        <ref-count count="26"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>AI has helped healthcare by giving new opportunities to recognize medical images. Thanks to DL in ML, it is now possible to spot important features in medical imaging data. With AI, doctors can be confident that they diagnose patients correctly and quickly, alleviating their workload. In this study, the application of DL for automated medical diagnosis, with a focus on AI treatment of X-rays, magnetic resonance imaging (MRI) scans, and computed tomography (CT) scans, is discussed. The initiative aims to present today's advancements in DL in healthcare, explore a more effective approach to diagnosis, and raise awareness of trust and transparency by highlighting XAI.</p><p>Medical imaging has long been employed to obtain essential insights into patient health and to support the diagnosis and monitoring of a wide range of diseases. It is best used when guided by the expertise of radiologists and clinicians but can be time-consuming, prone to errors, and require the use of trained professionals. As more medical images are being created, automatic systems must process a large amount of data without compromising their accuracy. Medical imaging classification and detection succeed mostly thanks to DL methods such as CNNs. Many applications in medical imaging have found that CNNs in DL tend to be very successful (<xref ref-type="bibr" rid="ref_10">He, 2024</xref>). Because these models can identify patterns in an image that form a hierarchy, they enable the detection of issues such as tumors or fractures by analyzing X-ray and MRI results (<xref ref-type="bibr" rid="ref_21">Wang et al., 2022</xref>). Earlier work on ImageNet enables fine-tuned models to aid in the diagnosis of various medical disorders (<xref ref-type="bibr" rid="ref_23">Zakaria et al., 2021</xref>).</p><p>Although DL is making significant progress in analyzing medical images, several challenges remain. The absence of well-marked medical data presents numerous challenges. Due to difficulties in privacy and medical imaging, it isn't easy to find large and clearly labeled datasets (<xref ref-type="bibr" rid="ref_11">Lehmann et al., 2002</xref>). Because DL models are opaque, people are less willing to use them in clinical fields. In addition, the reason why the model has reached its conclusions cannot be explained. Therefore, clinicians struggle to put faith in the diagnoses AI provides when mistakes are made (<xref ref-type="bibr" rid="ref_12">Ribeiro et al., 2016</xref>). Both improving models and making them clear and easy to understand, which can be achieved by XAI methods, are necessary to handle these challenges (<xref ref-type="bibr" rid="ref_24">Zeiler &amp;amp; Fergus, 2014</xref>).</p><p>This study is intended to develop an AI-enabled image recognition system that enhances the reliability of medical image analysis and provides interpretable explanations designed to strengthen clinician confidence in automated diagnostic outputs. In this study, a hybrid model constructed from CNN and ResNet was employed, with features incorporated to explain the model’s actions, such as the use of saliency maps and Grad-CAM. With this process, the system is expected to make accurate predictions and present them in a way that is easy for healthcare providers to explain (<xref ref-type="bibr" rid="ref_14">Selvaraju et al., 2020</xref>). While XAI has been explored in medical imaging, its role in enhancing clinician trust within chest X-ray diagnosis remains insufficiently addressed. This study contributes by integrating interpretable visualization methods that highlight decision-critical regions, thereby strengthening transparency in automated diagnosis.</p>
      
        <sec>
          
            <title>1.1. Background</title>
          
          <p>In the medical field, AI and DL have demonstrated success in image classification tasks. Previously, image classification was performed by machines via support vector machines (SVMs) and decision trees (DTs). However, making complex features by hand often prevented these methods from performing well. Thanks to CNNs, DL has now made it easier to identify useful features in images and, in turn, increased the diagnostic accuracy of imaging tools (<xref ref-type="bibr" rid="ref_15">Shamshirband et al., 2021</xref>). Many medical imaging problems, such as cancer, fractures, and diseases, have been addressed using CNNs, which have achieved results that match or exceed those of human radiologists (<xref ref-type="bibr" rid="ref_1">Agneya et al., 2024</xref>). It has been found that DL can catch lung cancer, breast cancer and brain tumors from CT scans and MRIs in radiology (<xref ref-type="bibr" rid="ref_6">Cai et al., 2020</xref>). The study by <xref ref-type="bibr" rid="ref_9">Esteva et al. (2017)</xref> illustrates that computers can classify skin cancer more accurately than dermatologists working with images. Similarly, <xref ref-type="bibr" rid="ref_17">Son et al. (2020)</xref> developed an AI system for accurately identifying diabetic retinopathy in retinal fundus photographs. These advancements have encouraged the continued investigation of DL techniques across a broader range of medical imaging modalities.</p><p>On the other hand, DL has not been adopted in healthcare as much as expected due to several problems. Currently, there is a lack of high-quality training data to build reliable systems in DL (<xref ref-type="bibr" rid="ref_8">Deheyab et al., 2022</xref>) effectively. Medical datasets are often limited in size, incomplete, and affected by substantial class imbalance, which can cause deep learning models to overfit and generalize poorly during clinical deployment. As a result, researchers have been utilizing data augmentation and transfer learning to construct models that can effectively operate with smaller datasets and accurately adapt to various settings and scanning devices (<xref ref-type="bibr" rid="ref_20">Van der Velden et al., 2022</xref>). A further issue is the comprehensibility of DL models. DL models are recognized for their excellent performance in image recognition; however, it is challenging to understand what enables these models to make their predictions (<xref ref-type="bibr" rid="ref_16">Singh et al., 2020</xref>). Because incorrect healthcare diagnoses may be fatal, it is very concerning that AI doesn’t explain itself well. To understand this issue, XAI approaches have been introduced, and saliency maps, class activation maps (CAMs) and Grad-CAM are examples used to highlight which areas of an image matter most to the model (<xref ref-type="bibr" rid="ref_5">Bhati et al., 2024</xref>). They provide an essential understanding of the model’s decision-making process, which can help clinicians make better medical choices.</p><p>Section 2 of this study summarizes relevant studies that utilize DL for medical image recognition. In Section 3, the approach is described, listing the data employed, the hybrid model structure and tools to improve both model accuracy and understanding. The results presented in Section 4 include performance metrics and comparisons with existing models. In the final section, findings are summarized, possible limitations are highlighted, and ideas for further research are presented. </p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>2. Related work</title>
      <p>This prospective observational study was conducted to evaluate the efficacy of empiric antibiotic therapies in DFIs and to develop a visual risk stratification model correlating antibiotic response with potential amputation risk. Ethical approval was obtained from the Postgraduate Medical Institute (PGMI), Hayatabad Peshawar. Additional institutional permissions were granted by the medical directors, deputy medical superintendents (DMS), heads of endocrinology wards, and heads of microbiology sections across participating hospitals. Written informed consent was obtained from all patients in accordance with ethical standards for research involving human participants.</p><p>The past few years have seen DL grow rapidly in medical image recognition, and researchers are now applying CNNs and similar models to help automate medical diagnosis. CNNs help identify and recognize diseases in medical images, but concerns remain due to their generalizability, limited data, and lack of explanation. This section summarizes the latest progress in using DL to address challenges in medical imaging. Medical experts are using DL more often to find diseases such as cancer, trouble with the brain and illnesses of the heart. Several DL models have been built to help detect breast cancer using mammography pictures. The method proposed by <xref ref-type="bibr" rid="ref_7">Chouhan et al. (2021)</xref> enabled a deep CNN to compete effectively with human radiologists in reviewing mammograms. Thanks to this technique, the model could detect cancerous tumors, making it a valuable tool in informing clinical decisions. In terms of breast cancer screening, the model proposed by <xref ref-type="bibr" rid="ref_2">Ahmad et al. (2023)</xref> achieved a high area under the curve (AUC) of 0.94 when detecting breast cancer on digital mammograms.</p><p>Using DL, doctors have made significant advancements in detecting lung illnesses, particularly pneumonia in X-ray images. <xref ref-type="bibr" rid="ref_19">Sunil Kumar Aithal &amp;amp; Rajashree (2023)</xref> used a deep CNN to distinguish between chest X-rays that were normal, pneumonia-positive, or showed signs of tuberculosis. The accuracy for this model was 92%, which is better than many other traditional measurement tools offer. It was found that by applying DL, many diseases could be recognized and immediate diagnoses given, despite not having specialist radiologists present at those sites. In terms of Alzheimer’s, DL has been applied to multiple sclerosis and brain tumors to look at brain MRI scans. <xref ref-type="bibr" rid="ref_13">Saikia &amp;amp; Kalita (2024)</xref> utilized a CNN on MRI images organized into structures to detect Alzheimer’s disease with a success rate of 95%. Findings from brain imaging allowed the model to separate those with Alzheimer’s from healthy people, showing that DL could detect the disease earlier than other approaches. <xref ref-type="bibr" rid="ref_3">Amran et al. (2022)</xref> developed a new method that utilizes DL to detect brain tumors in MRIs. Combining CNNs and recurrent neural networks (RNNs) made it easier for them to detect repeated events and identify tumors, which improved their performance in both segmentation and classification.</p><p>Doctors can detect heart problems using DL and identify certain diseases by analyzing echocardiograms and CT scans. <xref ref-type="bibr" rid="ref_22">Xie et al. (2024)</xref> developed a novel DL system for detecting atrial fibrillation, a common heart rhythm disorder, by analyzing electrocardiogram (ECG) signals. The traditional method of performing ECGs did not achieve the same sensitivity and specificity as the proposed ECG model. It further showed that DL tools can understand patient signals and help improve the accuracy of doctors’ diagnoses. While DL performs well for many medical image tasks, it remains challenging to ensure that these models function correctly across all medical communities, hospitals, and various types of imaging devices. To enhance the strength and utility of DL models, several studies have employed data augmentation, transfer learning, and domain adaptation. Thanks in large part to data augmentation introduced by <xref ref-type="bibr" rid="ref_26">Zhu et al. (2025)</xref>, the proposed model could better handle new hospitals and imaging circumstances. In addition, <xref ref-type="bibr" rid="ref_4">Ayana et al. (2024)</xref> employed transfer learning to fine-tune pre-trained CNNs for specific medical image classification tasks using a limited amount of data.</p><p>In addition to making predictions more accurate, researchers are making sure DL models can be understood by anyone who interprets them. With models in DL being difficult to explain, ongoing research has focused on developing methods that provide meaningful insight into the decision-making processes of AI systems in healthcare. According to <xref ref-type="bibr" rid="ref_18">Stadlhofer &amp;amp; Mezhuyev (2023)</xref>, the prediction of any ML model can be described using a new method called Local Interpretable Model-agnostic Explanations (LIME). It allows for precise, local explanations that are very important for clinicians to trust medical systems. Interestingly, <xref ref-type="bibr" rid="ref_25">Zhang &amp;amp; Ogasawara (2023)</xref> utilized Grad-CAM in medical image classification models, identifying the parts of the image with the most significant impact on the model’s outcome. Such XAI methods benefit the clinical community by providing insight into how AI models arrive at their conclusions. Advances in DL for medical image analysis have faced challenges in obtaining sufficient data, utilizing it effectively in other contexts, and understanding its underlying mechanisms. Additionally, healthcare professionals must utilize these models after they have been approved and proven reliable through proper testing. Research on these problems helps ensure that AI-based diagnostic systems can be easily adopted into clinicians’ work processes with confidence.</p>
    </sec>
    <sec sec-type="">
      <title>3. Methodology</title>
      <p>This section explains the establishment of a neural network for automatic medical diagnosis in medical images using DL techniques. The methodology is intended to increase the accuracy of models in healthcare while also making their behavior more transparent. The methodology contains four parts: data selection, design of the DL network structure, formulation of the equations, and the model training and evaluation procedures. The unique aspect of this research is the combination of modern DL and XAI approaches to achieve both accuracy and transparency.</p>
      
        <sec>
          
            <title>3.1. Dataset description</title>
          
          <p>This research was conducted using the ChestX-ray14 dataset, which is publicly available for detecting pneumonia, tuberculosis, and various diseases in chest X-ray images. The collection of over 100,000 frontal-view chest X-rays, all accompanied by disease information, makes it the ideal choice for testing the proposed DL algorithm. The data includes labels for 14 different diseases, such as pneumonia, tuberculosis and several other respiratory diseases. All images were normalized using dataset-specific mean and standard deviation values, ensuring consistent pixel intensity scaling. Preprocessing also included resizing to 224 × 224 pixels and intensity standardization to reduce variability across imaging devices. <xref ref-type="table" rid="table_1">Table 1</xref> shows the dataset parameters.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Dataset parameters</title>
              </caption>
              <table><tbody><tr><td><p>Parameter</p></td><td><p>Value</p></td></tr><tr><td><p>Image count</p></td><td><p>112,120 images</p></td></tr><tr><td><p>Disease labels</p></td><td><p>14 different disease labels</p></td></tr><tr><td><p>Image dimensions</p></td><td><p>1024 × 1024 pixels (resized)</p></td></tr><tr><td><p>Image format</p></td><td><p>JPEG</p></td></tr><tr><td><p>Annotation type</p></td><td><p>Multi-label classification</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>The data was split into training, validation, and test sets, with proportions of 70%, 15%, and 15%, respectively. As a result, the model was trained with a large number of images, and its performance was checked using unseen data to prevent overfitting. Minority class samples were added to the dataset through data augmentation to address class imbalance. To improve robustness across heterogeneous clinical settings, the dataset preprocessing pipeline was designed to account for variations in imaging devices, exposure levels, and patient positioning. Normalization and augmentation procedures mimic real-world imaging differences, helping the model generalize across diverse acquisition environments.</p><p>The dataset contains imbalanced distribution across the 14 disease categories. To mitigate this, augmentation techniques were employed, including ± 15° random rotations, horizontal flips with 0.5 probability, and controlled brightness and contrast adjustments. These steps increased representation of minority disease classes. Below is a representation of chest X-ray images from the dataset.</p>
          <p><xref ref-type="table" rid="table_2">Table 2</xref> shows the chest X-rays from three categories: pneumonia, tuberculosis and normal chest X-rays. The pneumonia image highlights red circles to indicate where the infection has occurred in the lungs. The image of tuberculosis reveals the usual signs, including abnormal conditions in the lungs. On a normal chest X-ray, the lungs are healthy and show no signs of abnormality. Many DL models in medical diagnostics were trained with these images.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Chest X-ray images from the dataset</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Pneumonia Image</p></td><td colspan="1" rowspan="1"><p>Tuberculosis Image</p></td><td colspan="1" rowspan="1"><p>Normal Chest X-ray</p></td></tr><tr><td colspan="1" rowspan="1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_eSMVW6HwFfL1C5TJ.jpeg" /></td><td colspan="1" rowspan="1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_3M52REb2Kj-1F0v0.jpeg" /></td><td colspan="1" rowspan="1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_NhtLvx5cwr_jodKN.jpeg" /></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Dl architecture</title>
          
          <p>The combination of ResNet and DenseNet was chosen due to their complementary strengths: residual connections preserve gradient flow in deep layers, while dense connectivity enhances feature reuse and representation richness. Together, these structures improve feature extraction efficiency for complex thoracic patterns. The proposed model was constructed with a hybrid approach that includes CNN from ResNet and DenseNet. The hybrid architecture enables the model to identify features more efficiently, with minimal impact on computer power. The design was selected to achieve high performance with reasonable computational demands. The architecture consists of the following components:</p><p>•  Preprocessing layer: This layer is responsible for resizing the input images to 224 × 224 pixels, normalization, and data augmentation (random rotations, flips, and zooms).</p><p>•  Base CNN layers: The convolutional layers in the base network extract low-level features such as edges and textures. This layer consists of a stack of convolutional layers with increasing filter sizes (3 × 3, 5 × 5, etc.) and ReLU activation functions.</p><p>•  Residual blocks: This strategy, which comes from ResNet, helps avoid the gradient problem and passes information smoothly into deeper layers. A block in this network includes two convolutional layers and skips the input to go right into the block’s output.</p><p>•  DenseNet layers: DenseNet blocks are added after the residual blocks to improve the efficiency of feature reuse. In a DenseNet block, each layer receives input from all previous layers, allowing the model to learn more compact and effective representations.</p><p>•  Fully connected layer: After the convolutional and dense blocks, the feature maps are flattened and passed through a fully connected layer with dropout regularization to reduce overfitting.</p><p>•  Output layer: The output layer uses a sigmoid activation function to provide probabilities for each of the 14 disease classes (multi-label classification).</p><p><xref ref-type="fig" rid="fig_1">Figure 1</xref> illustrates the architecture of a medical diagnosis system comprising multiple layers. The process begins with a layer that processes the input data. After that, the base CNN extracts essential features from the input. Residual blocks help training become faster and avoid the problem of overfitting. DenseNet layers enable us to share features more efficiently and facilitate the creation of deeper networks. Each feature is combined in the fully connected layer, allowing the output layer to display the diagnosis. Its structure supports purposes such as detecting objects in images and categorizing them appropriately in healthcare. The model treats each disease label as an independent sigmoid output while sharing a common feature backbone. This design prevents unintended coupling of unrelated labels while indirectly capturing correlations through shared feature representations learned by the hybrid architecture.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>DL architecture for medical diagnosis</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_HR1cxmQMKKJJ7nv1.png"/>
            </fig>
          
          <p>The hybrid architecture is designed as follows:</p><p>•  Input layer: 224 × 224 × 3 (RGB image input)</p><p>•  Convolutional layer 1: 64 filters, 3 × 3 kernel, stride 1, and ReLU activation</p><p>•  Residual block 1: 128 filters, 3 × 3 kernel, and skip connection</p><p>•  DenseNet block 1: 256 filters, 3 × 3 kernel, and dense connections</p><p>•  Flattening layer</p><p>•  Fully connected layer: 1024 units and dropout rate = 0.5</p><p>•  Output layer: 14 units (for 14 disease categories) and sigmoid activation</p>
          <p>As shown in <xref ref-type="table" rid="table_3">Table 3</xref>, the learning rate of 0.0001 and batch size of 32 were selected after iterative tuning experiments using a stepwise search. These values provided the best balance between convergence stability and validation performance when compared with higher and lower parameter ranges. The model employs a multi-label classification approach to predict the presence of multiple diseases simultaneously. The key mathematical formulation of the proposed model is as follows: <inline-formula>
  <mml:math id="m18ir50vle">
    <mml:mi>X</mml:mi>
    <mml:mo>∈</mml:mo>
    <mml:msup>
      <mml:mi>R</mml:mi>
      <mml:mrow>
        <mml:mn>224</mml:mn>
        <mml:mn>224</mml:mn>
        <mml:mn>3</mml:mn>
        <mml:mo>×</mml:mo>
        <mml:mo>×</mml:mo>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> is the input image; the output of the model, <inline-formula>
  <mml:math id="mrj9v2lqxc">
    <mml:mi>Y</mml:mi>
    <mml:mo>∈</mml:mo>
    <mml:msup>
      <mml:mi>R</mml:mi>
      <mml:mrow>
        <mml:mn>14</mml:mn>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula>, is a vector of probabilities, where each element corresponds to the probability of the presence of a specific disease class.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Model hyperparameters</title>
              </caption>
              <table><tbody><tr><td><p>Hyperparameter</p></td><td><p>Value</p></td></tr><tr><td><p>Learning rate</p></td><td><p>0.0001</p></td></tr><tr><td><p>Batch size</p></td><td><p>32</p></td></tr><tr><td><p>Optimizer</p></td><td><p>Adam</p></td></tr><tr><td><p>Epochs</p></td><td><p>50</p></td></tr><tr><td><p>Loss function</p></td><td><p>Binary cross-entropy</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>The model was trained by minimizing the binary cross-entropy loss function:</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="mfrsmpij8d">
                <mml:mrow>
                  <mml:mi>L</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mover>
                    <mml:mi>Y</mml:mi>
                    <mml:mo>^</mml:mo>
                  </mml:mover>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>[</mml:mo>
                  <mml:mo>⁡</mml:mo>
                  <mml:mo>+</mml:mo>
                  <mml:mo>⁡</mml:mo>
                  <mml:mo>]</mml:mo>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                  <mml:mi>log</mml:mi>
                  <mml:mi>log</mml:mi>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mover>
                          <mml:mi>y</mml:mi>
                          <mml:mo>^</mml:mo>
                        </mml:mover>
                      </mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mn>1</mml:mn>
                    <mml:msub>
                      <mml:mi>y</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mn>1</mml:mn>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mover>
                          <mml:mi>y</mml:mi>
                          <mml:mo>^</mml:mo>
                        </mml:mover>
                      </mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:mrow>
                <mml:mo>(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mi>Y</mml:mi>
                <mml:munderover>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mn>14</mml:mn>
                  </mml:mrow>
                </mml:munderover>
              </mml:math>
            </disp-formula>
          
          <p>where, <italic>Y</italic> is the ground truth label vector (with binary values indicating the presence or absence of diseases), <inline-formula>
  <mml:math id="m6pi24qzso">
    <mml:mrow>
      <mml:mover>
        <mml:mi>Y</mml:mi>
        <mml:mo>^</mml:mo>
      </mml:mover>
    </mml:mrow>
  </mml:math>
</inline-formula> is the predicted probability vector for each class, <italic>y<sub>i</sub></italic> is the true label for the <italic>i</italic>-th disease class, and is the predicted probability for the <italic>i</italic>-th disease class.</p><p>Adam was used as the optimizer, which adapted the learning rate based on the first and second moments of the gradient, and the learning rate was set to 10<sup>-4</sup> to ensure stable convergence during training. The following steps outline the training and evaluation process:</p>
          <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Algorithm</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">1.<span style="font-family: Times New Roman"> Data Preprocessing:</p><p style="text-align: justify">•<span style="font-family: Times New Roman">  Load and resize the chest X-ray images to 224 × 224 pixels.</p><p style="text-align: justify">•<span style="font-family: Times New Roman">  Apply random transformations such as rotations, flips, and zooms to augment the dataset and reduce overfitting.</p><p style="text-align: justify">•<span style="font-family: Times New Roman">  Normalize pixel values to the range [0, 1] by dividing by 255.</p><p style="text-align: justify">2.<span style="font-family: Times New Roman"> Model Training:</p><p style="text-align: justify">•<span style="font-family: Times New Roman">  Initialize the model with pre-trained weights from ImageNet using transfer learning.</p><p style="text-align: justify">• <span style="font-family: Times New Roman"> Fine-tune the model by training it on the medical dataset for 50 epochs with a batch size of 32.</p><p style="text-align: justify">•<span style="font-family: Times New Roman">  Use the Adam optimizer and binary cross-entropy loss for optimization.</p><p style="text-align: justify">During transfer learning, early convolutional layers inherited from ImageNet were kept frozen in the initial training stage to preserve general visual feature representations. Deeper layers were gradually unfrozen in subsequent epochs, allowing finer adaptation to chest X-ray–specific structures.</p><p style="text-align: justify">Training regularization included early stopping based on validation loss and adaptive learning rate decay, which reduced overfitting and improved model stability during fine-tuning.</p><p style="text-align: justify">3.<span style="font-family: Times New Roman"> Model Evaluation:</p><p style="text-align: justify">• <span style="font-family: Times New Roman"> After completing training, evaluate the model on the validation set using accuracy, precision, recall, F1-score, and AUC-ROC for each type of disease included.</p><p style="text-align: justify">• <span style="font-family: Times New Roman"> Compute CAMs and Grad-CAMs to illustrate the reasons behind the model’s predictions.</p><p style="text-align: justify">4.<span style="font-family: Times New Roman"> Interpretability and Visualization:</p><p style="text-align: justify">• <span style="font-family: Times New Roman"> Rely on saliency maps and CAMs to discover the essential parts of images that the model looks at when it forms a conclusion.</p><p style="text-align: justify">5.<span style="font-family: Times New Roman"> Model Deployment:</p><p style="text-align: justify">•  The deployed AI model utilizes chest X-ray images to generate real-time predictions about various diseases.</p></td></tr></tbody></table>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Results and discussion</title>
      <p>This section illustrates the outcomes from the proposed model using chest X-ray images. The model was built to achieve excellent accuracy in detecting multiple diseases from medical images and to explain its predictions in ways that are easy to understand using XAI methods. Key performance indicators were studied, and the achievements were checked against the latest model. The illustrations of the results make them more transparent.</p>
      
        <sec>
          
            <title>4.1. Evaluation criteria</title>
          
          <p>Several performance metrics were examined to assess how effectively the proposed model identified diseases from chest X-rays. The evaluation criteria consist of:</p><p>•  Accuracy: The proportion of correctly classified samples.</p><p>•  Precision: The proportion of true positive predictions among all positive predictions.</p><p>•  Recall: The proportion of true positive predictions among all actual positive cases.</p><p>•  F1-score: The harmonic mean of precision and recall, providing a balance between the two.</p><p>•  AUC-ROC: This metric assesses the model’s ability to distinguish between different classes.</p><p>•  Inference time: The time taken for the model to process and provide a prediction for a given image.</p><p>These measurements reveal the overall accuracy, precision and generalization of the model for several different diseases.</p>
        </sec>
      
      
        <sec>
          
            <title>4.2. Performance results</title>
          
          <p><xref ref-type="table" rid="table_4">Table 4</xref> summarizes the results obtained by the proposed model on the test dataset (15% of the total dataset), which includes 14 disease categories (multi-label classification):</p>
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Overall model performance</title>
              </caption>
              <table><tbody><tr><td><p>Metric</p></td><td><p>Value</p></td></tr><tr><td><p>Accuracy</p></td><td><p>92.5%</p></td></tr><tr><td><p>Precision</p></td><td><p>0.91</p></td></tr><tr><td><p>Recall</p></td><td><p>0.93</p></td></tr><tr><td><p>F1-score</p></td><td><p>0.92</p></td></tr><tr><td><p>AUC-ROC</p></td><td><p>0.96</p></td></tr><tr><td><p>Inference time</p></td><td><p>150 ms/image</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>According to <xref ref-type="table" rid="table_4">Table 4</xref>, the model achieved an accuracy of 92.5% and an F1-score of 0.92, indicating its good performance across each disease category. With an AUC-ROC of 0.96, the model demonstrates excellent performance in identifying positive and negative cases for medical image classification. For a more granular understanding, the performance for each of the 14 disease categories was assessed. The results are presented in <xref ref-type="table" rid="table_5">Table 5</xref>.</p>
          
            <table-wrap id="table_5">
              <label>Table 5</label>
              <caption>
                <title>Per-class performance</title>
              </caption>
              <table><tbody><tr><td><p>Disease</p></td><td><p>Precision</p></td><td><p>Recall</p></td><td><p>F1-score</p></td><td><p>AUC-ROC</p></td></tr><tr><td><p>Pneumonia</p></td><td><p>0.91</p></td><td><p>0.94</p></td><td><p>0.92</p></td><td><p>0.96</p></td></tr><tr><td><p>Tuberculosis</p></td><td><p>0.88</p></td><td><p>0.91</p></td><td><p>0.89</p></td><td><p>0.94</p></td></tr><tr><td><p>Lung cancer</p></td><td><p>0.94</p></td><td><p>0.93</p></td><td><p>0.94</p></td><td><p>0.97</p></td></tr><tr><td><p>Cardiomegaly</p></td><td><p>0.89</p></td><td><p>0.87</p></td><td><p>0.88</p></td><td><p>0.92</p></td></tr><tr><td><p>Consolidation</p></td><td><p>0.92</p></td><td><p>0.90</p></td><td><p>0.91</p></td><td><p>0.95</p></td></tr><tr><td><p>Atelectasis</p></td><td><p>0.87</p></td><td><p>0.88</p></td><td><p>0.87</p></td><td><p>0.91</p></td></tr><tr><td><p>Effusion</p></td><td><p>0.91</p></td><td><p>0.93</p></td><td><p>0.92</p></td><td><p>0.95</p></td></tr><tr><td><p>Infiltration</p></td><td><p>0.85</p></td><td><p>0.89</p></td><td><p>0.87</p></td><td><p>0.90</p></td></tr><tr><td><p>Edema</p></td><td><p>0.93</p></td><td><p>0.92</p></td><td><p>0.92</p></td><td><p>0.96</p></td></tr><tr><td><p>Emphysema</p></td><td><p>0.88</p></td><td><p>0.89</p></td><td><p>0.88</p></td><td><p>0.93</p></td></tr><tr><td><p>Fibrosis</p></td><td><p>0.92</p></td><td><p>0.90</p></td><td><p>0.91</p></td><td><p>0.94</p></td></tr><tr><td><p>Pleural thickening</p></td><td><p>0.86</p></td><td><p>0.87</p></td><td><p>0.86</p></td><td><p>0.90</p></td></tr><tr><td><p>Mass</p></td><td><p>0.90</p></td><td><p>0.92</p></td><td><p>0.91</p></td><td><p>0.95</p></td></tr><tr><td><p>Nodule</p></td><td><p>0.89</p></td><td><p>0.91</p></td><td><p>0.90</p></td><td><p>0.94</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>These per-class metrics show that the model performs particularly well in detecting conditions like lung cancer, pleural effusion, and consolidation, with AUC-ROC values exceeding 0.95 for many of the disease categories. Performance variations across diseases arise from multiple factors, including limited samples for certain categories, subtle disease manifestations, and overlapping radiographic patterns. Conditions such as infiltration and pleural thickening show lower AUC due to inherent visual similarity with other thoracic findings, reflecting challenges related to subtle radiographic characteristics and overlapping visual signatures, which naturally make these classes harder to distinguish.</p>
        </sec>
      
      
        <sec>
          
            <title>4.3. Comparison with existing models</title>
          
          <p>The performance of the proposed model was evaluated by comparing it with leading models used in medical image classification, including SVM, Random Forest (RF) and DL models like VGG16, ResNet50 and InceptionV3, all trained using the same ChestX-ray14 dataset. <xref ref-type="table" rid="table_6">Table 6</xref> summarizes the comparison.</p>
          
            <table-wrap id="table_6">
              <label>Table 6</label>
              <caption>
                <title>Comparison with existing models</title>
              </caption>
              <table><tbody><tr><td><p>Model</p></td><td><p>Accuracy</p></td><td><p>Precision</p></td><td><p>Recall</p></td><td><p>F1-score</p></td><td><p>AUC-ROC</p></td></tr><tr><td><p>Proposed model (CNN + ResNet + DenseNet)</p></td><td><p>92.5%</p></td><td><p>0.91</p></td><td><p>0.93</p></td><td><p>0.92</p></td><td><p>0.96</p></td></tr><tr><td><p>VGG16</p></td><td><p>85.3%</p></td><td><p>0.83</p></td><td><p>0.84</p></td><td><p>0.83</p></td><td><p>0.89</p></td></tr><tr><td><p>ResNet50</p></td><td><p>88.6%</p></td><td><p>0.85</p></td><td><p>0.86</p></td><td><p>0.85</p></td><td><p>0.91</p></td></tr><tr><td><p>InceptionV3</p></td><td><p>89.4%</p></td><td><p>0.87</p></td><td><p>0.88</p></td><td><p>0.87</p></td><td><p>0.92</p></td></tr><tr><td><p>SVM</p></td><td><p>83.2%</p></td><td><p>0.80</p></td><td><p>0.81</p></td><td><p>0.80</p></td><td><p>0.85</p></td></tr><tr><td><p>RF</p></td><td><p>84.1%</p></td><td><p>0.82</p></td><td><p>0.83</p></td><td><p>0.82</p></td><td><p>0.86</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>The results in the table demonstrate that the proposed hybrid model outperforms SVM, RF, VGG16, ResNet50, and InceptionV3. More accurate, precise, helpful, and reliable results are noted, revealing how a hybrid CNN-ResNet-DenseNet architecture, combined with XAI, works well.</p>
        </sec>
      
      
        <sec>
          
            <title>4.4. Visual results and model interpretability</title>
          
          <p>In healthcare, the ease with which a model can be interpreted is essential, as clarifying its decisions can increase clinicians’ trust and support for the model. This section presents several visualizations from the proposed model, including precision-recall (PR) curves, receiver operating characteristic (ROC) curves, and Grad-CAM heatmaps. They allow the model’s performance to be evaluated across different disease categories and reveal the image regions that the model attends to when generating predictions.</p>
          
            <sec>
              
                <title>4.4.1 Pr curve</title>
              
              <p>A PR curve is used to examine how the balance between precision and recall changes with varying thresholds. Because the model is built for multiple categories, a PR curve was created for each disease group it predicts. As shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>, a PR plot is provided for the disease pneumonia as an example.</p>
              
                <fig id="fig_2">
                  <label>Figure 2</label>
                  <caption>
                    <title>PR curve for pneumonia detection</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_3LUOnqd-ubGo94KB.png"/>
                </fig>
              
            </sec>
          
          
            <sec>
              
                <title>4.4.2 Roc curve</title>
              
              <p>Sensitivity against 1-specificity (false positive rate) can be seen on an ROC curve. It is an excellent method for judging how well the model can separate different categories. The AUC-ROC can be determined, which delivers a single number that allows us to compare the model’s results. <xref ref-type="fig" rid="fig_3">Figure 3</xref> shows the ROC curve for pneumonia detection.</p>
              
                <fig id="fig_3">
                  <label>Figure 3</label>
                  <caption>
                    <title>ROC curve for pneumonia detection</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_tblvZ6qjYnl8XSJC.png"/>
                </fig>
              
            </sec>
          
          
            <sec>
              
                <title>4.4.3 Evaluation of the results with charts</title>
              
              <p>In addition to the performance metrics, visual charts are provided to summarize the model’s classification accuracy and performance across different diseases. The confusion matrix (<xref ref-type="fig" rid="fig_4">Figure 4</xref>) provides insight into how well the model classifies each disease and helps identify specific diseases where the model may be making more errors.</p>
              
                <fig id="fig_4">
                  <label>Figure 4</label>
                  <caption>
                    <title>Confusion matrix for pneumonia detection</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_3xruTckA5j1T9Q3q.png"/>
                </fig>
              
            </sec>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>A DL model was designed in this study to automatically diagnose medical conditions, focusing on AI for image recognition in chest X-rays. With CNN, ResNet, and DenseNet, the model achieved an accuracy of 92.5%, surpassing the accuracies of VGG16 and ResNet50. Moreover, the model achieved impressive results in many disease categories, yielding high precision, recall, and F1-score values. The use of Grad-CAM and other XAI techniques makes the model predictions more straightforward and clinicians more confident in the model's decisions. Nevertheless, the study has a few limitations despite these positive outcomes. First, since the training data dealt only with chest X-ray images, its performance may not be well-suited for other image types commonly used in healthcare. Relying heavily on large, annotated datasets is problematic because data collection on a widespread scale can be challenging due to privacy concerns in specific locations. The model is also limited by its complexity, which could be challenging when trying to use it in real time on devices with few resources.</p><p>Improving the model's ability to work well with other data, as well as with hospitals and imaging methods, will be a primary focus moving forward. Incorporation of diverse images in the model, such as those from various locations and people, will help it perform more effectively. Computational efficiency is particularly essential for real-world deployment. Further study on how different forms of data can be combined (such as X-ray images and patient information) may help doctors make faster and more accurate diagnoses. Future work may incorporate multimodal inputs such as patient demographics, clinical history, and laboratory markers. Integrating these features through attention- or fusion-based architectures can enhance diagnostic reliability by combining radiological and clinical evidence. The model maintains an average inference time of approximately 150 ms per image, making it suitable for deployment on standard GPU workstations. The computational footprint supports real-time use in hospital environments without requiring high-end hardware.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>Conceptualization, A.K.P.; methodology, A.K.P.; software, A.K.P.; data curation, A.K.P.; investigation, A.K.P. and B.V.; validation, A.K.P. and B.V.; formal analysis, A.K.P.; resources, B.V.; writing—original draft preparation, A.K.P.; writing—review and editing, A.K.P. and K.V.K.; supervision, K.V.K.; project administration, K.V.K.; medical domain guidance, K.V.K.; All authors have read and agreed to the published version of the manuscript.</p>
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>1–5</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Agneya</surname>
              <given-names>D. A.</given-names>
            </name>
            <name>
              <surname>Shekar</surname>
              <given-names>M. S.</given-names>
            </name>
            <name>
              <surname>Bharadwaj</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Vineeth</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Neelima</surname>
              <given-names>M. L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/innova63080.2024.10847040</pub-id>
          <article-title>Deep learning in medical image analysis: A survey</article-title>
          <source>, http://dx.doi.org/10.1109/INNOVA63080.2024.10847040</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>108386-108397</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ahmad</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Akram</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Jaffar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Rashid</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Bhatti</surname>
              <given-names>S. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/access.2023.3311892</pub-id>
          <article-title>Breast cancer detection using deep learning: An investigation using the DDSM dataset and a customized AlexNet and support vector machine</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>3457</page-range>
          <issue>21</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Amran</surname>
              <given-names>G. A.</given-names>
            </name>
            <name>
              <surname>Alsharam</surname>
              <given-names>M. S.</given-names>
            </name>
            <name>
              <surname>Blajam</surname>
              <given-names>A. O. A.</given-names>
            </name>
            <name>
              <surname>Hasan</surname>
              <given-names>A. A.</given-names>
            </name>
            <name>
              <surname>Alfaifi</surname>
              <given-names>M. Y.</given-names>
            </name>
            <name>
              <surname>Amran</surname>
              <given-names>M. H.</given-names>
            </name>
            <name>
              <surname>Gumaei</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Eldin</surname>
              <given-names>S. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/electronics11213457</pub-id>
          <article-title>Brain tumor classification and detection using hybrid deep tumor network.</article-title>
          <source>Electronics</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>57</volume>
          <page-range>232</page-range>
          <issue>9</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ayana</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Dese</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Abagaro</surname>
              <given-names>A. M.</given-names>
            </name>
            <name>
              <surname>Jeong</surname>
              <given-names>K. C.</given-names>
            </name>
            <name>
              <surname>Yoon</surname>
              <given-names>S. D.</given-names>
            </name>
            <name>
              <surname>Choe</surname>
              <given-names>S. W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10462-024-10855-7</pub-id>
          <article-title>Multistage transfer learning for medical images</article-title>
          <source>Artif. Intell. Rev.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>239</page-range>
          <issue>10</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bhati</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Neha</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Amiruzzaman</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/jimaging10100239</pub-id>
          <article-title>A survey on explainable artificial intelligence (XAI) techniques for visualizing deep learning models in medical imaging</article-title>
          <source>J. Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>713</page-range>
          <issue>11</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cai</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.21037/atm.2020.02.44</pub-id>
          <article-title>A review of the application of deep learning in medical image classification and segmentation</article-title>
          <source>Ann. Transl. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>132</volume>
          <page-range>104318</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chouhan</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Shah</surname>
              <given-names>J. Z.</given-names>
            </name>
            <name>
              <surname>Hussnain</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>M. W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compbiomed.2021.104318</pub-id>
          <article-title>Deep convolutional neural network and emotional learning based breast cancer detection using digital mammography</article-title>
          <source>Comput. Biol. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>511-516</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Deheyab</surname>
              <given-names>A. O. A.</given-names>
            </name>
            <name>
              <surname>Alwan</surname>
              <given-names>M. H.</given-names>
            </name>
            <name>
              <surname>Rezzaqe</surname>
              <given-names>I. K. A.</given-names>
            </name>
            <name>
              <surname>Mahmood</surname>
              <given-names>O. A.</given-names>
            </name>
            <name>
              <surname>Hammadi</surname>
              <given-names>Y. I.</given-names>
            </name>
            <name>
              <surname>Kareem</surname>
              <given-names>A. N.</given-names>
            </name>
            <name>
              <surname>Ibrahim</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3584202.3584278</pub-id>
          <article-title>An overview of challenges in medical image processing</article-title>
          <source>, </source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>542</volume>
          <page-range>115-118</page-range>
          <issue>7639</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Esteva</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kuprel</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Novoa</surname>
              <given-names>R. A.</given-names>
            </name>
            <name>
              <surname>Ko</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Swetter</surname>
              <given-names>S. M.</given-names>
            </name>
            <name>
              <surname>Blau</surname>
              <given-names>H. M.</given-names>
            </name>
            <name>
              <surname>Thrun</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/nature21056</pub-id>
          <article-title>Dermatologist-level classification of skin cancer with deep neural networks</article-title>
          <source>Nature</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>537-540</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>He</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/iciibms62405.2024.10792859</pub-id>
          <article-title>Diving deep: The role of deep learning in medical image analysis, today and tomorrow</article-title>
          <source>, </source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>18</volume>
          <page-range>1049-1075</page-range>
          <issue>11</issue>
          <year>2002</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lehmann</surname>
              <given-names>T. M.</given-names>
            </name>
            <name>
              <surname>Gonner</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Spitzer</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/42.816070</pub-id>
          <article-title>Survey: Interpolation methods in medical image processing</article-title>
          <source>IEEE Trans. Med. Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>1135-1144</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ribeiro</surname>
              <given-names>M. T.</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Guestrin</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/2939672.2939778</pub-id>
          <article-title>“Why should I trust you?” Explaining the predictions of any classifier</article-title>
          <source>, </source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>507</page-range>
          <issue>5</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Saikia</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Kalita</surname>
              <given-names>S. K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s42979-024-02868-4</pub-id>
          <article-title>Alzheimer disease detection using MRI: Deep learning review</article-title>
          <source>SN Comput. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>128</volume>
          <page-range>336-359</page-range>
          <issue>2</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Selvaraju</surname>
              <given-names>R. R.</given-names>
            </name>
            <name>
              <surname>Cogswell</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Abhishek</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Ramakrishna</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Devi</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Dhruv</surname>
              <given-names>B</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11263-019-01228-7</pub-id>
          <article-title>Grad-CAM: Visual explanations from deep networks via gradient-based localization</article-title>
          <source>Int. J. Comput. Vis.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>113</volume>
          <page-range>103627</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shamshirband</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Fathi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Dehzangi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Chronopoulos</surname>
              <given-names>A. T.</given-names>
            </name>
            <name>
              <surname>Alinejad-Rokny</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.jbi.2020.103627</pub-id>
          <article-title>A review on deep learning approaches in healthcare systems: Taxonomies, challenges, and open issues</article-title>
          <source>J. Biomed. Inform.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <page-range>52</page-range>
          <issue>6</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Singh</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sengupta</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Lakshminarayanan</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/jimaging6060052</pub-id>
          <article-title>Explainable deep learning models in medical image analysis</article-title>
          <source>J. Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>127</volume>
          <page-range>85-94</page-range>
          <issue>1</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Son</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Shin</surname>
              <given-names>J. Y.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>H. D.</given-names>
            </name>
            <name>
              <surname>Jung</surname>
              <given-names>K. H.</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>K. H.</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>S. J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ophtha.2019.05.029</pub-id>
          <article-title>Development and validation of deep learning models for screening multiple abnormal findings in retinal fundus images</article-title>
          <source>Ophthalmology</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>10</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Stadlhofer</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Mezhuyev</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s44244-023-00009-z</pub-id>
          <article-title>Approach to provide interpretability in machine learning models for image classification</article-title>
          <source>Ind. Artif. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>664-668</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sunil Kumar Aithal</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Rajashree</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/iceca58529.2023.10395716</pub-id>
          <article-title>Deep learning based automated pneumonia detection from X-ray images</article-title>
          <source>, </source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>79</volume>
          <page-range>102470</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Van der Velden</surname>
              <given-names>B. H.</given-names>
            </name>
            <name>
              <surname>Kuijf</surname>
              <given-names>H. J.</given-names>
            </name>
            <name>
              <surname>Gilhuijs</surname>
              <given-names>K. G.</given-names>
            </name>
            <name>
              <surname>Viergever</surname>
              <given-names>M. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.media.2022.102470</pub-id>
          <article-title>Explainable artificial intelligence (XAI) in deep learning-based medical image analysis</article-title>
          <source>Med. Image Anal.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>15-20</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Slam</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/cvidliccea56201.2022.9824838</pub-id>
          <article-title>A novel deep convolution neural network model for CT image classification based on Covid-19</article-title>
          <source>, </source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Xie</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.31083/j.rcm2501008</pub-id>
          <article-title>Machine learning for detecting atrial fibrillation from ECGs: Systematic review and meta-analysis</article-title>
          <source>Rev. Cardiovasc. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>1-6</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zakaria</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Mohamed</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Abdelghani</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Sundaraj</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ai-csp52968.2021.9671124</pub-id>
          <article-title>VGG16, ResNet-50, and GoogLeNet deep learning architecture for breathing sound classification: A comparative study</article-title>
          <source>, </source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>818-833</page-range>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zeiler</surname>
              <given-names>M. D.</given-names>
            </name>
            <name>
              <surname>Fergus</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-319-10590-1_53</pub-id>
          <article-title>Visualizing and understanding convolutional networks</article-title>
          <source>, </source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>1070</page-range>
          <issue>9</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Ogasawara</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/bioengineering10091070</pub-id>
          <article-title>Grad-CAM-based explainable artificial intelligence related to medical text processing</article-title>
          <source>Bioengineering</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>101</volume>
          <page-range>103422</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Bolsterlee</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Meijering</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.media.2024.103422</pub-id>
          <article-title>Improving cross-domain generalizability of medical image segmentation using uncertainty and shape-aware continual test-time domain adaptation</article-title>
          <source>Med. Image Anal.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>