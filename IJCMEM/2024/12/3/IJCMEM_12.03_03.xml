<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IJCMEM</journal-id>
      <journal-id journal-id-type="doi">10.18280</journal-id>
      <journal-title-group>
        <journal-title>International Journal of Computational Methods and Experimental Measurements</journal-title>
        <abbrev-journal-title abbrev-type="issn">Int. J. Comput. Methods Exp. Meas.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IJCMEM</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2046-0554</issn>
      <issn publication-format="print">2046-0546</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-Xvpw7GSANyJle3AzEwJkk53rJn7sWijl</article-id>
      <article-id pub-id-type="doi">10.18280/ijcmem.120303</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Parallel Memory-Based Collaborative Filtering for Distributed Big Data Environments</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5268-158X</contrib-id>
          <name>
            <surname>Shree</surname>
            <given-names>Pallavi</given-names>
          </name>
          <email>pallavis.phd18.cs@nitp.ac.in</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8956-0175</contrib-id>
          <name>
            <surname>Suvvari</surname>
            <given-names>Somaraju</given-names>
          </name>
          <email/>
        </contrib>
        <aff id="aff_1">Department of Computer Science, National Institute of Technology Patna, Patna 80005, India</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>29</day>
        <month>09</month>
        <year>2024</year>
      </pub-date>
      <volume>12</volume>
      <issue>3</issue>
      <fpage>217</fpage>
      <lpage>225</lpage>
      <page-range>217-225</page-range>
      <history>
        <date date-type="received">
          <day>24</day>
          <month>06</month>
          <year>2024</year>
        </date>
        <date date-type="accepted">
          <day>18</day>
          <month>09</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>Â©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>The amount of information produced about any item or user has reached a very staggering level. Not only the volume of data, the velocity of data has reached an unprecedented magnitude. For any information retrieval or information processing system to work efficiently, it should be able to process massive amounts of data in real-time. Modern systems face a lot of challenges in managing data with high volume and velocity, especially when these systems are required to generate accurate predictions in a timely fashion. The most efficient way to ensure that modern information retrieval systems can adapt to the current volume and velocity of data is to implement them over a parallel and distributed environment. In this paper, we put forward a method for enhancing the scalability and performance of recommender systems in big data environments. By using the Euclidean distance to calculate the cosine similarity we introduce a technique which is efficient in parallelizing the algorithm for distributed environments. Thereby improving the computational efficiency and scalability of the recommender system. This enables such systems to manage large datasets with high accuracy and speed. With the help of parallel processing, our method can assist modern information retrieval systems keep up with the pace of ever-growing demands of data velocity and volume, ensuring real-time performance and robust scalability.</p></abstract>
      <kwd-group>
        <kwd>Memory-based</kwd>
        <kwd>Cosine similarity</kwd>
        <kwd>Euclidean distance</kwd>
        <kwd>PySpark</kwd>
        <kwd>Parallel and distributed environment</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="2"/>
        <fig-count count="3"/>
        <table-count count="6"/>
        <ref-count count="30"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>There has been an incredible development and business surge in the online commerce industry. The economy fueled by this growth has evolved into a connected economy, and due to the rapid expansion of data, the network has now stepped into the age of big data. Users cannot correctly use the information made available by ever-growing e-commerce platforms, as the amount of commodity information has reached an inconveniently large scale. This has led to information overdose for the users. This means that incoming information is above and beyond the processing capacity of recipients, users, and systems alike. Due to this astronomical data growth, parallel and distributed systems of recommendation are becoming increasingly important. One of the key benefits of such systems being distributed and parallel is their ability to process large datasets more quickly. Real-time recommendations to users have to be the main aim for recommender systems, such as those used by streaming services and e-commerce websites. Another benefit of parallel and distributed recommender systems is their ability to handle larger datasets. This is important for recommender systems, considering the number of parameters that are factored in while providing recommendations, including but not limited to user likings, item features, and information related to users and items. Additionally, parallelized distributed recommender systems are scalable, resilient, and cost-effective. This means they can be deployed on large-scale systems, handle failures, and be deployed on commodity hardware. We present an implementation of a memory-based collaborative filtering algorithm in a parallel and distributed environment in this paper. Our implementation uses several techniques to improve performance, including:</p><p style="text-align: justify">(1) Partitioning the user-item preference matrix across multiple processors.</p><p style="text-align: justify">(2) Using a more efficient, parallelizable version of the cosine similarity formula.</p><p style="text-align: justify">Our experimental results show that our parallel implementation of a memory-based collaborative filtering algorithm can significantly improve performance over a serial execution. The paper has been divided into six sections. In the second section, we discuss the basic terminologies used in recommender systems. The third section of the paper discusses the similar efforts in the field. Our methodology of parallel implementation of a memory-based collaborative filtering algorithm is laid out in the fourth section. All the experimental results of the proposed method have been presented using graphical and tabular data in the fifth section. We finally conclude the paper in the final and sixth sections.</p>
    </sec>
    <sec sec-type="">
      <title>2. Background</title>
      <p>Before we jump into recommender systems, the most imperative step should be to understand different categories of data processing systems referred to as Information Retrieval systems. Information retrieval (IR) can be defined as the process of using a source of data and extracting information pertinent to an inquiry done by, any user or any other system e.g., a movie based on genre from a streaming platform, a journal from a repository based on a subject, or results produced by the search engine based on a question asked by the user [<xref ref-type="bibr" rid="ref_1">1</xref>]. One the types of IR is recommender systems. Hence, we can define a recommender system (RS) as a subcategory of an information filtering system that calculates the most accurate rating a user would provide for an item [<xref ref-type="bibr" rid="ref_2">2</xref>]. RS as a software solution has its roots in the most basic human tendency of asking for suggestions or recommendations before trying out any new experience or object or even for making friends. The information provided by these systems helps the users make decisions like purchasing an item, renting a movie, etc. The recommendations presented are designed to assist individuals in making informed decisions across a range of contexts. This means that the primary objective of these systems is to provide personalized recommendations which is the major difference between recommender systems and information retrieval search engines [<xref ref-type="bibr" rid="ref_2">2</xref>]. Recommender systems have emerged as essential tools in electronic commerce, providing effective solutions for online users grappling with information overdose. Their significance lies in their ability to sift through vast amounts of data, enabling users to make informed decisions. These systems have become pivotal in addressing the challenges posed by the overwhelming volume of information on online platforms. Hence, numerous methods for generating recommendations have been put forward. Companies like Netflix, Amazon, Facebook, etc. have successfully applied and gained from these methods for recommending books, movies and friends. Any RS has two main objects: âItemsâ and âUsersâ. The topic or object for which the suggestions are generated is generally called âItemâ. Normally, RS is meant to recommend a specific type of item like movies, books, songs, restaurants, etc. Such systems are mainly aimed at individuals (referred to as âUserâ in RS) who are relatively new in a certain domain, like people looking for hotel suggestions before visiting a new place. Interaction between the items and users is called âTransactionâ. Transactions give us data about items, users and preferences. The recorded preferences of users act as an input to the RS. These inputs can be collected implicitly or explicitly. Explicit feedback [<xref ref-type="bibr" rid="ref_3">3</xref>] shows the direct preference of users for an item. Explicit ratings are mostly on a numerical scale like a range of 1 rating for worst and 5 being the best, like otherwise dislike, etc. Implicit feedback is extracted from user actions like the amount of time the user was on any given page, clicks performed by users on websites, whether the user purchased the item or watched the video, etc. Based on the way recommendations are generated, RS can be classified as given in <xref ref-type="fig" rid="fig_1">Figure 1</xref>.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Type of recommendation system</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_F2L7Vb3qqwNAgOsO.png"/>
        </fig>
      
      <p>1) Collaborative filtering: Collaborative filtering [<xref ref-type="bibr" rid="ref_4">4</xref>] recommender systems leverage the preferences and behaviours of other users to suggest items or content to a particular user. By analysing the choices and interactions of a diverse user base, these systems identify patterns and correlations, allowing them to generate personalized recommendations. This approach helps users navigate the abundance of available options, making their online experience more tailored and relevant. Through sophisticated algorithms and data analysis, recommender systems enhance user engagement and satisfaction by presenting them with choices aligned with their interests and preferences. For instance, a recommendation of a film for a viewer can be grounded on the explicit or implicit feedback given by various other viewers who have watched the movie.</p><p style="text-align: justify">(2) Content-based: Content-based [<xref ref-type="bibr" rid="ref_5">5</xref>] recommender systems use the previous interactions of the uses with the system and item properties of the items under consideration. Such systems might recommend movies based on the genre of the previously watched movies of the users or a book recommender tool might suggest books of the same author whose other books have already been read by the same user.</p><p style="text-align: justify">(3) Demographic: These systems use the date of birth, sex of the user, and their current geographical location to generate suggestions for the user. For instance, this type of recommender would recommend the product to any user on the same lines as the products that users of the same age and gender have purchased.</p><p style="text-align: justify">(4) Knowledge-based: Industry or domain-specific knowledge is used by such systems to provide useful suggestions to the user. The best example is a system suggesting a recipe to the user, considering the dietary restrictions and the ingredients they have on hand.</p><p style="text-align: justify">(5) Community-based: Community-based recommender systems recommend items to a user relying on the predilections of user clusters having similar features. For instance, this type of recommender would use all views of all users of the same online forum to which another user belongs and suggest a movie rated highly by the users of the forum.</p><p style="text-align: justify">(6) Hybrid filtering: These recommender systems are built using a combination of systems mentioned above.</p>
      
        <sec>
          
            <title>2.1. Content-based filtering algorithms</title>
          
          <p><span style="color: rgb(30, 30, 30); font-family: Lato, Helvetica Neue, Arial, sans-serif">Content-based filtering (CBF) generates a feature set of items and preference or behaviour profiles for users based on additional information about user demography, online behavior, their friend network, and the properties of items used by customers. CBF is classified as content-based when it provides recommendations grounded in the content details of items. Conversely, when these recommendations rely on the contextual information of users, CBF is termed context-based. In most situations, extracting relevant information about users or items becomes challenging due to a lack of information or information overload. This limits the performance and application of CBF.</p>
        </sec>
      
      
        <sec>
          
            <title>2.2. Collaborative filtering algorithms</title>
          
          <p style="text-align: justify">The majority of the recommender systems are based on Collaborative filtering algorithms. It can be defined as a method that generates suggestions, i.e., filters the information related to the choices of any person by gathering information from a large quantity of other people, i.e., collaborative filtering [<xref ref-type="bibr" rid="ref_6">6</xref>]. Breese et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] categorized CF techniques into systems which are based on memory while another type of system is built on models.</p>
          
            <sec>
              
                <title>2.2.1 Model-based cf techniques</title>
              
              <p>In the model-based approach, the system generates parameters to model the behavior of users and features of items, which enables it to make suggestions using the created parameters. Filtering techniques are collaborative in nature and build learning models based on machines for predicting ratings that an item might get from the user. These models are trained on a dataset of user ratings and item features. Once the models are trained, they can forecast the most probable rating an item would get from a specific user, even in scenarios where the user-item interaction would not have occurred earlier and no rating data is recorded for this user-item pair. As the main of such techniques is to predict rating, probability of purchase, etc., these systems are most commonly configured as supervised learning problems.</p>
            </sec>
          
          
            <sec>
              
                <title>2.2.2 Memory-based cf techniques</title>
              
              <p>Memory-based collaborative filtering techniques are relatively simple to implement and can be very effective for generating personalized recommendations. Memory or neighbourhood-based CF are implemented by calculating distance or similarity metrics. In memory-based CF, recommendations are based on similarities among users [<xref ref-type="bibr" rid="ref_8">8</xref>] or items [<xref ref-type="bibr" rid="ref_9">9</xref>].</p><p style="text-align: justify">(1) User-item Collaborative Filtering: items used or purchased or rated by users similar to us. Such systems first find users similar to the user under consideration and then generate recommendations for users based on their purchase or rating history.</p><p style="text-align: justify">(2) Item-item Collaborative Filtering: based on the segregation that suggests that users who show interest in specific items are more likely to be interested in these items. Here, we first find similarities among a bunch of items and recommend items that are most similar to items already rated by that user.</p><p style="text-align: justify">A detailed comparison in terms of the various characteristics of both these methods can be found in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
              
                <table-wrap id="table_1">
                  <label>Table 1</label>
                  <caption>
                    <title>Comparison of memory and model-based CF</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="1"><p>Characteristic</p></td><td colspan="1" rowspan="1"><p>Model-Based Collaborative Filtering</p></td><td colspan="1" rowspan="1"><p>Memory-Based Collaborative Filtering</p></td></tr><tr><td colspan="1" rowspan="1"><p>Simplicity</p></td><td colspan="1" rowspan="1"><p>More complex</p></td><td colspan="1" rowspan="1"><p>Simpler</p></td></tr><tr><td colspan="1" rowspan="1"><p>Interpretability</p></td><td colspan="1" rowspan="1"><p>Less interpretable</p></td><td colspan="1" rowspan="1"><p>More interpretable</p></td></tr><tr><td colspan="1" rowspan="1"><p>Flexibility</p></td><td colspan="1" rowspan="1"><p>Less flexible</p></td><td colspan="1" rowspan="1"><p>More flexible</p></td></tr><tr><td colspan="1" rowspan="1"><p>Cold-start handling</p></td><td colspan="1" rowspan="1"><p>Worse</p></td><td colspan="1" rowspan="1"><p>Better</p></td></tr><tr><td colspan="1" rowspan="1"><p>Scalability</p></td><td colspan="1" rowspan="1"><p>More scalable for large datasets (once models are trained)</p></td><td colspan="1" rowspan="1"><p>Less scalable for large datasets</p></td></tr><tr><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>More accurate, especially for sparse datasets</p></td><td colspan="1" rowspan="1"><p>Less accurate, especially for sparse datasets</p></td></tr><tr><td colspan="1" rowspan="1"><p>Explainability</p></td><td colspan="1" rowspan="1"><p>Less explainable</p></td><td colspan="1" rowspan="1"><p>More explainable</p></td></tr></tbody></table>
                </table-wrap>
              
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>2.3. Challenges of the recommender system</title>
          
          <p>Recommender systems are complex algorithms that use data to predict what users will like. They are used in various applications, like online shopping, streaming services, and social media. However, recommender systems also face several challenges, including:</p><p style="text-align: justify">Lack of data: Recommender systems need data to learn user preferences and make accurate recommendations. However, it is quite possible that there is not enough data for the users with very specific interests and this makes it very difficult to make useful and correct recommendations.</p><p style="text-align: justify">Cold start problem: It is a phenomenon faced by a recommendation system when a new user or item enters the system and recommendations have to be generated for such users or items. Being new to the system there is no associated data for such users or items. This makes it very hard to provide recommendations when no data is available for the userâs interests or the itemâs popularity. Thus, making it very challenging while generating recommendations.</p><p style="text-align: justify">Scalability: The most critical feature of any Recommender system should be its ability to scale up to the ever-increasing volume of data being generated by users and items. As most of the recommender systems run on very complex algorithms which are computationally demanding, scalability becomes one of the biggest challenges that should be considered while designing any recommender systems.</p><p style="text-align: justify">Sparsity: As the majority of the users do not interact with the majority of the items, the user-item matrix in a recommender system is often very sparse. This means data is not available for most of the user-item pairs. The absence of data makes it very difficult to make useful recommendations and understand the preferential pattern of the user or the popularity of items.</p><p style="text-align: justify">Bias: The methods or even data used to generate recommendations can be the source of an inherent bias towards certain items or users. Once the bias is present in the recommendations there is a very high chance of the same items being recommended to the majority of users and not taking into account the actual preferences of the user.</p><p style="text-align: justify">Privacy: There is an automatic concern relating to the collection and storage of data for making more accurate recommendations. This data can be transactional data or implicit data like browsing history and purchase history. This is not only a security concern but also raises ethical concerns as to what is the extent to which we should collect data without infringing the privacy of any user.</p><p style="text-align: justify">Apart from the challenges discussed above, there are numerous other challenges faced by any recommender system. It should be flexible enough to cater to the ever-evolving preferences of the user and also consider the new items which are regularly added to the inventory. It also should be robust to handle attacks such as shilling attacks. A shilling attack is an attack where the system is flooded by fake user profiles and their review of items which can either promote or paint a bad review for any item. Even with such challenges, the recommender system is a very useful tool which not only helps users identify the most suitable items for them but also discovers new content and products which they would normally not try.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Previous work</title>
      <p>With the increasing ease of accessibility to the internet and a large number of online and connected devices, the majority of the applications running on such devices have become data-centric. Data is now being generated at a very tremendous rate. Applications like search engines, social media platforms, content streaming and sharing platforms have data and intelligent usage at their core. They are processing data from a few gigabytes to several terabytes or even petabytes. Google for example is processing around twenty petabytes of data daily [<xref ref-type="bibr" rid="ref_10">10</xref>]. There have been various reviews of different recommender system techniques and applications. Lu et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] provided a comprehensive survey of real-world recommender system applications and categorizes the definite necessities for recommendation methods in each application field. The author has also systematically reviewed recommender systems (online software) by considering four aspects:</p><p style="text-align: justify">(1) Recommendation methods: This includes the different algorithms recommender systems use to generate recommendations, such as collaborative filtering, content-based filtering, and knowledge-based filtering.</p><p style="text-align: justify">(2) Recommender system software refers to specific applications that implement recommender systems, such as BizSeeker.</p><p style="text-align: justify">(3) Real-world application domains: This refers to the different areas in which recommender systems are used, such as e-business, e-learning, and entertainment.</p><p style="text-align: justify">(4) Application platforms: This refers to the different devices and platforms on which recommender systems are available, such as mobile phones, TVs, and websites.</p><p style="text-align: justify">Chen et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] provided a clear and concise overview of CF-based recommender systems, covering rudimentary ideas, different CF algorithms, and assessment metrics. They also discuss traditional CF methodsâ challenges, such as cold start, data sparsity and scalability. The authors introduce the hybrid CF methods based on social networks, which have shown promising results in addressing the challenges of traditional CF methods. This work discusses a wide range of memory and model-based techniques, including enhanced similarity measures, memory-based trust-aware CF, model-based social matrix factorization-based CF, and dimensionality reduction techniques.</p><p style="text-align: justify">Collaborative filtering algorithm is one of the most deployed personalized recommendation approaches especially in commercial recommendation systems [<xref ref-type="bibr" rid="ref_13">13</xref>], [<xref ref-type="bibr" rid="ref_14">14</xref>]. Scalability is a major concern for collaborative filtering. This has also been pointed out by Mishra et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] who consolidated the research problems in Recommendation Systems, scalability is one of the most challenging problems to be solved. Bobadilla [<xref ref-type="bibr" rid="ref_16">16</xref>] also studied the cold start problem present in all recommender systems alongside similarity metrics tailored for this problem. Authors have also dwelled on providing a survey of social filtering focusing on trust, reputation and credibility.</p><p style="text-align: justify">One of the approaches used in addressing such a problem is the reduction of data size [<xref ref-type="bibr" rid="ref_17">17</xref>]. This is done by either reducing the number of users by randomly sampling customers or by not considering users who have made fewer purchases. Items can also be reduced by selecting certain specific categories of items [<xref ref-type="bibr" rid="ref_18">18</xref>]. This approach of addressing scalability issues does not work as recommendation quality worsens significantly.</p><p style="text-align: justify">The segmentation method [<xref ref-type="bibr" rid="ref_19">19</xref>] has also been used to tackle scaling issues where users are segmented into groups of similar customers. After segments are generated, the similarity between users and the vector which summarizes each segment is calculated. Cluster models are efficient as compared to the data size reduction approach.</p><p style="text-align: justify">Most recommendation algorithms have tackled the scalability issue by moving the computationally heavy part of running any model into an offline phase. The same has been performed in <a target="_blank" rel="noopener noreferrer nofollow" href="http://Amazon.com">Amazon.com</a> recommendations [<xref ref-type="bibr" rid="ref_20">20</xref>] where it generates a similar item table and finds items similar to the items purchased by the user in offline mode. Part of the recommendation, which is only on a real-time basis, is listing the most similar items for that particular user. The real-time approach does not depend on the total number of items but only on the purchases made by that user, making item-item CF a highly scalable recommendation algorithm.</p><p style="text-align: justify">However, moving the calculation steps, which consume a maximum amount of time to the offline phase and saving intermediate results for the online phase helped in scaling as per dataset. Still, the offline phase is a step which consumes a large amount of time and a tremendous number of resources. Then researchers started using a parallel data processing method such as Map-Reduce over a distributed environment to implement collaborating filtering. Varanasi [<xref ref-type="bibr" rid="ref_21">21</xref>] implemented user-based collaborative filtering over Map-Reduce in a Hadoop environment where Jaccard distance was the similarity measure being calculated. The experiments in this approach do not include the effort for pre-processing as the performance measurement metric. Only the running time and data size are considered. It uses 6 MapReduce jobs.</p><p style="text-align: justify">Bobadilla et al. [<xref ref-type="bibr" rid="ref_22">22</xref>] identified the limitations of traditional similarity metrics, such as Pearson correlation, which are not well-suited for discrete data, and proposes a new metric that addresses these limitations by combining numerical and nonnumerical information. Three of the most widely used practical datasets were used by the author to evaluate the new metric and prove its much better performance than the traditional metrics regarding accuracy, coverage, and precision/recall.</p><p style="text-align: justify">Varanasi [<xref ref-type="bibr" rid="ref_23">23</xref>] implemented an item-item CF using Map-Reduce with multiple similarity measures namely Jaccard Similarity, Tanimoto Similarity, Cosine Similarity and Pearson Coefficient. The results show that as the authors increased the number of nodes execution time decreased. However, even with a 6-node cluster, the time consumed is well above 4 hours and reaches around 16 hours for certain datasets. This work uses 7 MapReduce jobs for implementation.</p><p style="text-align: justify">When parallel implementation of the basic recommendation algorithms [<xref ref-type="bibr" rid="ref_24">24</xref>] used Pearson correlation, adjusted cosine similarity and alternating least squares models on a platform like TensorFlow. The results pointed out that the adjusted cosine similarity neighbourhood approach provided the best accuracy, whereas the alternating least square method gave the lowest accuracy. The offline computation phase of adjusted cosine similarity on the other hand took around 8 hours in execution.</p><p style="text-align: justify">In another Map-Reduce-based approach used in âScalable Recommender System over MapReduceâ [<xref ref-type="bibr" rid="ref_25">25</xref>], item-item and user-user collaborative are directly implemented without changing the approach to calculate similarity. Here, they focus on the accuracy of the RS, not on the efficiency of the RS. It used 4 maps and 3 reduce jobs. A lot of contributions and work has been done where the Hadoop MapReduce framework has been used to process the calculation of collaborative filtering in a parallel manner [<xref ref-type="bibr" rid="ref_26">26</xref>], but there seems to be a lack of focus on the serial processing required in executing a MapReduce job. Hence it is imperative that fewer full scans and sequential access should be assured while executing MapReduce jobs as it is paramount for maintaining the superior efficacy of parallel processing because such jobs require disk operations on the data nodes for getting input data and writing back the processed information.</p><p style="text-align: justify">The author has put forward a new method for aggregating recommendations from multiple algorithms in paper [<xref ref-type="bibr" rid="ref_27">27</xref>]. The method, called Collaborative Rank Aggregation (CRA), uses a metaheuristic algorithm to find weights for each algorithmâs ranking, such that the aggregated ranking is more accurate than any of the individual rankings. But this method requires a training set to tune the weights of the individual algorithms. The CRA method may not be able to improve the accuracy of recommendations if the individual algorithms are not accurate and also may not be able to enhance the accuracy of suggestions for all users.</p><p style="text-align: justify">Dahdouh et al. [<xref ref-type="bibr" rid="ref_28">28</xref>] used Spark as processing system, recommendations are made to around 1218 learners from a list of more than 150 courses. The work has been done by using 3 node cluster and a dataset of 5000 transactions where the execution time is 55 seconds. Sun et al. [<xref ref-type="bibr" rid="ref_29">29</xref>] have proposed SACF model learns a similarity matrix that embeds features which are both related and unrelated to sequence, which is more informative for personalized e-government recommendations. SACF uses matrix factorization to learn the similarity matrix, which can effectively calculate the similarity between a pair of users having no items rated by both of them. SACF reduces the complexity of computing user similarity from quadratic to linear, making it more efficient for large-scale e-government recommendation tasks. It is evaluated on a live e-governance database and shows significant improvement over the cutting-edge methods.</p><p style="text-align: justify">All the works discussed above have used direct implementations of existing algorithms. This may improve the efficiency to a certain extent, but to completely parallelize any algorithms, we might have to use specific implementations of algorithms which are more feasible for parallel and distributed processing. In the next section, we discuss our approach for using a different version of existing cosine similarity in addition to parallel and distributed methods of processing.</p>
    </sec>
    <sec sec-type="">
      <title>4. Proposed work</title>
      <p style="text-align: justify">The most practical implementation of a memory-based CF is calculating the distance metric like cosine similarity [<xref ref-type="bibr" rid="ref_19">19</xref>], Pearson correlation [<xref ref-type="bibr" rid="ref_30">30</xref>] and Jaccard coefficient. We have focused on cosine similarity. It measures the similarity of two items, A and B, by measuring the cosine of the angle between the two vectors. The original formula for the cosine similarity is as given in Eq. (1):</p>
      
        <disp-formula>
          <label>(1)</label>
          <mml:math id="mw8i18n75t">
            <mml:mi>Similarity</mml:mi>
            <mml:mi>A</mml:mi>
            <mml:mi>B</mml:mi>
            <mml:mi>Cos</mml:mi>
            <mml:mi>A</mml:mi>
            <mml:mi>B</mml:mi>
            <mml:mo>(</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:mo>(</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mo>|</mml:mo>
                <mml:mo>â</mml:mo>
                <mml:mo>|</mml:mo>
                <mml:mi>A</mml:mi>
                <mml:mi>B</mml:mi>
              </mml:mrow>
              <mml:mrow>
                <mml:mo fence="false">â</mml:mo>
                <mml:mo fence="false">â</mml:mo>
                <mml:mo>â</mml:mo>
                <mml:mo fence="false">â</mml:mo>
                <mml:mo fence="false">â</mml:mo>
                <mml:mi>A</mml:mi>
                <mml:mi>B</mml:mi>
              </mml:mrow>
            </mml:mfrac>
          </mml:math>
        </disp-formula>
      
      <p>In this paper, item-item CF is implemented by using cosine similarity in parallelly in a parallel manner. For this parallel implementation, the proposed calculation of cosine similarity is given in Eq. (2):</p>
      
        <disp-formula>
          <label>(2)</label>
          <mml:math id="mujqkix3mm">
            <mml:mi>Similarity</mml:mi>
            <mml:mi>A</mml:mi>
            <mml:mi>B</mml:mi>
            <mml:mo>(</mml:mo>
            <mml:mo>,</mml:mo>
            <mml:mo>)</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:mfrac>
              <mml:mrow>
                <mml:mo>|</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>â</mml:mo>
                <mml:mi>A</mml:mi>
                <mml:mi>B</mml:mi>
                <mml:msup>
                  <mml:mrow>
                    <mml:mo>|</mml:mo>
                  </mml:mrow>
                  <mml:mn>2</mml:mn>
                </mml:msup>
                <mml:msup>
                  <mml:mrow>
                    <mml:mo>|</mml:mo>
                  </mml:mrow>
                  <mml:mn>2</mml:mn>
                </mml:msup>
                <mml:msup>
                  <mml:mi>C</mml:mi>
                  <mml:mn>2</mml:mn>
                </mml:msup>
                <mml:mrow>
                  <mml:mo>|</mml:mo>
                </mml:mrow>
              </mml:mrow>
              <mml:mrow>
                <mml:mn>2</mml:mn>
                <mml:mo>â</mml:mo>
                <mml:mo>â</mml:mo>
                <mml:mo>|</mml:mo>
                <mml:mrow>
                  <mml:mo>|</mml:mo>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>|</mml:mo>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>|</mml:mo>
                </mml:mrow>
                <mml:mi>A</mml:mi>
                <mml:mi>B</mml:mi>
              </mml:mrow>
            </mml:mfrac>
          </mml:math>
        </disp-formula>
      
      <p>where, A and B are item vector and C is the Euclidean distance between A and B.</p><p style="text-align: justify">When applying cosine similarity in item-item CF each vector corresponds to an item and vector dimension corresponds to users who have rated the item.</p><p style="text-align: justify">The following algorithm [<xref ref-type="bibr" rid="ref_19">19</xref>] provides an approach by calculating the similarity between a single item and all related items.</p>
      <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Algorithm Iterative approach to find likeness among any item and remaining associated items</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">1: Loop every item Ix</p><p style="text-align: justify">2: Loop every User U who rated Ix</p><p style="text-align: justify">3: Loop every item Iy rated by user U</p><p style="text-align: justify">4: Save values when a user rated Ix and Iy</p><p style="text-align: justify">5: Loop every item Iy</p><p style="text-align: justify">6: Calculate the similarity between Ix and Iy</p></td></tr></tbody></table>
      <p>The computation described above is the extremely time intensive. To improve the efficiency, it requires reducing the problem into manageable proportions. Number of users and items range in millions and become unmanageable. The approach taken in our work is to perform independent calculations in a parallelized and distributed manner. To achieve parallelization, the following steps are required.</p>
      <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Step 1: Load and partition data</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">1: Read item ID, user id &amp;amp; rating from csv source</p><p style="text-align: justify">2: Partition data with item ID column</p></td></tr></tbody></table>
      <p>Once we have partitioned data, we can carry on with further transformations. Given below are transformations applied.</p>
      <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Step 2: Consolidate dimensions of each item vector</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">1: Mapper 2: - Map data into key-value pair</p><p style="text-align: justify">2: Input: - Partition data from pre-processing step.</p><p style="text-align: justify">3: Output: - (Key(item id), value (user id, rating))</p><p style="text-align: justify">4: Reducer 1: - Consolidate all rating for each item</p><p style="text-align: justify">5: Input: - (Key(item id), value (user id, rating))</p><p style="text-align: justify">6: Output: - (Key(item id), value (Magnitude of item vector,((user 1, rating), (user N, rating))))</p></td></tr></tbody></table>
      <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Step 3: Generate item pair for similarity calculation</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">1: Mapper 3: - Generate item pair as key and value as pair of magnitude of vector and user &amp;amp; rating pair</p><p style="text-align: justify">2: Input: - (Key(item id), value (Magnitude of item vector, ((user 1, rating)(user N, rating))))</p><p style="text-align: justify">3: Output: - (Key(item item), value (Magnitude of item I vector, ((user 1, rating), (user N, rating))), (Magnitude of item J vector, ((user 1, rating), (user N, rating))</p></td></tr></tbody></table>
      <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: justify">Step 4: Calculate cosine similarity for each item pair</p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: justify">1: Mapper 4: - Use the formula in the Eq. (9) to calculate the cosine similarity.</p><p style="text-align: justify">2: Input: - (Key(itemi,itemj), value (Magnitude of item vector, ((user 1, rating), (user N, rating))))</p><p style="text-align: justify">3: Output: - (Key(itemi,itemj), value (Cosine similarity of item pairs))</p><p style="text-align: justify">4: Reducer2: - Produce a single file with item pair and corresponding cosine similarity</p></td></tr></tbody></table>
      <p>Our approach has multiple facets, which makes it more efficient.</p><p style="text-align: justify">1. The magnitude of item vectors is calculated in a parallelized manner.</p><p style="text-align: justify">2. Use of Euclidean distance for calculating cosine similarity.</p><p style="text-align: justify">3. Calculation of similarity in a distributed Spark cluster.</p><p style="text-align: justify">The Eq. (1) is slower because it computes the sum of products whereas Eq. (2) calculates sum of square distances. Multiplication is an expensive operation compared to subtraction and square. The Eq. (2) does not require the computation of the product, and is therefore faster. We break down and discuss each component of both formulae in the next paragraph.</p><p style="text-align: justify">When we calculate the square root of the sum of the squares of each corresponding element of any vector, we can say that we have calculated the norm of that particular vector is defined as the square root of the sum of the squares of its elements. For example, let us consider a vector A, the norm can be calculated by Eq. (3).</p>
      
        <disp-formula>
          <label>(3)</label>
          <mml:math id="m8o0gvny0u">
            <mml:mo fence="false">â</mml:mo>
            <mml:mo fence="false">â</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:mi>A</mml:mi>
            <mml:msqrt>
              <mml:mo>â</mml:mo>
              <mml:msup>
                <mml:mi>A</mml:mi>
                <mml:mn>2</mml:mn>
              </mml:msup>
            </mml:msqrt>
          </mml:math>
        </disp-formula>
      
      <p>We need to partition vector A into smaller blocks and then compute the norm of each block in parallel when the norm has to be calculated in a distributed environment. Then, all the intermediate norm of each block is summed up to achieve the final norm of the vector. Given, two vectors A and B, the dot product can be calculated by computing the sum of the products of their corresponding elements. This is also called a scalar product of the two vectors which is calculated using Eq. (4).</p>
      
        <disp-formula>
          <label>(4)</label>
          <mml:math id="mnbltbrm56">
            <mml:mi>A</mml:mi>
            <mml:mi>B</mml:mi>
            <mml:mi>A</mml:mi>
            <mml:mi>B</mml:mi>
            <mml:mo>â</mml:mo>
            <mml:mo>=</mml:mo>
            <mml:mo>â</mml:mo>
            <mml:mo>â</mml:mo>
          </mml:math>
        </disp-formula>
      
      <p>We need to partition the vectors A and B into smaller blocks and then compute the dot product of each block in parallel in a distributed environment. Then we can achieve the final dot product of the two vectors by adding up the dot products of the blocks. The Euclidean distance between two vectors is defined as the square root of the sum of the squares of the differences of their corresponding elements. In other words, for two vectors A and B, the Euclidean distance is given by: This can also be computed in a distributed environment by partitioning vectors A and B into smaller blocks and computing the parallel Euclidean distance between each block. Once the Euclidean distance between each block is computed, the overall Euclidean distance between the vectors can be computed by summing the Euclidean distances between the blocks.</p><p style="text-align: justify">In the proposed Eq. (1), we have precalculated the magnitude of the item vector, so this calculation does not contribute to execution time when calculating similarity in the final step. As the data is partitioned based on items, all the dimensions corresponding to users rating the same item are present in a single partition. This ensures minimum shuffle between the partitions.</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>Comparison of number of mapper and reducers</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Paper</p></td><td colspan="1" rowspan="1"><p>Mapper</p></td><td colspan="1" rowspan="1"><p>Reducer</p></td></tr><tr><td colspan="1" rowspan="1"><p>Varanasi [<xref ref-type="bibr" rid="ref_21">21</xref>]</p></td><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p>6</p></td></tr><tr><td colspan="1" rowspan="1"><p>Varanasi [<xref ref-type="bibr" rid="ref_23">23</xref>]</p></td><td colspan="1" rowspan="1"><p>7</p></td><td colspan="1" rowspan="1"><p>7</p></td></tr><tr><td colspan="1" rowspan="1"><p>Wang and Yao [<xref ref-type="bibr" rid="ref_25">25</xref>]</p></td><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>3</p></td></tr><tr><td colspan="1" rowspan="1"><p>Proposed Method</p></td><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>2</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>It can be understood from the <xref ref-type="table" rid="table_2">Table 2</xref> that number of MapReduce jobs is very important. Our work uses optimum number of mappers and reducers. Hence, we get the improved results.</p>
      
        <sec>
          
            <title>4.1. Roles of distributed system in recommendation</title>
          
          <p>Apart from the calculation changes. We also made sure to use of distributed and parallel computing as the two main weapons to fight the challenges and enhance the performance of these recommendation engines.</p><p style="text-align: justify">These programming paradigms have a two-pronged approach, where the computation work is spread or distributed across multiple computers whereas parallel processing utilized the multiple cores of each machine and the workload is further distributed in multiple cores of each machine. Furthermore, these systems can be upgraded by using a large number of commodity hardware and by scaling parallelly thereby reducing the cost of expensive vertical upgrades. This collection of computing resources makes it possible to handle large datasets and enables recommender systems to generate real-time recommendations.</p><p style="text-align: justify">There are a bunch of advantages provided by the use of distributed and parallel computation:</p><p style="text-align: justify">Scalability: A distributed system can easily scale to match the growing rate of data, users and items. As such systems scale up horizontally which means adding commodity hardware instead of expensive servers, it is much cheaper and becomes more viable for the future too. The proposed work uses the user-item interaction matrix which is partitioned over the distributed environment. The system can distribute the new workload across nodes when new data and users are added. This ensures that the system can easily handle larger datasets. If required we can just add more inexpensive hardware instead of high-end servers. Thus, horizontal scaling ensures scalability in a much cheaper manner than vertical scaling.</p><p style="text-align: justify">Faster Training: With the use of parallel processing the algorithms itself can be parallelized. This expedites the calculation of values like similarities. Thus, in turn improving the speed of the training of algorithms many folds, enabling them to learn from extensive datasets quickly. This use of parallel processing confirms that similarities and recommendations are always updated with the latest user-item interactions. Our implementation uses parallel processing as each node has 4 CPU cores. This speed up the calculations of the similarities as larger calculations are broken down into smaller tasks. This can be easily ascertained using the speedup measure of the results.</p><p style="text-align: justify">Real-time Recommendations: When distributed and parallel data processing is combined, the prospect of real-time recommendations becomes a possibility. This ensures that all recommendations are always updated with the most recent trends of user preferences and item popularity. In the proposed solution Apache Spark has been used as the engine which provides in-memory processing. This further compliment the new formula by providing near real-time calculations, so that similarity values can be always recalculated if there is any change in user preferences and popularity of the items.</p><p style="text-align: justify">Complex Models: Such systems also allow the researchers to use more sophisticated methods like deep learning-based recommendation models. These models can emulate the behavior of the users and user-item relations much more efficiently. In our approach, due to the use of Apache Spark which supports a large number of data science and ML libraries. We can easily build further complex models.</p><p style="text-align: justify">These implementation techniques have shown a good improvement in the execution time of cosine similarity; the results are discussed in detail in the next section.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>5. Experiments and results</title>
      
        <sec>
          
            <title>5.1. Experiments setup</title>
          
          <p>The setup used in the experiment is the Google Dataproc Spark cluster. It has a 4-node cluster setup on the Google Cloud platform with 8GB of RAM and 4 cores for each node. The cluster has 1 master and 3 slaves. Apache Spark is the processing engine for executing the code.</p><p style="text-align: justify">Dataproc is a platform managed by Google Cloud platform. It provides Hadoop and Spark services. It is a very useful tool for batch processing, machine learning and stream processing. It is very user-friendly as it lets users create clusters and manage them using the Google Cloud platform dashboard. <xref ref-type="fig" rid="fig_2">Figure 2</xref> gives a screengrab of the VM instance list of the Google Cloud platform.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Cluster setup in Dataproc</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_cYSUTtH46bvasuPa.jpeg"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>5.2. Datasets</title>
          
          <p>Dataset used in the experiment is called MovieLens. It contains ratings given by a number of users from the MovieLens website. This data has been aggregated by the MovieLens website over a large amount of time. The dataset used in our work has 1000209 ratings provided by 6040 unique users for 3706 movies.</p>
        </sec>
      
      
        <sec>
          
            <title>5.3. Measures of performance</title>
          
          <p>5.3 Measures of performance</p><p style="text-align: justify">The most common measures for determining the performance of a parallel system are as follows:</p><p style="text-align: justify">(1) Execution Time: It is the most basic and intuitive measure which tracks the time taken between submission of a job for similarity calculation and job completion.</p><p style="text-align: justify">(2) Speedup: This is a ratio of the execution time of an application on a single core and the execution time when the same application is executed using parallel computation [<xref ref-type="bibr" rid="ref_14">14</xref>]. It signifies the improvement in the execution time when using parallel computation. It is given in Eq. (5).</p>
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="mhgqj0r4dn">
                <mml:mi>S</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>here, the time of execution with one processor is T (1), and the execution time with n processors is T(n).</p><p style="text-align: justify">(3) Efficiency: The percentage of time during which a machine is effectively utilized in parallel computing. It is also calculated by dividing the speedup by the number of processors [<xref ref-type="bibr" rid="ref_28">28</xref>].</p>
          
            <disp-formula>
              <label>(6)</label>
              <mml:math id="mlktcccoz7">
                <mml:mi>E</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>S</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                  <mml:mi>n</mml:mi>
                </mml:mfrac>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>n</mml:mi>
                    <mml:mi>T</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mo>â</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>In the formula given above speedup is denoted as S(n), whereas the time of execution with one processor is T (1), and the execution time with n processors is T(n).</p>
        </sec>
      
      
        <sec>
          
            <title>5.4. Results</title>
          
          <p>Experiments were run with different data volumes for proposed cosine similarity Eq. (2) and existing cosine similarity Eq. (1).</p><p style="text-align: justify">(1) Execution Time: For calculating this measure, we executed the proposed cosine and original cosine both on the 4-node cluster and recorded execution time for 5k, 10k, 20k, 50k, 100k, 200k and 500k, 1M and 2M number of transactions. Execution time was recorded for different volumes and different number of partitions. The results are captured in the <xref ref-type="table" rid="table_3">Table 3</xref>.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Comparison of execution time</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Data</p><p>Volume</p></td><td colspan="1" rowspan="1"><p>No. of Users</p></td><td colspan="1" rowspan="1"><p>Partitions</p></td><td colspan="1" rowspan="1"><p>Proposed Cosine</p><p>Similarity<sup>1</sup></p></td><td colspan="1" rowspan="1"><p>Cosine Similarity<sup>1</sup></p></td></tr><tr><td colspan="1" rowspan="3"><p>5001</p></td><td colspan="1" rowspan="3"><p>2645</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>25.22</p></td><td colspan="1" rowspan="1"><p>32.06</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>16.81</p></td><td colspan="1" rowspan="1"><p>21.37</p></td></tr><tr><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p>20.17</p></td><td colspan="1" rowspan="1"><p>25.64</p></td></tr><tr><td colspan="1" rowspan="3"><p>10000</p></td><td colspan="1" rowspan="3"><p>3722</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>26.31</p></td><td colspan="1" rowspan="1"><p>55.89</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>17.54</p></td><td colspan="1" rowspan="1"><p>37.26</p></td></tr><tr><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p>21.05</p></td><td colspan="1" rowspan="1"><p>44.71</p></td></tr><tr><td colspan="1" rowspan="3"><p>20000</p></td><td colspan="1" rowspan="3"><p>4680</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>31.79</p></td><td colspan="1" rowspan="1"><p>104.54</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>21.19</p></td><td colspan="1" rowspan="1"><p>69.69</p></td></tr><tr><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p>25.4</p></td><td colspan="1" rowspan="1"><p>83.5</p></td></tr><tr><td colspan="1" rowspan="3"><p>50000</p></td><td colspan="1" rowspan="3"><p>5637</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>74.01</p></td><td colspan="1" rowspan="1"><p>290.97</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>49.34</p></td><td colspan="1" rowspan="1"><p>193.98</p></td></tr><tr><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p>59.21</p></td><td colspan="1" rowspan="1"><p>232</p></td></tr><tr><td colspan="1" rowspan="3"><p>100000</p></td><td colspan="1" rowspan="3"><p>5966</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>246.80</p></td><td colspan="1" rowspan="1"><p>762.95</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>164.5</p></td><td colspan="1" rowspan="1"><p>508.63</p></td></tr><tr><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p>197.4</p></td><td colspan="1" rowspan="1"><p>610.46</p></td></tr><tr><td colspan="1" rowspan="3"><p>200000</p></td><td colspan="1" rowspan="3"><p>6037</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>1123</p></td><td colspan="1" rowspan="1"><p>2308</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>748.96</p></td><td colspan="1" rowspan="1"><p>1539.43</p></td></tr><tr><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p>898</p></td><td colspan="1" rowspan="1"><p>1847.32</p></td></tr><tr><td colspan="1" rowspan="3"><p>500000</p></td><td colspan="1" rowspan="3"><p>6040</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>5245.4</p></td><td colspan="1" rowspan="1"><p>9759</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>3503</p></td><td colspan="1" rowspan="1"><p>6506</p></td></tr><tr><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p>4203</p></td><td colspan="1" rowspan="1"><p>7807</p></td></tr><tr><td colspan="1" rowspan="3"><p>1000000</p></td><td colspan="1" rowspan="3"><p>6040</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>6834</p></td><td colspan="1" rowspan="1"><p>19665</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>4556</p></td><td colspan="1" rowspan="1"><p>13110</p></td></tr><tr><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p>5467</p></td><td colspan="1" rowspan="1"><p>15732</p></td></tr><tr><td colspan="1" rowspan="3"><p>2000000</p></td><td colspan="1" rowspan="3"><p>20000</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>15990</p></td><td colspan="1" rowspan="1"><p>48375</p></td></tr><tr><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>10660</p></td><td colspan="1" rowspan="1"><p>32250</p></td></tr><tr><td colspan="1" rowspan="1"><p>6</p></td><td colspan="1" rowspan="1"><p>12792</p></td><td colspan="1" rowspan="1"><p>38700</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>Results clearly state that the proposed implementation of cosine similarity is much more efficient than the original implementation. As the data volume increases, the execution time increases for both approaches significantly. The difference between execution time is less when small volumes of data. However, for 2M rows of data, the execution time is more than 50 % less in the proposed solution.</p><p style="text-align: justify">Another observation that can be made is that increasing the number of partitions improves execution time for both methods.  It can also be seen that 4 partitions are ideal for the dataset as using 2 partitions reduces the performance. However, increasing the number of partitions to 6 also causes the execution time to increase due to overheads. Further, this approach is also able to scale according to the increasing number of users. <xref ref-type="fig" rid="fig_3">Figure 3</xref> showcases that execution time growth for the proposed solution does increase exponentially with the increasing volume.</p><p style="text-align: justify">The authors [<xref ref-type="bibr" rid="ref_28">28</xref>] have used a 3-node cluster and worked on a maximum of 5000 transactions. So, we also set up one more 3-node cluster to compare with the results provided by the user. The results for execution time have been noted in <xref ref-type="table" rid="table_4">Table 4</xref> which also states that it is faster than the similar items calculated in the previous work.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Execution time comparison</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_xYeh4G7fprVa5bZw.png"/>
            </fig>
          
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Comparison of execution time 5k rows for 3 node cluster</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Data Volume</p></td><td colspan="1" rowspan="1"><p>Proposed Cosine Similarity<sup>1</sup></p></td><td colspan="1" rowspan="1"><p>FP-Growth Algorithm [<xref ref-type="bibr" rid="ref_28">28</xref>]<sup>1</sup></p></td></tr><tr><td colspan="1" rowspan="1"><p>5001</p></td><td colspan="1" rowspan="1"><p>22.25</p></td><td colspan="1" rowspan="1"><p>50</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>(2) Speed Up: For calculating this measure, we executed data once on a single core of a CPU with memory (RAM) of 8 GB and then using a CPU with quad cores without any changes in the configuration of RAM for both proposed cosine similarity and original cosine similarity.</p><p style="text-align: justify"><xref ref-type="table" rid="table_5">Table 5</xref> shows that parallel implementation of any algorithm considerably speeds up the algorithm. However, our approach speeds up the similarity calculation 3 times against 2 times when using the original cosine formula.</p>
          
            <table-wrap id="table_5">
              <label>Table 5</label>
              <caption>
                <title>Comparison of speed up measure</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Algorithm</p></td><td colspan="1" rowspan="1"><p>T(1)</p><p>Seconds</p></td><td colspan="1" rowspan="1"><p>T(4)</p><p>Seconds</p></td><td colspan="1" rowspan="1"><p>S(4)=T(1)/T(4)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Proposed Cosine Similarity</p></td><td colspan="1" rowspan="1"><p>55.64</p></td><td colspan="1" rowspan="1"><p>16.81</p></td><td colspan="1" rowspan="1"><p>3.31</p></td></tr><tr><td colspan="1" rowspan="1"><p>Cosine similarity</p></td><td colspan="1" rowspan="1"><p>47</p></td><td colspan="1" rowspan="1"><p>21.37</p></td><td colspan="1" rowspan="1"><p>2.20</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>(3) Efficiency is based on the speedup measure calculated above. Using the values in the <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
          
            <table-wrap id="table_6">
              <label>Table 6</label>
              <caption>
                <title>Comparison of efficiency measure</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Algorithm</p></td><td colspan="1" rowspan="1"><p>S(p), where p=4</p></td><td colspan="1" rowspan="1"><p>p=4</p></td><td colspan="1" rowspan="1"><p>E(4)=S(4)/4</p></td></tr><tr><td colspan="1" rowspan="1"><p>Proposed Cosine Similarity</p></td><td colspan="1" rowspan="1"><p>3.31</p></td><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>0.82</p></td></tr><tr><td colspan="1" rowspan="1"><p>Cosine similarity</p></td><td colspan="1" rowspan="1"><p>2.20</p></td><td colspan="1" rowspan="1"><p>4</p></td><td colspan="1" rowspan="1"><p>0.55</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>The results shown above in <xref ref-type="table" rid="table_6">Table 6</xref> prove that our approach has a better percentage of time during which a machine is effectively utilized in parallel computing. Parallel processing also ensures more efficient use of utilization of resources of each node of the cluster. This means that computational resources are more efficiently utilized in performing all the calculations and processing large volumes of data.</p>
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>6. Conclusions</title>
      <p>All the experimental results provided above showcase that the proposed method provides improved scalability and performance. These features are critical for a recommendation system to be considered useful in a real-world scenario. Given below are a few important considerations showcasing the usefulness of this approach:</p><p style="text-align: justify">(1) Improved execution time: In the above results it is clear that execution time is nearly reduced to half of that of the original formula. For example, in <xref ref-type="table" rid="table_3">Table 3</xref> for 2000000 rows of data execution time is 10000 seconds as compared to that of 32000 seconds in the original cosine formula.</p><p style="text-align: justify">(2) Better parallelization: The greater speedup factor noted in <xref ref-type="table" rid="table_5">Table 5</xref> also showcases that our implementation is very much suitable for parallel execution as increasing the number of cores reduces the execution time by a factor of more than 3 whereas the execution time in the traditional approach only improved by a factor of 2.</p><p style="text-align: justify">(3) Ability to handle more users: As the volume of the data is increased thereby increasing the number of users the results in <xref ref-type="table" rid="table_3">Table 3</xref> again show that the proposed method can manage increment in the data without a proportion increase in execution time.</p><p style="text-align: justify">(4) Better resource utilization: The proposed work showcases an improved efficiency of 0.82 compared to 0.55 of the original approach. This means that our approach utilizes the available resources more effectively than the current approach.</p><p style="text-align: justify">There are many papers which have focused on the parallel implementation of collaborative filtering. However, the focus has always been on using the direct implementations of existing algorithms. We have proved with the experiments that even adjusting the derivation of cosine similarity can tremendously improve execution time. Further, we can also safely state that the formula used in this paper has a better speedup. The use of the new formula also utilizes the resources much more efficiently. All these parameters remain consistently in favour of using the new formula.</p><p style="text-align: justify">Though the use of a suitable formula did improve the efficiency of the memory-based collaborative filtering, there were no changes done to improve the accuracy. We plan to focus next on improving the accuracy of such a system. This would provide us with more efficient and accurate novel approaches to make sense of the ever-growing data.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation/>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation/>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation/>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation/>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation/>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation/>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation/>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation/>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation/>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation/>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation/>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation/>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation/>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation/>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation/>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation/>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation/>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation/>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation/>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation/>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation/>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation/>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation/>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation/>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation/>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation/>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation/>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation/>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation/>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation/>
      </ref>
    </ref-list>
  </back>
</article>