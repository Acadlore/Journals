<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">JCHE</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Journal of Civil and Hydraulic Engineering</journal-title>
        <abbrev-journal-title abbrev-type="issn">J. Civ. Hydraul. Eng.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">JCHE</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-0587</issn>
      <issn publication-format="print">2958-0579</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-o8AZ64WS9jN8YFO-L7dDVRpnDXnP33RC</article-id>
      <article-id pub-id-type="doi">10.56578/jche020403</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>An Intelligent Recording Method for Field Geological Survey Data in Hydraulic Engineering Based on Speech Recognition</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-5605-1055</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Zuguang</given-names>
          </name>
          <email>zuguang_zhang@tju.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7005-0722</contrib-id>
          <name>
            <surname>Ren</surname>
            <given-names>Qiubing</given-names>
          </name>
          <email>qbren@tju.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1,2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-3617-5849</contrib-id>
          <name>
            <surname>Zhao</surname>
            <given-names>Wenchao</given-names>
          </name>
          <email>157983533@qq.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-3010-0892</contrib-id>
          <name>
            <surname>Li</surname>
            <given-names>Mingchao</given-names>
          </name>
          <email>lmc@tju.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-2687-9292</contrib-id>
          <name>
            <surname>Liu</surname>
            <given-names>Leping</given-names>
          </name>
          <email>liuleping@tju.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-6292-8557</contrib-id>
          <name>
            <surname>Lyu</surname>
            <given-names>Yuangeng</given-names>
          </name>
          <email>lyg@tju.edu.cn</email>
        </contrib>
        <aff id="aff_1">State Key Laboratory of Hydraulic Engineering Intelligent Construction and Operation, Tianjin University, 300350 Tianjin, China</aff>
        <aff id="aff_2">Bei Fang Investigation, Design &amp; Research Corporation Limited, 300222 Tianjin, China</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>31</day>
        <month>10</month>
        <year>2024</year>
      </pub-date>
      <volume>2</volume>
      <issue>4</issue>
      <fpage>220</fpage>
      <lpage>237</lpage>
      <page-range>220-237</page-range>
      <history>
        <date date-type="received">
          <day>29</day>
          <month>06</month>
          <year>2024</year>
        </date>
        <date date-type="accepted">
          <day>24</day>
          <month>09</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>Â©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Field data collection is a crucial component of geological surveys in hydraulic engineering. Traditional methods, such as manual handwriting and data entry, are cumbersome and inefficient, failing to meet the demands of digital and intelligent recording processes. This study develops an intelligent speech recognition and recording method tailored for hydraulic engineering geology, leveraging specialized terminology and speech recognition technology. Initially, field geological work documents are collected and processed to create audio data through manual recording and speech synthesis, forming a speech recognition training dataset. This dataset is used to train and construct a speech-to-text recognition model specific to hydraulic engineering geology, including fine-tuning a Conformer acoustic model and building an N-gram language model to achieve accurate mapping between speech and specialized vocabulary. The model's effectiveness and superiority are validated in practical engineering applications through comparative experiments focusing on decoding speed and character error rate (CER). The results demonstrate that the proposed method achieves a word error rate of only 2.6% on the hydraulic engineering geology dataset, with a single character decoding time of 15.5ms. This performance surpasses that of typical speech recognition methods and mainstream commercial software for mobile devices, significantly improving the accuracy and efficiency of field geological data collection. The method provides a novel technological approach for data collection and recording in hydraulic engineering geology.</p></abstract>
      <kwd-group>
        <kwd>Hydraulic engineering</kwd>
        <kwd>Geological survey</kwd>
        <kwd>Intelligent speech recognition</kwd>
        <kwd>Deep learning</kwd>
        <kwd>Digital recording</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="6"/>
        <fig-count count="15"/>
        <table-count count="5"/>
        <ref-count count="43"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Field data collection and recording in geological surveys are crucial tasks in hydraulic engineering construction, characterized by their necessity and preeminence [<xref ref-type="bibr" rid="ref_1">1</xref>]. The raw data from hydraulic engineering geological surveys are critical for describing the on-site geological environment and serve as a primary source for geological big data [<xref ref-type="bibr" rid="ref_2">2</xref>], featuring large data volumes and diverse types. The complex work environment of hydraulic engineering geological surveys makes data collection by handwriting or typing in the field inconvenient, hindering the accurate and efficient gathering and recording of geological survey data. Therefore, designing and developing a novel method for field geological data recording that simplifies the data collection process is essential for reducing the difficulty of data acquisition and improving the accuracy and efficiency of hydraulic engineering geological data collection.</p><p>Since the 1970s, methods for geological data collection have undergone significant reform and innovation. Traditional methods involved recording data in field notebooks and later storing the collected data in databases for management, which often resulted in issues such as non-standardized recording formats and low collection efficiency [<xref ref-type="bibr" rid="ref_3">3</xref>]. To address these problems, the Queensland Geological Survey combined handheld devices with supporting software to develop a mobile GIS-based field geological data collection technology [<xref ref-type="bibr" rid="ref_4">4</xref>]. This approach offers advantages such as low power consumption and ease of operation, integrating data input, storage, management, and output. However, it also has drawbacks, including insufficient utilization of built-in sensors and cumbersome data collection processes [<xref ref-type="bibr" rid="ref_5">5</xref>].</p><p>In recent years, as mobile devices have become increasingly intelligent and hardware performance has rapidly improved, the variety of hardware sensors has also expanded. Utilizing these latest software environments and hardware devices to make field data collection more efficient and accurate is a growing trend in modern information technology. Mobile GIS-based field geological data collection technology has thus become a more efficient collection method [<xref ref-type="bibr" rid="ref_6">6</xref>]. Furthermore, the significant advancements in artificial intelligence (AI) have led to its widespread application across various fields. The use of AI-based mobile devices for data collection is simple and convenient, enhancing the efficiency of fieldwork for geologists [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>], and holds substantial significance for revolutionizing traditional field data collection methods.</p><p>The complex fieldwork environment in hydraulic engineering geological surveys makes geological investigation and data collection particularly challenging. For intelligent geological data collection, it is necessary to quickly and conveniently record observed data into devices by applying various advanced technologies to achieve efficient and convenient data recording. However, using mobile devices for field data collection and recording often requires one hand to observe samples while the other operates the mobile device. This sometimes requires additional tools such as magnifying glasses, geological hammers, and compasses [<xref ref-type="bibr" rid="ref_9">9</xref>], making single-handed operation challenging, prone to errors, and inefficient. Therefore, a data collection method that avoids occupying both hands is needed to make the geological data collection process more convenient, reducing the complexity and operational difficulty of field geological work.</p><p>With the rapid development of deep learning methods, human-computer interaction has gradually evolved from mouse-keyboard interaction and touchscreen interaction to natural language interaction [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>]. Using voice control for field geological data collection, where data is collected by voice control of mobile devices without manual operation, allows data to be converted to text through speech recognition, which is faster than manual text entry on mobile devices, significantly improving collection efficiency. Speech recognition technology has been widely applied in areas such as terminal control [<xref ref-type="bibr" rid="ref_12">12</xref>], software interaction [<xref ref-type="bibr" rid="ref_13">13</xref>], and meeting transcription [<xref ref-type="bibr" rid="ref_14">14</xref>]. Currently, open platforms like iFlytek and Baidu Voice provide technologies such as speech recognition, lexical analysis, and speech synthesis, which can support general field geological data observation and recording [<xref ref-type="bibr" rid="ref_15">15</xref>]. However, there are limitations in recognizing specialized geological terminology.</p><p>The current implementation methods for speech recognition mainly include offline and online speech recognition, each suited to different application scenarios [<xref ref-type="bibr" rid="ref_16">16</xref>]. Offline speech recognition is characterized by high accuracy and fast recognition speed, but its primary drawback is the limited scope of recognizable speech, as it can only recognize content within its built-in language models and acoustic model libraries [<xref ref-type="bibr" rid="ref_17">17</xref>]. Online speech recognition, on the other hand, has a broader recognition range, supporting multiple languages such as Chinese and English [<xref ref-type="bibr" rid="ref_18">18</xref>]. It is suitable for general tasks such as daily communication, software interaction, and intelligent device control, but its recognition accuracy for specialized geological terminology is relatively lower than offline methods, as it lacks a specialized geological lexicon. Using a general lexicon for field geological data collection often results in high rates of misrecognition. Considering the challenges of weak signals and low bandwidth in fieldwork environments, it is necessary to develop an offline speech recognition method tailored for hydraulic engineering geological survey scenarios. This would enable the verbal description of observed geological information in the field, which is then intelligently converted into text records, improving the efficiency of field recording and addressing the inefficiencies in field geological data collection.</p><p>In response to these challenges, this paper proposes an intelligent recording method for field geological survey data in hydraulic engineering based on speech recognition interaction. This method simplifies the data collection process in geological surveys, improving the accuracy and efficiency of data collection in hydraulic engineering geological surveys. Initially, geological survey text data were collected and used to create a geological survey speech dataset through manual recording and speech synthesis. Then, a speech recognition acoustic model tailored for geological surveys was trained based on the Conformer acoustic model architecture [<xref ref-type="bibr" rid="ref_19">19</xref>], and a geological survey-specific language model was trained using the N-gram algorithm [<xref ref-type="bibr" rid="ref_20">20</xref>]. Finally, the recognition performance of the proposed method was evaluated using CER and single-character decoding time as metrics, and its effectiveness was validated through comparative testing. Experimental results demonstrate that the proposed offline recognition method outperforms classical speech recognition models and mainstream commercial software, efficiently and accurately enabling the collection and recording of hydraulic engineering geological survey data, effectively addressing the current inefficiencies and error-prone nature of field geological data collection.</p>
    </sec>
    <sec sec-type="">
      <title>2. Hydraulic engineering geological survey specialized speech dataset</title>
      
        <sec>
          
            <title>2.1. Data collection</title>
          
          <p>Hydraulic engineering geological terminology is characterized by its strong domain specificity, high level of specialization, low frequency of common usage, and a high occurrence of uncommon characters, making it difficult to be accurately recognized by mainstream speech recognition models and programs currently available on the market. Therefore, geological-related corpora are used to construct a speech recognition dataset for hydraulic engineering geological surveys, which is then used to train the speech recognition model. The geological specialized speech data includes audio and its corresponding text, with the text data sourced from various geological professional materials, mainly including hydraulic engineering geological monographs, textbooks on hydraulic engineering geology, fieldwork specifications for geological surveys in water conservancy and hydropower engineering, geological borehole exploration entry materials, and engineering geological survey reports.</p><p>To convert the text data into dataset labels for input into the acoustic model, the following four preprocessing tasks are required: (1) Content Extraction: Convert the geological specialized materials into editable text and manually remove information sections with low relevance to geological surveys. (2) Text Cleaning: This includes removing meaningless data such as spaces and punctuation marks, replacing Arabic numerals, letters, and other non-Chinese characters with Chinese characters, and removing meaningless data such as spaces and punctuation marks [<xref ref-type="bibr" rid="ref_21">21</xref>]. (3) Sentence Segmentation: Break down long sentences into multiple short sentences of 2 to 25 characters based on key punctuation marks such as commas, periods, and semicolons, forming 16,352 sentences of text data. (4) Word Segmentation: For each short sentence, segment the sentence into word information based on sentence components and vocabulary attributes, using spaces as delimiters, while retaining specialized geological terms. The overall preprocessing workflow is shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>The preprocessing of text information</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/10/img_c5Ht_0Ga4_PaTo9v.png"/>
            </fig>
          
          <p>To obtain audio files matching the text data, audio data is generated through two methods: human voice recording and speech synthesis. Human voice recording is carried out by 15 recording personnel who read the text data content in Mandarin in a quiet environment, recording each line using a mobile phone microphone. Speech synthesis is done by randomly using multiple Text to Speech (TTS) models [<xref ref-type="bibr" rid="ref_22">22</xref>] to generate audio sentence by sentence, simulating multiple different speakers to enhance the diversity of the audio data, thereby ensuring the generalization ability of the speech recognition model. The TTS models used for speech synthesis are shown in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>TTS model for speech synthesis</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p>Speaker Type</p></th><th colspan="1" rowspan="1"><p>Language</p></th></tr><tr><td colspan="1" rowspan="1"><p>Speedyspeech CSMSC</p></td><td colspan="1" rowspan="1"><p>Single</p></td><td colspan="1" rowspan="1"><p>Chinese</p></td></tr><tr><td colspan="1" rowspan="1"><p>Fastspeech2_CSMSC</p></td><td colspan="1" rowspan="1"><p>Single</p></td><td colspan="1" rowspan="1"><p>Chinese</p></td></tr><tr><td colspan="1" rowspan="1"><p>Fastspeech2 LJSPEECH</p></td><td colspan="1" rowspan="1"><p>Single</p></td><td colspan="1" rowspan="1"><p>English</p></td></tr><tr><td colspan="1" rowspan="1"><p>Fastspeech2_AISHELL3</p></td><td colspan="1" rowspan="1"><p>Multiple</p></td><td colspan="1" rowspan="1"><p>Chinese</p></td></tr><tr><td colspan="1" rowspan="1"><p>Fastspeech2 VCTK</p></td><td colspan="1" rowspan="1"><p>Multiple</p></td><td colspan="1" rowspan="1"><p>English</p></td></tr><tr><td colspan="1" rowspan="1"><p>Fastspeech2_MIX</p></td><td colspan="1" rowspan="1"><p>Multiple</p></td><td colspan="1" rowspan="1"><p>Chinese/English</p></td></tr><tr><td colspan="1" rowspan="1"><p>Tacotron2_CSMSC</p></td><td colspan="1" rowspan="1"><p>Single</p></td><td colspan="1" rowspan="1"><p>Chinese</p></td></tr><tr><td colspan="1" rowspan="1"><p>Tacotron2 LJSPEECH</p></td><td colspan="1" rowspan="1"><p>Single</p></td><td colspan="1" rowspan="1"><p>English</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>2.2. Speech dataset construction</title>
          
          <p>Referring to the format of the Aishell dataset [<xref ref-type="bibr" rid="ref_23">23</xref>], the text sequences and audio sequences are mapped to one-to-one relationships, and placed line by line in the record file, constructing the speech recognition dataset for hydraulic engineering geological surveys. This dataset contains 16,532 audio files with a total duration of 13.53 hours. In terms of professional domain recognition, this dataset can ensure recognition accuracy while effectively maintaining the model's generalization performance due to its large volume, thereby ensuring its anti-interference stability during actual use. Considering the large scale of the dataset, it is randomly divided into training, validation, and test sets according to a ratio of 18:1:1 [<xref ref-type="bibr" rid="ref_24">24</xref>] to simulate field geological survey conditions as closely as possible, ensuring the reference value of the model evaluation. The process of constructing the dataset is shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Building of the dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/10/img_EbrPaYlh02WDMxIi.png"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Hydraulic engineering geological survey specialized speech recognition model</title>
      
        <sec>
          
            <title>3.1. Acoustic signal conversion</title>
          
          <p>The conversion of acoustic signals is the fundamental task of speech recognition. According to the waveform of the speech signal, acoustic signals are converted into effective acoustic features to describe and capture different speech signals. The effectiveness of feature extraction directly impacts the accuracy of subsequent speech recognition.</p><p>To fully extract the features of the human voice in the speech signal while reducing the impact of background noise on linguistic information, this study uses Mel-scale Frequency Cepstral Coefficients (MFCCs) as the features of the input acoustic signal, which are widely used in speech recognition [<xref ref-type="bibr" rid="ref_25">25</xref>]. MFCCs are cepstral parameters extracted in the Mel-scale frequency domain. The Mel scale describes the nonlinear characteristics of human ear frequency perception, and its relationship with frequency is as follows:</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="m7wvkjs8xh">
                <mml:mi>Mel</mml:mi>
                <mml:mi>f</mml:mi>
                <mml:mi>lg</mml:mi>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>Ã</mml:mo>
                <mml:mo>â¡</mml:mo>
                <mml:mn>2595</mml:mn>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>+</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mn>1</mml:mn>
                  <mml:mfrac>
                    <mml:mi>f</mml:mi>
                    <mml:mn>700</mml:mn>
                  </mml:mfrac>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>Based on the audio files, MFCC features are obtained through the following eight steps:</p><p>(1) Pre-emphasis. The high-frequency part of the speech is easily lost after being emitted by the human vocal organs. Pre-emphasis is used to compensate for the amplitude of the high-frequency part of the speech signal. The calculation formula is as follows:</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="mwcmjromri">
                <mml:msub>
                  <mml:mi>x</mml:mi>
                  <mml:mrow>
                    <mml:mi>e</mml:mi>
                    <mml:mi>m</mml:mi>
                    <mml:mi>p</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>â</mml:mo>
                <mml:mo>â</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>â</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mi>n</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mi>Î±</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mn>1</mml:mn>
              </mml:math>
            </disp-formula>
          
          <p> where, <inline-formula>
  <mml:math id="mz8shnnszu">
    <mml:mi>x</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is the $n<inline-formula>
  <mml:math id="majr1k7v4q">
    <mml:mo>â</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\alpha$ is the pre-emphasis coefficient.</p><p>(2) Framing. Speech signals exhibit short-term stationarity, so a segment of speech over a short period is taken as a frame. To ensure continuity of the speech signal after framing, a portion of the previous frame is retained in the next frame, called the frame shift. Typically, the frame shift is set to 10ms, and the frame length is set to 25ms.</p><p>(3) Windowing. Windowing is associated with framing and smooths the frame through a window function. A Hamming window is used to retain the frequency characteristics of the speech signal effectively, and its function is as follows:</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="mu0yxrhi6w">
                <mml:mi>w</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mi>cos</mml:mi>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>â</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>â</mml:mo>
                <mml:mo>â</mml:mo>
                <mml:mo>â¡</mml:mo>
                <mml:mn>1</mml:mn>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>â</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mn>2</mml:mn>
                      <mml:mi>Ï</mml:mi>
                      <mml:mi>n</mml:mi>
                    </mml:mrow>
                    <mml:mi>N</mml:mi>
                  </mml:mfrac>
                  <mml:mn>1</mml:mn>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p> where, $a<inline-formula>
  <mml:math id="mpqel6n2bm">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mn>0.46</mml:mn>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>N$ is the window length.</p><p>(4) Fourier Transform. The Fourier Transform converts the signal from the time domain to the frequency domain for spectrum analysis. The Short-Time Fourier Transform (STFT) is suitable for short-term stationary signals, and its expression is:</p>
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="mycejekmiu">
                <mml:msub>
                  <mml:mi>T</mml:mi>
                  <mml:mi>t</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>b</mml:mi>
                  <mml:mi>k</mml:mi>
                </mml:msub>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>â</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>â</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mi>k</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mi>n</mml:mi>
                <mml:mi>w</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:munderover>
                  <mml:mo>â</mml:mo>
                  <mml:mrow>
                    <mml:mi>n</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>0</mml:mn>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>N</mml:mi>
                    <mml:mo>â</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:munderover>
              </mml:math>
            </disp-formula>
          
          <p> where, $x<inline-formula>
  <mml:math id="mdb7fw4d44">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>t<inline-formula>
  <mml:math id="m65mj9ae06">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>w<inline-formula>
  <mml:math id="mevtz2b6fg">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>b<inline-formula>
  <mml:math id="myfepf645d">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>k<inline-formula>
  <mml:math id="mwbe828efk">
    <mml:mo>â</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>â</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>B</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mn>5</mml:mn>
    <mml:mn>3.</mml:mn>
  </mml:math>
</inline-formula>m<inline-formula>
  <mml:math id="mx5noedjke">
    <mml:mo>â</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>k$-th frequency point can be calculated by the following formula:</p>
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="mhwjsufxzq">
                <mml:msub>
                  <mml:mi>H</mml:mi>
                  <mml:mi>m</mml:mi>
                </mml:msub>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mi>k</mml:mi>
                <mml:mrow>
                  <mml:mo>{</mml:mo>
                  <mml:mo fence="true"/>
                  <mml:mtable columnalign="left" columnspacing="1em" rowspacing="4pt">
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mn>0</mml:mn>
                        <mml:mn>1</mml:mn>
                        <mml:mo>,</mml:mo>
                        <mml:mo>&lt;</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>â</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>k</mml:mi>
                        <mml:mi>f</mml:mi>
                        <mml:mi>m</mml:mi>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mfrac>
                          <mml:mrow>
                            <mml:mi>k</mml:mi>
                            <mml:mi>f</mml:mi>
                            <mml:mi>m</mml:mi>
                            <mml:mo>â</mml:mo>
                            <mml:mo>(</mml:mo>
                            <mml:mo>â</mml:mo>
                            <mml:mo>)</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>f</mml:mi>
                            <mml:mi>m</mml:mi>
                            <mml:mi>f</mml:mi>
                            <mml:mi>m</mml:mi>
                            <mml:mo>(</mml:mo>
                            <mml:mo>)</mml:mo>
                            <mml:mo>â</mml:mo>
                            <mml:mo>(</mml:mo>
                            <mml:mo>â</mml:mo>
                            <mml:mo>)</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:mfrac>
                        <mml:mo>,</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>â</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mo>â¤</mml:mo>
                        <mml:mo>&lt;</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>f</mml:mi>
                        <mml:mi>m</mml:mi>
                        <mml:mi>k</mml:mi>
                        <mml:mi>f</mml:mi>
                        <mml:mi>m</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mfrac>
                          <mml:mrow>
                            <mml:mi>f</mml:mi>
                            <mml:mi>m</mml:mi>
                            <mml:mi>k</mml:mi>
                            <mml:mo>(</mml:mo>
                            <mml:mo>+</mml:mo>
                            <mml:mo>)</mml:mo>
                            <mml:mo>â</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>f</mml:mi>
                            <mml:mi>m</mml:mi>
                            <mml:mi>f</mml:mi>
                            <mml:mi>m</mml:mi>
                            <mml:mo>(</mml:mo>
                            <mml:mo>+</mml:mo>
                            <mml:mo>)</mml:mo>
                            <mml:mo>â</mml:mo>
                            <mml:mo>(</mml:mo>
                            <mml:mo>)</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:mfrac>
                        <mml:mo>,</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mo>â¤</mml:mo>
                        <mml:mo>&lt;</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>+</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>f</mml:mi>
                        <mml:mi>m</mml:mi>
                        <mml:mi>k</mml:mi>
                        <mml:mi>f</mml:mi>
                        <mml:mi>m</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mn>0</mml:mn>
                        <mml:mn>1</mml:mn>
                        <mml:mo>,</mml:mo>
                        <mml:mo>â¥</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>+</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>k</mml:mi>
                        <mml:mi>f</mml:mi>
                        <mml:mi>m</mml:mi>
                      </mml:mtd>
                    </mml:mtr>
                  </mml:mtable>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p> where, <inline-formula>
  <mml:math id="m2jvi79im8">
    <mml:mi>f</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is the center frequency of the filter.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Equal area Mel filter bank</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/10/img_jBzLoorURmx6Q-j0.png"/>
            </fig>
          
          <p>(6) Logarithmic Power Operation. The amplitude spectrum obtained from the Fourier Transform is squared to obtain the short-term power spectrum. By multiplying and accumulating through the filter, the logarithm is taken to obtain the logarithmic power spectrum, which relatively amplifies the low-frequency signal. The resulting feature is the F-bank feature of the audio, and its expression is as follows:</p>
          
            <disp-formula>
              <label>(6)</label>
              <mml:math id="mbe1nmgo1y">
                <mml:mi>s</mml:mi>
                <mml:mi>m</mml:mi>
                <mml:mi>ln</mml:mi>
                <mml:mi>m</mml:mi>
                <mml:mi>M</mml:mi>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>â¡</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>&lt;</mml:mo>
                <mml:mo>&lt;</mml:mo>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mo>â</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:munderover>
                    <mml:mo>â</mml:mo>
                    <mml:mrow>
                      <mml:mi>k</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mn>0</mml:mn>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>N</mml:mi>
                      <mml:mo>â</mml:mo>
                      <mml:mn>1</mml:mn>
                    </mml:mrow>
                  </mml:munderover>
                  <mml:msub>
                    <mml:mi>H</mml:mi>
                    <mml:mi>m</mml:mi>
                  </mml:msub>
                  <mml:mi>k</mml:mi>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mo>|</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>|</mml:mo>
                      <mml:msub>
                        <mml:mi>T</mml:mi>
                        <mml:mi>t</mml:mi>
                      </mml:msub>
                      <mml:mi>k</mml:mi>
                    </mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:msup>
                </mml:mrow>
                <mml:mn>0</mml:mn>
              </mml:math>
            </disp-formula>
          
          <p> where, $M$ is the total number of filters.</p><p>(7) Discrete Cosine Transform (DCT). The logarithmic energy obtained above is input into the DCT to concentrate the signal's energy, and the MFCCs are obtained according to the following formula:</p>
          
            <disp-formula>
              <label>(7)</label>
              <mml:math id="mpqhya4t91">
                <mml:mi>C</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>s</mml:mi>
                <mml:mi>m</mml:mi>
                <mml:mi>cos</mml:mi>
                <mml:mi>i</mml:mi>
                <mml:mi>L</mml:mi>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>â</mml:mo>
                <mml:mo>â¡</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>â¯</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:munderover>
                  <mml:mo>â</mml:mo>
                  <mml:mrow>
                    <mml:mi>m</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>0</mml:mn>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>N</mml:mi>
                    <mml:mo>â</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:munderover>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mi>Ï</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>m</mml:mi>
                      <mml:mo>(</mml:mo>
                      <mml:mo>â</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mn>0.5</mml:mn>
                    </mml:mrow>
                    <mml:mi>M</mml:mi>
                  </mml:mfrac>
                </mml:mrow>
                <mml:mn>1</mml:mn>
                <mml:mn>2</mml:mn>
              </mml:math>
            </disp-formula>
          
          <p> where, $L<inline-formula>
  <mml:math id="mpu8rmjebj">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mn>12</mml:mn>
  </mml:math>
</inline-formula>\sim$16.</p><p>(8) Dynamic Differential. The DCT only obtains static MFCC features. Dynamic MFCC features can be obtained by taking the differential of the static features. The calculation method for the first-order differential coefficient is as shown in Eq. (8), and the second-order differential coefficient is obtained by repeatedly substituting the first-order differential coefficient into the equation. The combination of static, first-order, and second-order coefficients results in the complete MFCC features.</p>
          
            <disp-formula>
              <label>(8)</label>
              <mml:math id="mlwa2deaor">
                <mml:msub>
                  <mml:mi>d</mml:mi>
                  <mml:mi>t</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:munderover>
                      <mml:mo>â</mml:mo>
                      <mml:mrow>
                        <mml:mi>n</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mi>N</mml:mi>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:munderover>
                    <mml:mi>n</mml:mi>
                    <mml:mi>C</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mi>C</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mi>n</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>â</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>â</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mn>2</mml:mn>
                    <mml:munderover>
                      <mml:mo>â</mml:mo>
                      <mml:mrow>
                        <mml:mi>n</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>N</mml:mi>
                    </mml:munderover>
                    <mml:msup>
                      <mml:mi>n</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msup>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p> where, $t<inline-formula>
  <mml:math id="mi102zzm7w">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>N$ represents the frame sequence difference of the first-order derivative, which can be 1 or 2.</p><p>The overall extraction process of MFCCs features is shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>The calculation process for extracting MFCCs</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/10/img_9RTVzbCLHuqQWWju.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Conformer acoustic model</title>
          
          <p>The acoustic model is one of the most important modules in a speech recognition system, directly affecting the system's performance [<xref ref-type="bibr" rid="ref_26">26</xref>]. The acoustic model takes the vector sequence obtained from feature extraction of the speech signal as input and establishes a mapping relationship between speech features and phonemes to obtain the probability of the speech waveform corresponding to the model's output speech signal.</p><p>The Conformer model combines the strengths of the Transformer [<xref ref-type="bibr" rid="ref_27">27</xref>] and Convolutional Neural Network (CNN), leveraging CNN's advantage in capturing local features while retaining Transformer's capability of acquiring long-range dependencies. This enhances the network's ability to model both global and local dependencies simultaneously, demonstrating outstanding performance in recognition accuracy, inference speed, and model parameter size.</p><p>The core component of the Conformer model is the Conformer block, which mainly consists of four modules: the feedforward network, multi-head attention mechanism module, convolution module, and the second feedforward network. Its structure is similar to a macaron structure, where the two feedforward networks each contribute half of the Conformer's output. The speech features xi are processed by the Conformer model as follows:</p>
          
            <disp-formula>
              <label>(9)</label>
              <mml:math id="mw66ky4sv3">
                <mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt">
                  <mml:mtr>
                    <mml:mtd/>
                    <mml:mtd>
                      <mml:msubsup>
                        <mml:mi>X</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mrow>
                          <mml:mi>â²</mml:mi>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:mo>=</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:msub>
                        <mml:mi>x</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                      <mml:mn>0.5</mml:mn>
                      <mml:mi>F</mml:mi>
                      <mml:mi>F</mml:mi>
                      <mml:mi>N</mml:mi>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mi>x</mml:mi>
                          <mml:mi>i</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd/>
                    <mml:mtd>
                      <mml:msubsup>
                        <mml:mi>X</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mrow>
                          <mml:mi>â²</mml:mi>
                          <mml:mi>â²</mml:mi>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:msubsup>
                        <mml:mi>x</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mrow>
                          <mml:mi>â²</mml:mi>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:mo>=</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mrow>
                        <mml:mi>M</mml:mi>
                        <mml:mi>H</mml:mi>
                        <mml:mi>S</mml:mi>
                        <mml:mi>A</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msubsup>
                          <mml:mi>x</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mrow>
                            <mml:mi>â²</mml:mi>
                          </mml:mrow>
                        </mml:msubsup>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd/>
                    <mml:mtd>
                      <mml:msubsup>
                        <mml:mi>X</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mrow>
                          <mml:mi>â²</mml:mi>
                          <mml:mi>â²</mml:mi>
                          <mml:mi>â²</mml:mi>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:msubsup>
                        <mml:mi>x</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mrow>
                          <mml:mi>â²</mml:mi>
                          <mml:mi>â²</mml:mi>
                        </mml:mrow>
                      </mml:msubsup>
                      <mml:mo>=</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mrow>
                        <mml:mi>M</mml:mi>
                        <mml:mi>H</mml:mi>
                        <mml:mi>S</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msubsup>
                          <mml:mi>x</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mrow>
                            <mml:mi>â²</mml:mi>
                            <mml:mi>â²</mml:mi>
                          </mml:mrow>
                        </mml:msubsup>
                      </mml:mrow>
                      <mml:mi>A</mml:mi>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd/>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>Y</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mi>L</mml:mi>
                      <mml:mi>N</mml:mi>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>+</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msubsup>
                          <mml:mi>x</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mrow>
                            <mml:mi>â²</mml:mi>
                            <mml:mi>â²</mml:mi>
                            <mml:mi>â²</mml:mi>
                          </mml:mrow>
                        </mml:msubsup>
                        <mml:mn>0.5</mml:mn>
                        <mml:mi>F</mml:mi>
                        <mml:mi>F</mml:mi>
                        <mml:mi>N</mml:mi>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:msubsup>
                            <mml:mi>x</mml:mi>
                            <mml:mi>i</mml:mi>
                            <mml:mrow>
                              <mml:mi>â²</mml:mi>
                              <mml:mi>â²</mml:mi>
                              <mml:mi>â²</mml:mi>
                            </mml:mrow>
                          </mml:msubsup>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p> where, <inline-formula>
  <mml:math id="m3qowdqa4o">
    <mml:mi>F</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>N</mml:mi>
  </mml:math>
</inline-formula> represents the feedforward network, <inline-formula>
  <mml:math id="mpj5a79zu2">
    <mml:mi>M</mml:mi>
    <mml:mi>H</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>A</mml:mi>
  </mml:math>
</inline-formula> stands for the multi-head self-attention module, and <inline-formula>
  <mml:math id="morxzlrefj">
    <mml:mi>L</mml:mi>
    <mml:mi>N</mml:mi>
  </mml:math>
</inline-formula> represents layer normalization. Residual connections are used between each module.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Structure of conformer model</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/10/img_dfJCi4VDMEzi_PDw.png"/>
            </fig>
          
          <p>In the encoding stage, the audio is first processed by a convolutional subsampling layer, and then multiple Conformer blocks are used for further processing, as shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>. In the figure, (a) represents the Conformer model structure, (b) shows the upsampling and downsampling operations for aligning the input feature space, and (c) illustrates the fully connected layer structure of the feedforward network.</p>
          
            <sec>
              
                <title>3.2.1 Mhsa mechanism</title>
              
              <p>The multi-head attention mechanism [<xref ref-type="bibr" rid="ref_28">28</xref>] module replaces the positional encoding in the Transformer with the relative sinusoidal positional encoding from Transformer-XL, allowing the attention module to generalize better across different input lengths and making the generated encoder more stable to changes in audio sequence length.</p><p>The self-attention mechanism is a variant of the attention mechanism, reducing the dependency on external information and being more adept at capturing the internal correlations within data or features. In the self-attention mechanism, the $Q<inline-formula>
  <mml:math id="mvb9sc402d">
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula>K<inline-formula>
  <mml:math id="mzrdzix0ey">
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>k</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>V<inline-formula>
  <mml:math id="mptcmz6edp">
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>X$ obtained from character encoding. This means that after the model reads the input information, it determines the most important information based on the input itself. The calculation process is shown in <xref ref-type="fig" rid="fig_6">Figure 6</xref>.</p>
              
                <fig id="fig_6">
                  <label>Figure 6</label>
                  <caption>
                    <title>Self-attention mechanism</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/10/img_rFLdqOp0B3PnqN_T.png"/>
                </fig>
              
              <p>In this study, the correlation between $Q<inline-formula>
  <mml:math id="mrqc7t761u">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>K<inline-formula>
  <mml:math id="mver7sl5e8">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>â</mml:mo>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>Q<inline-formula>
  <mml:math id="m8oouwdv94">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>K<inline-formula>
  <mml:math id="mfpwziy59l">
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>V$ values are weighted and summed according to the attention weight coefficients to obtain the Self-Attention Value, with the calculation formula as follows:</p>
              
                <disp-formula>
                  <label>(10)</label>
                  <mml:math id="mgqn3xecdp">
                    <mml:mrow>
                      <mml:mi>A</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>n</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>n</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>s</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>f</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>m</mml:mi>
                      <mml:mi>a</mml:mi>
                      <mml:mi>x</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>Q</mml:mi>
                          <mml:msup>
                            <mml:mi>K</mml:mi>
                            <mml:mi>T</mml:mi>
                          </mml:msup>
                        </mml:mrow>
                        <mml:msqrt>
                          <mml:msub>
                            <mml:mi>dim</mml:mi>
                            <mml:mi>k</mml:mi>
                          </mml:msub>
                        </mml:msqrt>
                      </mml:mfrac>
                    </mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>â</mml:mo>
                    <mml:mi>X</mml:mi>
                    <mml:mi>V</mml:mi>
                  </mml:math>
                </disp-formula>
              
              <p>In the formula, <inline-formula>
  <mml:math id="mu6t8zhsiy">
    <mml:mi>Q</mml:mi>
    <mml:mi>K</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>, and $V<inline-formula>
  <mml:math id="makf4urgzk">
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>X<inline-formula>
  <mml:math id="mxk3bzr79e">
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>W_Q, W_K<inline-formula>
  <mml:math id="m4q844jydk">
    <mml:mo>,</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>W_V<inline-formula>
  <mml:math id="moaec7kxad">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>\operatorname{dim}_k<inline-formula>
  <mml:math id="m89wczoee5">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
  </mml:math>
</inline-formula>K<inline-formula>
  <mml:math id="mp41vi9oyk">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>V$.</p>
              
                <fig id="fig_7">
                  <label>Figure 7</label>
                  <caption>
                    <title>MHSA mechanism</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/10/img_NDG8W_XKWL7_jEhG.png"/>
                </fig>
              
              <p>When encoding the information at the current position, the self-attention mechanism tends to focus excessively on its own position, making it less effective than CNN in capturing useful information. To address this issue, the multi-head attention mechanism is used. It applies $h<inline-formula>
  <mml:math id="mo17yqu7v7">
    <mml:mo>(</mml:mo>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>h<inline-formula>
  <mml:math id="m1z4plam7n">
    <mml:mo>=</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mn>8</mml:mn>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>X<inline-formula>
  <mml:math id="m1ups1x9rv">
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
  </mml:math>
</inline-formula>Q<inline-formula>
  <mml:math id="masolm10zz">
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>K<inline-formula>
  <mml:math id="mheps4txnm">
    <mml:mo>,</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>V<inline-formula>
  <mml:math id="mw57zic402">
    <mml:mo>.</mml:mo>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>h<inline-formula>
  <mml:math id="mkojb4axd8">
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>Q<inline-formula>
  <mml:math id="mb3vqk9wyz">
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>K<inline-formula>
  <mml:math id="mkjn0ewoht">
    <mml:mo>,</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>V<inline-formula>
  <mml:math id="mtnoroidlz">
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>V</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>â</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>â</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>h<inline-formula>
  <mml:math id="mqnsud775h">
    <mml:mi>S</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>V</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>â</mml:mo>
  </mml:math>
</inline-formula>W_0$ to produce the final Multi-Head Self-Attention output. The overall calculation process is shown in <xref ref-type="fig" rid="fig_7">Figure 7</xref>.</p>
            </sec>
          
          
            <sec>
              
                <title>3.2.2 Mhsa mechanism</title>
              
              <p>The convolution module consists of five parts: a pointwise convolution layer, a Gated Linear Unit (GLU) activation function, a depth-wise convolution layer, a Swish activation function, and a second pointwise convolution layer. The structure is shown in <xref ref-type="fig" rid="fig_8">Figure 8</xref>.</p>
              
                <fig id="fig_8">
                  <label>Figure 8</label>
                  <caption>
                    <title>Convolution module of Conformer</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/10/img_n2dh4dT6hT5QewbB.png"/>
                </fig>
              
              <p>Depth-wise convolution and pointwise convolution are used together as an efficient combination to replace traditional CNNs, effectively reducing the number of network parameters and improving computational efficiency. In depth-wise convolution, one convolution kernel is responsible for one channel, meaning that each channel is convolved by only one convolution kernel, focusing solely on the dependencies within the sequence in each channel, without considering dependencies between different channels. On the other hand, pointwise convolution is very similar to regular convolution operations, with a convolution kernel size of 1Ã1Ã$M<inline-formula>
  <mml:math id="moi0b7homw">
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>M$ equals to the number of channels in the previous layer. Its convolution operation weights and combines the feature maps from the previous step along the depth direction to generate a new set of feature maps with the same number as the convolution kernels, thus focusing on dependencies between different channels while ignoring intra-channel dependencies.</p><p>The GLU activation function is an activation function used in neural networks that incorporates a gating mechanism, helping the network better capture long-term dependencies in sequence data. The GLU activation function is defined as follows:</p>
              
                <disp-formula>
                  <label>(11)</label>
                  <mml:math id="m39ry5sivr">
                    <mml:mrow>
                      <mml:mi>G</mml:mi>
                      <mml:mi>L</mml:mi>
                      <mml:mi>U</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>x</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>S</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>g</mml:mi>
                      <mml:mi>m</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>d</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mi>g</mml:mi>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mi>x</mml:mi>
                          <mml:mi>i</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:mo>â</mml:mo>
                    <mml:msub>
                      <mml:mi>x</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                  </mml:math>
                </disp-formula>
              
              <p> where, $X<inline-formula>
  <mml:math id="m5k654ajjb">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\otimes<inline-formula>
  <mml:math id="m69jxpxqdg">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>â</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>g\left(x_i\right)$ is the intermediate vector obtained through the convolution layer, and the Sigmoid function is defined as follows:</p>
              
                <disp-formula>
                  <label>(12)</label>
                  <mml:math id="mt5oqwt3j9">
                    <mml:mrow>
                      <mml:mi>S</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>g</mml:mi>
                      <mml:mi>m</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>d</mml:mi>
                    </mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>+</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mn>1</mml:mn>
                        <mml:msup>
                          <mml:mi>e</mml:mi>
                          <mml:mrow>
                            <mml:mo>â</mml:mo>
                            <mml:mi>x</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>â</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msup>
                  </mml:math>
                </disp-formula>
              
              <p>The Swish activation function has characteristics such as a lower bound, smoothness, and non-monotonicity, effectively addressing the problems of gradient vanishing and neuron death encountered in the ReLU activation function. Its calculation formula is:</p>
              
                <disp-formula>
                  <label>(13)</label>
                  <mml:math id="m47ch0xpyb">
                    <mml:mrow>
                      <mml:mi>S</mml:mi>
                      <mml:mi>w</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>s</mml:mi>
                      <mml:mi>h</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>x</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>S</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>g</mml:mi>
                      <mml:mi>m</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>d</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mi>Î²</mml:mi>
                      <mml:msub>
                        <mml:mi>x</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:mo>â</mml:mo>
                    <mml:msub>
                      <mml:mi>x</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                  </mml:math>
                </disp-formula>
              
              <p> where, <inline-formula>
  <mml:math id="my23cu9lh6">
    <mml:mi>Î²</mml:mi>
  </mml:math>
</inline-formula> is a trainable parameter.</p>
            </sec>
          
          
            <sec>
              
                <title>3.2.3 Feedforward module</title>
              
              <p>A FFN is a fully connected feedforward neural network consisting of two fully connected layers and a nonlinear activation function. The feedforward module in the Conformer model uses the Swish activation function. The first fully connected layer is used for dimensionality expansion, and the second fully connected layer is used for dimensionality reduction. This design aims to perform nonlinear transformations and mappings of the embedding vectors, allowing the model to learn more abstract features. Additionally, layer normalization, the Dropout mechanism, and residual summation are introduced to accelerate network training, improve the model's generalization ability, and alleviate the gradient vanishing problem [<xref ref-type="bibr" rid="ref_29">29</xref>]. The feedforward module's calculation process is shown in <xref ref-type="fig" rid="fig_9">Figure 9</xref>.</p>
              
                <fig id="fig_9">
                  <label>Figure 9</label>
                  <caption>
                    <title>Calculation process of feedforward module</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/10/img_pLEbpT0w3RJ1-EDE.png"/>
                </fig>
              
            </sec>
          
          
            <sec>
              
                <title>3.2.4 Loss function</title>
              
              <p>During training, the Connectionist Temporal Classification (CTC) loss and attention loss are used to supervise the training of the CNN and Transformer branches, respectively [<xref ref-type="bibr" rid="ref_30">30</xref>], to obtain features that exhibit both CNN and Transformer characteristics. During inference, the outputs of these two classifiers are summed to serve as the prediction results. The cross-entropy function is expressed as follows:</p>
              
                <disp-formula>
                  <label>(14)</label>
                  <mml:math id="mmkhmp97p1">
                    <mml:msub>
                      <mml:mi>L</mml:mi>
                      <mml:mrow>
                        <mml:mi>C</mml:mi>
                        <mml:mi>E</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>T</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mo>â</mml:mo>
                    <mml:mo>â¡</mml:mo>
                    <mml:munderover>
                      <mml:mo>â</mml:mo>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>n</mml:mi>
                    </mml:munderover>
                    <mml:mi>log</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>P</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p> where, <inline-formula>
  <mml:math id="m0yy9yftzm">
    <mml:mi>T</mml:mi>
    <mml:mi>i</mml:mi>
  </mml:math>
</inline-formula> represents the actual probability distribution, and <inline-formula>
  <mml:math id="mspwqdlmlc">
    <mml:mi>P</mml:mi>
    <mml:mi>i</mml:mi>
  </mml:math>
</inline-formula> represents the predicted probability distribution, both of which are normalized using the softmax function.</p><p>The CTC loss function measures the gap between the predicted result and the actual result when no annotated text is provided. For an input sequence $x<inline-formula>
  <mml:math id="mwlcjr2spk">
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
  </mml:math>
</inline-formula>T<inline-formula>
  <mml:math id="mn2fd65jwo">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>y<inline-formula>
  <mml:math id="mslfnv03on">
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
  </mml:math>
</inline-formula>U$, the cross-entropy loss function is:</p>
              
                <disp-formula>
                  <label>(15)</label>
                  <mml:math id="m825x7iqgd">
                    <mml:msub>
                      <mml:mi>L</mml:mi>
                      <mml:mrow>
                        <mml:mi>C</mml:mi>
                        <mml:mi>T</mml:mi>
                        <mml:mi>C</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>â</mml:mo>
                    <mml:mo>â¡</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>â£</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mi>y</mml:mi>
                    <mml:mi>log</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>y</mml:mi>
                    <mml:mi>x</mml:mi>
                  </mml:math>
                </disp-formula>
              
              <p> where, <inline-formula>
  <mml:math id="mp2ahzndeu">
    <mml:mi>P</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>â£</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> represents the probability of the output sequence $y<inline-formula>
  <mml:math id="m944kq61lv">
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>x<inline-formula>
  <mml:math id="m449pimc0n">
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:msup>
      <mml:mi>l</mml:mi>
      <mml:mo>â²</mml:mo>
    </mml:msup>
  </mml:math>
</inline-formula>x<inline-formula>
  <mml:math id="mcxbfru13u">
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
  </mml:math>
</inline-formula>T<inline-formula>
  <mml:math id="mdjx8ub3qf">
    <mml:mo>,</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>y<inline-formula>
  <mml:math id="moz98z6m81">
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
  </mml:math>
</inline-formula>U<inline-formula>
  <mml:math id="m4hnu2f0ow">
    <mml:mo>,</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>a$, the loss function is expressed as:</p>
              
                <disp-formula>
                  <label>(16)</label>
                  <mml:math id="moh65ms86o">
                    <mml:msub>
                      <mml:mi>L</mml:mi>
                      <mml:mrow>
                        <mml:mi>A</mml:mi>
                        <mml:mi>T</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>a</mml:mi>
                      <mml:mrow>
                        <mml:mi>u</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mo>,</mml:mo>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>â</mml:mo>
                    <mml:mo>â¡</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mi>y</mml:mi>
                    <mml:mi>a</mml:mi>
                    <mml:mi>log</mml:mi>
                    <mml:munderover>
                      <mml:mo>â</mml:mo>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>T</mml:mi>
                    </mml:munderover>
                    <mml:munderover>
                      <mml:mo>â</mml:mo>
                      <mml:mrow>
                        <mml:mi>u</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>U</mml:mi>
                    </mml:munderover>
                    <mml:msubsup>
                      <mml:mi>y</mml:mi>
                      <mml:mi>u</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:msubsup>
                  </mml:math>
                </disp-formula>
              
              <p> where, <inline-formula>
  <mml:math id="mogqpfdslw">
    <mml:msub>
      <mml:mi>a</mml:mi>
      <mml:mrow>
        <mml:mi>u</mml:mi>
        <mml:mi>t</mml:mi>
        <mml:mo>,</mml:mo>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> represents the degree to which the model focuses on the $t<inline-formula>
  <mml:math id="mv5idimn0m">
    <mml:mo>â</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
  </mml:math>
</inline-formula>y_u<inline-formula>
  <mml:math id="mmdbqcrqui">
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>y_u^t<inline-formula>
  <mml:math id="mz9fcp85td">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>u$-th character. The logarithmic likelihood of these two parts is combined in a weighted sum.</p><p>The loss function for the joint training process of decoding usually includes both cross-entropy loss and attention loss, calculated as follows:</p>
              
                <disp-formula>
                  <label>(17)</label>
                  <mml:math id="munoimqs82">
                    <mml:msub>
                      <mml:mi>L</mml:mi>
                      <mml:mrow>
                        <mml:mi>C</mml:mi>
                        <mml:mi>T</mml:mi>
                        <mml:mi>C</mml:mi>
                        <mml:mi>A</mml:mi>
                        <mml:mi>T</mml:mi>
                        <mml:mo>â</mml:mo>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>L</mml:mi>
                      <mml:mrow>
                        <mml:mi>C</mml:mi>
                        <mml:mi>T</mml:mi>
                        <mml:mi>C</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>L</mml:mi>
                      <mml:mrow>
                        <mml:mi>A</mml:mi>
                        <mml:mi>T</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>â</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>Î»</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>y</mml:mi>
                    <mml:mi>Î»</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>y</mml:mi>
                    <mml:mi>a</mml:mi>
                    <mml:mn>1</mml:mn>
                  </mml:math>
                </disp-formula>
              
              <p> where, <inline-formula>
  <mml:math id="mz0wyhi2gb">
    <mml:mi>Î»</mml:mi>
  </mml:math>
</inline-formula> is used to set the weights of the two types of loss, generally set to 0.3.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. N-gram language model</title>
          
          <p>The purpose of the language model is to further decode the output of the acoustic model, converting the speech signal into the corresponding text sequence [<xref ref-type="bibr" rid="ref_31">31</xref>], and it is used for model CTC beam search decoding prediction. The CTC beam search method involves having B-Size candidate sequences, and at each time step, generating a new set of the best B-Size candidate sequences. The final result is the sequence with the highest probability among the B-Size candidate sequences. The principle of CTC beam search is shown in <xref ref-type="fig" rid="fig_10">Figure 10</xref>.</p>
          
            <fig id="fig_10">
              <label>Figure 10</label>
              <caption>
                <title>CTC beam search</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/10/img_MFjYj14xlu3IOjv0.png"/>
            </fig>
          
          <p>A language model is a knowledge representation of a sequence of words. By using a statistical-based language model and processing a large specific text corpus, the probability distribution of a given word sequence can be obtained, further determining the likelihood of a text sequence and finally obtaining the accurate prediction result of the text sequence with the highest probability.</p><p>Currently, the N-gram algorithm is the most commonly used language model in speech recognition systems. Its basic idea is to introduce the Markov assumption, considering that the probability of each word occurring is only related to several preceding words, and it ignores longer-distance context. Based on this assumption, the joint probability of words in a sentence is calculated to determine the likelihood of the sentence. N is usually an integer between 1 and 5, and when $N$ is 3, the corresponding Tri-gram algorithm is as follows:</p>
          
            <disp-formula>
              <label>(18)</label>
              <mml:math id="m1jn8kkhx7">
                <mml:mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mi>P</mml:mi>
                      <mml:mi>S</mml:mi>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mi/>
                      <mml:mi>P</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>â¯</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mi>w</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>w</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>w</mml:mi>
                          <mml:mi>n</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd/>
                    <mml:mtd>
                      <mml:mi/>
                      <mml:mi>P</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:munderover>
                        <mml:mo>â</mml:mo>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mi>n</mml:mi>
                      </mml:munderover>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>â£</mml:mo>
                        <mml:mo>â¯</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mi>w</mml:mi>
                          <mml:mi>i</mml:mi>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>w</mml:mi>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                            <mml:mo>â</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>w</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd/>
                    <mml:mtd>
                      <mml:mi/>
                      <mml:mi>P</mml:mi>
                      <mml:mi>P</mml:mi>
                      <mml:mi>P</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mo>â¯</mml:mo>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mi>w</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>â£</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mi>w</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>w</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>â£</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mi>w</mml:mi>
                          <mml:mi>n</mml:mi>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>w</mml:mi>
                          <mml:mrow>
                            <mml:mi>n</mml:mi>
                            <mml:mo>â</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>w</mml:mi>
                          <mml:mrow>
                            <mml:mi>n</mml:mi>
                            <mml:mo>â</mml:mo>
                            <mml:mn>2</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p> where, $S<inline-formula>
  <mml:math id="m0802hhxqg">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>w_i<inline-formula>
  <mml:math id="m5a7kvf8iz">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>W<inline-formula>
  <mml:math id="mi4jus5cpu">
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>X$ be the input speech. The final score is calculated as follows:</p>
          
            <disp-formula>
              <label>(19)</label>
              <mml:math id="mextsiplt0">
                <mml:mrow>
                  <mml:mi>s</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:mi>o</mml:mi>
                  <mml:mi>r</mml:mi>
                  <mml:mi>e</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>|</mml:mo>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>â£</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>â</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>â</mml:mo>
                <mml:msub>
                  <mml:mi>P</mml:mi>
                  <mml:mrow>
                    <mml:mi>a</mml:mi>
                    <mml:mi>m</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>P</mml:mi>
                  <mml:mrow>
                    <mml:mi>l</mml:mi>
                    <mml:mi>m</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mi>W</mml:mi>
                <mml:mi>X</mml:mi>
                <mml:mi>W</mml:mi>
                <mml:mi>W</mml:mi>
                <mml:msup>
                  <mml:mo>)</mml:mo>
                  <mml:mi>Î±</mml:mi>
                </mml:msup>
                <mml:msup>
                  <mml:mo>|</mml:mo>
                  <mml:mi>Î²</mml:mi>
                </mml:msup>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mn4uh98tl4">
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
  </mml:math>
</inline-formula> represents the acoustic model score, <inline-formula>
  <mml:math id="m30dz7exse">
    <mml:mi>l</mml:mi>
    <mml:mi>m</mml:mi>
  </mml:math>
</inline-formula> represents the language model score, <inline-formula>
  <mml:math id="m6hgve5048">
    <mml:mi>Î±</mml:mi>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mg0r140d6y">
    <mml:mi>Î²</mml:mi>
  </mml:math>
</inline-formula> are the set hyperparameters.</p><p>Field geological survey text data in hydraulic engineering involves many specialized terms and rare characters. The current N-gram language models are mostly trained based on common Chinese text [<xref ref-type="bibr" rid="ref_32">32</xref>], such as the large-scale Giga Chinese model (zh_giga.no_cna_cmn.prune01244.klm, 2.75 GB) and the lightweight Peopleâs Daily 2014 corpus model (people_2014_corpus_char.klm, 0.14 GB). These language models, due to their corpora not fully encompassing the content of the hydraulic engineering geological domain, result in higher CER in recognition and may lead to situations where specific characters cannot be displayed in the recognition results, as shown in <xref ref-type="fig" rid="fig_11">Figure 11</xref>.</p><p>To solve the problem of missing characters and further optimize the speech recognition results, geological specialized text data was converted to form a geological specialized Chinese text corpus. Additionally, by combining the ToRCH2009 Modern Chinese Balanced, ToRCH2014 Modern Chinese Balanced, ToRCH2019 Modern Chinese Balanced, and The BFSU DiSCUSS four open-source general Chinese corpora, the KenLM toolkit was used to train the model and compress it into binary format, forming a specialized language model for hydraulic engineering field geological surveys. To ensure that the model accurately and comprehensively learns the probability distribution of text sequences, the N value of the N-gram model was set to 5 [<xref ref-type="bibr" rid="ref_33">33</xref>].</p>
          
            <fig id="fig_11">
              <label>Figure 11</label>
              <caption>
                <title>Missing characters in recognition results</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/10/img_NBb3YR6nUsDlyVhK.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.4. Model fine-tuning</title>
          
          <p>Model fine-tuning is an important technique for addressing the challenge of limited computational power under large-scale data conditions [<xref ref-type="bibr" rid="ref_34">34</xref>]. It allows a general model to quickly adapt to the specific needs of a domain using a small amount of labeled data, thereby improving model performance, increasing recognition speed, and enhancing the accuracy of recognizing targets in specific fields. Additionally, training on top of an existing large-scale dataset helps the model fully learn acoustic features, enhancing its generalization capability and anti-interference stability [<xref ref-type="bibr" rid="ref_35">35</xref>].</p><p>The task of specialized speech recognition for hydraulic engineering field surveys involves numerous geological terms and related expressions that are rarely encountered in everyday language. The current mainstream speech recognition models are trained on datasets mostly derived from publicly available common language materials, where geological survey-specific vocabulary and speech account for a small proportion, resulting in weak recognition capabilities for geological domain-specific speech. Therefore, it is necessary to fine-tune the existing mainstream speech recognition models for the geological domain to improve their performance in this field.</p><p>When fine-tuning the geological-specific speech recognition model, the WenetSpeech [<xref ref-type="bibr" rid="ref_36">36</xref>] large-scale Chinese speech dataset is first used to train the model as a base model for transfer learning. Then, fine-tuning is performed on the specialized dataset for geological field surveys, updating all parameters of the entire network during the fine-tuning process. Cepstral Mean and Variance Normalization (CMVN) features [<xref ref-type="bibr" rid="ref_37">37</xref>] are used to normalize the speech signal and remove noise, while CTC loss and attention loss are used for supervision.</p>
        </sec>
      
      
        <sec>
          
            <title>3.5. Evaluation metrics</title>
          
          <p>To ensure the recognition performance of the model, this study uses decoding speed and CER as metrics to evaluate the performance of the proposed speech recognition model. The time taken to decode a single character represents the time required by the model to decode the audio corresponding to a single character, and it is used to assess the model's recognition efficiency. The smaller the unit decoding time, the higher the model's recognition efficiency. CER is used to evaluate the degree of difference between the predicted text and the original reference text, reflecting the accuracy of text recognition. The lower the CER, the better the model's recognition performance. The calculation method is as follows:</p>
          
            <disp-formula>
              <label>(20)</label>
              <mml:math id="mg5eoamrcd">
                <mml:mrow>
                  <mml:mi data-mjx-auto-op="false">CER</mml:mi>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>S</mml:mi>
                    <mml:mi>D</mml:mi>
                    <mml:mi>I</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                  <mml:mi>N</mml:mi>
                </mml:mfrac>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>S</mml:mi>
                    <mml:mi>D</mml:mi>
                    <mml:mi>I</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>S</mml:mi>
                    <mml:mi>D</mml:mi>
                    <mml:mi>C</mml:mi>
                    <mml:mo>+</mml:mo>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p> where, $S<inline-formula>
  <mml:math id="md2cx7mo8r">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>D<inline-formula>
  <mml:math id="mdwh9mijcg">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>I<inline-formula>
  <mml:math id="m6hmqinxfl">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>N<inline-formula>
  <mml:math id="muis7u9i0f">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>C$ represents the number of correctly recognized characters in the predicted result.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Model validation</title>
      
        <sec>
          
            <title>4.1. Data preprocessing</title>
          
          <p>The audio editing software WavePad was used to trim all audio files, removing silent sections. FFmpeg was used to modify the sampling rate of all audio to 16 kHz, lock the channel to mono, and convert the files to WAV format. The parameter information is shown in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Speech data parameter information</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Sampling Rate</p></th><th colspan="1" rowspan="1"><p>Bit Depth</p></th><th colspan="1" rowspan="1"><p>Channel</p></th><th colspan="1" rowspan="1"><p>File Format</p></th></tr><tr><td colspan="1" rowspan="1"><p>16 kHz</p></td><td colspan="1" rowspan="1"><p>16 bit</p></td><td colspan="1" rowspan="1"><p>Mono</p></td><td colspan="1" rowspan="1"><p>WAV</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>After processing the audio, a file-by-file reading and matching method was used to generate a data list file, which was used to index the audio files and their corresponding text information. Each line of data includes the relative path of the audio file and the annotated content corresponding to that audio file. The format is shown in <xref ref-type="fig" rid="fig_12">Figure 12</xref>.</p>
          
            <fig id="fig_12">
              <label>Figure 12</label>
              <caption>
                <title>Format of the data list file</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets"/>
            </fig>
          
          <p>Afterward, all characters contained in the dataset are counted to generate a vocabulary file. Finally, the number of frames, mean, and standard deviation are calculated for CMVN operations, with the default being to use all speech data to calculate the mean and standard deviation.</p><p>Finally, feature extraction is performed on all audio files to obtain their MFCC features, and JSON-formatted data feature list files are generated for both the training set and the test set for model training. In practice, the features of the same phoneme may differ due to the influence of different microphones, recording environments, and audio channels. Through CMVN operations, standard features with a mean of 0 and a variance of 1 can be obtained, thereby improving the anti-interference stability of the acoustic features. The MFCCs spectrogram is shown in <xref ref-type="fig" rid="fig_13">Figure 13</xref>.</p>
          
            <fig id="fig_13">
              <label>Figure 13</label>
              <caption>
                <title>MFCC spectrogram</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/10/img_V-GJ71rRD_O2TK80.png"/>
            </fig>
          
          <p>To further enrich the diversity of the data and thereby enhance the model's generalization and anti-interference capabilities, small random perturbations were added to the original audio during model training to generate new audio for data augmentation. The augmentation methods include noise perturbation, speed perturbation, volume perturbation, and SpecAugment [<xref ref-type="bibr" rid="ref_38">38</xref>]. The overall data preprocessing process is shown in <xref ref-type="fig" rid="fig_14">Figure 14</xref>.</p>
          
            <fig id="fig_14">
              <label>Figure 14</label>
              <caption>
                <title>Data preprocessing process</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/10/img_13sGII8EnHl5xdAo.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.2. Hyperparameter adjustment</title>
          
          <p>All model experiments were conducted on the same computer equipment, with the operating system being Windows 10 Professional, the programming language Python 3.11, the CPU being IntelÂ® Coreâ¢ i5-13490F, and the GPU being NVIDIAÂ® GeForce RTXâ¢ 3070.</p><p>In this study, the configuration parameters for the training and validation stages of the Conformer acoustic model were determined based on relevant literature in the field [<xref ref-type="bibr" rid="ref_39">39</xref>]. The specific parameter information is shown in <xref ref-type="table" rid="table_3">Table 3</xref>. The number of epochs represents the total number of training cycles for the model; the batch size is the number of audio samples the model processes simultaneously in one cycle; gradient accumulation is used to achieve the effect of expanding the batch size; the initial learning rate and its decay determine the convergence speed of the objective function; and MFCC size represents the dimensions of the MFCCs.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Model training hyperparameter settings</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Parameter</p></th><th colspan="1" rowspan="1"><p>Value</p></th></tr><tr><td colspan="1" rowspan="1"><p>Number of epochs</p></td><td colspan="1" rowspan="1"><p>200</p></td></tr><tr><td colspan="1" rowspan="1"><p>Batch size</p></td><td colspan="1" rowspan="1"><p>8</p></td></tr><tr><td colspan="1" rowspan="1"><p>Gradient accumulation</p></td><td colspan="1" rowspan="1"><p>4</p></td></tr><tr><td colspan="1" rowspan="1"><p>Optimizer type</p></td><td colspan="1" rowspan="1"><p>Adamm [<xref ref-type="bibr" rid="ref_40">40</xref>]</p></td></tr><tr><td colspan="1" rowspan="1"><p>Initial learning rate</p></td><td colspan="1" rowspan="1"><p>0.001</p></td></tr><tr><td colspan="1" rowspan="1"><p>Learning rate decay</p></td><td colspan="1" rowspan="1"><p>0.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>Minimum learning rate</p></td><td colspan="1" rowspan="1"><p>1.0e-5</p></td></tr><tr><td colspan="1" rowspan="1"><p>Weight decay coefficient</p></td><td colspan="1" rowspan="1"><p>1.0e-6</p></td></tr><tr><td colspan="1" rowspan="1"><p>MFCC size</p></td><td colspan="1" rowspan="1"><p>40</p></td></tr><tr><td colspan="1" rowspan="1"><p>Input audio length</p></td><td colspan="1" rowspan="1"><p>0.5<mml:math id="mmixjfh132">
  <mml:mo>â¼</mml:mo>
</mml:math>20</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>4.3. Recognition result analysis</title>
          
          <p>The Conformer model was trained for 200 epochs, with validation after each epoch. The model converged at the 87th epoch and achieved the best validation performance at the 194th epoch, with a CER of 4.38%, as shown in <xref ref-type="fig" rid="fig_15">Figure 15</xref>. After combining with the geological specialized language model, the CER was reduced to 2.36%, and the single-character decoding time was 15.5 ms.</p><p>To further evaluate the performance of the proposed model, the constructed hydraulic engineering field geological survey speech dataset was used as an example, with other advanced deep learning speech recognition models selected for comparison under the same configuration parameters as the hydraulic engineering geological survey speech recognition model. CER and decoding time were used as evaluation metrics. The decoding times and CERs of each model are listed in <xref ref-type="table" rid="table_4">Table 4</xref>. The comparison results show that the proposed language model, due to its specialization in the geological domain, effectively reduced the CER compared to the Giga Chinese model. It can be seen that this method shows no significant difference in single-character decoding time compared to other methods, meeting the requirements for real-time speech recognition; the CER metric is superior to other methods, indicating that it is more suitable for the speech recognition tasks involved in hydraulic engineering field geological survey data collection. Additionally, fine-tuning and loading the specialized language model significantly improved the recognition performance of the Conformer model, further demonstrating the advantages of the proposed method.</p>
          
            <fig id="fig_15">
              <label>Figure 15</label>
              <caption>
                <title>Validation effect and test loss of Conformer model</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/10/img_UaoMYL4RLDGvHPPy.png"/>
            </fig>
          
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Comparison results of speech recognition models</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Acoustic Model</p></th><th colspan="1" rowspan="1"><p>Fine-Tuning</p></th><th colspan="1" rowspan="1"><p>Language Model</p></th><th colspan="1" rowspan="1"><p>Single Character Decoding Time/ms</p></th><th colspan="1" rowspan="1"><p>CER/%</p></th></tr><tr><td colspan="1" rowspan="1"><p>Conformer</p></td><td colspan="1" rowspan="1"><p>Yes</p></td><td colspan="1" rowspan="1"><p>Geological Survey</p></td><td colspan="1" rowspan="1"><p>15.5</p></td><td colspan="1" rowspan="1"><p>2.6</p></td></tr><tr><td colspan="1" rowspan="1"><p>Conformer</p></td><td colspan="1" rowspan="1"><p>Yes</p></td><td colspan="1" rowspan="1"><p>Giga Chinese</p></td><td colspan="1" rowspan="1"><p>15.4</p></td><td colspan="1" rowspan="1"><p>3.9</p></td></tr><tr><td colspan="1" rowspan="1"><p>Conformer</p></td><td colspan="1" rowspan="1"><p>No</p></td><td colspan="1" rowspan="1"><p>Giga Chinese</p></td><td colspan="1" rowspan="1"><p>15.8</p></td><td colspan="1" rowspan="1"><p>3.7</p></td></tr><tr><td colspan="1" rowspan="1"><p>Deepspeech [<xref ref-type="bibr" rid="ref_41">41</xref>]</p></td><td colspan="1" rowspan="1"><p>No</p></td><td colspan="1" rowspan="1"><p>Giga Chinese</p></td><td colspan="1" rowspan="1"><p>29.5</p></td><td colspan="1" rowspan="1"><p>5.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>Transformer</p></td><td colspan="1" rowspan="1"><p>No</p></td><td colspan="1" rowspan="1"><p>Giga Chinese</p></td><td colspan="1" rowspan="1"><p>19.1</p></td><td colspan="1" rowspan="1"><p>8.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>Efficient Conformer [<xref ref-type="bibr" rid="ref_42">42</xref>]</p></td><td colspan="1" rowspan="1"><p>Yes</p></td><td colspan="1" rowspan="1"><p>Giga Chinese</p></td><td colspan="1" rowspan="1"><p>15.1</p></td><td colspan="1" rowspan="1"><p>3.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>Squesezeformer [<xref ref-type="bibr" rid="ref_43">43</xref>]</p></td><td colspan="1" rowspan="1"><p>Yes</p></td><td colspan="1" rowspan="1"><p>Giga Chinese</p></td><td colspan="1" rowspan="1"><p>21.6</p></td><td colspan="1" rowspan="1"><p>17.1</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>4.4. Comparison with mainstream commercial software recognition performance</title>
          
          <p>To directly demonstrate the recognition performance of the proposed model, actual field data recording speech was used as a sample, comparing the predicted results of this model with the output of mainstream commercial software. This speech sample contains a large number of geological terms, with a duration of 23 seconds and a text length of 95 characters. The software and their recognition results, along with the CER, are shown in <xref ref-type="table" rid="table_5">Table 5</xref>.</p>
          
            <table-wrap id="table_5">
              <label>Table 5</label>
              <caption>
                <title>Comparison of recognition performance between our model and mainstream commercial software</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model/Software</p></th><th colspan="1" rowspan="1"><p>Network Status</p></th><th colspan="1" rowspan="1"><p>Recognition Result</p></th><th colspan="1" rowspan="1"><p>CER/%</p></th></tr><tr><td colspan="1" rowspan="1"><p>Actual Corpus</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>è±å²å²© æµèçº¢è² ç²ç²ç»æ åç¶æé  è¡¨å±å²©ä½åå¼ºé£åç¶ å²©ä½å¼ºåº¦è¾ä½ é¤å»æç¢ ç¿ç©æåä»¥é¿ç³ ç³è± äºæ¯åè§éªç³ä¸ºä¸» å¡é¢è¦çå±ä¸ºç¢ç³å ç¢ç³å«éç¾åä¹åè³åäº åé¶ç¹äºè³ä¸ç±³ æ¤è¢«åè² è¾¹å¡èªç¶å¡åº¦ååè³ååäºåº¦</p></td><td colspan="1" rowspan="1"><p>-</p></td></tr><tr><td colspan="1" rowspan="1"><p>Our Model</p></td><td colspan="1" rowspan="1"><p>Offline</p></td><td colspan="1" rowspan="1"><p>è±å²å²© æµèçº¢è² ç²ç²ç»æ åç¶æé  è¡¨å±å²©ä½åå¼ºé£åç¶ å²©ä½å¼ºåº¦è¾ä½ é¤å»æç¢ ç¿ç©æåä»¥é¿ç³ ç³è± äºæ¯åè§éªç³ä¸ºä¸» å¡é¢è¦çå±ä¸ºç¢ç³å æç³å«éç¾åä¹åè³åäº <u>è³</u>é¶ç¹äºè³ä¸ç±³ <u>å­</u>è¢«åè² è¾¹å¡èªç¶å¡åº¦ååè³ååäºåº¦</p></td><td colspan="1" rowspan="1"><p>3.16</p></td></tr><tr><td colspan="1" rowspan="1"><p>iOS-15.3.1 Native Keyboard</p></td><td colspan="1" rowspan="1"><p>Offline</p></td><td colspan="1" rowspan="1"><p>è±å²å²© <u>å</u>èçº¢è² <u>å¤çç»æ</u> åç¶æé  è¡¨å±<u>äºº</u>ä½<u>ç</u>å¼ºé£å<u>å¦</u> <u>äºº</u>ä½å¼ºåº¦è¾ä½ é¤å»<u>ä¸å²</u> ç¿ç©æå<u>éèå¼</u> ç³è± äºæ¯åè§éªç³ä¸ºä¸» <u>å</u>é¢è¦ç<u>æ</u>ä¸º<u>å²æ¶å¾</u> <u>å²æ¶</u>å«é10%è³15 <u>å·</u>0.2<u>æ¥</u>ä¸<u>é¸£</u> æ¤è¢«<u>æ³è¯­</u> è¾¹åèªç¶å¡åº¦40-45åº¦</p></td><td colspan="1" rowspan="1"><p>28.42</p></td></tr><tr><td colspan="1" rowspan="1"><p>iOS-15.3.1 Native Keyboard</p></td><td colspan="1" rowspan="1"><p>Offline</p></td><td colspan="1" rowspan="1"><p>è±å²å²© æµ<u>RAW</u>çº¢è² <u>å¤çç»æ</u> åç¶æé  è¡¨å±<u>ä¸¥æ</u>æå¼ºé£å<u>å¦</u> <u>æ©</u>ä½å¼ºåº¦è¾ä½ é¤å»<u>ä¸å²</u> ç¿ç©æå<u>éèå¼</u> ç³è± äºæ¯åè§éªç³ä¸ºä¸» <u>å</u>é¢è¦ç<u>æ</u>ä¸º<u>å²æ¶å¾</u> <u>ç¡ç®</u>å«é10%è³15 <u>å·</u>0.2æ¥ä¸<u>é¸£</u> æ¤è¢«<u>æ³è¯­</u> è¾¹<u>å</u>èªç¶<u>è¿æ¸¡</u>40-45åº¦</p></td><td colspan="1" rowspan="1"><p>29.47</p></td></tr><tr><td colspan="1" rowspan="1"><p>Baidu Custom Input Method v8.2.39.795</p></td><td colspan="1" rowspan="1"><p>Offline</p></td><td colspan="1" rowspan="1"><p>è±å²å²© <u>å</u>èçº¢è² <u>åå</u>ç»æ åç¶æé  è¡¨å±<u>ç</u>ä½<u>å</u>å¼ºé£åç¶ <u>é¢</u>ä½å¼ºåº¦è¾ä½ <u>åè</u>æç¢ ç¿ç©æå<u>å·²å¸¸å</u> <u>å</u>è± äºæ¯å<u>èäº§å</u>ä¸ºä¸» <u>å</u>é¢è¦çæ¾ä¸ºç¢ç³å ç¢ç³å«é10%è³15 <u>å</u>0.2è³ä¸ç±³ æ¤è¢«åè² è¾¹å¡èªç¶<u>è¿åº¦</u>40è³45åº¦</p></td><td colspan="1" rowspan="1"><p>20.00</p></td></tr><tr><td colspan="1" rowspan="1"><p>Baidu Custom Input Method v8.2.39.795</p></td><td colspan="1" rowspan="1"><p>Offline</p></td><td colspan="1" rowspan="1"><p>è±å²å²© æµèçº¢è² ç²ç²ç»æ åç¶æé  è¡¨å±å²©ä½åå¼ºé£åç¶ å²©ä½å¼ºåº¦è¾ä½ é¤<u>åº</u>æç¢ ç¿ç©æåä»¥<u>å¸¸è¯</u> ç³è± äºæ¯åè§éªç³ä¸ºä¸» å¡é¢è¦çæ¾ä¸ºç¢ç³åç¢ç³å«é10%è³15% <u>å</u>0.2è³1ç±³ æ¤è¢«åè² è¾¹å¡èªç¶å¡åº¦40è³45åº¦</p></td><td colspan="1" rowspan="1"><p>5.26</p></td></tr><tr><td colspan="1" rowspan="1"><p>Sogou Custom Input Method V8.31.22</p></td><td colspan="1" rowspan="1"><p>Offline</p></td><td colspan="1" rowspan="1"><p><u>å«é¢ç </u> <u>æ½å¥</u>çº¢è² <u>åºå</u>ç»æ <u>ä¼æ</u>æé  è¡¨<u>æ¾</u>å²©ä½<u>åå¢ä¸°ååº</u> å²©ä½å¼ºåº¦<u>å«ç¹</u> é¤<u>åº</u>æç¢ ç¿ç©æå<u>å¼å¸¸æ¯</u> ç³è± <u>éµæ¯</u>å<u>è</u>éª<u>è§</u>ä¸ºä¸» <u>ç»</u>é¢è¦çå±ä¸º<u>è¯´å®å¾</u> ç¢ç³å«éç¾åä¹åè³åäº <u>å·</u>é¶ç¹äº<u>ä¹</u>ä¸ç±³ <u>èä½èç</u> <u>æå</u>èªç¶<u>è¿åº¦</u>ååè³ååäºåº¦</p></td><td colspan="1" rowspan="1"><p>41.05</p></td></tr><tr><td colspan="1" rowspan="1"><p>Sogou Custom Input Method V8.31.22</p></td><td colspan="1" rowspan="1"><p>Offline</p></td><td colspan="1" rowspan="1"><p>è±å²å²© æµèçº¢è² ç²ç²ç»æ åç¶æé  è¡¨å±å²©ä½åå¼ºé£åç¶ å²©ä½å¼ºåº¦è¾ä½ é¤å»æç¢ ç¿ç©æåä»¥<u>å¸¸è¯</u> ç³è± äºæ¯åè§éªç³ä¸ºä¸» <u>æ³¼</u>é¢è¦ç<u>æ</u>ä¸ºç¢ç³å ç¢ç³å«é10%<mml:math id="m4ju1y9pkl">
  <mml:mo>â¼</mml:mo>
</mml:math>15 <u>å</u>0ç¹2<mml:math id="mhjzk6wp5r">
  <mml:mo>â¼</mml:mo>
</mml:math>1m æ¤è¢«åè² è¾¹å¡èªç¶å¡åº¦40~45åº¦</p></td><td colspan="1" rowspan="1"><p>6.32</p></td></tr><tr><td colspan="1" rowspan="1"><p>Deepspeech2-Aishell</p></td><td colspan="1" rowspan="1"><p>Offline</p></td><td colspan="1" rowspan="1"><p><u>åä¸</u>+ <u>éè·¯çº¢é¹¤</u> <u>æ¹é</u>ç»æ <u>åççå¬</u> <u>è¡¨æäºº</u>ä½<u>æ¨ªè¡ä¸­åæ¬</u> <u>äºº</u>ä½å¼ºåº¦è¾ä½ <u>æ åä¸ç»</u> ç¿ç©æå+<u>è¡</u>ç³ ç³è± äº+å<u>è¾å°é£</u>ä¸ºä¸» <u>å½ç­å¤</u>ç<u>æ</u>ä¸º<u>åºé£å</u> <u>éé£</u>å«éç¾åä¹åè³åäº <u>å</u>é¶ç¹äºè³ä¸ç±³ <u>ç´</u>è¢«<u>å«æ¥</u> <u>å°</u>+èªç¶<u>è¿æ¸¡</u>ååè³ååäº<u>é¨</u></p></td><td colspan="1" rowspan="1"><p>51.58</p></td></tr><tr><td colspan="1" rowspan="1"><p>Conformer-Wenetspeech</p></td><td colspan="1" rowspan="1"><p>Offline</p></td><td colspan="1" rowspan="1"><p>è±<u>å</u>å²© æµèçº¢è² ç²+ç»æ +ç¶æé  è¡¨å±å²©ä½++é£å<u>å¦</u> <u>æ©</u>ä½å¼ºåº¦è¾ä½ <u>å</u>++ç¢ ç¿ç©æåä»¥é¿<u>æ¶</u> +è± äºæ¯å<u>è</u>++ä¸ºä¸» ++è¦ç<u>æ</u>ä¸ºç¢ç³å ç¢ç³å«éç¾åä¹åè³åäº <u>å</u>é¶ç¹äº<u>å</u>+ç±³ +è¢«åè² ++èªç¶å¡åº¦ååè³ååäºåº¦</p></td><td colspan="1" rowspan="1"><p>25.26</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>It can be seen that the CER of the proposed model is 3.16%, which is 2.10% lower than that of the Baidu Input Method in online mode. Compared to the offline versions of Baidu, iOS, and Sogou Input Methods, the CER is reduced by 16.84%, 25.26%, and 37.89%, respectively, showing better performance than the existing mainstream commercial software. For mainstream commercial software, their online recognition is more geared toward everyday conversational dialogue. Although their dataset size is larger than that used in the proposed method, the total number of geological terms is low, and their proportion is small during training, resulting in poor model performance in the related field, leading to more recognition errors, especially when geological terms are misrecognized as common conversational words. Notably, the proposed model shows a significant advantage in offline recognition, effectively recognizing geological terms, meeting the needs for recording speech data into text during field geological data collection.</p>
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>This study proposed a method for the collection and recording of hydraulic engineering geological survey data in the field, based on speech recognition interaction, to meet the demands for digital and intelligent data collection in hydraulic engineering geological surveys. The proposed method effectively improves the efficiency and accuracy of field data collection for hydraulic engineering geological surveys. The main conclusions are as follows:</p><p>(1) Various geological professional materials were compiled to form a specialized speech dataset for hydraulic engineering geological field surveys. This dataset includes 16,352 sentences of geological survey professional text data, totaling 218,498 characters, and 13.53 hours of speech data generated through both human voice recording and speech synthesis.</p><p>(2) Based on the self-made dataset and fine-tuning on the WenetSpeech large-scale dataset model, a Conformer acoustic model was trained using CTC-Attention joint loss supervision. Compared to the WenetSpeech pre-trained model, the CER was reduced to 3.9%.</p><p>(3) A speech recognition model for hydraulic engineering geological field surveys was developed. Using tokenized text data combined with open-source Chinese corpora, an N-gram algorithm was used to train and generate a specialized language model for hydraulic engineering geological field surveys, which was used for CTC beam search decoding. The proposed method achieved a single-character decoding time of 15.5 ms and a CER of only 2.6%, a reduction of 1.3%, outperforming other speech recognition methods such as Deepspeech2, Transformer, Efficient Conformer, and Squeezeformer.</p><p>(4) By combining the Conformer acoustic model with the N-gram language model, an intelligent speech recognition and efficient recording method tailored for hydraulic engineering geological field surveys was proposed. Compared with mainstream commercial software, this method achieved better results in terms of CER and showed significant advantages in offline recognition, making it suitable for mobile data environments in field geological surveys.</p><p>The proposed method performed well on the self-made dataset and in practical speech tests for geological surveys, but there is still room for improvement in recognition accuracy. Future work will include further expanding the dataset, such as collecting geological survey data for bridges, underground projects, and increasing the number of recording personnel. Additionally, the model architecture will be improved using the EfficientNet network to further optimize performance. Moreover, the acoustic and language models developed in this study are still relatively large after export, so future research will explore model lightweighting using knowledge distillation techniques.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p>This work was supported by the National Natural Science Foundation of China (Grant No.: 52179139) and the Major Science and Technology Projects of the Ministry of Water Resources (Grant No.: SKS-2022147).</p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>79</volume>
          <page-range>24413-24428</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Xiang</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Yan</given-names>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names>Jun</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Chen</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>Zhi</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-020-09064-5</pub-id>
          <article-title>Geological big data acquisition based on speech recognition</article-title>
          <source>Multimed. Tools Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>163-175</page-range>
          <year>2012</year>
          <person-group person-group-type="author">
            <name>
              <surname>Merriam</surname>
              <given-names>Daniel F.</given-names>
            </name>
            <name>
              <surname>Brady</surname>
              <given-names>Lawrence L.</given-names>
            </name>
            <name>
              <surname>Newell</surname>
              <given-names>K. David</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11053-011-9164-y</pub-id>
          <article-title>Kansas energy sources: A geological review</article-title>
          <source>Nat. Resour. Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>69</volume>
          <page-range>121-139</page-range>
          <issue>2</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Castellini</surname>
              <given-names>Mirko</given-names>
            </name>
            <name>
              <surname>Di Prima</surname>
              <given-names>Simone</given-names>
            </name>
            <name>
              <surname>Moret-FernÃ¡ndez</surname>
              <given-names>David</given-names>
            </name>
            <name>
              <surname>Lassabatere</surname>
              <given-names>Laurent</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.2478/johh-2021-0002</pub-id>
          <article-title>Rapid and accurate measurement methods for determining soil hydraulic properties: A review</article-title>
          <source>J. Hydrol. Hydromech.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>10365</page-range>
          <issue>21</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Safari Bazargani</surname>
              <given-names>Jalal</given-names>
            </name>
            <name>
              <surname>Sadeghi-Niaraki</surname>
              <given-names>Abolghasem</given-names>
            </name>
            <name>
              <surname>Choi</surname>
              <given-names>Soo-Mi</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app112110365</pub-id>
          <article-title>A survey of GIS and IoT integration: Applications and architecture</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>44</volume>
          <page-range>24-30</page-range>
          <year>2012</year>
          <person-group person-group-type="author">
            <name>
              <surname>Weng</surname>
              <given-names>Y. H.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>F. S.</given-names>
            </name>
            <name>
              <surname>Grigsby</surname>
              <given-names>J. D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.cageo.2012.02.027</pub-id>
          <article-title>GeoTools: An android phone application in geology</article-title>
          <source>Comput. Geosci.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>1019-1029</page-range>
          <issue>2</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Saint-Martin</surname>
              <given-names>Clotilde</given-names>
            </name>
            <name>
              <surname>Javelle</surname>
              <given-names>Pierre</given-names>
            </name>
            <name>
              <surname>Vinet</surname>
              <given-names>Freddy</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.5194/essd-10-1019-2018</pub-id>
          <article-title>DamaGIS: A multisource geodatabase for collection of flood-related damage data</article-title>
          <source>Earth Syst. Sci. Data</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>133</volume>
          <page-range>104312</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Han</surname>
              <given-names>Shuang</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Hui</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Meng</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>Xiang</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.cageo.2019.104312</pub-id>
          <article-title>Measuring rock surface strength based on spectrograms with deep convolutional networks</article-title>
          <source>Comput. Geosci.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>93</volume>
          <page-range>64-65</page-range>
          <issue>s3</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Stephenson</surname>
              <given-names>M. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1111/1755-6724.14247</pub-id>
          <article-title>The uses and benefits of big data for geological surveys</article-title>
          <source>Acta Geol. Sin. (Engl. Ed.)</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <page-range>275-294</page-range>
          <issue>3</issue>
          <year>2010</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pavlis</surname>
              <given-names>Terry L.</given-names>
            </name>
            <name>
              <surname>Langford</surname>
              <given-names>Richard</given-names>
            </name>
            <name>
              <surname>Hurtado</surname>
              <given-names>Jose</given-names>
            </name>
            <name>
              <surname>Serpa</surname>
              <given-names>Laura</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1130/GES00503.1</pub-id>
          <article-title>Computer-based data acquisition and visualization systems in field geology: Results from 12 years of experimentation and future potential</article-title>
          <source>Geosphere</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>56</volume>
          <page-range>85-100</page-range>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Besacier</surname>
              <given-names>Laurent</given-names>
            </name>
            <name>
              <surname>Barnard</surname>
              <given-names>Etienne</given-names>
            </name>
            <name>
              <surname>Karpov</surname>
              <given-names>Alexey</given-names>
            </name>
            <name>
              <surname>Schultz</surname>
              <given-names>Tanja</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.specom.2013.07.008</pub-id>
          <article-title>Automatic speech recognition for under-resourced languages: A survey</article-title>
          <source>Speech Commun.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>78</volume>
          <page-range>97-111</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Agarwalla</surname>
              <given-names>Swapnil</given-names>
            </name>
            <name>
              <surname>Sarma</surname>
              <given-names>Kandarpa Kumar</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neunet.2015.12.010</pub-id>
          <article-title>Machine learning based sample extraction for automatic speech recognition using dialectal Assamese speech</article-title>
          <source>Neural Networks</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>89</volume>
          <page-range>2783-2793</page-range>
          <issue>11</issue>
          <year>2006</year>
          <person-group person-group-type="author">
            <name>
              <surname>Fujimoto</surname>
              <given-names>Masakiyo</given-names>
            </name>
            <name>
              <surname>Takeda</surname>
              <given-names>Kazuya</given-names>
            </name>
            <name>
              <surname>Nakamura</surname>
              <given-names>Satoshi</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1093/ietisy/e89-d.11.2783</pub-id>
          <article-title>CENSREC-3: An evaluation framework for Japanese speech recognition in real car-driving environments</article-title>
          <source>IEICE Trans. Inf. &amp; Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>131</volume>
          <page-range>1599-1607</page-range>
          <issue>7</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Rohlfing</surname>
              <given-names>Meredith L.</given-names>
            </name>
            <name>
              <surname>Buckley</surname>
              <given-names>Daniel P.</given-names>
            </name>
            <name>
              <surname>Piraquive</surname>
              <given-names>Julian</given-names>
            </name>
            <name>
              <surname>Stepp</surname>
              <given-names>Cara E.</given-names>
            </name>
            <name>
              <surname>Tracy</surname>
              <given-names>Lauren F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1002/lary.29082</pub-id>
          <article-title>Hey Siri: How effective are common voice recognition systems at recognizing dysphonic voices?</article-title>
          <source>Laryngoscope</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>146</volume>
          <page-range>3073-3074</page-range>
          <issue>S4</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Morgan</surname>
              <given-names>Matthew M.</given-names>
            </name>
            <name>
              <surname>Bhattacharya</surname>
              <given-names>Indrani</given-names>
            </name>
            <name>
              <surname>Radke</surname>
              <given-names>Richard</given-names>
            </name>
            <name>
              <surname>Braasch</surname>
              <given-names>Jonas</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1121/1.5137665</pub-id>
          <article-title>Automatic speech emotion recognition using deep learning for analysis of collaborative group meetings</article-title>
          <source>J. Acoust. Soc. Am.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>131858-131876</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Alharbi</surname>
              <given-names>Sarah</given-names>
            </name>
            <name>
              <surname>Alrazgan</surname>
              <given-names>Muna</given-names>
            </name>
            <name>
              <surname>Alrashed</surname>
              <given-names>Abdullah</given-names>
            </name>
            <name>
              <surname>Alnomasi</surname>
              <given-names>Talal</given-names>
            </name>
            <name>
              <surname>Almojel</surname>
              <given-names>Reham</given-names>
            </name>
            <name>
              <surname>Alharbi</surname>
              <given-names>Raghad</given-names>
            </name>
            <name>
              <surname>others</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2021.3112535</pub-id>
          <article-title>Automatic speech recognition: Systematic literature review</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>96</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Peivandi</surname>
              <given-names>Saeid</given-names>
            </name>
            <name>
              <surname>Ahmadian</surname>
              <given-names>Leila</given-names>
            </name>
            <name>
              <surname>Farokhzadian</surname>
              <given-names>Jebrail</given-names>
            </name>
            <name>
              <surname>Jahani</surname>
              <given-names>Yousef</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s12911-022-01835-4</pub-id>
          <article-title>Evaluation and comparison of errors on nursing notes created by online and offline speech recognition technology and handwritten: An interventional study</article-title>
          <source>BMC Med. Inform. Decis. Mak.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>23</volume>
          <page-range>1530</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Guo</surname>
              <given-names>Lun Yi</given-names>
            </name>
            <name>
              <surname>Mu</surname>
              <given-names>Shi Ning</given-names>
            </name>
            <name>
              <surname>Deng</surname>
              <given-names>Yi Jie</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>Chao Fan</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>Bo</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>Zhuo Ling</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s23031530</pub-id>
          <article-title>Efficient binary weight convolutional network accelerator for speech recognition</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>74</volume>
          <page-range>101349</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sterpu</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Harte</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.csl.2022.101349</pub-id>
          <article-title>Taris: An online speech recognition framework with sequence to sequence neural networks for both audio-only and audio-visual speech</article-title>
          <source>Comput. Speech Lang.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>5036--5040</page-range>
          <year>2020</year>
          <publisher-name>ISCA-INT Speech Communication Assoc,</publisher-name>
          <person-group person-group-type="author">
            <name>
              <surname>Gulati</surname>
              <given-names>Anmol</given-names>
            </name>
            <name>
              <surname>Qin</surname>
              <given-names>James</given-names>
            </name>
            <name>
              <surname>Chiu</surname>
              <given-names>Chung-Cheng</given-names>
            </name>
            <name>
              <surname>Parmar</surname>
              <given-names>Niki</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Yu</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>Jiahui</given-names>
            </name>
            <name>
              <surname>Han</surname>
              <given-names>Wei</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Shibo</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Zhengdong</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Yonghui</given-names>
            </name>
            <name>
              <surname>Pang</surname>
              <given-names>Ruoming</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2005.08100</pub-id>
          <article-title>Conformer: Convolution-augmented transformer for speech recognition</article-title>
          <source>, undefined</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>280</volume>
          <page-range>110964</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yan</surname>
              <given-names>Xiao</given-names>
            </name>
            <name>
              <surname>Fang</surname>
              <given-names>Zhen</given-names>
            </name>
            <name>
              <surname>Jin</surname>
              <given-names>Yibing</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.knosys.2023.110964</pub-id>
          <article-title>An adaptive n-gram transformer for multi-scale scene text recognition</article-title>
          <source>Knowl.-Based Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>41</volume>
          <page-range>1-12</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>Tao</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>S. R.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Z. H.</given-names>
            </name>
            <name>
              <surname>Guan</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.11660/slfdxb.20220701</pub-id>
          <article-title>Text intelligent analysis for hydraulic construction accidents based on BERT-BiLSTM hybrid model</article-title>
          <source>J. Hydroelectr. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>164</volume>
          <page-range>103112</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Soleymanpour</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Johnson</surname>
              <given-names>M.T.</given-names>
            </name>
            <name>
              <surname>Soleymanpour</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Berry</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.specom.2024.103112</pub-id>
          <article-title>Accurate synthesis of dysarthric speech for ASR data augmentation</article-title>
          <source>Speech Commun.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>1-5</page-range>
          <year>2017</year>
          <publisher-name>IEEE</publisher-name>
          <person-group person-group-type="author">
            <name>
              <surname>Bu</surname>
              <given-names>Haiwei</given-names>
            </name>
            <name>
              <surname>Du</surname>
              <given-names>Jun</given-names>
            </name>
            <name>
              <surname>Na</surname>
              <given-names>Xuewei</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Binghuai</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>Haoran</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICSDA.2017.8384449</pub-id>
          <article-title>Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline</article-title>
          <source>, undefined</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>1163</page-range>
          <issue>10</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>LieskovskÃ¡</surname>
              <given-names>Eva</given-names>
            </name>
            <name>
              <surname>Jakubec</surname>
              <given-names>MaroÅ¡</given-names>
            </name>
            <name>
              <surname>Jarina</surname>
              <given-names>Roman</given-names>
            </name>
            <name>
              <surname>ChmulÃ­k</surname>
              <given-names>Michal</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/electronics10101163</pub-id>
          <article-title>A review on speech emotion recognition using deep learning and attention mechanism</article-title>
          <source>Electronics</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>45</volume>
          <page-range>283-295</page-range>
          <issue>2</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Atmani</surname>
              <given-names>Youcef</given-names>
            </name>
            <name>
              <surname>Rechak</surname>
              <given-names>Said</given-names>
            </name>
            <name>
              <surname>Mesloub</surname>
              <given-names>Ammar</given-names>
            </name>
            <name>
              <surname>Hemmouch</surname>
              <given-names>Larbi</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.24425/aoa.2020.133149</pub-id>
          <article-title>Enhancement in bearing fault classification parameters using gaussian mixture models and mel frequency cepstral coefficients features</article-title>
          <source>Arch. Acoust.</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <page-range>470-488</page-range>
          <issue>2</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Coppieters de Gibson</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Garner</surname>
              <given-names>P.N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/acoustics6020025</pub-id>
          <article-title>Training a filter-based model of the cochlea in the context of pre-trained acoustic models</article-title>
          <source>Acoustics</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>1-11</page-range>
          <year>2017</year>
          <publisher-name>La Jolla, CA: NIPS</publisher-name>
          <person-group person-group-type="author">
            <name>
              <surname>Vaswani</surname>
              <given-names>Ashish</given-names>
            </name>
            <name>
              <surname>Shazeer</surname>
              <given-names>Noam</given-names>
            </name>
            <name>
              <surname>Parmar</surname>
              <given-names>Niki</given-names>
            </name>
            <name>
              <surname>Uszkoreit</surname>
              <given-names>Jakob</given-names>
            </name>
            <name>
              <surname>Jones</surname>
              <given-names>Llion</given-names>
            </name>
            <name>
              <surname>Gomez</surname>
              <given-names>Aidan N. et al.</given-names>
            </name>
          </person-group>
          <article-title>Attention Is all you need</article-title>
          <source>, undefined</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>1601</page-range>
          <issue>13</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>RendÃ³n-Segador</surname>
              <given-names>Fernando J.</given-names>
            </name>
            <name>
              <surname>GarcÃ­a</surname>
              <given-names>Juan A.</given-names>
            </name>
            <name>
              <surname>EnrÃ­quez</surname>
              <given-names>Fernando</given-names>
            </name>
            <name>
              <surname>Deniz</surname>
              <given-names>Oscar</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/electronics10131601</pub-id>
          <article-title>Violencenet: Dense multi-head self-attention with bidirectional convolutional LSTM for detecting violence</article-title>
          <source>Electronics</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>10638</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Joshi</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>V. K.</given-names>
            </name>
            <name>
              <surname>Vishwakarma</surname>
              <given-names>D. K.</given-names>
            </name>
            <name>
              <surname>Ghorbani</surname>
              <given-names>M. A.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Gupta</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>others</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-024-61339-1</pub-id>
          <article-title>A comparative survey between Cascade Correlation Neural Network (CCNN) and Feedforward Neural Network (FFNN) machine learning models for forecasting suspended sediment concentration</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <volume>81</volume>
          <page-range>41295-41308</page-range>
          <issue>28</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>W. Q.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-022-12136-3</pub-id>
          <article-title>A hybrid CTC+ Attention model based on end-to-end framework for multilingual speech recognition</article-title>
          <source>Multimed. Tools Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="journal">
          <volume>226</volume>
          <page-range>71-77</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hou</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Jian</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Quan</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ymeth.2024.04.013</pub-id>
          <article-title>Language model based on deep learning network for biomedical named entity recognition</article-title>
          <source>Methods</source>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <volume>23</volume>
          <page-range>1145</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mukhamadiyev</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Mukhiddinov</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Khujayarov</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Ochilov</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Cho</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s23031145</pub-id>
          <article-title>Development of language models for continuous uzbek speech recognition system</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <volume>95</volume>
          <page-range>2308-2317</page-range>
          <issue>9</issue>
          <year>2012</year>
          <person-group person-group-type="author">
            <name>
              <surname>Naptali</surname>
              <given-names>Welly</given-names>
            </name>
            <name>
              <surname>Tsuchiya</surname>
              <given-names>Masatoshi</given-names>
            </name>
            <name>
              <surname>Nakagawa</surname>
              <given-names>Seiichi</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1587/transinf.E95.D.2308</pub-id>
          <article-title>Class-based n-gram language model for new words using out-of-vocabulary to in-vocabulary similarity</article-title>
          <source>IEICE Transactions on Information and Systems</source>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>9945</page-range>
          <issue>24</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Nahar</surname>
              <given-names>Raufun</given-names>
            </name>
            <name>
              <surname>Miwa</surname>
              <given-names>Shogo</given-names>
            </name>
            <name>
              <surname>Kai</surname>
              <given-names>Atsuhiko</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s22249945</pub-id>
          <article-title>Domain adaptation with augmented data by deep neural network based method using re-recorded speech for automatic speech recognition in real environment</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_35">
        <label>35.</label>
        <element-citation publication-type="journal">
          <volume>2024</volume>
          <page-range>29</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Qu</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICASSP43922.2022.9746682</pub-id>
          <article-title>Exploration of whisper fine-tuning strategies for low-resource ASR</article-title>
          <source>EURASIP J. Audio Speech Music Proc.</source>
        </element-citation>
      </ref>
      <ref id="ref_36">
        <label>36.</label>
        <element-citation publication-type="conf-paper">
          <page-range>6182-6186</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Lv</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Shao</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names>L. et al.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICASSP43922.2022.9746682</pub-id>
          <article-title>Wenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition</article-title>
          <source>IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source>
        </element-citation>
      </ref>
      <ref id="ref_37">
        <label>37.</label>
        <element-citation publication-type="journal">
          <page-range>1-16</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Farahani</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1904.08779</pub-id>
          <article-title>Autocorrelation-based noise subtraction method with smoothing, overestimation, energy, and cepstral mean and variance normalization for noisy speech recognition</article-title>
          <source>EURASIP J. Audio Speech Music Proc.</source>
        </element-citation>
      </ref>
      <ref id="ref_38">
        <label>38.</label>
        <element-citation publication-type="conf-paper">
          <page-range>2613-2617</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Park</surname>
              <given-names>D. S.</given-names>
            </name>
            <name>
              <surname>Chan</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Chiu</surname>
              <given-names>C. C.</given-names>
            </name>
            <name>
              <surname>Zoph</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Cubuk</surname>
              <given-names>E. D.</given-names>
            </name>
            <name>
              <surname>Le</surname>
              <given-names>Q. V.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1904.08779</pub-id>
          <article-title>Specaugment: A simple data augmentation method for automatic speech recognition</article-title>
          <source>INTERSPEECH 2019</source>
        </element-citation>
      </ref>
      <ref id="ref_39">
        <label>39.</label>
        <element-citation publication-type="journal">
          <volume>77</volume>
          <page-range>911-940</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jiang</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Pan</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.32604/cmc.2023.041772</pub-id>
          <article-title>A robust conformer-based speech recognition model for mandarin air traffic control</article-title>
          <source>Comput. Mater. Continua</source>
        </element-citation>
      </ref>
      <ref id="ref_40">
        <label>40.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-15</page-range>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kingma</surname>
              <given-names>P. D.</given-names>
            </name>
            <name>
              <surname>Ba</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Adam: A method for stochastic optimization</article-title>
          <source>International Conference on Learning Representations</source>
        </element-citation>
      </ref>
      <ref id="ref_41">
        <label>41.</label>
        <element-citation publication-type="conf-paper">
          <page-range>173-182</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Amodei</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Ananthanarayanan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Anubhai</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Bai</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Battenberg</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Case</surname>
              <given-names>C. et al.</given-names>
            </name>
          </person-group>
          <article-title>Deep speech 2: End-to-end speech recognition in English and Mandarin</article-title>
          <source>International Conference on Machine Learning</source>
        </element-citation>
      </ref>
      <ref id="ref_42">
        <label>42.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>448-453</page-range>
          <year>2021</year>
          <publisher-name>IEEE</publisher-name>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X. L.</given-names>
            </name>
          </person-group>
          <article-title>Efficient conformer-based speech recognition with linear attention</article-title>
          <source>, undefined</source>
        </element-citation>
      </ref>
      <ref id="ref_43">
        <label>43.</label>
        <element-citation publication-type="journal">
          <volume>35</volume>
          <page-range>9361-9373</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kim</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Gholami</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Shaw</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Mangalam</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Malik</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>others</surname>
            </name>
          </person-group>
          <article-title>Squeezeformer: An efficient transformer for automatic speech recognition</article-title>
          <source>Adv. Neural Inf. Process. Syst.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>