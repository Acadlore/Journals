<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-fS4CvFS3voFJkGb2juaYNXBglwwtblgI</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml010105</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>A Dual-Selective Channel Attention Network for Osteoporosis Prediction in Computed Tomography Images of Lumbar Spine</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5753-0120</contrib-id>
          <name>
            <surname>Xue</surname>
            <given-names>Linyan</given-names>
          </name>
          <email>lyxue@hbu.edu.cn</email>
          <xref ref-type="aff" rid="aff_1,2,3">1,2,3</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6587-8610</contrib-id>
          <name>
            <surname>Hou</surname>
            <given-names>Ya</given-names>
          </name>
          <email>houya@stumail.hbu.edu.cn</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1090-6944</contrib-id>
          <name>
            <surname>Wang</surname>
            <given-names>Shiwei</given-names>
          </name>
          <email>20217020050@stumail.hbu.edu.cn</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3228-4477</contrib-id>
          <name>
            <surname>Luo</surname>
            <given-names>Cheng</given-names>
          </name>
          <email>20208023017@stumail.hbu.edu.cn</email>
          <xref ref-type="aff" rid="aff_4">4</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8069-4185</contrib-id>
          <name>
            <surname>Xia</surname>
            <given-names>Zhiyin</given-names>
          </name>
          <email>20207023046@stumail.hbu.edu.cn</email>
          <xref ref-type="aff" rid="aff_4">4</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7750-6470</contrib-id>
          <name>
            <surname>Qin</surname>
            <given-names>Geng</given-names>
          </name>
          <email>20208020011@stumail.hbu.edu.cn</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9552-1364</contrib-id>
          <name>
            <surname>Liu</surname>
            <given-names>Shuang</given-names>
          </name>
          <email>whlius@hbu.edu.cn</email>
          <xref ref-type="aff" rid="aff_1,2,3">1,2,3</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8989-4044</contrib-id>
          <name>
            <surname>Wang</surname>
            <given-names>Zhongliang</given-names>
          </name>
          <email>20191704022@stumail.hbu.edu.cn</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9841-6810</contrib-id>
          <name>
            <surname>Gao</surname>
            <given-names>Wenshan</given-names>
          </name>
          <email>wsg6813@hbu.edu.cn</email>
          <xref ref-type="aff" rid="aff_4">4</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-6487-7512</contrib-id>
          <name>
            <surname>Yang</surname>
            <given-names>Kun</given-names>
          </name>
          <email>yangkun@hbu.edu.cn</email>
          <xref ref-type="aff" rid="aff_1,2,3">1,2,3</xref>
        </contrib>
        <aff id="aff_1">College of Quality and Technical Supervision, Hebei University, 071002 Baoding, China</aff>
        <aff id="aff_2">Hebei Technology Innovation Center for Lightweight of New Energy Vehicle Power System, 071002 Baoding, China</aff>
        <aff id="aff_3">National &amp; Local Joint Engineering Research Center of Metrology Instrument and System, Hebei University, 071002 Baoding, China</aff>
        <aff id="aff_4">Department of Orthopedics, Affiliated Hospital of Hebei University, 071002 Baoding, China</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>19</day>
        <month>11</month>
        <year>2022</year>
      </pub-date>
      <volume>1</volume>
      <issue>1</issue>
      <fpage>30</fpage>
      <lpage>39</lpage>
      <page-range>30-39</page-range>
      <history>
        <date date-type="received">
          <day>08</day>
          <month>07</month>
          <year>2022</year>
        </date>
        <date date-type="accepted">
          <day>11</day>
          <month>09</month>
          <year>2022</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>Â©2022 by the author(s)</copyright-statement>
        <copyright-year>2022</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Osteoporosis is a common systemic bone disease with insidious onset and low treatment efficiency. Once it occurs, it will increase bone fragility and lead to fractures. Computed tomography (CT) is a non-invasive medical examination method that can identify the bone condition of patients. In this paper, we propose a novel channel attention module, which is subsequently integrated into the supervised deep convolutional neural network (DCNN) termed DSNet, which can perform feature fusion from two different scales, and use the method of quadratic weight calculation to enhance the interconnection among feature map channels and improve the detection and classification performance for the bone condition in lumbar spine CT images. To train and test the proposed framework, we retrospectively collect 4805 CT images of 133 patients, using DXA as the gold standard. According to the T-value diagnostic criteria defined by WHO, the vertebral bodies of L1 - L4 in CT images are labeled and classified into osteoporosis, osteopenia and normal bone mineral density. Meanwhile, the training set and test set are constructed in the ratio of 4:1. As a result, the DSNet achieves a prediction accuracy of 83.4% and a recall rate of 90.0% on the test set, indicating that the proposed model has the potential to assist clinicians in diagnosing individuals with abnormal BMD and may alert patients at high risk of osteoporosis for timely treatment.</p></abstract>
      <kwd-group>
        <kwd>Deep convolutional neural network (DCNN)</kwd>
        <kwd>DSNet</kwd>
        <kwd>Dual-selective channel attention</kwd>
        <kwd>CT image</kwd>
        <kwd>Osteoporosis</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="10"/>
        <fig-count count="4"/>
        <table-count count="3"/>
        <ref-count count="33"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Osteoporosis is a common and systemic bone disease, and its early symptoms are not obvious. Most patients with osteoporosis undergo relevant examinations when complications arise, which are usually in the late stage and bring a lot of inconveniences and even death [<xref ref-type="bibr" rid="ref_1">1</xref>]. Therefore, early screening is crucial for the timely prevention and treatment of osteoporosis fractures [<xref ref-type="bibr" rid="ref_2">2</xref>]. Meanwhile, all kinds of orthopedic surgery need to refer to a bone status evaluation to formulate a better surgical plan. As the gold standard for bone mass measurement, DXA testing is expensive. Even in many developed countries, the opportunity to use DXA is still insufficient. CT image examination has a large number and clear images, which is of great significance in early screening for the prevention and treatment of osteoporotic fractures [<xref ref-type="bibr" rid="ref_3">3</xref>]. </p><p>In order to reduce this preventable injury and subsequent complications, more and more researchers are focusing on methods that combine computer-aided detection and machine learning with radionics to assist clinicians with osteoporosis prediction. For example, Aouache et al. [<xref ref-type="bibr" rid="ref_4">4</xref>] designed a fuzzy decision tree (FDT) model to identify osteoporosis by identifying patients' cervical spine images. Devikanniga and Raj [<xref ref-type="bibr" rid="ref_5">5</xref>] proposed an artificial neural network optimized for monarch butterflies, which identified osteoporosis and normal subjects by recognizing hip X-ray images of patients and combining them with demographic attributes. However, their algorithms were based on small data sets and required complex data processing before feature extraction, causing inaccuracy in medical image processing.</p><p>In recent years, convolutional neural networks (CNNs) have achieved high performance in visual recognition tasks. Instead, this technique can automatically locate the region of interest (ROI) and extract features, avoiding empirical errors in manual feature extraction [<xref ref-type="bibr" rid="ref_6">6</xref>]. Therefore, the application of deep learning in medical imaging diagnosis has received extensive attention for osteoporosis classification [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>] and bone mineral density (BMD) prediction [<xref ref-type="bibr" rid="ref_11">11</xref>], [<xref ref-type="bibr" rid="ref_12">12</xref>], [<xref ref-type="bibr" rid="ref_13">13</xref>]. For instance, Pan et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] developed a deep learning-based system for bone mineral density (BMD) classification in chest CT images. To extract the density information of trabecular bone more accurately, they firstly segmented and labeled all vertebral bodies, then their system can automatically extract the rectangular area in the center of the trabecular bone as the ROI, thereby realizing automatic measure BMD and make two classifications predictions of osteoporosis and osteopenia. Yasaka et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] used an improved CNN model to extract the features of the manually labeled central circular region of trabecular bone from unenhanced abdominal CT images, achieving a correlation of 0.840 between the CNN network and the corresponding DXA results. Lee et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] proposed a method based on the combination of the VGG network and random forest to classify of normal and abnormal BMD in spinal X-ray images. To extract features more accurately, they selected the central region of the fourth lumbar spine as ROI and achieved a 71% accuracy for the two-category classification. Gonzalez et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] selected chest CT images as input data, proving that deep neural network has an excellent performance in osteoporosis identification. </p><p>Despite the above studies yielded exciting results in identifying osteoporosis, they still left significant limitations on the diagnostic ability of clinical osteoporosis recognition. Firstly, those designed network models were limited to two classifications: Osteoporosis and osteopenia, which could not meet the clinical requirements of various conditions of bone condition. Moreover, these studies mostly selected the trabecular structure as ROI, while ignoring the cortical bone thickness which plays an important role in the real discrimination scenarios of bone by clinical experts. Finally, the methods proposed in these studies were not effective in bone image texture recognition due to the insufficient ability of feature extraction, which affected the prediction accuracy. </p><p>In view of the limitations of CNN-based methods for accurate osteoporosis prediction, we proposed an improved network based on Faster R-CNN, in which a new channel attention module was integrated to enhance the texture, shape and other features of vertebral trabecular bone in CT images. The specific analysis is as follows: 1) We have achieved a three-category classification of osteoporosis, osteopenia and normal bone mineral density, which brings greater applicability to professional doctors in clinical diagnosis; 2) According to the actual clinical needs, the whole vertebral body including cortical bone and cancellous bone was tested for the region of interest; and 3) We adopt an improved channel attention mechanism, named DS module, to improve the interrelation among feature map channels and deepen the learning of extracted feature information.</p>
    </sec>
    <sec sec-type="">
      <title>2. Methodology</title>
      <p>This paper aims to improve the osteoporosis detection performance for the lumbar spine CT images based on an improved Faster R-CNN network.</p>
      
        <sec>
          
            <title>2.1. Faster r-cnn</title>
          
          <p>The general framework of the proposed DSNet is based on the mainstream detection network of Faster R-CNN due to the high performance in many visual detection tasks [<xref ref-type="bibr" rid="ref_18">18</xref>]. In Faster R-CNN, the backbone network extracts feature from input images using ResNet [<xref ref-type="bibr" rid="ref_19">19</xref>], LeNet5 [<xref ref-type="bibr" rid="ref_20">20</xref>], AlexNet [<xref ref-type="bibr" rid="ref_21">21</xref>] or GoogLeNet [<xref ref-type="bibr" rid="ref_22">22</xref>]. The backbone network loads the officially trained model parameters to extract features, which can reduce the amount of model training data and speed up the training speed. Afterward, the target proposal boxes generated by the region proposal network (RPN) are projected to the feature map through the region of interest pooling layer (ROI pooling). Finally, the features are calculated by classification and bounding regression to achieve end-end detection. The framework of Faster R-CNN is shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>Faster R-CNN</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2022/11/img_2iPKFXes5Zzg4Y9B.jpeg"/>
            </fig>
          
          <p style="text-indent: 10.2pt;">In clinical practice, radiologists commonly distinguish normal, osteoporosis, and osteopenia BMD from the thickness of cortical bone and the sparsity of cancellous bone. For the automatic osteoporosis detection task, the network needs to pay more attention to the texture details of the whole vertebral body. With respect to these aspects, we pronounce a novel attention module and subsequently integrate it into the backbone of ResNet50 to improve the interconnection among feature map channels and deepen the learning of the extracted texture feature information. As a result, the proposed DSNet extracts more texture features of the entire vertebral body than the traditional ResNet50.</p>
        </sec>
      
      
        <sec>
          
            <title>2.2. Channel attention module</title>
          
          <p style="text-indent: 10.2pt;">Channel attention is one of the most widely used attention mechanisms in various fields such as natural language processing (NLP), computer vision (CV) and speech recognition. The representative ones are SENet [<xref ref-type="bibr" rid="ref_23">23</xref>], SKNet [<xref ref-type="bibr" rid="ref_24">24</xref>], ECANet [<xref ref-type="bibr" rid="ref_25">25</xref>], etc. As channel attention modules, they can be easily embedded in the deep learning network to achieve improved performance. As shown in subgraph (a) of <xref ref-type="fig" rid="fig_2">Figure 2</xref>, the input feature map with the size of H*W*C is mapped to the feature map with the size of H1*W1*C1 through a transformation of a 3*3 convolution. Then the SE attention mechanism establishes the inter-dependence between feature channels through a global pooling and a full convolution. Its mechanism is that each channelâs feature map is assigned a weight, which represents the correlation between the channel and the key information. The larger the weight, the higher the correlation. In this way, it simulates the brain signal processing mechanism of human vision and filters useful channel information through the weight. The SK attention mechanism introduces a 5*5 spatial dimension on the basis of SE to fuse feature channels, as shown in subgraph (b) of <xref ref-type="fig" rid="fig_2">Figure 2</xref>. The fusion feature map from two different scales captures attention through a global pooling and a full convolution. Then it assigns weights by introducing a softmax calculation to select adaptively different spatial scales of information without sacrificing the amount of computation. Although SKNet captures target features at different scales, it fuses feature maps before weight assignment and allocates the same attention weight to the feature map after the global pooling and the full convolution, thereby losing the interactive feature information from different scales and not adequately condensing the model's attention ability.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Schema of different attention modules</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2022/11/img_2PhooykHloLgfaDc.jpeg"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>2.3. The improved dual-selective attention module</title>
          
          <p style="text-indent: 10.2pt;">This paper proposes a new attention mechanism module named dual-selective, as shown in subgraph (c) of <xref ref-type="fig" rid="fig_2">Figure 2</xref>. The feature map extracted by Backbone is taken as the input. Our attention mechanism module performs feature fusion from two different scales of kernel sizes 3 and 5. The global pooling layer and the full convolution layer are used to fuse the weights of each channel. Then the two fused branches are superimposed. In order to adaptively adjust the multi-scale feature information and capture more accurate information from the target objects with different scales, we adopt a dilated convolution with a 3Ã3 kernel and a dilation size of 2 to fuse the output features. Therefore, our DS module can fuse more resolution information in the convolutional feature map from different attention weights, and deepen the texture information of the feature map from different scales by the secondary weight calculation. It is conducive to paying more accurate attention to the feature information of the image texture.</p><p style="text-indent: 10.2pt;">In subgraph (c) of <xref ref-type="fig" rid="fig_2">Figure 2</xref>, the computational steps can be mathematically expressed as follows. The input of the channel attention block is a feature map <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>x</mi>
    <mo>â</mo>
    <msup>
      <mi>R</mi>
      <mrow data-mjx-texclass="ORD">
        <mi>W</mi>
        <mi>H</mi>
        <mi>C</mi>
        <mo>â</mo>
        <mo>â</mo>
      </mrow>
    </msup>
  </math>
</inline-formula>, in which W, H and C are the width, the height and the channel, respectively [<xref ref-type="bibr" rid="ref_26">26</xref>]. The feature map $x<inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>i</mi>
    <mi>s</mi>
    <mi>c</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>d</mi>
    <mi>u</mi>
    <mi>c</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>d</mi>
    <mi>w</mi>
    <mi>i</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>t</mi>
    <mi>w</mi>
    <mi>o</mi>
    <mi>t</mi>
    <mi>r</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>s</mi>
    <mi>f</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mi>m</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>s</mi>
    <mi>w</mi>
    <mi>i</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>k</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>n</mi>
    <mi>e</mi>
    <mi>l</mi>
    <mi>s</mi>
    <mi>i</mi>
    <mi>z</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>a</mi>
    <mi>n</mi>
    <mi>d</mi>
    <mi>w</mi>
    <mi>h</mi>
    <mi>o</mi>
    <mi>s</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>u</mi>
    <mi>l</mi>
    <mi>t</mi>
    <mi>s</mi>
    <mi>a</mi>
    <mi>r</mi>
    <mi>e</mi>
    <mi>r</mi>
    <mi>e</mi>
    <mi>p</mi>
    <mi>r</mi>
    <mi>e</mi>
    <mi>s</mi>
    <mi>e</mi>
    <mi>n</mi>
    <mi>t</mi>
    <mi>e</mi>
    <mi>d</mi>
    <mi>a</mi>
    <mi>s</mi>
    <mi>e</mi>
    <mi>m</mi>
    <mi>x</mi>
    <mi>e</mi>
    <mi>m</mi>
    <mi>s</mi>
    <mi>u</mi>
    <mi>b</mi>
    <mi>e</mi>
    <mi>m</mi>
    <mi>c</mi>
    <mi>e</mi>
    <mi>m</mi>
    <mi>s</mi>
    <mi>u</mi>
    <mi>b</mi>
    <mi>T</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>e</mi>
    <mi>m</mi>
    <mi>x</mi>
    <mi>e</mi>
    <mi>m</mi>
    <mi>s</mi>
    <mi>u</mi>
    <mi>b</mi>
    <mi>e</mi>
    <mi>m</mi>
    <mi>c</mi>
    <mi>e</mi>
    <mi>m</mi>
    <mi>s</mi>
    <mi>u</mi>
    <mi>b</mi>
    <mi>i</mi>
    <mi>s</mi>
    <mi>e</mi>
    <mi>m</mi>
    <mi>b</mi>
    <mi>e</mi>
    <mi>d</mi>
    <mi>t</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>g</mi>
    <mi>l</mi>
    <mi>o</mi>
    <mi>b</mi>
    <mi>a</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>f</mi>
    <mi>o</mi>
    <mi>r</mi>
    <mi>m</mi>
    <mi>a</mi>
    <mi>t</mi>
    <mi>i</mi>
    <mi>o</mi>
    <mi>n</mi>
    <mi>b</mi>
    <mi>y</mi>
    <mi>a</mi>
    <mi>g</mi>
    <mi>l</mi>
    <mi>o</mi>
    <mi>b</mi>
    <mi>a</mi>
    <mi>l</mi>
    <mi>p</mi>
    <mi>o</mi>
    <mi>o</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>g</mi>
    <mi>T</mi>
    <mi>h</mi>
    <mi>e</mi>
    <mi>g</mi>
    <mi>l</mi>
    <mi>o</mi>
    <mi>b</mi>
    <mi>a</mi>
    <mi>l</mi>
    <mi>p</mi>
    <mi>o</mi>
    <mi>o</mi>
    <mi>l</mi>
    <mi>i</mi>
    <mi>n</mi>
    <mi>g</mi>
    <mn>3</mn>
    <mn>5</mn>
    <mo>,</mo>
    <mo>&amp;lt;</mo>
    <mo>&amp;gt;</mo>
    <mo>&amp;lt;</mo>
    <mo>&amp;gt;&amp;lt;</mo>
    <mo>&amp;gt;&amp;lt;</mo>
    <mo>&amp;gt;</mo>
    <mo>&amp;lt;</mo>
    <mo>&amp;gt;&amp;lt;</mo>
    <mo>&amp;gt;</mo>
    <mo>.</mo>
    <mo>&amp;lt;</mo>
    <mo>&amp;gt;</mo>
    <mo>&amp;lt;</mo>
    <mo>&amp;gt;&amp;lt;</mo>
    <mo>&amp;gt;&amp;lt;</mo>
    <mo>&amp;gt;</mo>
    <mo>&amp;lt;</mo>
    <mo>&amp;gt;&amp;lt;</mo>
    <mo>&amp;gt;</mo>
    <mo>.</mo>
    <mrow data-mjx-texclass="ORD">
      <mo>/</mo>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo>/</mo>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo>/</mo>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo>/</mo>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo>/</mo>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo>/</mo>
    </mrow>
  </math>
</inline-formula>F_P\left(x_c\right)$ is calculated as:</p>
          
            <disp-formula>
              <label>(1)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <msub>
                  <mi>F</mi>
                  <mi>P</mi>
                </msub>
                <msub>
                  <mi>x</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mi>i</mi>
                    <mi>j</mi>
                  </mrow>
                </msub>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <msub>
                    <mi>x</mi>
                    <mi>c</mi>
                  </msub>
                </mrow>
                <mo>=</mo>
                <mfrac>
                  <mn>1</mn>
                  <mrow>
                    <mi>H</mi>
                    <mi>W</mi>
                    <mo>Ã</mo>
                  </mrow>
                </mfrac>
                <msubsup>
                  <mi mathvariant="normal">Î£</mi>
                  <mi>W</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                </msubsup>
                <msubsup>
                  <mi mathvariant="normal">Î£</mi>
                  <mi>H</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mi>j</mi>
                    <mo>=</mo>
                    <mn>1</mn>
                  </mrow>
                </msubsup>
              </math>
            </disp-formula>
          
          <p style="text-indent: 10.2pt;">Further, the output feature map <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>x</mi>
      <mi>o</mi>
    </msub>
    <mo>â</mo>
    <msup>
      <mi>R</mi>
      <mrow data-mjx-texclass="ORD">
        <mn>1</mn>
        <mn>1</mn>
        <mo>â</mo>
        <mo>â</mo>
        <mi>C</mi>
      </mrow>
    </msup>
  </math>
</inline-formula> after the global pooling is the one-dimensional vector. The feature map <italic>x</italic><sub><italic>0 </italic></sub>achieves precise and adaptive selections by a full convolution. The full convolution <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>F</mi>
      <mi>c</mi>
    </msub>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">(</mo>
      <mo data-mjx-texclass="CLOSE">)</mo>
      <msub>
        <mi>x</mi>
        <mi>o</mi>
      </msub>
    </mrow>
  </math>
</inline-formula> is calculated is:</p>
          
            <disp-formula>
              <label>(2)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <msub>
                  <mi>F</mi>
                  <mi>c</mi>
                </msub>
                <msub>
                  <mi>x</mi>
                  <mi>o</mi>
                </msub>
                <msub>
                  <mi>W</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mi>i</mi>
                    <mi>j</mi>
                  </mrow>
                </msub>
                <msub>
                  <mi>B</mi>
                  <mi>i</mi>
                </msub>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <msub>
                    <mi>x</mi>
                    <mi>o</mi>
                  </msub>
                </mrow>
                <mo>=</mo>
                <mo>+</mo>
              </math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>W</mi>
      <mrow data-mjx-texclass="ORD">
        <mi>i</mi>
        <mi>j</mi>
      </mrow>
    </msub>
    <mo>â</mo>
    <msup>
      <mi>R</mi>
      <mrow data-mjx-texclass="ORD">
        <mi>i</mi>
        <mi>j</mi>
        <mo>â</mo>
      </mrow>
    </msup>
  </math>
</inline-formula> is the weight matrix and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>B</mi>
    <mo>â</mo>
    <msup>
      <mi>R</mi>
      <mrow data-mjx-texclass="ORD">
        <mi>i</mi>
        <mo>â</mo>
        <mn>1</mn>
      </mrow>
    </msup>
  </math>
</inline-formula> is the one-dimensional bias [<xref ref-type="bibr" rid="ref_27">27</xref>].</p>
        </sec>
      
      
        <sec>
          
            <title>2.4. Dual-selective channel attention network</title>
          
          <p style="text-indent: 10.2pt;">Dual-selective channel attention network (DSNet) serves as the backbone of the improved Faster R-CNN, which is a fusion network of ResNet50 and DS module. As shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>, the first stage has a relatively simple structure, consisting of a 7*7 convolutional kernel and a 3*3 max pooling layer. The second stage consists of a DS module and a residual layer. Stages 3 and 4 consist of one residual layer, respectively. The last stage consists of a residual layer and a subsequent DS module. Each residual layer is composed of 1*1, 3*1, and 1*1 convolution kernels in sequence. In this way, DSNet improves the network's ability to extract texture features and can use transfer learning to improve the stability and training speed of the model.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Pipeline of the proposed DSNet</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2022/11/img_Sq4kNc1vGJZJ1eDb.jpeg"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Experiments</title>
      <p style="text-indent: 10.2pt;">We used axial unenhanced lumbar spine CT images (L1 - L4) as input data and the DXA report of each lumbar spine as reference data. All data were manually annotated for the entire lumbar vertebrae and labeled with the bone condition category of osteoporosis, osteopenia and normal. The training set and test set were divided at a ratio of 4:1. The training set is used to train the network model. The performance of the network model is evaluated by the test set.</p>
      
        <sec>
          
            <title>3.1. Subjects and dataset description</title>
          
          <p style="text-indent: 10.2pt;">This study was approved by the ethical board of the affiliated hospital of Hebei university. We retrospectively collected personal data of lumbar spine CT scan (full bone window) and DXA examination between May 2016 and April 2020 from the Department of Orthopedics, Affiliated Hospital of Hebei University, with a total of 50,296 lumbar spine CT images and corresponding DXA detection results of 1,132 patients. Moreover, we conducted screening and disorientation processing for patient privacy. All patients underwent single-photon emission computed tomography (CT) scans of the lumbar spine (from L1 to L4). Their gender, height, and weight were recorded, and the original images of the cases were in DICOM format. For all data, we performed rigorous data cleaning by learning from medical professionals: (1) Exclude images of poor quality, such as artifacts produced by spinal implants (instruments) implanted during surgery; (2) Exclude non-lumbar regions or images showing lumbar L1-L4 insufficiency, leaving only the lumbar cones Complete images; (3) Image data that has undergone lumbar spine surgery, such as bone nails and bone cement filling; (4) Individuals with a history of fractures, history of spinal surgery, primary or metastatic tumors, bone hyperplasia, or vertebral bodies are excluded. Finally, we achieved 4805 images (including any of L1, L2, L3 or L4 for the training dataset; including all L1, L2, L3 and L4 for the test dataset) of 133 patients (men, n=29, women, n=104). The whole vertebral body (including cortical bone and cancellous bone) was manually annotated by experienced physicians. </p><p style="text-indent: 10.2pt;">According to the criteria for diagnosis of osteoporosis defined by WHO [<xref ref-type="bibr" rid="ref_28">28</xref>]. We divided the subjects into three groups, namely the normal group with T value â¥ -1.0 SD, the osteopenia group with -2.5 SD &amp;amp;lt; T value &amp;amp;lt; -1.0 SD, and the osteoporosis group with T value â¤ -2.5 SD. Finally, we divide all the data at a ratio of 4:1, one part is used for the training of network parameters, namely the training set, which includes 3,844 CT images, and the other part is used to evaluate the generalization performance of the network, namely the test set, which includes 961 CT images.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Experimental details</title>
          
          <p style="text-indent: 10.2pt;">A new convolutional neural network should be firstly initialized the weights. Otherwise, the activation layer function will fail to output during the training of deep neural networks, which will result in an explosion of the loss gradient and a prolonged convergence of the network. Therefore, we use the weight parameters learned from the COCO dataset as the initial network parameters, which can avoid effectively the problem of overfitting during the training process and improve the stability and training speed of the model.</p><p style="text-indent: 10.2pt;">Deep learning model training was performed on a computer equipped with a core I7-7800x 3.5-GHz central processing unit, 256 GB memory, and a 2080 Ti graphics processing unit. We used the programming language of python 3.7 and the deep learning framework of PyTorch. The loss function of the model was the dice loss, SGD was used as the optimizer of the model, the learning rate was set to 0.01, the batch size was 9, and the training epoch was 100. </p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Evaluation indicators</title>
          
          <p style="text-indent: 10.2pt;">To evaluate the performance of the proposed model, the metrics of mean average precision (Map), recall, accuracy, and frame per second (Fps) are adopted for object detection. The vertebral bodies of the lumbar CT images (IOU â¥ 0.5) are detected and classified correctly as correct predictions, otherwise, it is considered incorrect predictions. </p><p style="text-indent: 10.2pt;">Mean average precision (Map) is the proportion of the predictions for all categories that successfully predicted the true target, which is defined as:</p>
          
            <disp-formula>
              <label>(3)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>M</mi>
                <mi>a</mi>
                <mi>p</mi>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                  </mrow>
                  <mrow>
                    <mi>N</mi>
                    <mi>T</mi>
                    <mi>P</mi>
                    <mi>F</mi>
                    <mi>P</mi>
                    <mo stretchy="false">(</mo>
                    <mo>+</mo>
                    <mo stretchy="false">)</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          <p>Recall rate (Recall) represents the ability of the model to find all relevant targets, which is the number of true targets in the results predicted by the model. It is calculated as follows:</p>
          
            <disp-formula>
              <label>(4)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>Recall</mi>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                  </mrow>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                    <mi>F</mi>
                    <mi>N</mi>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          <p>Accuracy (Acc) represents the proportion of all predictions that the model predicts correctly.</p>
          
            <disp-formula>
              <label>(5)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>A</mi>
                <mi>c</mi>
                <mi>c</mi>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                  </mrow>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                    <mi>T</mi>
                    <mi>N</mi>
                    <mi>F</mi>
                    <mi>N</mi>
                    <mo>+</mo>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          <p>Frame per second (Fps) is defined as the number of pictures processed by the model per second. The higher the frame per second, the faster the model processing speed, and vice versa.</p>
          
            <disp-formula>
              <label>(6)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>F</mi>
                <mi>p</mi>
                <mi>s</mi>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mi>F</mi>
                    <mi>n</mi>
                  </mrow>
                  <mi>S</mi>
                </mfrac>
              </math>
            </disp-formula>
          
          <p>where, <italic>TP</italic> is the number of predicted bounding boxes with correct classification and correct bounding box coordinates. <italic>FP</italic> is the number of predicted bounding box classification errors or bounding box coordinates that do not meet the standard. <italic>FN</italic> is the number of ground truths falsely detected. Number (<italic>N</italic>) represents the number of categories for classification. <italic>Fn</italic> is the number of picture frames processed by the network. <italic>S</italic> represents the time.</p>
        </sec>
      
    </sec>
    <sec sec-type="results">
      <title>4. Results</title>
      <p>In this study, we use 1,605 CT images of osteoporosis, 1,600 CT images of osteopenia, and 1,600 CT images of normal bone density for analysis. To estimate the detection performance of the proposed attention mechanism and the novel DSNet, we compared the effects of ResNet50, VGG16, and MobileNet as the backbone under different attention mechanisms.</p>
      
        <sec>
          
            <title>4.1. Comparison of different attention modules based on resnet50</title>
          
          <p style="text-indent: 10.2pt;">We adopted Faster R-CNN as the main framework and used the backbone with different attentional mechanisms to extract features. We performed an ablation experiment using the modules of SK, SE, and our DS based on the traditional ResNet50 without the integration of the channel attention algorithm. The metrics of Map, Recall, Acc with the IOU of 0.5, and Fps were used to evaluate and compare the detection performances among different methods. As shown in <xref ref-type="table" rid="table_1">Table 1</xref>, ResNet50 integrated with the attention module achieved increased Map and Recall, indicating that the attention module can improve the networkâs performance. The integration of DS attention into ResNet50 achieved the best performance and the detection speed is the closest to the ResNet50, reaching a Map of 83.4%, a Recall of 90.0%, an Acc of 75.9%, and a Fps of 0.1846, indicating that our DS module has better performance on network feature extraction. Meanwhile, our DS module improved the shortcomings of other attention mechanisms on Acc, with a 1.8% improvement over Resnet50. Therefore, our model was effective at fusing multiscale attention weights.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Detection results of the resnet50 with different attention modules</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Method</p></td><td colspan="1" rowspan="1"><p>Backbone</p></td><td colspan="1" rowspan="1"><p>Map (%)</p></td><td colspan="1" rowspan="1"><p>Recall (%)</p></td><td colspan="1" rowspan="1"><p>Acc (%)</p></td><td colspan="1" rowspan="1"><p>Fps (f/s)</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>ResNet50</p></td><td colspan="1" rowspan="1"><p>78.2</p></td><td colspan="1" rowspan="1"><p>88.1</p></td><td colspan="1" rowspan="1"><p>74.1</p></td><td colspan="1" rowspan="1"><p>0.1849</p></td></tr><tr><td colspan="1" rowspan="1"><p>Faster R-CNN</p></td><td colspan="1" rowspan="1"><p>ResNet50+SE</p></td><td colspan="1" rowspan="1"><p>80.4</p></td><td colspan="1" rowspan="1"><p>88.5</p></td><td colspan="1" rowspan="1"><p>73.7</p></td><td colspan="1" rowspan="1"><p>0.1834</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>ResNet50+SK</p></td><td colspan="1" rowspan="1"><p>81.4</p></td><td colspan="1" rowspan="1"><p>89.2</p></td><td colspan="1" rowspan="1"><p>73.8</p></td><td colspan="1" rowspan="1"><p>0.1829</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>ResNet50+DS (ours)</p></td><td colspan="1" rowspan="1"><p>83.4</p></td><td colspan="1" rowspan="1"><p>90.0</p></td><td colspan="1" rowspan="1"><p>75.9</p></td><td colspan="1" rowspan="1"><p>0.1846</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>The visual detection results of different backbone networks</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2022/11/img_l__biwuNacoDoSiO.jpeg"/>
            </fig>
          
          <p>To evaluate the detection performance of the network more intuitively, we performed a visual analysis to compare the results of different modules. <xref ref-type="fig" rid="fig_4">Figure 4</xref> shows the detection results of different backbone networks with the vertebraeâs boundary and a single probability value. The first column is the ground truth of normal bone density, osteopenia, and osteoporosis. The remaining columns are the visual detection results of different backbone networks, followed by Resnet50, Resnet50+SE, Resnet50+SK, and Resnet50+DS. All backbone networks accurately identified the normal class. But because the texture feature of osteopenia is more difficult to distinguish than another two classes, the Resnet50 and Resnet50+SE mistakenly identified osteopenia as normal. Meanwhile, the Resnet50 mistakenly identified osteoporosis as osteopenia, indicating that the attention module can improve the networkâs detection performance. This can seriously affect the clinician's diagnosis and delay the patient's treatment. For the three categories, our Resnet50+DS module can accurately identify with the highest confidence. It can be seen that the ResNet50+DS has a higher detection efficiency for improving the networkâs attention ability.</p>
        </sec>
      
      
        <sec>
          
            <title>4.2. Comparison of our module network with other backbone networks</title>
          
          <p style="text-indent: 10.2pt;">To verify the applicability of the proposed DS attention module, we tested the performance of different backbone networks with and without the fusion DS attention module. To be fair, these three sets of comparative experiments were tested under the same training conditions and the same data set (4,805 images of 133 patients). As shown in <xref ref-type="table" rid="table_2">Table 2</xref>, ResNet50 with the DS attention module has improvements of 0.54%, 0.19%, and 0.18% in Map, Recall, and Acc with a 0.0003f/s decrease in Fps. We further compared the performance with or without the DS attention module on VGG [<xref ref-type="bibr" rid="ref_29">29</xref>], and MobileNet [<xref ref-type="bibr" rid="ref_30">30</xref>] and found that the integration of the DS attention module would also reduce their speed. The VGG16 integrated with the DS attention module achieved increased Map, Recall, and Acc. The integration of DS attention into MobileNet achieved better performance on Map and Recall with a 0.3% reduction in Acc. Among ResNet50 with the integration DS module has the best improvement. In general, our proposed DS module is also applicable to other backbones and the fusion method of ResNet50 had the best effect, which can effectively improve the feature extraction ability of the network.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Detection results of the faster R-CNN with different backbones</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Method</p></td><td colspan="1" rowspan="1"><p>Backbone</p></td><td colspan="1" rowspan="1"><p>DS</p></td><td colspan="1" rowspan="1"><p>Map (%)</p></td><td colspan="1" rowspan="1"><p>Recall (%)</p></td><td colspan="1" rowspan="1"><p>Acc (%)</p></td><td colspan="1" rowspan="1"><p>Fps (f/s)</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="2"><p>ResNet50</p></td><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>78.2</p></td><td colspan="1" rowspan="1"><p>88.1</p></td><td colspan="1" rowspan="1"><p>74.1</p></td><td colspan="1" rowspan="1"><p>0.1849</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>â</p></td><td colspan="1" rowspan="1"><p>83.4</p></td><td colspan="1" rowspan="1"><p>90.0</p></td><td colspan="1" rowspan="1"><p>75.9</p></td><td colspan="1" rowspan="1"><p>0.1846</p></td></tr><tr><td colspan="1" rowspan="1"><p>Faster</p><p>R-CNN</p></td><td colspan="1" rowspan="2"><p>VGG16</p></td><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>80.0</p></td><td colspan="1" rowspan="1"><p>88.5</p></td><td colspan="1" rowspan="1"><p>71.1</p></td><td colspan="1" rowspan="1"><p>0.1873</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>â</p></td><td colspan="1" rowspan="1"><p>80.2</p></td><td colspan="1" rowspan="1"><p>89.9</p></td><td colspan="1" rowspan="1"><p>71.8</p></td><td colspan="1" rowspan="1"><p>0.1870</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="2"><p>MobileNet</p></td><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>80.9</p></td><td colspan="1" rowspan="1"><p>86.4</p></td><td colspan="1" rowspan="1"><p>73.0</p></td><td colspan="1" rowspan="1"><p>0.1856</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>â</p></td><td colspan="1" rowspan="1"><p>83.0</p></td><td colspan="1" rowspan="1"><p>88.1</p></td><td colspan="1" rowspan="1"><p>72.7</p></td><td colspan="1" rowspan="1"><p>0.1846</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>4.3. Comparison of our method with other methods</title>
          
          <p style="text-indent: 10.2pt;"><xref ref-type="table" rid="table_3">Table 3</xref> illustrated the comparative results between our improved network method and the other osteoporosis prediction networks. Our method produced remarkable recall and detection categories on the biggest number of images. The traditional osteoporosis prediction methods had the results of the two classifications. Our method added a detection category of osteopenia to alert patients before they reach the severity of osteoporosis. As we all know, as the detection category increases, the detection effect of the network decreases. Therefore, the Acc of our proposed method was reduced than the other two-classification tasks. But our proposed method achieved the highest Recall of 89.2% with a tri-classification identification of osteoporosis, osteopenia, and normal. (We have no the results of AUC due to the tri-classification detection task.) Thus, the overall detection and classification effect of our method achieved better performance and provide value for the prediction of three categories of bone status in the future [<xref ref-type="bibr" rid="ref_31">31</xref>].</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Comparative result of the proposed method with other methods</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Methods</p></td><td colspan="1" rowspan="1"><p>Number of images</p></td><td colspan="1" rowspan="1"><p>Category</p></td><td colspan="1" rowspan="1"><p>Acc (%)</p></td><td colspan="1" rowspan="1"><p>AUC (%)</p></td><td colspan="1" rowspan="1"><p>Recall (%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Machine learning [<xref ref-type="bibr" rid="ref_32">32</xref>]</p></td><td colspan="1" rowspan="1"><p>120</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>83.0</p></td><td colspan="1" rowspan="1"><p>-</p></td></tr><tr><td colspan="1" rowspan="1"><p>LeNet [<xref ref-type="bibr" rid="ref_33">33</xref>]</p></td><td colspan="1" rowspan="1"><p>4000</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>88.4</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>-</p></td></tr><tr><td colspan="1" rowspan="1"><p>3D U-net [<xref ref-type="bibr" rid="ref_14">14</xref>]</p></td><td colspan="1" rowspan="1"><p>200</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>92.7</p></td><td colspan="1" rowspan="1"><p>85.7</p></td></tr><tr><td colspan="1" rowspan="1"><p>VGGnet16+BCR [<xref ref-type="bibr" rid="ref_16">16</xref>]</p></td><td colspan="1" rowspan="1"><p>334</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>71.0</p></td><td colspan="1" rowspan="1"><p>74.0</p></td><td colspan="1" rowspan="1"><p>-</p></td></tr><tr><td colspan="1" rowspan="1"><p>ENSEMBLE [<xref ref-type="bibr" rid="ref_31">31</xref>]</p></td><td colspan="1" rowspan="1"><p>247</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>92.0</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>88.0</p></td></tr><tr><td colspan="1" rowspan="1"><p>Faster R-CNN+ResNet50+DS (Ours)</p></td><td colspan="1" rowspan="1"><p>4805</p></td><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p>75.9</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>90.0</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p style="text-indent: 10.2pt;">This study proposes a new deep-learning method for the automatic detection of lumbar vertebrae and classification of bone status using 4,805 lumbar CT images of 133 patients. We train the network model with a large amount of clinical data and compare the test results with DXA diagnosis results. Meanwhile, the network model has been improved and optimized. At present, the prediction of osteoporosis is based on the measurement of bone density. However, bone density can only reflect about 70% of the degree of osteoporosis. The geometric characteristics of bone microstructure and the heterogeneity of density structure also have a certain impact on the diagnosis of osteoporosis. Therefore, the thickness of cortical bone and the thinning of cancellous bone are the basis for doctors to judge whether there is osteoporosis. Different from previous studies, our model is highly close to the doctor's diagnostic level. The entire lumbar vertebral body including cortical and cancellous bones is annotated as regions of interest, as the dataset for model training.</p><p style="text-indent: 10.2pt;">In this paper, we propose a new backbone integrating ResNet50 and DS module based on Faster R-CNN. In this network, the lumbar vertebral body is identified and detected first, and the detection results are marked in the form of rectangular boxes with predicted confidence. Then the vertebral body structure is extracted to class the categories of osteoporosis, osteopenia, and normal. The proposed module improves the performance of feature extraction and pays more attention to the texture features of cortical and cancellous bones. Compared with other attention mechanisms, our module achieves an improvement on Map, Recall, and Acc reaching 83.4% and 90.0%, and 75.9% respectively. The feasibility, effectiveness, and compatibility of the model are also verified.</p><p style="text-indent: 10.2pt;">This paper can expand the application of artificial intelligence-assisted diagnosis systems and contribute to the clinical identification of osteoporosis. To a certain extent, it can solve the problem that fracture osteoporosis cannot be detected in time to improve the treatment rate of osteoporosis so that it has important theoretical value and clinical significance.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p style="text-indent: 10.2pt;">L.X. and Y.H. devised the project and drafted the manuscript; G.Q. and S.W. carried out the data collection and analyses; C.L., Z.X. and S.L. participated in the design of the study and Z.W., W.G. and K.Y. contributed to analyzing the results of the experiment. All authors have read and agreed to the published version of the manuscript.</p>
    </notes>
    <notes>
      <title>Funding</title>
      <p>This paper was funded by Hebei University (Grant No.: DXK201914); the President of Hebei University (Grant No.: XZJJ201914); the Post-graduateâs Innovation Fund Project of Hebei University (Grant No.: HBU2022SS003); and the Special Project for Cultivating College Students' Scientific and Technological Innovation Ability in Hebei Province (Grant No.: 22E50041D).</p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p style="text-indent: 10.2pt;">We declare that we do not have any commercial or associative interest that represents a conflict of interest in connection with the work submitted.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names/>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names/>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names/>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Sosa</surname>
              <given-names/>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Mo</surname>
              <given-names/>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names/>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Li</surname>
              <given-names/>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Fully automated radiomic screening pipeline for osteoporosis and abnormal bone density with a deep learning-based segmentation using a short lumbar mDixon sequence</article-title>
          <source>Quant. Imaging. Med. Surg.</source>
          <year>2022</year>
          <volume>12</volume>
          <issue>2</issue>
          <page-range>1198-1213</page-range>
          <fpage>1198</fpage>
          <lpage>1213</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.21037/qims-21-587</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Snodgrass</surname>
              <given-names/>
            </name>
            <name>
              <surname>Zou</surname>
              <given-names/>
            </name>
            <name>
              <surname>Gruntmanis</surname>
              <given-names/>
            </name>
            <name>
              <surname>Gitajn</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Osteoporosis diagnosis, management, and referral practice after fragility fractures</article-title>
          <source>Curr. Osteoporos. Rep.</source>
          <year>2022</year>
          <volume>20</volume>
          <issue>3</issue>
          <page-range>163-169</page-range>
          <fpage>163</fpage>
          <lpage>169</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/s11914-022-00730-1</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Ruan</surname>
              <given-names/>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names/>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Diagnostic and gradation model of osteoporosis based on improved deep u-net network</article-title>
          <source>J. Med. Syst.</source>
          <year>2020</year>
          <volume>44</volume>
          <issue>1</issue>
          <page-range>15-23</page-range>
          <fpage>15</fpage>
          <lpage>23</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/s10916-019-1502-3</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Aouache</surname>
              <given-names/>
            </name>
            <name>
              <surname>Hussain</surname>
              <given-names/>
            </name>
            <name>
              <surname>Zulkifley</surname>
              <given-names/>
            </name>
            <name>
              <surname>Zaki</surname>
              <given-names/>
            </name>
            <name>
              <surname>Husain</surname>
              <given-names/>
            </name>
            <name>
              <surname>Hamid</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Anterior osteoporosis classification in cervical vertebrae using fuzzy decision tree</article-title>
          <source>Multimedia Tools Appl.</source>
          <year>2018</year>
          <volume>77</volume>
          <issue>3</issue>
          <page-range>4011-4045</page-range>
          <fpage>4011</fpage>
          <lpage>4045</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/MWSCAS.2003.1562304</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Devikanniga</surname>
              <given-names/>
            </name>
            <name>
              <surname>Raj</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Classification of osteoporosis by artificial neural network based on monarch butterfly optimisation algorithm</article-title>
          <source>Healthcare Technol Lett.</source>
          <year>2018</year>
          <volume>5</volume>
          <issue>2</issue>
          <page-range>70-75</page-range>
          <fpage>70</fpage>
          <lpage>75</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1049/htl.2017.0059</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pedoia</surname>
              <given-names/>
            </name>
            <name>
              <surname>Caliva</surname>
              <given-names/>
            </name>
            <name>
              <surname>Kazakia</surname>
              <given-names/>
            </name>
            <name>
              <surname>Burghardt</surname>
              <given-names/>
            </name>
            <name>
              <surname>Majumdar</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Augmenting osteoporosis imaging with machine learning</article-title>
          <source>Curr. Osteoporos. Rep.</source>
          <year>2021</year>
          <volume>19</volume>
          <issue>6</issue>
          <page-range>699-709</page-range>
          <fpage>699</fpage>
          <lpage>709</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/s11914-021-00701-y</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Geng</surname>
              <given-names/>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Ding</surname>
              <given-names/>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Ye</surname>
              <given-names/>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Ruan</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Deep learning-based self-efficacy X-ray images in the evaluation of Rheumatoid Arthritis combined with osteoporosis nursing</article-title>
          <source>Sci. Programming-Neth.</source>
          <year>2021</year>
          <volume>2021</volume>
          <page-range>1-8</page-range>
          <fpage>1</fpage>
          <lpage>8</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1155/2021/9959617</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lee</surname>
              <given-names/>
            </name>
            <name>
              <surname>Jung</surname>
              <given-names/>
            </name>
            <name>
              <surname>Ryu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Shin</surname>
              <given-names/>
            </name>
            <name>
              <surname>Choi</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Evaluation of transfer learning with deep convolutional neural networks for screening osteoporosis in dental panoramic radiographs</article-title>
          <source>Jpn. J. Clin. Med.</source>
          <year>2020</year>
          <volume>9</volume>
          <issue>2</issue>
          <page-range>392-400</page-range>
          <fpage>392</fpage>
          <lpage>400</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.3390/jcm9020392</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Poole</surname>
              <given-names/>
            </name>
            <name>
              <surname>Chappell</surname>
              <given-names/>
            </name>
            <name>
              <surname>Clark</surname>
              <given-names/>
            </name>
            <name>
              <surname>Fleming</surname>
              <given-names/>
            </name>
            <name>
              <surname>Shepstone</surname>
              <given-names/>
            </name>
            <name>
              <surname>Turmezei</surname>
              <given-names/>
            </name>
            <name>
              <surname>Wagner</surname>
              <given-names/>
            </name>
            <name>
              <surname>Willoughby</surname>
              <given-names/>
            </name>
            <name>
              <surname>Kaptoge</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>PHOENIX (Picking up hidden osteoporosis effectively during normal CT imaging without additional X-rays): Protocol for a randomised, multicentre feasibility study</article-title>
          <source>BMJ Open.</source>
          <year>2022</year>
          <volume>12</volume>
          <issue>1</issue>
          <page-range>1-8</page-range>
          <fpage>1</fpage>
          <lpage>8</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1136/bmjopen-2021-050343</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Li</surname>
              <given-names/>
            </name>
            <name>
              <surname>Li</surname>
              <given-names/>
            </name>
            <name>
              <surname>Li</surname>
              <given-names/>
            </name>
            <name>
              <surname>Cai</surname>
              <given-names/>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names/>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>CNN-based qualitative detection of bone mineral density via diagnostic CT slices for osteoporosis screening</article-title>
          <source>Osteoporosis Int.</source>
          <year>2021</year>
          <volume>32</volume>
          <issue>5</issue>
          <page-range>971-979</page-range>
          <fpage>971</fpage>
          <lpage>979</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/s00198-020-05673-w</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Xiao</surname>
              <given-names/>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names/>
            </name>
            <name>
              <surname>Han</surname>
              <given-names/>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Prediction of trabecular bone architectural features by deep learning models using simulated DXA images</article-title>
          <source>Bone Rep.</source>
          <year>2020</year>
          <volume>13</volume>
          <page-range>1-8</page-range>
          <fpage>1</fpage>
          <lpage>8</lpage>
          <pub-id pub-id-type="doi">https://doi.org/ 10.1016/j.bonr.2020.100295</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lim</surname>
              <given-names/>
            </name>
            <name>
              <surname>il Ha</surname>
              <given-names/>
            </name>
            <name>
              <surname>Park</surname>
              <given-names/>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Comparison of the diagnostic performance of CT Hounsfield unit histogram analysis and dual-energy X-ray absorptiometry in predicting osteoporosis of the femur</article-title>
          <source>Eur. Radiolo.</source>
          <year>2019</year>
          <volume>29</volume>
          <issue>4</issue>
          <page-range>1831-1840</page-range>
          <fpage>1831</fpage>
          <lpage>1840</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/s00330-018-5728-0</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>T.</given-names>
              <surname>Ho-Le</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Eisman</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Nguyen</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Nguyen</surname>
            </name>
          </person-group>
          <article-title>Prediction of hip fracture in post-menopausal women using artificial neural network approach</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>Annual International Conference of the IEEE Engineering in Medicine and Biology Society</conf-name>
          <conf-acronym>EMBC 2017</conf-acronym>
          <conf-loc>Jeju, South Korea</conf-loc>
          <conf-date>July 11-15, 2017</conf-date>
          <year>2017</year>
          <page-range>4207-4210</page-range>
          <fpage>4207</fpage>
          <lpage>4210</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/EMBC.2017.8037784.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pan</surname>
              <given-names/>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names/>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names/>
            </name>
            <name>
              <surname>Cui</surname>
              <given-names/>
            </name>
            <name>
              <surname>Cheng</surname>
              <given-names/>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Automatic opportunistic osteoporosis screening using low-dose chest computed tomography scans obtained for lung cancer screening</article-title>
          <source>Eur. Radiolo.</source>
          <year>2020</year>
          <volume>30</volume>
          <issue>7</issue>
          <page-range>4107-4116</page-range>
          <fpage>4107</fpage>
          <lpage>4116</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/s00330-020-06679-y</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yasaka</surname>
              <given-names/>
            </name>
            <name>
              <surname>Akai</surname>
              <given-names/>
            </name>
            <name>
              <surname>Kunimatsu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Kiryu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Abe</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Prediction of bone mineral density from computed tomography: Application of deep learning with a convolutional neural network</article-title>
          <source>Eur. Radiolo.</source>
          <year>2020</year>
          <volume>30</volume>
          <issue>6</issue>
          <page-range>3549-3557</page-range>
          <fpage>3549</fpage>
          <lpage>3557</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/s00330-020-06677-0</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lee</surname>
              <given-names/>
            </name>
            <name>
              <surname>Choe</surname>
              <given-names/>
            </name>
            <name>
              <surname>Kang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Yoon</surname>
              <given-names/>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>The exploration of feature extraction and machine learning for predicting bone density from simple spine X-ray images in a Korean population</article-title>
          <source>Skeletal Radiolo.</source>
          <year>2020</year>
          <volume>49</volume>
          <issue>4</issue>
          <page-range>613-618</page-range>
          <fpage>613</fpage>
          <lpage>618</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/s00256-019-03342-6</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>G.</given-names>
              <surname>Gonzalez</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Washko</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Estepar</surname>
            </name>
          </person-group>
          <article-title>Deep learning for biomarker regression: Application to osteoporosis and emphysema on chest CT scans</article-title>
          <publisher-name>SPIE</publisher-name>
          <conf-name>Proceedings of SPIE-the International Society for Optical Engineering</conf-name>
          <conf-acronym>ISOE 2018</conf-acronym>
          <conf-loc>Houston, Texas, United States, March 2</conf-loc>
          <conf-date>2018</conf-date>
          <year>2018</year>
          <page-range>52-60</page-range>
          <fpage>52</fpage>
          <lpage>60</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1117/12.2293455.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names/>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names/>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names/>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names/>
            </name>
            <name>
              <surname>Li</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Detection of cervical cancer cells in whole slide images using deformable and global context aware faster RCNN-FPN</article-title>
          <source>Curr. Oncolo.</source>
          <year>2021</year>
          <volume>28</volume>
          <issue>5</issue>
          <page-range>3585-3601</page-range>
          <fpage>3585</fpage>
          <lpage>3601</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.3390/curroncol28050307</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>K.</given-names>
              <surname>He</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Ren</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Sun</surname>
            </name>
          </person-group>
          <article-title>Deep residual learning for image recognition</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition</conf-name>
          <conf-acronym>CVPR 2016</conf-acronym>
          <conf-loc>Las Vegas, NV, USA</conf-loc>
          <conf-date>June 27-30, 2016</conf-date>
          <year>2016</year>
          <page-range>770-778</page-range>
          <fpage>770</fpage>
          <lpage>778</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/CVPR.2016.90.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lecun</surname>
              <given-names/>
            </name>
            <name>
              <surname>Bottou</surname>
              <given-names/>
            </name>
            <name>
              <surname>Bengio</surname>
              <given-names/>
            </name>
            <name>
              <surname>Haffner</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Gradient-based learning applied to document recognition</article-title>
          <source>P. IEEE</source>
          <year>1998</year>
          <volume>86</volume>
          <issue>1</issue>
          <page-range>2278-2324</page-range>
          <fpage>2278</fpage>
          <lpage>2324</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/5.726791</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Krizhevsky</surname>
              <given-names/>
            </name>
            <name>
              <surname>Sutskever</surname>
              <given-names/>
            </name>
            <name>
              <surname>Hinton</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>ImageNet classification with deep convolutional neural networks</article-title>
          <source>Commun ACM.</source>
          <year>2017</year>
          <volume>60</volume>
          <issue>6</issue>
          <page-range>84-90</page-range>
          <fpage>84</fpage>
          <lpage>90</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1145/3065386</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Szegedy</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Jia</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Sermanet</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Reed</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Anguelov</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Rabinovich</surname>
            </name>
          </person-group>
          <article-title>Going deeper with convolutions</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</conf-name>
          <conf-acronym>CVPR 2015</conf-acronym>
          <conf-loc>Boston, MA</conf-loc>
          <conf-date>June 07-12, 2015</conf-date>
          <year>2015</year>
          <page-range>1-9</page-range>
          <fpage>1</fpage>
          <lpage>9</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/CVPR.2015.7298594.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names/>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names/>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Squeeze-and-excitation networks</article-title>
          <source>IEEE T. Pattern Anal.</source>
          <year>2019</year>
          <volume>42</volume>
          <issue>8</issue>
          <page-range>2011-2023</page-range>
          <fpage>2011</fpage>
          <lpage>2023</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/TPAMI.2019.2913372</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>X.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Hu</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Yang</surname>
            </name>
          </person-group>
          <article-title>Selective kernel networks</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>
          <conf-acronym>CVPR 2019</conf-acronym>
          <conf-loc>Beach, CA, USA</conf-loc>
          <conf-date>June 15-20, 2019</conf-date>
          <year>2019</year>
          <page-range>510-519</page-range>
          <fpage>510</fpage>
          <lpage>519</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/CVPR.2019.00060.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>Q.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Wu</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Zhu</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Zuo</surname>
            </name>
            <name>
              <given-names>Q.</given-names>
              <surname>Hu</surname>
            </name>
          </person-group>
          <article-title>ECA-Net: Efficient channel attention for deep convolutional neural networks</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>
          <conf-acronym>CVPR 2020</conf-acronym>
          <conf-loc>Seattle, WA, USA</conf-loc>
          <conf-date>June 13-19, 2020</conf-date>
          <year>2020</year>
          <page-range>11531-11539</page-range>
          <fpage>11531</fpage>
          <lpage>11539</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/CVPR42600.2020.01155.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lin</surname>
              <given-names/>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names/>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Network in network</article-title>
          <source>ArXiv.</source>
          <year>2014</year>
          <volume>1</volume>
          <pub-id pub-id-type="doi">https://doi.org/10.48550/arXiv.1312.4400</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>X.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Girshick</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Gupta</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>He</surname>
            </name>
          </person-group>
          <article-title>Non-local neural networks</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name>
          <conf-acronym>CVPR 2018</conf-acronym>
          <conf-loc>Salt Lake City, UT, USA</conf-loc>
          <conf-date>June 18-23, 2018</conf-date>
          <year>2018</year>
          <page-range>7794-7803</page-range>
          <fpage>7794</fpage>
          <lpage>7803</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/CVPR.2018.00813.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Si</surname>
              <given-names/>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names/>
            </name>
            <name>
              <surname>Cong</surname>
              <given-names/>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names/>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Fagan</surname>
              <given-names/>
            </name>
            <name>
              <surname>Mur</surname>
              <given-names/>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Ji</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>A hierarchical opportunistic screening model for osteoporosis using machine learning applied to clinical data and CT images</article-title>
          <source>BMC Bioinformatics</source>
          <year>2022</year>
          <volume>23</volume>
          <issue>1</issue>
          <page-range>1-10</page-range>
          <fpage>1</fpage>
          <lpage>10</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1186/s12859-022-04596-z</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Simonyan</surname>
              <given-names/>
            </name>
            <name>
              <surname>Zisserman</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Very deep convolutional networks for large-scale image recognition</article-title>
          <source>ArXiv.</source>
          <year>2014</year>
          <volume>1</volume>
          <pub-id pub-id-type="doi">https://doi.org/10.48550/arXiv.1409.1556</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Howard</surname>
              <given-names/>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names/>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names/>
            </name>
            <name>
              <surname>Kalenichenko</surname>
              <given-names/>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names/>
            </name>
            <name>
              <surname>Weyand</surname>
              <given-names/>
            </name>
            <name>
              <surname>Adam</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Mobilenets: Efficient convolutional neural networks for mobile vision applications</article-title>
          <source>ArXiv.</source>
          <year>2017</year>
          <volume>2017</volume>
          <pub-id pub-id-type="doi">https://doi.org/10.48550/arXiv.1704.04861</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Islam</surname>
              <given-names/>
            </name>
            <name>
              <surname>Aowal</surname>
              <given-names/>
            </name>
            <name>
              <surname>Minhaz</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Abnormality detection and localization in chest x-rays using deep convolutional neural networks</article-title>
          <source>ArXiv.</source>
          <year>2017</year>
          <volume>1</volume>
          <pub-id pub-id-type="doi">https://doi.org/10.48550/arXiv.1705.09850</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Muehlematter</surname>
              <given-names/>
            </name>
            <name>
              <surname>Mannil</surname>
              <given-names/>
            </name>
            <name>
              <surname>Becker</surname>
              <given-names/>
            </name>
            <name>
              <surname>Vokinger</surname>
              <given-names/>
            </name>
            <name>
              <surname>FinkenstÃ¤dt</surname>
              <given-names/>
            </name>
            <name>
              <surname>Osterhoff</surname>
              <given-names/>
            </name>
            <name>
              <surname>Fischer</surname>
              <given-names/>
            </name>
            <name>
              <surname>Guggenberger</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Vertebral body insufficiency fractures: Detection of vertebrae at risk on standard CT images using texture analysis and machine learning</article-title>
          <source>Eur. Radiol.</source>
          <year>2019</year>
          <volume>29</volume>
          <issue>5</issue>
          <page-range>2207-2217</page-range>
          <fpage>2207</fpage>
          <lpage>2217</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/s00330-018-5846-8</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tecle</surname>
              <given-names/>
            </name>
            <name>
              <surname>Teitel</surname>
              <given-names/>
            </name>
            <name>
              <surname>Morris</surname>
              <given-names/>
            </name>
            <name>
              <surname>Sani</surname>
              <given-names/>
            </name>
            <name>
              <surname>Mitten</surname>
              <given-names/>
            </name>
            <name>
              <surname>Hammert</surname>
              <given-names/>
            </name>
          </person-group>
          <article-title>Convolutional neural network for second metacarpal radiographic osteoporosis screening</article-title>
          <source>J. Hand Surg.</source>
          <year>2020</year>
          <volume>45</volume>
          <issue>3</issue>
          <page-range>175-181</page-range>
          <fpage>175</fpage>
          <lpage>181</lpage>
          <pub-id pub-id-type="doi">https://doi.org/ 10.1016/j.jhsa.2019.11.019</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>