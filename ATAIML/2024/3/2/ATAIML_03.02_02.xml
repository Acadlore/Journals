<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-Mv2CuALo6aeVOYoOqnhrlFF7ZqEoZeUI</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml030202</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Enhanced Named Entity Recognition Based on Multi-Feature Fusion Using Dual Graph Neural Networks</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-3273-2256</contrib-id>
          <name>
            <surname>Gu</surname>
            <given-names>Hanzhao</given-names>
          </name>
          <email>212106560620@hyit.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3612-5051</contrib-id>
          <name>
            <surname>Ma</surname>
            <given-names>Jialin</given-names>
          </name>
          <email>majl@hyit.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0008-1760-8887</contrib-id>
          <name>
            <surname>Zhao</surname>
            <given-names>Yanran</given-names>
          </name>
          <email>guyy.wxu@edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7715-3070</contrib-id>
          <name>
            <surname>Khadka</surname>
            <given-names>Ashim</given-names>
          </name>
          <email>ashim.khadka@ncit.edu.np</email>
        </contrib>
        <aff id="aff_1">Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, 223003 Huaian, China</aff>
        <aff id="aff_2">Faculty of Internet of Things Engineering, Wuxi Taihu University, 214063 Wuxi, China</aff>
        <aff id="aff_3">Nepal College of Information Technology, Pokhara University, 44700 Lalitpur, Nepal</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>02</day>
        <month>04</month>
        <year>2024</year>
      </pub-date>
      <volume>3</volume>
      <issue>2</issue>
      <fpage>84</fpage>
      <lpage>93</lpage>
      <page-range>84-93</page-range>
      <history>
        <date date-type="received">
          <day>14</day>
          <month>01</month>
          <year>2024</year>
        </date>
        <date date-type="accepted">
          <day>27</day>
          <month>03</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Named Entity Recognition (NER), a pivotal task in information extraction, is aimed at identifying named entities of various types within text. Traditional NER methods, however, often fall short in providing sufficient semantic representation of text and preserving word order information. Addressing these challenges, a novel approach is proposed, leveraging dual Graph Neural Networks (GNNs) based on multi-feature fusion. This approach constructs a co-occurrence graph and a dependency syntax graph from text sequences, capturing textual features from a dual-graph perspective to overcome the oversight of word interdependencies. Furthermore, Bidirectional Long Short-Term Memory Networks (BiLSTMs) are utilized to encode text, addressing the issues of neglecting word order features and the difficulty in capturing contextual semantic information. Additionally, to enable the model to learn features across different subspaces and the varying degrees of information significance, a multi-head self-attention mechanism is introduced for calculating internal dependency weights within feature vectors. The proposed model achieves F1-scores of 84.85% and 96.34% on the CCKS-2019 and Resume datasets, respectively, marking improvements of 1.13 and 0.67 percentage points over baseline models. The results affirm the effectiveness of the presented method in enhancing performance on the NER task.</p></abstract>
      <kwd-group>
        <kwd>Named Entity Recognition</kwd>
        <kwd>Graph Neural Networks</kwd>
        <kwd>Dependency syntax graph</kwd>
        <kwd>Co-occurrence graph</kwd>
        <kwd>Multi-feature fusion</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="4"/>
        <fig-count count="5"/>
        <table-count count="3"/>
        <ref-count count="23"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>NER, one of the significant research directions in the field of Natural Language Processing (NLP), is recognized for its capability to identify specific entities such as names of people, places, organizations, and other named entities within textual data. In various downstream tasks of NLP, including information retrieval, knowledge graphs, sentiment analysis, and question-answering systems, NER plays a crucial role. Consequently, the effective and accurate identification of specific entity information from text holds significant importance for computer processing of textual data.</p><p>In recent years, GNNs [<xref ref-type="bibr" rid="ref_1">1</xref>] have demonstrated exceptional performance in capturing topological information within data, making them particularly suitable for handling irregular graph-structured data. They have achieved remarkable results in both computer vision and NLP domains. Yao et al. [<xref ref-type="bibr" rid="ref_2">2</xref>] constructed a heterogeneous graph for the entire corpus based on word co-occurrence and text relationships, followed by the extraction of graph features using Graph Convolutional Networks (GCNs) [<xref ref-type="bibr" rid="ref_1">1</xref>]. To obtain more information within the GCN, Hu et al. [<xref ref-type="bibr" rid="ref_3">3</xref>] introduced thematic nodes during graph construction, while Xin et al. [<xref ref-type="bibr" rid="ref_4">4</xref>] incorporated label information. Zhang et al. [<xref ref-type="bibr" rid="ref_5">5</xref>] constructed co-occurrence graphs for each text, utilizing Gated Graph Neural Networks (GGNNs) for information propagation. The models mentioned above relied solely on co-occurrence information to build text graphs, neglecting other types of information such as semantics. Sui et al. [<xref ref-type="bibr" rid="ref_6">6</xref>] proposed a Trigger-GNN for nested NER, which obtains complementary annotation embeddings through entity trigger encoding and semantic matching, utilizing an efficient graph message passing mechanism. Peng et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] introduced a method using semantic and syntactic graphs for aspect-level sentiment analysis. Wu et al. [<xref ref-type="bibr" rid="ref_8">8</xref>] modeled documents as graphs to capture non-contiguous and long-distance semantics, extracting features from graphs across different domains through joint training of hierarchical GNNs. Some of these GNN models require constructing graphs for the entire corpus, leading to high space complexity. Others build text networks solely from the perspective of neighborhood word co-occurrence, overlooking the inherent syntactic dependency relationships within textual language, i.e., neglecting the representation of textual information at the level of syntactic dependency relationships in the language itself.</p><p>Addressing the aforementioned challenges, this study explores various graph construction methods to comprehensively capture textual features from multiple dimensions, thereby enhancing NER performance. A novel NER model based on multi-feature fusion using dual GNN (M-DGNN) is proposed. This approach involves constructing a co-occurrence graph and a dependency syntax graph to learn textual features from a dual-graph perspective; incorporating BiLSTMs to enhance the model's capability in capturing contextual information. Two separate GCNs are employed to learn the global semantic information of both graphs, i.e., the original distance dependency relationships. These are then fused with contextual information through a multi-head self-attention mechanism, enabling the model to learn features across different subspaces and the varying degrees of information significance. This, in turn, allows for a more effective learning of graphical representations. The main contributions of this study are as follows:</p><p>(i) The introduction of a joint updating module for GCNs based on co-occurrence graphs and dependency syntax graphs, extracting textual features from a dual-graph perspective. This module learns structured information representations dependent on context and mitigates the impact of incorrect dependency labels.</p><p>(ii) Comparative experiments were conducted on two public datasets with several mainstream methods. The results demonstrate that the M-DGNN model outperforms other methods.</p>
    </sec>
    <sec sec-type="">
      <title>2. Related work</title>
      <p>Currently, NER methods based on deep learning capitalize on the strong representational learning capabilities of deep neural networks, which autonomously learn features related to named entities from textual data. Models based on Convolutional Neural Networks (CNNs) are particularly powerful in generating local features for sentences [<xref ref-type="bibr" rid="ref_9">9</xref>], and they are very fast in their extended forms due to their parallelism [<xref ref-type="bibr" rid="ref_10">10</xref>]. Models based on Recurrent Neural Networks (RNN) excel in modeling sequential information [<xref ref-type="bibr" rid="ref_11">11</xref>]. Huang et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] proposed to capture contextual information utilizing BiLSTMs and to model label relationships through Conditional Random Field (CRF) models, thereby enhancing the accuracy of NER. Li et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] introduced a BiLSTM sub-classification model into the base model, effectively addressing the issue of nested medical entities. Ma and Hovy [<xref ref-type="bibr" rid="ref_14">14</xref>] presented the BiLSTM-CNN-CRF model, which first extracts character-level features using CNN, followed by further extraction of contextual features and final output using the BiLSTM-CRF model. Gui et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] processed the task of Chinese NER in parallel with a CNN model that integrates dictionary information and improved model performance through a novel feedback mechanism to resolve ambiguity between words. Dang et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] optimized word vectors with linguistic features in the D3NER model, which incorporates a BiLSTM-CRF framework. Strubell et al. [<xref ref-type="bibr" rid="ref_10">10</xref>] utilized dilated convolutions to extend the sequence coverage distance, thereby acquiring more contextual information. This method effectively balances computational speed and feature extraction for long sequences in NER tasks.</p><p>In recent years, GNNs have garnered increasing attention in the field of NLP, as traditional CNNs and RNNs have shown limitations in effectively utilizing structural information between documents, hierarchical classifications, and dependency trees among other graph data. GNN models, through the use of recurrent neural structures, propagate information from surrounding nodes, iteratively reaching a stable fixed point to obtain the vector representation of target nodes. Driven by GNNs, scholars, drawing on the concepts of CNNs and RNNs, have defined and designed GCNs for processing graph-structured data, applying them to classification tasks [<xref ref-type="bibr" rid="ref_2">2</xref>]. The convolution operation in GCNs combines the feature vectors of nodes with the graph structure between them. With each graph convolution operation, a node's feature vector is updated through the graph structure using information from neighboring nodes, thereby ensuring that similar nodes possess similar feature vectors. Existing evidence demonstrates the powerful feature extraction capabilities of GCNs, capable of extracting the data features of graph structures and applying them in areas such as relation classification and label classification. This study utilizes the existing capabilities of GCNs in processing graph-structured data to extract features from long distances.</p>
    </sec>
    <sec sec-type="">
      <title>3. The ner model based on m-dgnn</title>
      <p>The NER model based on M-DGNN, as proposed in this study, is depicted in <xref ref-type="fig" rid="fig_1">Figure 1</xref>. The encoding layer of the model comprises two sub-modules: a BiLSTM network module for extracting contextual features and a GCN module for extracting global features. Initially, textual sequences are input into the BiLSTM model to learn context feature vectors. Concurrently, by utilizing textual sequences in conjunction with co-occurrence graphs and dependency syntax graphs, dual-graph GCNs are constructed to obtain global feature vectors. Subsequently, these two types of feature vectors are input into a multi-head self-attention layer for feature fusion. Finally, a CRF model is used to decode the optimal encoding sequence from the fused feature vectors.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>The NER model based on M-DGNN</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_FH-TgDRr5rse6bMO.png"/>
        </fig>
      
      
        <sec>
          
            <title>3.1. Contextual semantic learning</title>
          
          <p>Deep learning-based NER models typically transform text sequences into vector sequences, where traditional one-hot encoding representations fail to effectively capture the contextual relationships and issues of data sparsity within sentences. In recent years, low-dimensional dense vector representations, such as word2vec and Global Vectors for Word Representation (GloVe), have gradually replaced traditional methods, offering advantages in extracting semantic information, conserving computational resources, and exhibiting strong generalization capabilities. In this study, the GloVe model is utilized for unsupervised training of text sequences to generate word vector representations rich in semantic information. Let the preprocessed text sequence be denoted as <inline-formula>
  <mml:math id="moubm8x8i0">
    <mml:mi>W</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mrow>
      <mml:mo>{</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>…</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>…</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>}</mml:mo>
      <mml:msub>
        <mml:mi>w</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>w</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>w</mml:mi>
        <mml:mn>3</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>w</mml:mi>
        <mml:mi>n</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula>. The pre-trained word vector model is employed to represent the text <inline-formula>
  <mml:math id="mi3hx6vq1j">
    <mml:msub>
      <mml:mi>w</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>, resulting in <inline-formula>
  <mml:math id="mi65pc95hr">
    <mml:msub>
      <mml:mi>w</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mrow>
      <mml:mo>{</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>…</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>…</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>}</mml:mo>
      <mml:msubsup>
        <mml:mi>w</mml:mi>
        <mml:mi>i</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msubsup>
      <mml:msubsup>
        <mml:mi>w</mml:mi>
        <mml:mi>i</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msubsup>
      <mml:msubsup>
        <mml:mi>w</mml:mi>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:msubsup>
      <mml:msubsup>
        <mml:mi>w</mml:mi>
        <mml:mi>i</mml:mi>
        <mml:mi>n</mml:mi>
      </mml:msubsup>
    </mml:mrow>
  </mml:math>
</inline-formula>, where, <inline-formula>
  <mml:math id="mb2jomn98h">
    <mml:msubsup>
      <mml:mi>w</mml:mi>
      <mml:mi>i</mml:mi>
      <mml:mi>j</mml:mi>
    </mml:msubsup>
    <mml:mo>∈</mml:mo>
    <mml:msup>
      <mml:mi>R</mml:mi>
      <mml:mrow>
        <mml:msub>
          <mml:mi>d</mml:mi>
          <mml:mi>w</mml:mi>
        </mml:msub>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula>, $n<inline-formula>
  <mml:math id="mtywdofm1i">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>d_w<inline-formula>
  <mml:math id="mqji345qqh">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>B</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>B</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo stretchy="false">(</mml:mo>
    <mml:mo stretchy="false">)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>w_i<inline-formula>
  <mml:math id="m4tussegry">
    <mml:mo>,</mml:mo>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>h_t$. The specific formulas are as follows:</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="me5w044q80">
                <mml:msub>
                  <mml:mi>f</mml:mi>
                  <mml:mi>t</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mi>σ</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>⋅</mml:mo>
                  <mml:mo>+</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>W</mml:mi>
                    <mml:mi>f</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>b</mml:mi>
                    <mml:mi>f</mml:mi>
                  </mml:msub>
                  <mml:mrow>
                    <mml:mo>[</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>]</mml:mo>
                    <mml:msub>
                      <mml:mi>h</mml:mi>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                        <mml:mo>−</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>x</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="mwogt7m6q9">
                <mml:msub>
                  <mml:mi>i</mml:mi>
                  <mml:mi>t</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mi>σ</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>⋅</mml:mo>
                  <mml:mo>+</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>W</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>b</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                  <mml:mrow>
                    <mml:mo>[</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>]</mml:mo>
                    <mml:msub>
                      <mml:mi>h</mml:mi>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                        <mml:mo>−</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>x</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="mr8nfwmcyx">
                <mml:msub>
                  <mml:mrow>
                    <mml:mover>
                      <mml:mi>C</mml:mi>
                      <mml:mo stretchy="false">ˇ</mml:mo>
                    </mml:mover>
                  </mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mtext>tanh</mml:mtext>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>⋅</mml:mo>
                  <mml:mo>+</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>W</mml:mi>
                    <mml:mi>C</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>b</mml:mi>
                    <mml:mi>C</mml:mi>
                  </mml:msub>
                  <mml:mrow>
                    <mml:mo>[</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>]</mml:mo>
                    <mml:msub>
                      <mml:mi>h</mml:mi>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                        <mml:mo>−</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>x</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="m4liucvhek">
                <mml:msub>
                  <mml:mi>C</mml:mi>
                  <mml:mi>t</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>f</mml:mi>
                  <mml:mi>t</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>C</mml:mi>
                  <mml:mrow>
                    <mml:mi>t</mml:mi>
                    <mml:mo>−</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>i</mml:mi>
                  <mml:mi>t</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mrow>
                    <mml:mover>
                      <mml:mi>C</mml:mi>
                      <mml:mo stretchy="false">ˇ</mml:mo>
                    </mml:mover>
                  </mml:mrow>
                  <mml:mi>t</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>∗</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>∗</mml:mo>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="mvrxgij86x">
                <mml:msub>
                  <mml:mi>o</mml:mi>
                  <mml:mi>t</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mi>σ</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>⋅</mml:mo>
                  <mml:mo>+</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>W</mml:mi>
                    <mml:mi>o</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>b</mml:mi>
                    <mml:mi>o</mml:mi>
                  </mml:msub>
                  <mml:mrow>
                    <mml:mo>[</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>]</mml:mo>
                    <mml:msub>
                      <mml:mi>h</mml:mi>
                      <mml:mrow>
                        <mml:mi>t</mml:mi>
                        <mml:mo>−</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>x</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(6)</label>
              <mml:math id="mx7090e7z5">
                <mml:msub>
                  <mml:mi>h</mml:mi>
                  <mml:mi>t</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>o</mml:mi>
                  <mml:mi>t</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>∗</mml:mo>
                <mml:mtext>tanh</mml:mtext>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>C</mml:mi>
                    <mml:mi>t</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mfic7m950y">
    <mml:msub>
      <mml:mi>f</mml:mi>
      <mml:mi>t</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>i</mml:mi>
      <mml:mi>t</mml:mi>
    </mml:msub>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="m4f5qrr0t5">
    <mml:msub>
      <mml:mi>o</mml:mi>
      <mml:mi>t</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> respectively represent the forget gate, input gate, and output gate at time $t<inline-formula>
  <mml:math id="mk3ymmu1tb">
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>\check{C}_t<inline-formula>
  <mml:math id="mx1jyau0wz">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>t<inline-formula>
  <mml:math id="m1jvmkdelw">
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>C_t<inline-formula>
  <mml:math id="mubypeatgy">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>t<inline-formula>
  <mml:math id="m18q04ynxc">
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>h_t<inline-formula>
  <mml:math id="mk1znm8wi5">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>;</mml:mo>
    <mml:mo>∗</mml:mo>
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>W<inline-formula>
  <mml:math id="mf9ikfcctf">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>b<inline-formula>
  <mml:math id="m51tfip4zv">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>\sigma(\cdot)<inline-formula>
  <mml:math id="mg7nqzyp60">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>tanh (\cdot)$ denote the sigmoid activation function and hyperbolic tangent activation function, respectively.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Global semantic learning based on dual-graph gcns</title>
          
          <p>GCNs are adept at capturing relationships between nodes and have been widely applied in NLP. To effectively capture the dependencies between words, GCNs are utilized to extract features from both co-occurrence graphs and syntactic dependency graphs. Through the dual-graph structure, a complementary acquisition of textual sequential information and global information is achieved.</p>
          
            <sec>
              
                <title>3.2.1 Construction of co-occurrence graphs</title>
              
              <p>Co-occurrence graphs utilize graph structures to express the co-occurrence relationships between words, aiming to showcase semantic connections between vocabularies by capturing words that frequently appear in the same context within texts. Words in text sequences are represented as nodes, and the edges between nodes denote the co-occurrence relationships between words. A predefined sliding window is moved from left to right along the text sequence. When words appear within the same sliding window, they are considered to have a co-occurrence relationship, thereby constructing a co-occurrence graph, as illustrated in <xref ref-type="fig" rid="fig_2">Figure 2</xref>.</p>
              
                <fig id="fig_2">
                  <label>Figure 2</label>
                  <caption>
                    <title>Example of co-occurrence graph construction</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_sVyyQP8ZItb1ufCv.png"/>
                </fig>
              
            </sec>
          
          
            <sec>
              
                <title>3.2.2 Construction of syntactic dependency graphs</title>
              
              <p>Syntactic parsing, a fundamental component of NLP, plays a pivotal role in presenting the grammatical dependency relationships between words within sentences. By analyzing the dependency relationships among words in sentences, the syntactic structure of sentences is determined, revealing their hierarchical structure and grammatical dependencies. Text sequences are analyzed using the StanfordNLP syntactic parser to effectively transform text sequences into graph network data structures. The result of the syntactic analysis for a given text sequence is shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>.</p><p>From the example in <xref ref-type="fig" rid="fig_3">Figure 3</xref>, it can be observed that the syntactic analysis results obtained from the syntactic parser not only provide the grammatical roles of each word but also mark the dependency relationships between words. For instance, the relationship between “suffers” and “pain” is labeled as “ccomp,” indicating a clausal complement. Although the dependency relationships obtained from syntactic analysis are represented as directed edges, for the purposes of constructing syntactic dependency graphs and feature extraction, this study opts to consider directed edges as undirected. By analyzing the dependency relationships among words in sentences with a syntactic parser, syntactic dependency graphs are constructed, enabling a more effective expression of the dependency relationships between words and the overall text structure.</p>
              
                <fig id="fig_3">
                  <label>Figure 3</label>
                  <caption>
                    <title>Example of syntactic dependency analysis</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_t_60gJI5LKbxucr1.png"/>
                </fig>
              
              <p>Through the construction of graphs, two textual graphs are obtained: a co-occurrence graph <inline-formula>
  <mml:math id="mhm660slq0">
    <mml:msub>
      <mml:mi>G</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mo stretchy="false">(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo stretchy="false">)</mml:mo>
    <mml:mi>V</mml:mi>
  </mml:math>
</inline-formula> and a syntactic dependency graph <inline-formula>
  <mml:math id="mqun8b79sr">
    <mml:msub>
      <mml:mi>G</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>E</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mo stretchy="false">(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo stretchy="false">)</mml:mo>
    <mml:mi>V</mml:mi>
  </mml:math>
</inline-formula>. $X<inline-formula>
  <mml:math id="m0yhskpt85">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>V<inline-formula>
  <mml:math id="mtdqit06mi">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>E_1<inline-formula>
  <mml:math id="m51lifs63p">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>E_2$ is the set of edges in the syntactic dependency graph.</p><p>Two GCNs are employed to learn the topological structures of the two text networks separately. Through neighborhood aggregation operations based on the word connection relationships in the two text networks, the embedding representations of textual words are obtained. The computational formulas are as follows:</p>
              
                <disp-formula>
                  <label>(7)</label>
                  <mml:math id="m85w3jouno">
                    <mml:msub>
                      <mml:mi>H</mml:mi>
                      <mml:mi>M</mml:mi>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mi>R</mml:mi>
                    <mml:mi>E</mml:mi>
                    <mml:mi>L</mml:mi>
                    <mml:mi>U</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>A</mml:mi>
                        <mml:mi>m</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>H</mml:mi>
                        <mml:mi>M</mml:mi>
                      </mml:msub>
                      <mml:msup>
                        <mml:mi>W</mml:mi>
                        <mml:mi>l</mml:mi>
                      </mml:msup>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(8)</label>
                  <mml:math id="mhrkz6dh2b">
                    <mml:msub>
                      <mml:mi>H</mml:mi>
                      <mml:mi>S</mml:mi>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mi>R</mml:mi>
                    <mml:mi>E</mml:mi>
                    <mml:mi>L</mml:mi>
                    <mml:mi>U</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>A</mml:mi>
                        <mml:mi>s</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>H</mml:mi>
                        <mml:mi>S</mml:mi>
                      </mml:msub>
                      <mml:msup>
                        <mml:mi>W</mml:mi>
                        <mml:mi>l</mml:mi>
                      </mml:msup>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="m105wbbhlc">
    <mml:msub>
      <mml:mi>H</mml:mi>
      <mml:mi>M</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> represents the embedding matrix of the co-occurrence graph, and <inline-formula>
  <mml:math id="mdhr72m4ij">
    <mml:msub>
      <mml:mi>H</mml:mi>
      <mml:mi>S</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> denotes the embedding matrix of the syntactic dependency graph. <inline-formula>
  <mml:math id="mhrb1x8ren">
    <mml:msubsup>
      <mml:mi>H</mml:mi>
      <mml:mi>M</mml:mi>
      <mml:mn>0</mml:mn>
    </mml:msubsup>
    <mml:mo>=</mml:mo>
    <mml:mi>X</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>∈</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mi>X</mml:mi>
      <mml:msup>
        <mml:mi>R</mml:mi>
        <mml:mrow>
          <mml:mi>N</mml:mi>
          <mml:mi>L</mml:mi>
          <mml:mo>×</mml:mo>
        </mml:mrow>
      </mml:msup>
    </mml:mrow>
  </mml:math>
</inline-formula> is the input to the first layer of the GCN, $N<inline-formula>
  <mml:math id="mk69lc9s92">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>L<inline-formula>
  <mml:math id="mg1lfe15zf">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>A \in R^{N \times N}<inline-formula>
  <mml:math id="mmtgvmzq0s">
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mn>0</mml:mn>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula>W^l<inline-formula>
  <mml:math id="mdu06oquet">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>R E L U(\cdot)<inline-formula>
  <mml:math id="mslw0fdqbo">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>H_M<inline-formula>
  <mml:math id="mmu37ohcy5">
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mo>−</mml:mo>
  </mml:math>
</inline-formula>G_1<inline-formula>
  <mml:math id="moci9khf8u">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>H_S<inline-formula>
  <mml:math id="myvj91kzm8">
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
  </mml:math>
</inline-formula>G_2<inline-formula>
  <mml:math id="mgq8qjk216">
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>H_C$ represents the global features obtained by word embeddings through the dual graphs:</p>
              
                <disp-formula>
                  <label>(9)</label>
                  <mml:math id="mvedx6k5r5">
                    <mml:msub>
                      <mml:mi>H</mml:mi>
                      <mml:mi>C</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>H</mml:mi>
                      <mml:mi>M</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>H</mml:mi>
                      <mml:mi>S</mml:mi>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mo>⊕</mml:mo>
                  </mml:math>
                </disp-formula>
              
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. Feature fusion</title>
          
          <p>The multi-head self-attention mechanism aids in capturing syntactic and semantic features between words in sentences, particularly focusing on long-distance dependencies between words, enabling a more effective understanding of internal dependencies within feature vectors. To fuse feature vectors extracted by the GCN and BiLSTM modules, this study introduces a multi-head self-attention mechanism, concentrating on the structural features of the fusion vectors. The fusion vector representing global semantic information and contextual information is denoted as <inline-formula>
  <mml:math id="ms354aue2i">
    <mml:mi>R</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mrow>
      <mml:mo>[</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>…</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>]</mml:mo>
      <mml:msub>
        <mml:mi>r</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>r</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>r</mml:mi>
        <mml:mi>n</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula>, where <inline-formula>
  <mml:math id="mrf0u4osf8">
    <mml:msub>
      <mml:mi>r</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is the vector of the i-th character in the sentence. After undergoing linear transformation, three different vector matrices Q(query), K(Key) and V(Value) are obtained, and $Q$=$K$=$V$=$R<inline-formula>
  <mml:math id="mkgkekr000">
    <mml:mo>.</mml:mo>
    <mml:mi>B</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
  </mml:math>
</inline-formula>K<inline-formula>
  <mml:math id="mvkfg3a98l">
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
  </mml:math>
</inline-formula>Q$, the results are scaled and normalized using a softmax function to obtain attention weights. The specific computational formula is as follows:</p>
          
            <disp-formula>
              <label>(10)</label>
              <mml:math id="mjoux5naw8">
                <mml:mtext>Self Attention</mml:mtext>
                <mml:mtext>softmax</mml:mtext>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo stretchy="false">)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mi>Q</mml:mi>
                <mml:mi>K</mml:mi>
                <mml:mi>V</mml:mi>
                <mml:mi>V</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mi>Q</mml:mi>
                      <mml:msup>
                        <mml:mi>K</mml:mi>
                        <mml:mi>T</mml:mi>
                      </mml:msup>
                    </mml:mrow>
                    <mml:msqrt>
                      <mml:msub>
                        <mml:mi>d</mml:mi>
                        <mml:mi>k</mml:mi>
                      </mml:msub>
                    </mml:msqrt>
                  </mml:mfrac>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mkqqabegha">
    <mml:msqrt>
      <mml:msub>
        <mml:mi>d</mml:mi>
        <mml:mi>k</mml:mi>
      </mml:msub>
    </mml:msqrt>
  </mml:math>
</inline-formula> represents the dimensionality of key, and <inline-formula>
  <mml:math id="mluwbxnxot">
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo stretchy="false">(</mml:mo>
    <mml:mo>·</mml:mo>
    <mml:mo stretchy="false">)</mml:mo>
  </mml:math>
</inline-formula> indicates probability mapping of the input elements.</p><p>Through the multi-head self-attention mechanism, high-dimensional feature vectors are subjected to multiple parallel attention computations, with the number of heads being $e<inline-formula>
  <mml:math id="mwyyt006d6">
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>E</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
  </mml:math>
</inline-formula>e$ heads is concatenated and then linearly transformed to obtain comprehensive feature information fused with attention, represented as:</p>
          
            <disp-formula>
              <label>(11)</label>
              <mml:math id="mdwpyc0c3x">
                <mml:msub>
                  <mml:mi>M</mml:mi>
                  <mml:mi>i</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mtext>Self Attention</mml:mtext>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mi>Q</mml:mi>
                  <mml:mi>K</mml:mi>
                  <mml:mi>V</mml:mi>
                  <mml:msub>
                    <mml:mi>W</mml:mi>
                    <mml:mi>Q</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>W</mml:mi>
                    <mml:mi>K</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>W</mml:mi>
                    <mml:mi>V</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(12)</label>
              <mml:math id="mwgiqjnakn">
                <mml:mtext>MulAtt</mml:mtext>
                <mml:mtext>Concat</mml:mtext>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo stretchy="false">)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mi>Q</mml:mi>
                <mml:mi>K</mml:mi>
                <mml:mi>V</mml:mi>
                <mml:mi>W</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>…</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>M</mml:mi>
                    <mml:mn>1</mml:mn>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>M</mml:mi>
                    <mml:mn>2</mml:mn>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>M</mml:mi>
                    <mml:mi>e</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mql6oe19o3">
    <mml:msub>
      <mml:mi>W</mml:mi>
      <mml:mi>Q</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>W</mml:mi>
      <mml:mi>K</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>W</mml:mi>
      <mml:mi>V</mml:mi>
    </mml:msub>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula> denotes the weight matrix learned during model training, <inline-formula>
  <mml:math id="mshr2vulk0">
    <mml:mi>C</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo stretchy="false">(</mml:mo>
    <mml:mo>⋅</mml:mo>
    <mml:mo stretchy="false">)</mml:mo>
  </mml:math>
</inline-formula> represents the concatenation of feature vectors extracted by $e<inline-formula>
  <mml:math id="mwbtfarve4">
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>W$ is the weight matrix generated in the process.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Crf decoding</title>
          
          <p>While the multi-head self-attention mechanism effectively fuses features and independently identifies the most probable label for each word, it does not address the issue of the rationality between adjacent labels. To resolve this, a CRF model is employed to introduce effective constraints between labels, thereby ensuring that reasonable label sequences attain higher probability values, resulting in the optimal predicted sequence. For a given sequence <inline-formula>
  <mml:math id="mvmomepum7">
    <mml:mi>X</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mrow>
      <mml:mo>{</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>…</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>}</mml:mo>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mi>n</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> and its predicted sequence <inline-formula>
  <mml:math id="myxnvrdad1">
    <mml:mi>y</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mrow>
      <mml:mo>{</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>…</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>}</mml:mo>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mn>2</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>y</mml:mi>
        <mml:mi>n</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula>, the computed score can be represented as:</p>
          
            <disp-formula>
              <label>(13)</label>
              <mml:math id="mv7yk2upl9">
                <mml:mi>S</mml:mi>
                <mml:mi>X</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo stretchy="false">)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:munderover>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>n</mml:mi>
                </mml:munderover>
                <mml:munderover>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>n</mml:mi>
                </mml:munderover>
                <mml:msub>
                  <mml:mi>A</mml:mi>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>y</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>y</mml:mi>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:mo>+</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>,</mml:mo>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>P</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:msub>
                      <mml:mi>y</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:msub>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mjhl0uz2n5">
    <mml:msub>
      <mml:mi>A</mml:mi>
      <mml:mrow>
        <mml:msub>
          <mml:mi>y</mml:mi>
          <mml:mi>i</mml:mi>
        </mml:msub>
        <mml:msub>
          <mml:mi>y</mml:mi>
          <mml:mrow>
            <mml:mi>i</mml:mi>
            <mml:mo>+</mml:mo>
            <mml:mn>1</mml:mn>
          </mml:mrow>
        </mml:msub>
        <mml:mo>,</mml:mo>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> denotes the probability of transitioning <inline-formula>
  <mml:math id="m2z4sg87wb">
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> to <inline-formula>
  <mml:math id="m9kp5n9vjg">
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mo>+</mml:mo>
        <mml:mn>1</mml:mn>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mi>P</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mo>,</mml:mo>
        <mml:msub>
          <mml:mi>y</mml:mi>
          <mml:mi>i</mml:mi>
        </mml:msub>
      </mml:mrow>
    </mml:msub>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula> represents the probability of the i-th word being labeled <inline-formula>
  <mml:math id="mdfpv01vvc">
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="myvpghauqy">
    <mml:mi>S</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo stretchy="false">(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo stretchy="false">)</mml:mo>
  </mml:math>
</inline-formula> signifies the probability of the input sentence sequence $x<inline-formula>
  <mml:math id="m1rfq26yxn">
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>y<inline-formula>
  <mml:math id="mm2u245ffo">
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>y<inline-formula>
  <mml:math id="m7roicik1t">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>X$ is:</p>
          
            <disp-formula>
              <label>(14)</label>
              <mml:math id="migm74da9m">
                <mml:mi>p</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mi>X</mml:mi>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mo>∣</mml:mo>
                <mml:mo stretchy="false">)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:msup>
                    <mml:mi>e</mml:mi>
                    <mml:mrow>
                      <mml:mi>S</mml:mi>
                      <mml:mi>X</mml:mi>
                      <mml:mi>y</mml:mi>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo stretchy="false">)</mml:mo>
                    </mml:mrow>
                  </mml:msup>
                  <mml:mrow>
                    <mml:munder>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mover>
                            <mml:mi>y</mml:mi>
                            <mml:mo stretchy="false">~</mml:mo>
                          </mml:mover>
                        </mml:mrow>
                        <mml:mo>∈</mml:mo>
                        <mml:msub>
                          <mml:mi>Y</mml:mi>
                          <mml:mi>X</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                    </mml:munder>
                    <mml:mi>S</mml:mi>
                    <mml:mi>X</mml:mi>
                    <mml:mo stretchy="false">(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo stretchy="false">)</mml:mo>
                    <mml:mrow>
                      <mml:mover>
                        <mml:mi>y</mml:mi>
                        <mml:mo stretchy="false">~</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mk6oqnzgbo">
    <mml:msub>
      <mml:mi>Y</mml:mi>
      <mml:mi>X</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> represents the set of all possible labels under the text sequence $X$. Subsequently, the likelihood function is solved, and the optimal label sequence is decoded to obtain the prediction result, as follows:</p>
          
            <disp-formula>
              <label>(15)</label>
              <mml:math id="mrmxphe5jp">
                <mml:mi>log</mml:mi>
                <mml:mi>p</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mi>X</mml:mi>
                <mml:mi>S</mml:mi>
                <mml:mi>X</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mi>log</mml:mi>
                <mml:mo>⁡</mml:mo>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mo>∣</mml:mo>
                <mml:mo stretchy="false">)</mml:mo>
                <mml:mo stretchy="false">)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo stretchy="false">)</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>⁡</mml:mo>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:munder>
                    <mml:mo>∑</mml:mo>
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mover>
                          <mml:mi>y</mml:mi>
                          <mml:mo stretchy="false">~</mml:mo>
                        </mml:mover>
                      </mml:mrow>
                      <mml:mo>∈</mml:mo>
                      <mml:msub>
                        <mml:mi>Y</mml:mi>
                        <mml:mi>X</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                  </mml:munder>
                  <mml:msup>
                    <mml:mi>e</mml:mi>
                    <mml:mrow>
                      <mml:mi>S</mml:mi>
                      <mml:mi>X</mml:mi>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo stretchy="false">)</mml:mo>
                      <mml:mrow>
                        <mml:mover>
                          <mml:mi>y</mml:mi>
                          <mml:mo stretchy="false">~</mml:mo>
                        </mml:mover>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:msup>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(16)</label>
              <mml:math id="md5vypo3qu">
                <mml:msup>
                  <mml:mi>y</mml:mi>
                  <mml:mo>∗</mml:mo>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mo stretchy="false">(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo stretchy="false">)</mml:mo>
                <mml:mtext>argmax</mml:mtext>
                <mml:mi>S</mml:mi>
                <mml:mi>X</mml:mi>
                <mml:mrow>
                  <mml:mover>
                    <mml:mi>y</mml:mi>
                    <mml:mo stretchy="false">~</mml:mo>
                  </mml:mover>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Results and analysis</title>
      
        <sec>
          
            <title>4.1. Datasets</title>
          
          <p>Experiments were conducted on the Chinese Electronic Medical Records Entity Dataset released by the China Conference on Knowledge Graph and Semantic Computing (CCKS-2019) and the Chinese Resume Dataset. The distribution of the two datasets is shown in <xref ref-type="table" rid="table_1">Table 1</xref>. The datasets employ the BIO tagging scheme: “B” represents Beginning, indicating the start of an entity; “I” represents Inside, indicating the middle part of an entity; “O” represents Outside, indicating information unrelated to any entity.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Introduction of the datasets</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Dataset</p></td><td colspan="1" rowspan="1"><p>Dataset Division</p></td><td colspan="1" rowspan="1"><p>Number of Sentences/Documents</p></td><td colspan="1" rowspan="1"><p>Entity Types</p></td></tr><tr><td colspan="1" rowspan="1"><p>CCKS-2019</p></td><td colspan="1" rowspan="1"><p>Training set</p><p>Development set</p><p>Test set</p></td><td colspan="1" rowspan="1"><p>1000</p><p>79</p><p>300</p></td><td colspan="1" rowspan="1"><p>Surgical procedures, body parts, medications, imaging examinations, diseases and diagnoses, laboratory tests.</p></td></tr><tr><td colspan="1" rowspan="1"><p></p><p>Resume</p></td><td colspan="1" rowspan="1"><p>Training set</p><p>Development set</p><p>Test set</p></td><td colspan="1" rowspan="1"><p>3821</p><p>463</p><p>477</p></td><td colspan="1" rowspan="1"><p>Nationalities, educational institutions, addresses, personal names, organization names.</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>4.2. Evaluation metrics</title>
          
          <p>To accurately measure the effectiveness of entity recognition, this study employs three evaluation metrics: precision (P), recall (R), and F1-score (F1). The specific formulas are as follows:</p><p style="text-align: center"><inline-formula>
  <mml:math id="m2yo2tw2qt">
    <mml:mtable displaystyle="true" columnspacing="1em" rowspacing="3pt">
      <mml:mtr>
        <mml:mtd>
          <mml:mi>P</mml:mi>
          <mml:mi>T</mml:mi>
          <mml:mi>P</mml:mi>
          <mml:mi>T</mml:mi>
          <mml:mi>P</mml:mi>
          <mml:mi>F</mml:mi>
          <mml:mi>P</mml:mi>
          <mml:mo>=</mml:mo>
          <mml:mo stretchy="false">(</mml:mo>
          <mml:mo>+</mml:mo>
          <mml:mo stretchy="false">)</mml:mo>
          <mml:mrow>
            <mml:mo>/</mml:mo>
          </mml:mrow>
        </mml:mtd>
      </mml:mtr>
      <mml:mtr>
        <mml:mtd>
          <mml:mi>R</mml:mi>
          <mml:mi>T</mml:mi>
          <mml:mi>P</mml:mi>
          <mml:mi>T</mml:mi>
          <mml:mi>P</mml:mi>
          <mml:mi>F</mml:mi>
          <mml:mi>N</mml:mi>
          <mml:mo>=</mml:mo>
          <mml:mo stretchy="false">(</mml:mo>
          <mml:mo>+</mml:mo>
          <mml:mo stretchy="false">)</mml:mo>
          <mml:mrow>
            <mml:mo>/</mml:mo>
          </mml:mrow>
        </mml:mtd>
      </mml:mtr>
      <mml:mtr>
        <mml:mtd>
          <mml:msub>
            <mml:mi>F</mml:mi>
            <mml:mn>1</mml:mn>
          </mml:msub>
          <mml:mo>=</mml:mo>
          <mml:mo stretchy="false">(</mml:mo>
          <mml:mo>+</mml:mo>
          <mml:mo stretchy="false">)</mml:mo>
          <mml:mn>2</mml:mn>
          <mml:mi>P</mml:mi>
          <mml:mi>R</mml:mi>
          <mml:mi>P</mml:mi>
          <mml:mi>R</mml:mi>
          <mml:mrow>
            <mml:mo>/</mml:mo>
          </mml:mrow>
        </mml:mtd>
      </mml:mtr>
    </mml:mtable>
  </mml:math>
</inline-formula></p>
          <p>where, <inline-formula>
  <mml:math id="m39c16oftr">
    <mml:mi>T</mml:mi>
    <mml:mi>P</mml:mi>
  </mml:math>
</inline-formula> represents the number of correctly identified entities, <inline-formula>
  <mml:math id="mjitygfudf">
    <mml:mi>F</mml:mi>
    <mml:mi>P</mml:mi>
  </mml:math>
</inline-formula> denotes the number of entities with either boundary determination errors or category classification errors, and <inline-formula>
  <mml:math id="mz05naaooz">
    <mml:mi>F</mml:mi>
    <mml:mi>N</mml:mi>
  </mml:math>
</inline-formula> denotes the number of unrecognized entities.</p>
        </sec>
      
      
        <sec>
          
            <title>4.3. Parameter settings</title>
          
          <p>The word vectors were initialized using GloVe, with an embedding dimension set to 300. The embedding dimension for BiLSTM was set to 100. The graph convolutional module utilized two layers of convolution, with a word embedding dimension of 300. The number of heads in the self-attention mechanism was set to 6. The Adam optimizer was used for training with epochs=200, an initial learning rate of 0.01, and a Dropout rate of 0.5.</p>
        </sec>
      
      
        <sec>
          
            <title>4.4. Experimental results and analysis</title>
          
          <p>To validate the effectiveness of the model proposed in this study, comparative experiments were conducted on the CCKS-2019 and Resume datasets. The performance comparison of various models on the CCKS-2019 dataset is presented in <xref ref-type="table" rid="table_2">Table 2</xref> and <xref ref-type="table" rid="table_3">Table 3</xref>.</p><p>Chiu and Nichols [<xref ref-type="bibr" rid="ref_17">17</xref>] utilized a CNN model to capture character features, which were then combined with word vectors and other additional features for input into a BiLSTM network. Li et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] employed both sentence and bi-word vectors in their model. Dang et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] proposed the D3NER model, which introduces linguistic features into the BiLSTM-CRF structure to optimize word vectors. Peters et al. [<xref ref-type="bibr" rid="ref_19">19</xref>] introduced the Embeddings from Language Models (ELMo), utilizing BiLSTM as a pre-training structure to obtain dynamic word vectors. Through experimental comparison, the model presented in this study demonstrated superior performance on the CCKS-2019 dataset, with an F1-score exceeding the next best model by 1.13 percentage points.</p><p>The performance comparison of models on the Resume dataset is shown in <xref ref-type="table" rid="table_3">Table 3</xref>. Zhang and Yang [<xref ref-type="bibr" rid="ref_20">20</xref>] first proposed using Lattice-LSTM to achieve character-word fusion, enhancing the performance of Chinese NER. Building on this, Liu et al. [<xref ref-type="bibr" rid="ref_21">21</xref>] further optimized the model with four strategies for acceleration, enabling the model to support larger batch sizes. Gui et al. [<xref ref-type="bibr" rid="ref_22">22</xref>] introduced graph methods to the field of NER, utilizing co-occurrence relationships for sequence tagging. Li et al. [<xref ref-type="bibr" rid="ref_23">23</xref>] improved the self-attention mechanism and introduced position encoding to improve the F1-score for NER. Through comparative experiments, the model presented in this study achieved the best results on the Resume dataset, with an F1-score exceeding the next best model by 0.67 percentage points.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Comparison of evaluation metrics for different models on the CCKS-2019 dataset</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="1" rowspan="1"><p>P (%)</p></td><td colspan="1" rowspan="1"><p>R (%)</p></td><td colspan="1" rowspan="1"><p>F1 (%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>BCEL [<xref ref-type="bibr" rid="ref_17">17</xref>]</p></td><td colspan="1" rowspan="1"><p>83.65</p></td><td colspan="1" rowspan="1"><p>82.56</p></td><td colspan="1" rowspan="1"><p>83.24</p></td></tr><tr><td colspan="1" rowspan="1"><p>VT-BiLSTM [<xref ref-type="bibr" rid="ref_18">18</xref>]</p></td><td colspan="1" rowspan="1"><p>83.45</p></td><td colspan="1" rowspan="1"><p>83.68</p></td><td colspan="1" rowspan="1"><p>83.54</p></td></tr><tr><td colspan="1" rowspan="1"><p>D3NER [<xref ref-type="bibr" rid="ref_16">16</xref>]</p></td><td colspan="1" rowspan="1"><p>82.35</p></td><td colspan="1" rowspan="1"><p>83.41</p></td><td colspan="1" rowspan="1"><p>83.62</p></td></tr><tr><td colspan="1" rowspan="1"><p>ELMo [<xref ref-type="bibr" rid="ref_19">19</xref>]</p></td><td colspan="1" rowspan="1"><p>83.65</p></td><td colspan="1" rowspan="1"><p>83.52</p></td><td colspan="1" rowspan="1"><p>83.72</p></td></tr><tr><td colspan="1" rowspan="1"><p>M-DGNN</p></td><td colspan="1" rowspan="1"><p>84.72</p></td><td colspan="1" rowspan="1"><p>84.49</p></td><td colspan="1" rowspan="1"><p>84.85</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Comparison of evaluation metrics for different models on the resume dataset</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="1" rowspan="1"><p>P (%)</p></td><td colspan="1" rowspan="1"><p>R (%)</p></td><td colspan="1" rowspan="1"><p>F1 (%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Lattice-LSTM [<xref ref-type="bibr" rid="ref_20">20</xref>]</p></td><td colspan="1" rowspan="1"><p>94.62</p></td><td colspan="1" rowspan="1"><p>94.23</p></td><td colspan="1" rowspan="1"><p>94.65</p></td></tr><tr><td colspan="1" rowspan="1"><p>WC-LSTM [<xref ref-type="bibr" rid="ref_21">21</xref>]</p></td><td colspan="1" rowspan="1"><p>95.33</p></td><td colspan="1" rowspan="1"><p>94.60</p></td><td colspan="1" rowspan="1"><p>94.74</p></td></tr><tr><td colspan="1" rowspan="1"><p>LB-GNN [<xref ref-type="bibr" rid="ref_22">22</xref>]</p></td><td colspan="1" rowspan="1"><p>95.58</p></td><td colspan="1" rowspan="1"><p>95.52</p></td><td colspan="1" rowspan="1"><p>95.40</p></td></tr><tr><td colspan="1" rowspan="1"><p>FL-Transformer [<xref ref-type="bibr" rid="ref_22">22</xref>]</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>95.67</p></td></tr><tr><td colspan="1" rowspan="1"><p>M-DGNN</p></td><td colspan="1" rowspan="1"><p>96.12</p></td><td colspan="1" rowspan="1"><p>95.86</p></td><td colspan="1" rowspan="1"><p>96.34</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>The experimental results indicate that the introduction of GCNs into NER tasks, combined with the co-occurrence and syntactic dependency information within text sequences, is effective in enhancing the accuracy of NER. Constructing text sequences into graph structures facilitates the exploration of hidden structural information within the text. GCN models can effectively model the global co-occurrence relationships of words, with embedding representations considering both topological information and word co-occurrence information. Simultaneously, combining BiLSTMs to capture the bidirectional contextual semantic information of texts addresses the insufficiency of GCNs in neglecting textual word order features.</p>
        </sec>
      
      
        <sec>
          
            <title>4.5. Parameter analysis</title>
          
          <p> <xref ref-type="fig" rid="fig_4">Figure 4</xref> presents the impact of different sliding window sizes on model performance across two datasets. It can be observed that, on both the CCKS-2019 and Resume datasets, the model's F1-score reaches its peak when the sliding window size is set to 3, and shows a declining trend when the size is either decreased or further increased. Therefore, setting the sliding window too large or too small is not the optimal strategy. A too large sliding window might lead to unnecessary node connections. Conversely, a too small setting reduces the number of words within the window, decreasing the number of node connections in the constructed text graph. This could result in the loss of some important connections between nodes, thereby losing key information and reducing the performance of entity recognition.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Impact of different sliding window sizes on model performance</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_zq5ddLC1im2YBlll.png"/>
            </fig>
          
          <p> <xref ref-type="fig" rid="fig_5">Figure 5</xref> illustrates the performance of the model under different numbers of graph convolutional layers. It is observed that the model exhibits the highest accuracy when the number of graph convolutional layers is set to 3. Compared to having one or two layers of graph convolution, both accuracy and F1-score show improvement. However, an increase to four layers results in a decrease in both accuracy and F1-score. This decline can be attributed to the issue of over-smoothing that may occur with too many stacked layers of graph convolution, leading to the features of adjacent nodes becoming increasingly similar and thus negatively impacting the performance of entity recognition.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Impact of different numbers of graph convolutional layers on model performance</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_wWGJLUc1WJmpjyaI.png"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>This study introduced a NER method based on dual GNNs with multi-feature fusion. The model leverages GCNs, combining co-occurrence and syntactic dependency graphs, to learn the feature information of text word nodes. In addition, a multi-head self-attention mechanism is introduced to calculate the internal dependency significance of feature vectors. Experimental results demonstrate that fully considering the grammatical characteristics of text language and introducing syntactic dependency information significantly enhances the effectiveness of NER. A limitation of this study is the construction of static text graphs. Future work could involve constructing dynamic text graphs and exploring various methods for constructing text graphs, including those based on semantic associations and entity relationships. Furthermore, different variants of GNNs based on text graph features could be experimented with for further improvements.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kipf</surname>
              <given-names>Thomas N</given-names>
            </name>
            <name>
              <surname>Welling</surname>
              <given-names>Max</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1609.02907</pub-id>
          <article-title>Semi-supervised classification with graph convolutional networks</article-title>
          <source>arXiv preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="conf-paper">
          <page-range>7370–7377</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yao</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Mao</surname>
              <given-names>C. S.</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1809.05679</pub-id>
          <article-title>Graph convolutional networks for text classification</article-title>
          <source>The Thirty- Third AAAI Conference on Artificial Intelligence (AAAI-19), Hawaii, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="conf-paper">
          <page-range>4821–4830</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hu</surname>
              <given-names>L. M.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>T. C.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Ji</surname>
              <given-names>H.Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>X. L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18653/v1/D19-1488</pub-id>
          <article-title>Heterogeneous graph attention networks for semi-supervised short text classification</article-title>
          <source>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="conf-paper">
          <page-range>8892–8898</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Xin</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>L. L.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>J. L.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J. Q.</given-names>
            </name>
            <name>
              <surname>Sheng</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Y. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICPR48806.2021.9413086</pub-id>
          <article-title>Label incorporated graph neural networks for text classification</article-title>
          <source>Proceedings of the 25th International Conference on Pattern Recognition, Milan, Italy</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="conf-paper">
          <page-range>334–339</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Y. F.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>X. L.</given-names>
            </name>
            <name>
              <surname>Cui</surname>
              <given-names>Z. Y.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wen</surname>
              <given-names>Z. Z.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18653/v1/2020.acl-main.31</pub-id>
          <article-title>Every document owns its structure: Inductive text classification via graph neural networks</article-title>
          <source>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-8</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sui</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Bu</surname>
              <given-names>F. Y.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>Y. T.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/IJCNN55064.2022.9892555</pub-id>
          <article-title>TriggerGNN: A trigger-based graph neural network for nested named entity recognition</article-title>
          <source>2022 International Joint Conference on Neural Networks (IJ-CNN), Padua, Italy</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1063–1072</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Peng</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J. X.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Y. P.</given-names>
            </name>
            <name>
              <surname>Bao</surname>
              <given-names>M. J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L. H.</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>Y. Q.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Q.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3178876.3186005</pub-id>
          <article-title>Large-scale hierarchical text classification with recursively regularized deep graph-CNN</article-title>
          <source>Proceedings of the 2018 World Wide Web Conference, Lyon, France</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="conf-paper">
          <page-range>648–657</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Pan</surname>
              <given-names>S. R.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>X. Q.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Pan</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICDM.2019.00075</pub-id>
          <article-title>Domain-adversarial graph neural networks for text classification</article-title>
          <source>2019 IEEE International Conference on Data Mining (ICDM), Beijing, China</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>2493-2537</page-range>
          <year>2011</year>
          <person-group person-group-type="author">
            <name>
              <surname>Collobert</surname>
              <given-names>Ronan</given-names>
            </name>
            <name>
              <surname>Weston</surname>
              <given-names>Jason</given-names>
            </name>
            <name>
              <surname>Bottou</surname>
              <given-names>Léon</given-names>
            </name>
            <name>
              <surname>Karlen</surname>
              <given-names>Michael</given-names>
            </name>
            <name>
              <surname>Kavukcuoglu</surname>
              <given-names>Koray</given-names>
            </name>
            <name>
              <surname>Kuksa</surname>
              <given-names>Pavel</given-names>
            </name>
          </person-group>
          <article-title>Natural language processing (almost) from scratch</article-title>
          <source>J. Mach. Learn. Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="conf-paper">
          <page-range>2670–2680</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Strubell</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Verga</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Belanger</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>McCallum</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18653/v1/D17-1283</pub-id>
          <article-title>Fast and accurate entity recognition with iterated dilated convolutions</article-title>
          <source>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Copenhagen, Denmark</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>283</volume>
          <page-range>31-37</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kadari</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>W. N.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Ting</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neucom.2017.12.050</pub-id>
          <article-title>CCG supertagging via Bidirectional LSTM-CRF neural architecture</article-title>
          <source>Neurocomputing</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <article-title>Bidirectional LSTM-CRF models for sequence tagging</article-title>
          <source>arXiv preprint</source>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Huang</surname>
              <given-names>Z. H.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.1508.01991</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>105</volume>
          <page-range>105-113</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Fei</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Mei Shan</given-names>
            </name>
            <name>
              <surname>Tian</surname>
              <given-names>Bo</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Bo</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>Guo Hong</given-names>
            </name>
            <name>
              <surname>Ji</surname>
              <given-names>Dong Hong</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.patrec.2017.06.009</pub-id>
          <article-title>Recognizing irregular entities in biomedical text via deep neural networks</article-title>
          <source>Pattern Recognit. Lett.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <article-title>End-to-end sequence Labeling via Bi-directional LSTM-CNNs-CRF</article-title>
          <source>arXiv preprint</source>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ma</surname>
              <given-names>X. Z.</given-names>
            </name>
            <name>
              <surname>Hovy</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.1603.01354</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="conf-paper">
          <page-range>4982–4988</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gui</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>R. T.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>L. J.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>Y. G.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>X. J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.24963/ijcai.2019/692</pub-id>
          <article-title>CNN-based Chinese NER with lexicon rethinking</article-title>
          <source>Proceedings of the 28th International Joint Conference on Artificial Intelligence, Macao</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>34</volume>
          <page-range>3539-3546</page-range>
          <issue>20</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Dang</surname>
              <given-names>Thanh Hai</given-names>
            </name>
            <name>
              <surname>Le</surname>
              <given-names>Hoang Quynh</given-names>
            </name>
            <name>
              <surname>Nguyen</surname>
              <given-names>Trang M</given-names>
            </name>
            <name>
              <surname>Vu</surname>
              <given-names>Sinh T</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1093/bioinformatics/bty356</pub-id>
          <article-title>D3NER: Biomedical named entity recognition using CRF-biLSTM improved with fine-tuned embeddings of various linguistic information</article-title>
          <source>Bioinformatics</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>4</volume>
          <page-range>357-370</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chiu</surname>
              <given-names>J. P. C.</given-names>
            </name>
            <name>
              <surname>Nichols</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1162/tacl_a_00104</pub-id>
          <article-title>Named entity recognition with bidirectional LSTM-CNNs</article-title>
          <source>Trans. Assoc. Comput. Linguist.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>L. S.</given-names>
            </name>
            <name>
              <surname>Jin</surname>
              <given-names>L. K.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>Y. X.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Recognizing biomedical named entities based on the sentence vector/twin word embeddings conditioned bidirectional LSTM</article-title>
          <source>Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data</source>
          <year>2016</year>
          <page-range>165–176</page-range>
          <pub-id pub-id-type="doi">10.1007/978-3-319-47674-215</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="conf-paper">
          <page-range>2227–2237</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Peters</surname>
              <given-names>M. E.</given-names>
            </name>
            <name>
              <surname>Neumann</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Iyyer</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Gardner</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Clark</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zettlemoyer</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18653/v1/N18-1202</pub-id>
          <article-title>Deep contextualized word representations</article-title>
          <source>Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, New Orleans, Louisiana, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1554–1564</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18653/v1/P18-1144</pub-id>
          <article-title>Chinese NER using lattice LSTM</article-title>
          <source>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Melbourne, Australia</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="conf-paper">
          <page-range>2379–2389</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>T. G.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Q. H.</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>J. Y.</given-names>
            </name>
            <name>
              <surname>Zu</surname>
              <given-names>Y. R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18653/v1/N19-1247</pub-id>
          <article-title>An encoding strategy based word-character lstm for Chinese NER</article-title>
          <source>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Minneapolis, Minnesota, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1040–1050</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gui</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Zou</surname>
              <given-names>Y. C.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Peng</surname>
              <given-names>M. L.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>J. L.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>Z. Y.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>X. J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18653/v1/D19-1096</pub-id>
          <article-title>A lexicon based graph neural network for Chinese NER</article-title>
          <source>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Hong Kong, China</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="conf-paper">
          <page-range>6836–6842</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>X. N.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Qiu</surname>
              <given-names>X. P.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>X. J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18653/v1/2020.acl-main.611</pub-id>
          <article-title>FLAT: Chinese NER using flat lattice transformer</article-title>
          <source>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>