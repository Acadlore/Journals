<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IDA</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Information Dynamics and Applications</journal-title>
        <abbrev-journal-title abbrev-type="issn">Inf. Dyn. Appl.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IDA</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-1494</issn>
      <issn publication-format="print">2958-1486</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-7io3z8BO2B8y8wo8vAkIA3a89f9zizrk</article-id>
      <article-id pub-id-type="doi">10.56578/ida030302</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Enhanced Defect Detection in Insulator Iron Caps Using Improved YOLOv8n</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-7202-5740</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Qiming</given-names>
          </name>
          <email>190319086@qq.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-8647-7541</contrib-id>
          <name>
            <surname>Liu</surname>
            <given-names>Ying</given-names>
          </name>
          <email>1220170066@mail.xhu.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8023-9366</contrib-id>
          <name>
            <surname>Tang</surname>
            <given-names>Song</given-names>
          </name>
          <email>st_4455@126.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-5646-3960</contrib-id>
          <name>
            <surname>Kang</surname>
            <given-names>Kui</given-names>
          </name>
          <email>1421330890@qq.com</email>
        </contrib>
        <aff id="aff_1">Sichuan Global Insulators Co., Ltd, 610100 Chengdu, China</aff>
        <aff id="aff_2">School of Mechanical Engineering, Xihua University, 610039 Chengdu, China</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>04</day>
        <month>09</month>
        <year>2024</year>
      </pub-date>
      <volume>3</volume>
      <issue>3</issue>
      <fpage>162</fpage>
      <lpage>170</lpage>
      <page-range>162-170</page-range>
      <history>
        <date date-type="received">
          <day>23</day>
          <month>06</month>
          <year>2024</year>
        </date>
        <date date-type="accepted">
          <day>28</day>
          <month>08</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>To address the challenges in detecting surface defects on insulator iron caps, particularly due to the complex backgrounds that hinder accurate identification, an improved defect detection algorithm based on YOLOv8n, whose full name is You Only Look Once version 8 nano, was proposed. The C2f convolutional layers in both the backbone and neck networks were replaced by the C2f-Spatial and Channel Reconstruction Convolution (SCConv) convolutional network, which strengthens the model's capacity to extract detailed surface defect features. Additionally, a Convolutional Block Attention Module (CBAM) was incorporated after the Spatial Pyramid Pooling - Fast (SPPF) layer, enhancing the extraction of deep feature information. Furthermore, the original feature fusion method in YOLOv8n was replaced with a Bidirectional Feature Pyramid Network (BiFPN), significantly improving the detection accuracy. Extensive experiments conducted on a self-constructed dataset demonstrated the effectiveness of this approach, with improvements of 2.7% and 2.9% in mAP@0.5 and mAP@0.95, respectively. The results confirm that the proposed algorithm exhibits strong robustness and superior performance in detecting insulator iron cap defects under varied conditions.</p></abstract>
      <kwd-group>
        <kwd>Insulator iron cap</kwd>
        <kwd>YOLOv8n</kwd>
        <kwd>Defect detection</kwd>
        <kwd>Spatial and channel convolution</kwd>
        <kwd>Attention mechanism</kwd>
        <kwd>Bidirectional Feature Pyramid Network</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="4"/>
        <fig-count count="9"/>
        <table-count count="2"/>
        <ref-count count="20"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>In recent years, with the continuous development of the power system, the scale of China's power grid has been gradually expanding, covering more than 95% of the country's territory. China's power grid system has the world's largest voltage span and the longest transmission lines by using a variety of power generation and transmission methods. Insulators in the power grid system are one of the important external insulation equipment in transmission and distribution lines. Their main role includes supporting and fixing the current-carrying conductor, the current-carrying conductor and the ground to form good insulation between. Common types of insulators include porcelain insulators, glass insulators and composite insulators. Porcelain and glass insulators are composed of insulating parts and fittings, and the fittings include the steel caps and feet of the insulators. In actual operation, the insulator is easy to be affected by natural conditions and environmental changes because it is exposed to the complex environment for a long time. Insulators corrode from time to time, resulting in the decline of their electrical and mechanical properties [<xref ref-type="bibr" rid="ref_1">1</xref>]. In wet conditions, leakage current flows through the insulator’s steel cap surface, which constitutes electrolytic corrosion of the chemical primary battery. In this reaction, the steel cap loses electrons to the negative reaction, thus losing the metal. The mechanical bearing capacity of corroded insulator steel caps also reduces, which in serious cases causes broken insulator strings and other consequences, resulting in great economic losses.</p><p>Insulators are usually continuously exposed to the natural environment. The cross-influence of a variety of harsh weather and climate conditions leads to corrosion, breakage, and deformation of the insulator iron caps and other failures, which is a serious threat to the reliable operation of electrical equipment. In order to increase the service life of the insulator iron caps, the surface is usually plated with a layer of zinc to enhance their corrosion resistance. However, due to the lack of zinc in the production and transport process, damage and other defects inevitably occur, which reduces the service life of the iron cap, thereby increasing the likelihood of the broken string. Therefore, in order to reduce the probability of this situation, it is necessary to carry out defect detection on the insulator iron cap before it leaves the factory, aiming to efficiently and accurately detect the defective target state of the iron cap.</p><p>In summary, it is of great significance to investigate an intelligent recognition method based on deep learning for the defect detection of insulator iron caps [<xref ref-type="bibr" rid="ref_2">2</xref>]. Therefore, this study proposes a target detection algorithm based on the YOLOv8n algorithm, and produces a dataset, including defects of insulator iron caps. Experiments show that the improved algorithm enhances the detection accuracy of the iron caps.</p>
    </sec>
    <sec sec-type="">
      <title>2. The yolov8n algorithm</title>
      <p>As shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>, the structure of the YOLOv8n algorithm [<xref ref-type="bibr" rid="ref_3">3</xref>] includes three parts: the backbone network (backbone), the feature fusion network (neck) and the prediction network (head). Compared with the YOLOv5 algorithm, YOLOv8n introduces the C2f module instead of the C3 module in the backbone and neck parts, which not only keeps the model lightweight but also enables the model to obtain richer gradient flow information. As for the head part, a decoupling head is adopted to separate the classification head from the detection head. In addition, the long-used anchor-base approach is changed to the anchor-free approach, which removes the step of generating prediction frames and avoids a large number of Intersection over Union (IoU) calculations. This enables the memory of the computer occupied by the model during the training process to become low. In the allocation method, YOLOv8n abandons the previous IoU matching or unilateral proportion allocation method, and instead adopts the task-aligned assigner positive-negative sample matching method, which not only dynamically allocates the proportion of positive-negative samples during the training process but also better adapts to different datasets and models. In terms of the loss function, the classification loss uses Binary Cross Entropy (BCELoss), and the regression loss not only uses Complete Intersection over Union Loss (CIoU Loss) but also introduces Distribution Focal Loss (DFL) in order to cooperate with the anchor-free idea. Therefore, the network can quickly learn the information near the target location and thus locate the target faster.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>YOLOv8n network architecture</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/9/img_Brkmou-V4jqs_1pM.png"/>
        </fig>
      
    </sec>
    <sec sec-type="">
      <title>3. Yolov8n algorithm improvement</title>
      
        <sec>
          
            <title>3.1. C2f-scconv module</title>
          
          <p>The complexity of texture features and the amount of interference information in the defective images of insulator surfaces lead to a large amount of redundant information in the extracted feature maps in the spatial and channel dimensions, thus weakening the adaptability and generalisation power of Convolutional Neural Networks (CNNs). In addition, the C2f structure in the original YOLOv8n network is more complex, resulting in a larger model computation and slower detection speed, which does not facilitate the deployment of embedded devices in industrial automation application scenarios. SCConv [<xref ref-type="bibr" rid="ref_4">4</xref>] is a lightweight convolution module proposed in 2023 to compress redundant features in CNNs. The structure of SCConv is shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>. The SCConv module consists of a Spatial Reconstruction Unit (SRU) and a Channel Reconstruction Unit (CRU). When dealing with the intermediate feature <italic>X</italic> in the bottleneck residual block, the spatial details are firstly optimised by the SRU unit, and then the CRU unit further refines the channel features to form the feature <italic>Y</italic>. After the SCConv module, the channel features are further refined to form the feature <italic>Y</italic>. Then the 1×1 convolution of the SCConv module reduces the number of channels to reduce the computation amount. Finally, the convolution becomes the corresponding number of channels to calculate the loss. This method effectively reduces the redundancy of space and channels in the CNN, promotes the learning of critical features, and shares the original two-branch structure parameterised to form a smaller and lighter head structure [<xref ref-type="bibr" rid="ref_5">5</xref>].</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>SCConv module</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/9/img_K4Hb1drNzE3UjXtp.png"/>
            </fig>
          
          <p>In this study, SCConv was adopted to replace the bottleneck structure in the original C2f module, which can effectively reduce the feature redundancy in the spatial and channel dimensions as well as the complexity and computational cost of the model, thereby realising the lightweight of the model. The improved C2f module is named C2f-SCConv module, and its structure is shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>. Using the C2f-SCConv module to replace the original C2f structure in the neck network can improve the detection performance of the model while reducing resource consumption.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>C2f-SCConv structure</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/9/img_5xLi_bMgct6QfB1-.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Cbam</title>
          
          <p>The deepening of the network layers helps the model to extract the features of defects, but the model also loses the detail information during the downsampling operation, which is not conducive to the target defect detection of insulator iron caps [<xref ref-type="bibr" rid="ref_6">6</xref>]. Therefore, in order to improve the model's ability to capture key information in various types of defects and reduce the interference of irrelevant information, this study introduces the CBAM [<xref ref-type="bibr" rid="ref_7">7</xref>] after the SPPF layer, which integrates the channel attention mechanism with the spatial attention mechanism. As shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>, the feature map is first input into the channel attention module, and the corresponding attention map is output. Therefore, the channel attention mechanism can be used to highlight the channels related to the defective features of the insulator iron cap and suppress the irrelevant channels, thereby enhancing the ability of the model to extract the defective features of the insulator iron cap in terms of channels [<xref ref-type="bibr" rid="ref_8">8</xref>]. Then the input feature map is multiplied with the attention map, and the output goes through the spatial attention module. The result obtained after the spatial attention multiplication is multiplied with the original result to obtain the output result. The spatial attention mechanism highlights image regions of interest and reduces the influence of interfering regions by weighting the features at each spatial location of the surface defects of iron caps [<xref ref-type="bibr" rid="ref_9">9</xref>]. This enables the model to improve the attention of the YOLOv8n-SCConv model to the features of the insulator iron cap defects from both channel and spatial aspects, which better distinguishes the background from the target and thus improves the accuracy of the detection results.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>CBAM module</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/9/img_KokHFSB5XEQsQf3-.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. Bifpn</title>
          
          <p>Path Aggregation Network (PANet) [<xref ref-type="bibr" rid="ref_10">10</xref>] is used in YOLOv8n. PANet [<xref ref-type="bibr" rid="ref_11">11</xref>] has both top-down and bottom-up paths, which makes it fuse top and bottom feature information. Although this method can improve the detection accuracy, it is easy to miss tiny target objects [<xref ref-type="bibr" rid="ref_12">12</xref>]. To solve this problem, this study replaces PANet with BiFPN [<xref ref-type="bibr" rid="ref_13">13</xref>]. The structure of PANet and BiFPN is shown in subgraphs (a) and (b) of <xref ref-type="fig" rid="fig_5">Figure 5</xref>. PANet introduces a reverse path on the basis of Feature Pyramid Network (FPN) [<xref ref-type="bibr" rid="ref_14">14</xref>] to convey the missing position information, as shown in subgraph (a) of <xref ref-type="fig" rid="fig_5">Figure 5</xref>. BiFPN is a weighted BiFPN, as shown in subgraph (b) of <xref ref-type="fig" rid="fig_5">Figure 5</xref>. It simplifies the network structure based on PANet by removing a single input node to ensure that no important information is lost. Meanwhile, additional features are integrated by adding extra edges when the input and output nodes are located in the same layer. Specifically, BiFPN designs a top-down path from p7 to p3 that passes semantic information from higher-level features to the bottom layer, and a bottom-up path from p3 to p7 that passes new information about the location of the bottom layer to higher levels. In addition, BiFPN adds a connection from p4 to p6 that directly connects the input and output nodes of the same layer across intermediate layers for deeper feature fusion. As the model is trained deeper, this dynamic weight adjustment mechanism allows BiFPN to gradually optimise its feature fusion strategy so that it can more accurately mine and integrate useful information from different layers. The learned features are then used to continuously update the weights to obtain more valuable information [<xref ref-type="bibr" rid="ref_15">15</xref>].</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Model structure of PANet and BiFPN: (a) PANet; (b) BiFPN</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/9/img_bYnLHxuguDcT2JAX.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/9/img_uVOA1Q_l7DkBit5f.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.4. Improved yolov8n-scpn network structure</title>
          
          <p>YOLOv8n was improved using the above method to obtain a model, whose structure is shown in <xref ref-type="fig" rid="fig_6">Figure 6</xref>. In this study, SCConv was used to replace the bottleneck structure in the original C2f module to form the C2f-SCConv structure, which enables the backbone network to use a larger sensory field and obtain more semantic information, effectively reducing the feature redundancy in the spatial and channel dimensions, the complexity of the model and the cost of computation, thereby realising the model's lightweight. Secondly, the CBAM was introduced between the backbone network and the neck network to improve the network's attention to the defective target during feature extraction, thus achieving the purpose of optimising the network performance. Finally, the PANet in the neck was replaced with BiFPN, and the introduction of BiFPN can exchange a small amount of computational cost for a larger model performance gain.</p>
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Improved YOLOv8-SCPN network structure</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/9/img_IT74-7uJ51d7XT1B.png"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Experimental validation and analysis of results</title>
      
        <sec>
          
            <title>4.1. Pre-processing of the dataset</title>
          
          <p>In this study, the dataset uses the industrial greyscale camera A3A20MG8, lens M2016-12MP-2 of Huarui Technology to take images of the insulator iron cap defects for experiments. However, due to the irregular surfaces of the collected insulator iron caps, the images usually have background interference or their features are not obvious enough. Therefore, the image quality can be improved by the pre-processing method.</p><p>Image enhancement mainly aims to improve the recognition of image feature information in the background and interference elements. Filtering the casting image can suppress noise, but the image regions have a certain blurring and defects in the image colour and geometric information in the image of the contrast decrease. In order to reduce the impact of filtering and highlight the texture feature information to improve the subsequent algorithms for regional processing and the stability and accuracy of feature recognition, there is a need for image enhancement processing, aiming to improve defective features and image quality through pre-processing methods. In order to reduce the effect of filtering and highlight the texture feature information to improve the stability and accuracy of the subsequent algorithm and feature recognition, it is necessary to enhance the image to improve the recognition of defect feature information. In this study, the gamma transform was used to enhance the image of insulator iron cap defects.</p><p>Because the gamma transform has the advantages of improving the visual effect of the image [<xref ref-type="bibr" rid="ref_16">16</xref>], correcting the image illumination, and having simple operations, it is widely used in the enhancement of low-quality images. According to the different values of the input parameter, it has a homogeneous effect on the global image value, and has a good effect on the image brightness and overexposed area suppression. The principle of its action is as follows: different pixel values of the image are homogeneous according to the value of the gamma value, but the difference of the transformation changes the contrast of the image feature information. The gamma transform formula is as follows:</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="m1nrvhos91">
                <mml:mi>s</mml:mi>
                <mml:mi>c</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:msup>
                  <mml:mi>r</mml:mi>
                  <mml:mi>r</mml:mi>
                </mml:msup>
              </mml:math>
            </disp-formula>
          
          <p> where, $c<inline-formula>
  <mml:math id="m3oiznm7fa">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>1 ; \gamma<inline-formula>
  <mml:math id="mi0hr6i7z0">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>r<inline-formula>
  <mml:math id="mcioi95byb">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>s<inline-formula>
  <mml:math id="m35lkevbnr">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>\gamma<inline-formula>
  <mml:math id="m7yhkv0utm">
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\gamma$ values was plotted, as shown in <xref ref-type="fig" rid="fig_7">Figure 7</xref>, where the horizontal coordinate is the input and the vertical coordinate is the output.</p>
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>Curves of different gamma values</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/9/img_TtFl8Od-73NSAzaD.png"/>
            </fig>
          
          <p>As can be seen from the figure, when <inline-formula>
  <mml:math id="mvaacbb1rl">
    <mml:mi>γ</mml:mi>
  </mml:math>
</inline-formula> is taken less than 1, the pixel value of the image increases; when <inline-formula>
  <mml:math id="m9ici5bwkz">
    <mml:mi>γ</mml:mi>
  </mml:math>
</inline-formula> is taken more than 1, the pixel value of the image decreases; when the value of <inline-formula>
  <mml:math id="mcjrcwjs0o">
    <mml:mi>γ</mml:mi>
  </mml:math>
</inline-formula> is taken to be 0.8, 1.3, and 1.5, the effect of the processing of the insulator cap is shown in <xref ref-type="fig" rid="fig_8">Figure 8</xref>.</p>
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>Comparison of different gamma values with the original figure: (a) Original diagram of the iron cap; (b) $\gamma=1.3$; (c) $\gamma=1.5$; (d) $\gamma=0.8$</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/9/img_eKI9N-Bc9OtPM96m.png"/>
            </fig>
          
          <p>As can be seen from the figure, when the value of <inline-formula>
  <mml:math id="myhw33fcwx">
    <mml:mi>γ</mml:mi>
  </mml:math>
</inline-formula> is greater than 1, the overall brightness of the image is enhanced, but the part of the defective features with a lower grey value is enhanced relatively large, resulting in a decrease in the contrast between the defective and normal areas. When the value of <inline-formula>
  <mml:math id="m9dsf55byg">
    <mml:mi>γ</mml:mi>
  </mml:math>
</inline-formula> is less than 1, the overall brightness of the image is reduced, and the contrast of the defects in the normal surface is enhanced, but a larger value also blurs the edges and the background boundary information.</p><p>After the above image enhancement, the dataset was divided and labelled for the collected defect images. The 4,000 defect images were divided into training set, validation set and test set according to the ratio of 8:1:1, among which the training set is 3200, the validation set is 400, and the test set is 400. <xref ref-type="fig" rid="fig_9">Figure 9</xref> shows the partial sample examples of the defects in the insulator's iron cap, and the dataset was manually labelled by Labelimg software.</p>
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>
                <title>Sample defects</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/9/img_e2Fx9CmQ1yvDoaGv.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.2. Experimental environment and parameter settings</title>
          
          <p>The Python version used in this experiment is 3.11, the training framework is Pytorch 2.3.1, and the graphics card is NVDIA GeForce 4070 with 12GB of video memory. CUDA 12.1 was selected to train the model on the Win11 system. The training number was set to 150 epochs, batch size to 16, learning rate to 0.001, and momentum to 0.937.</p>
        </sec>
      
      
        <sec>
          
            <title>4.3. Evaluation indicators</title>
          
          <p>In order to validate the effectiveness of the proposed model, three metrics, mean accuracy (mAP), precision (P) and recall (R), were used for comprehensive evaluation [<xref ref-type="bibr" rid="ref_17">17</xref>].</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="m9mykr2tyz">
                <mml:mi>P</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="mx96uclbgy">
                <mml:mi>R</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="mtu1rgr6l0">
                <mml:mi>m</mml:mi>
                <mml:mi>A</mml:mi>
                <mml:mi>P</mml:mi>
                <mml:mi>d</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mn>1</mml:mn>
                  <mml:mi>N</mml:mi>
                </mml:mfrac>
                <mml:munderover>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>N</mml:mi>
                </mml:munderover>
                <mml:msubsup>
                  <mml:mo>∫</mml:mo>
                  <mml:mn>0</mml:mn>
                  <mml:mn>1</mml:mn>
                </mml:msubsup>
                <mml:msub>
                  <mml:mi>P</mml:mi>
                  <mml:mi>i</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>R</mml:mi>
                  <mml:mi>i</mml:mi>
                </mml:msub>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>R</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="msouuchm8j">
    <mml:mi>T</mml:mi>
    <mml:mi>P</mml:mi>
  </mml:math>
</inline-formula> denotes the number of positive samples predicted as positive samples, <inline-formula>
  <mml:math id="mej3epzoav">
    <mml:mi>F</mml:mi>
    <mml:mi>P</mml:mi>
  </mml:math>
</inline-formula> denotes the number of negative samples predicted as positive samples, and <inline-formula>
  <mml:math id="m4i7pf6m84">
    <mml:mi>F</mml:mi>
    <mml:mi>N</mml:mi>
  </mml:math>
</inline-formula> denotes the number of positive samples predicted as negative samples in the number of samples. <inline-formula>
  <mml:math id="mysh5bfzvy">
    <mml:msub>
      <mml:mi>p</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is the value of accuracy, <inline-formula>
  <mml:math id="mabch4qawo">
    <mml:msub>
      <mml:mi>R</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is the value of recall, and $n<inline-formula>
  <mml:math id="mjs7gajdog">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>[</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mn>18</mml:mn>
  </mml:math>
</inline-formula>m A P$ is the average of the area of each Precision-Recall (PR) curve, and takes the value ranging from 0 to 1.</p>
        </sec>
      
      
        <sec>
          
            <title>4.4. Comparative experiments with different algorithms</title>
          
          <p>In order to further verify the feasibility of the YOLOv8n algorithm proposed in this study, it was compared with the Faster Region-based CNN (Faster R-CNN) algorithm [<xref ref-type="bibr" rid="ref_19">19</xref>], the Single Shot MultiBox Detector (SSD) algorithm [<xref ref-type="bibr" rid="ref_20">20</xref>], and other algorithms of the YOLO series, YOLOv5s and YOLOv8n, in the dataset. mAP@0.5 was used as a measure of the performance of the model, and the results of the comparison experiments are shown in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Comparison of detection results using different models</title>
              </caption>
              <table><tr><th >Model</th><th >mAP@0.5</th></tr><tr><td >Faster R-CNN</td><td >87.6</td></tr><tr><td >SSD</td><td >85.1</td></tr><tr><td >YOLOv5s</td><td >88.2</td></tr><tr><td >YOLOv8n-SCPN</td><td >90.6</td></tr></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>4.5. Ablation experiments</title>
          
          <p>In order to verify the effectiveness of the improved algorithm proposed in this study, different experimental groups were set up and ablation experiments were conducted. As shown in <xref ref-type="table" rid="table_2">Table 2</xref>, Experiment 1 was trained using YOLOv8n, whose data served as the control group. In Experiments 2 to 8, different modules were progressively replaced or added to the network.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Ablation experiments</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1" colwidth="67"></td><td colspan="1" rowspan="1" colwidth="140"><p>BiFPN</p></td><td colspan="1" rowspan="1" colwidth="146"><p>C2f-SCConv </p></td><td colspan="1" rowspan="1" colwidth="136"><p>CBAM</p></td><td colspan="1" rowspan="1" colwidth="130"><p>P</p></td><td colspan="1" rowspan="1" colwidth="125"><p>R</p></td><td colspan="1" rowspan="1" colwidth="135"><p>mAP@0.5</p></td><td colspan="1" rowspan="1" colwidth="133"><p>mAP@0.95</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="67"><p>1</p></td><td colspan="1" rowspan="1" colwidth="140"><p><mml:math id="mzo5l4oo1a">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="146"><p><mml:math id="mvmk46e96j">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="136"><p><mml:math id="m52jive6fn">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="130"><p>0.841</p></td><td colspan="1" rowspan="1" colwidth="125"><p>0.728</p></td><td colspan="1" rowspan="1" colwidth="135"><p>0.879</p></td><td colspan="1" rowspan="1" colwidth="133"><p>0.513</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="67"><p>2</p></td><td colspan="1" rowspan="1" colwidth="140"><p><mml:math id="m9q2vtn6xw">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="146"><p><mml:math id="msmk54qehb">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="136"><p><mml:math id="m3hov8r9v8">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="130"><p>0.851</p></td><td colspan="1" rowspan="1" colwidth="125"><p>0.751</p></td><td colspan="1" rowspan="1" colwidth="135"><p>0.883</p></td><td colspan="1" rowspan="1" colwidth="133"><p>0.528</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="67"><p>3</p></td><td colspan="1" rowspan="1" colwidth="140"><p><mml:math id="m1x0ov0ejm">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="146"><p><mml:math id="m2rjzxfa78">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="136"><p><mml:math id="mftrdirnt3">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="130"><p>0.853</p></td><td colspan="1" rowspan="1" colwidth="125"><p>0.742</p></td><td colspan="1" rowspan="1" colwidth="135"><p>0.894</p></td><td colspan="1" rowspan="1" colwidth="133"><p>0.516</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="67"><p>4</p></td><td colspan="1" rowspan="1" colwidth="140"><p><mml:math id="m1878w73ua">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="146"><p><mml:math id="m59xmpe73i">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="136"><p><mml:math id="mpov0f1emc">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="130"><p>0.859</p></td><td colspan="1" rowspan="1" colwidth="125"><p>0.753</p></td><td colspan="1" rowspan="1" colwidth="135"><p>0.892</p></td><td colspan="1" rowspan="1" colwidth="133"><p>0.532</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="67"><p>5</p></td><td colspan="1" rowspan="1" colwidth="140"><p><mml:math id="mm2ihdi5u9">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="146"><p><mml:math id="m7n26aa2f6">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="136"><p><mml:math id="mp49it385x">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="130"><p>0.865</p></td><td colspan="1" rowspan="1" colwidth="125"><p>0.760</p></td><td colspan="1" rowspan="1" colwidth="135"><p>0.892</p></td><td colspan="1" rowspan="1" colwidth="133"><p>0.521</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="67"><p>6</p></td><td colspan="1" rowspan="1" colwidth="140"><p><mml:math id="m1p4vgxg3l">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="146"><p><mml:math id="mudcqo2tr2">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="136"><p><mml:math id="mj5muxa5w9">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="130"><p>0.855</p></td><td colspan="1" rowspan="1" colwidth="125"><p>0.763</p></td><td colspan="1" rowspan="1" colwidth="135"><p>0.901</p></td><td colspan="1" rowspan="1" colwidth="133"><p>0.524</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="67"><p>7</p></td><td colspan="1" rowspan="1" colwidth="140"><p><mml:math id="m725pe9yua">
  <mml:mo>×</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="146"><p><mml:math id="m0zjji24fi">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="136"><p><mml:math id="m81h85cd1v">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="130"><p>0.871</p></td><td colspan="1" rowspan="1" colwidth="125"><p>0.759</p></td><td colspan="1" rowspan="1" colwidth="135"><p>0.895</p></td><td colspan="1" rowspan="1" colwidth="133"><p>0.514</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="67"><p>8</p></td><td colspan="1" rowspan="1" colwidth="140"><p><mml:math id="mj6gn7l8s0">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="146"><p><mml:math id="mvlmcctyah">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="136"><p><mml:math id="ma3305zq4j">
  <mml:msqrt/>
</mml:math></p></td><td colspan="1" rowspan="1" colwidth="130"><p>0.882</p></td><td colspan="1" rowspan="1" colwidth="125"><p>0.776</p></td><td colspan="1" rowspan="1" colwidth="135"><p>0.906</p></td><td colspan="1" rowspan="1" colwidth="133"><p>0.542</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>From Experiments 2-4, it can be seen that compared with YOLOv8n, the improved algorithm has different degrees of improvement in mAP@0.5 and mAP@0.95 when using only the BiFPN module, the C2f-SCConv module or the CBAM. This indicates that the problem of defective targets, which BiFPN can easily identify incorrectly, is solved after using the BiFPN module. The receptive field of the network was enlarged after using the C2f-SCConv module, which effectively improves the feature extraction capability of the network. The addition of the CBAM makes the network focus more on the defective region. From Experiments 5-7, it can be seen that combining any two of the three modules added to the network improves the detection accuracy compared to YOLOv8n. This further illustrates the effectiveness of these module improvements. From Experiment 8, it can be seen that after the simultaneous addition of the three improved modules, its mAP@0.5 and mAP@0.95 reached the highest among all experimental groups. Compared with YOLOv8n, the improvement is 2.7% and 2.9%, respectively. It illustrates the effectiveness of the three modules added to the network at the same time. In conclusion, the YOLOv8n-SCBP algorithm outperforms the original algorithm as well as the algorithms of Experiments 2-7 in the ablation experiments in terms of performance metrics, proving the superiority of the proposed algorithm in the task of stacked workpiece detection.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>Aiming at the defects on the surface of insulator iron caps, the difficulty to distinguish the complex defect background leads to certain leakage and the wrong detection phenomenon of existing defect detection algorithms. Therefore, this study proposes a new YOLOv8n-SCPN algorithm on the basis of YOLOv8n. Firstly, the C2f convolutional network was replaced by the C2f-SCConv convolutional network in the backbone and neck network, which enhanced the ability of the model to extract the features on the surface of the insulator iron cap. Secondly, the CBAM was introduced after the SPPF layer, which enables the feature extraction network to extract more effective information about the surface defects of insulator caps. Finally, a BiFPN was added to the neck to replace the original feature fusion method in YOLOv8n in order to improve the model's ability to detect defects in insulator caps. Compared with the YOLOv8n algorithm, the YOLOv8n-SCPN algorithm proposed in this study has significantly improved the accuracy, recall and mAP on the home-made dataset. However, the YOLOv8n-SCPN algorithm still has low precision in detecting complex surface defects and still needs to be optimised in terms of the number of parameters. Therefore, the algorithm needs to be further improved in detecting complex surface defects so that it can achieve more lightweight and efficient detection.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>543-549</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sanyal</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Jeon</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>Y. J.</given-names>
            </name>
            <name>
              <surname>Yi</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Choi</surname>
              <given-names>I. H.</given-names>
            </name>
            <name>
              <surname>Son</surname>
              <given-names>J. A.</given-names>
            </name>
            <name>
              <surname>Koo</surname>
              <given-names>J. B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s42341-020-00239-3</pub-id>
          <article-title>Influence of corrosion on electrical and mechanical properties of porcelain suspension insulators: An overview</article-title>
          <source>Trans. Electr. Electron. Mater.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>677</page-range>
          <issue>7</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hussain</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/machines11070677</pub-id>
          <article-title>YOLO-v1 to YOLO-v8, the rise of YOLO and its complementary nature toward digital manufacturing and industrial defect detection</article-title>
          <source>Machines</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Khare</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Gandhi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Rahalkar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Mane</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>YOLOv8-based visual detection of road hazards: Potholes, sewer covers, and manHoles</article-title>
          <source>2023 IEEE Pune Section International Conference (PuneCon), Pune, India</source>
          <year>2023</year>
          <page-range>1-6</page-range>
          <pub-id pub-id-type="doi">10.1109/PuneCon58714.2023.10449999</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wen</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>SCConv: Spatial and channel reconstruction convolution for feature redundancy</article-title>
          <source>2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Vancouver, BC, Canada</source>
          <year>2023</year>
          <page-range>6153-6162</page-range>
          <pub-id pub-id-type="doi">10.1109/CVPR52729.2023.00596</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>148</volume>
          <page-range>108982</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Souza</surname>
              <given-names>B. J.</given-names>
            </name>
            <name>
              <surname>Stefenon</surname>
              <given-names>S. F.</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Freire</surname>
              <given-names>R. Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ijepes.2023.108982</pub-id>
          <article-title>Hybrid-YOLO for classification of insulators defects in transmission lines based on UAV</article-title>
          <source>Int. J. Electr. Power Energy Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Dey</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Dehaerne</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Halder</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Towards improving challenging stochastic defect detection in SEM images based on improved YOLOv5</article-title>
          <source>Photomask Technology 2022, Monterey, California, United States</source>
          <year>2022</year>
          <volume>12293</volume>
          <page-range>28-37</page-range>
          <pub-id pub-id-type="doi">10.1117/12.2645402</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Woo</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>J. Y.</given-names>
            </name>
            <name>
              <surname>Kweon</surname>
              <given-names>I. S.</given-names>
            </name>
          </person-group>
          <article-title>CBAM: Convolutional block attention module</article-title>
          <source>Proceedings of the European Conference on Computer Vision (ECCV)</source>
          <year>2018</year>
          <page-range>3-19</page-range>
          <pub-id pub-id-type="doi">10.48550/arXiv.1807.06521</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>304</page-range>
          <issue>5</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Yiting</given-names>
            </name>
            <name>
              <surname>Fan</surname>
              <given-names>Qingsong</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Haisong</given-names>
            </name>
            <name>
              <surname>Han</surname>
              <given-names>Zhenggong</given-names>
            </name>
            <name>
              <surname>Gu</surname>
              <given-names>Qiang</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/drones7050304</pub-id>
          <article-title>A modified YOLOv8 detection network for UAV aerial image recognition</article-title>
          <source>Drones</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <page-range>452</page-range>
          <issue>9</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sami</surname>
              <given-names>Abdullah As</given-names>
            </name>
            <name>
              <surname>Sakib</surname>
              <given-names>Saadman</given-names>
            </name>
            <name>
              <surname>Deb</surname>
              <given-names>Kaushik</given-names>
            </name>
            <name>
              <surname>Sarker</surname>
              <given-names>Iqbal H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/a16090452</pub-id>
          <article-title>Improved YOLOv5-based real-time road pavement damage detection in road infrastructure management</article-title>
          <source>Algorithms</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Qi</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Qin</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Jia</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Path aggregation network for instance segmentation</article-title>
          <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
          <year>2018</year>
          <page-range>8759-8768</page-range>
          <pub-id pub-id-type="doi">10.48550/arXiv.1803.01534</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Liew</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Zou</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Feng</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>PANet: Few-shot image semantic segmentation with prototype alignment</article-title>
          <source>2019 IEEE/CVF International Conference on Computer Vision (ICCV), Seoul, Korea (South)</source>
          <year>2019</year>
          <page-range>9196-9205</page-range>
          <pub-id pub-id-type="doi">10.1109/iccv.2019.00929</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>83138-83151</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2022.3197146</pub-id>
          <article-title>AB-DLM: An improved deep learning model based on attention mechanism and BiFPN for driver distraction behavior detection</article-title>
          <source>IEEE Access.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Tan</surname>
              <given-names>Mingxing</given-names>
            </name>
            <name>
              <surname>Pang</surname>
              <given-names>Ruoming</given-names>
            </name>
            <name>
              <surname>Le</surname>
              <given-names>Quoc V.</given-names>
            </name>
          </person-group>
          <article-title>EfficientDet: Scalable and efficient object detection</article-title>
          <source>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>
          <year>2020</year>
          <page-range>10781-10790</page-range>
          <pub-id pub-id-type="doi">10.48550/arXiv.1911.09070</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Lin</surname>
              <given-names>T. Y.</given-names>
            </name>
            <name>
              <surname>Dollar</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Girshick</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Hariharan</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Belongie</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Feature pyramid networks for object detection</article-title>
          <source>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
          <year>2017</year>
          <page-range>2117-2125</page-range>
          <pub-id pub-id-type="doi">10.48550/arXiv.1612.03144</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>577-586</page-range>
          <issue>4</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Alibek</surname>
              <given-names>Esanov</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>Kang Chul</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.13067/JKIECS.2022.17.4.577</pub-id>
          <article-title>Metal surface defect detection and classification using EfficientNetV2 and YOLOv5</article-title>
          <source>J. Korea Inst. Electron. Commun. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>2021</volume>
          <page-range>5563698</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Wencheng</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>Xiaohui</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Zhenxue</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>XiaoJin</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>Zairui</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2021/5563698</pub-id>
          <article-title>Weak-light image enhancement method based on adaptive local gamma transform and color compensation</article-title>
          <source>J. Sens.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>139</volume>
          <page-range>102919</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kwon</surname>
              <given-names>Jung Eun</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>Jae Hyeon</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>Ju Hyun</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>Yun Hak</given-names>
            </name>
            <name>
              <surname>Cho</surname>
              <given-names>Sung In</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ndteint.2023.102919</pub-id>
          <article-title>Context and scale-aware YOLO for welding defect detection</article-title>
          <source>NDT E Int.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>23</volume>
          <page-range>7190</page-range>
          <issue>16</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Gang</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Yanfei</given-names>
            </name>
            <name>
              <surname>An</surname>
              <given-names>Pei</given-names>
            </name>
            <name>
              <surname>Hong</surname>
              <given-names>Hanyu</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>Jinghu</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Tiange</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s23167190</pub-id>
          <article-title>UAV-YOLOv8: A small-object-detection model based on improved YOLOv8 for UAV aerial photography scenarios</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>39</volume>
          <page-range>1137-1149</page-range>
          <issue>6</issue>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ren</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Girshick</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TPAMI.2016.2577031</pub-id>
          <article-title>Faster R-CNN: Towards real-time object detection with region proposal networks</article-title>
          <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>Wei</given-names>
            </name>
            <name>
              <surname>Anguelov</surname>
              <given-names>Dragomir</given-names>
            </name>
            <name>
              <surname>Erhan</surname>
              <given-names>Dumitru</given-names>
            </name>
            <name>
              <surname>Szegedy</surname>
              <given-names>Christian</given-names>
            </name>
            <name>
              <surname>Reed</surname>
              <given-names>Scott</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>Cheng Yang</given-names>
            </name>
            <name>
              <surname>Berg</surname>
              <given-names>Alexander C.</given-names>
            </name>
          </person-group>
          <article-title>SSD: Single shot MultiBox detector</article-title>
          <source>Computer Vision–ECCV 2016</source>
          <publisher-name>Springer, Cham</publisher-name>
          <year>2016</year>
          <volume>9905</volume>
          <page-range>21-37</page-range>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>