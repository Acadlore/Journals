<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IJKIS</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>International Journal of Knowledge and Innovation Studies</journal-title>
        <abbrev-journal-title abbrev-type="issn">Int J. Knowl. Innov Stud.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IJKIS</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">3005-6101</issn>
      <issn publication-format="print">3005-6098</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-cpHXWQUFHtSc3mLg3w-fbWs1Ao-WAOFi</article-id>
      <article-id pub-id-type="doi">10.56578/ijkis020105</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Enhanced Detection of Soybean Leaf Diseases Using an Improved Yolov5 Model</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3989-8367</contrib-id>
          <name>
            <surname>Peng</surname>
            <given-names>Shiqin</given-names>
          </name>
          <email>pengshiqin@byau.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-0725-3164</contrib-id>
          <name>
            <surname>Xi</surname>
            <given-names>Guiqing</given-names>
          </name>
          <email>xiguiqing@byau.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-8917-8568</contrib-id>
          <name>
            <surname>Wei</surname>
            <given-names>Yongshun</given-names>
          </name>
          <email>weiyongshun@byau.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-3207-5380</contrib-id>
          <name>
            <surname>Yu</surname>
            <given-names>Ling</given-names>
          </name>
          <email>yuling@byau.edu.cn</email>
        </contrib>
        <aff id="aff_1">School of Information and Electrical Engineering, Heilongjiang Bayi Agricultural University, 163319 Daqing, China</aff>
        <aff id="aff_2">BGP INC.,China National Petroleum Corporation, 163319 Daqing, China</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>30</day>
        <month>03</month>
        <year>2024</year>
      </pub-date>
      <volume>2</volume>
      <issue>1</issue>
      <fpage>45</fpage>
      <lpage>56</lpage>
      <page-range>45-56</page-range>
      <history>
        <date date-type="received">
          <day>08</day>
          <month>01</month>
          <year>2024</year>
        </date>
        <date date-type="accepted">
          <day>21</day>
          <month>03</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>To facilitate early intervention and control efforts, this study proposes a soybean leaf disease detection method based on an improved Yolov5 model. Initially, image preprocessing is applied to two datasets of diseased soybean leaf images. Subsequently, the original Yolov5s network model is modified by replacing the Spatial Pyramid Pooling (SPP) module with a simplified SimSPPF for more efficient and precise feature extraction. The backbone Convolutional Neural Network (CNN) is enhanced with the Bottleneck transformer (BotNet) self-attention mechanism to accelerate detection speed. The Complete Intersection over Union (CIoU) loss function is replaced by EIoU-Loss to increase the model's inference speed, and Enhanced Intersection over Union (EIoU)-Non-Maximum Suppression (NMS) is used instead of traditional NMS to optimize the handling of prediction boxes. Experimental results demonstrate that the modified Yolov5s model increases the mean Average Precision (mAP) value by 4.5% compared to the original Yolov5 network model for the detection and identification of soybean leaf diseases. Therefore, the proposed method effectively detects and identifies soybean leaf diseases and can be validated for practicality in actual production environments.</p></abstract>
      <kwd-group>
        <kwd>Soybean leaf disease</kwd>
        <kwd>Improved Yolov5</kwd>
        <kwd>Bottleneck transformer (BotNet)</kwd>
        <kwd>Disease detection</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="4"/>
        <fig-count count="10"/>
        <table-count count="3"/>
        <ref-count count="34"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Soybean is one of the important grain crops in China. During the growth of soybeans, the occurrence of diseases can cause the plants to weaken and become infected, affecting the yield and quality of soybeans. Therefore, it is extremely important to quickly detect and carry out early prevention and control tasks to avoid the economic losses caused by diseases in soybean cultivation each year [<xref ref-type="bibr" rid="ref_1">1</xref>].</p><p>Traditional detection methods mainly fall into two categories. One is manual detection and identification, which requires a large amount of manpower, material resources, and time costs, and the detection results are susceptible to human subjective consciousness, leading to misjudgments. The other is based on image-based machine learning methods. Shrivastava and Hooda<span style="font-family: Times, serif"> [<xref ref-type="bibr" rid="ref_2">2</xref>] proposed a method based on digital image processing technology to detect and classify soybean leaf blight and gray spot disease, with identification accuracies of 70% and 80%, respectively. This method extracts the shape feature vectors of leaf images and uses the K-Nearest Neighbors (KNN) classifier for detection and classification. However, the recognition accuracy of this method is not enough, and the extraction of image shape feature vectors is relatively simple, which cannot distinguish leaves with complex backgrounds and deformation features. Araujo and Peixoto [<xref ref-type="bibr" rid="ref_3">3</xref>] proposed a digital image processing technique combining color moments, Local Binary Patterns (LBP), and Bag of Visual Words (BoVW) models, using the extracted image features as inputs for a Support Vector Machine (SVM) to achieve disease classification. However, the recognition rate of this method only reached 75.8%, which is not sufficient for application in real environments. Traditional machine learning requires a series of complex data processing steps, and generally uses simpler function forms, lacking the expressive power of complex models, leading to overfitting and low recognition accuracy in real environment disease detection.</p><p>Currently, researchers both domestic and international mainly focus on deep learning for the detection and identification of soybean diseases. For example, Li et al. [<xref ref-type="bibr" rid="ref_4">4</xref>] proposed combining the feature pyramid model with the Faster R-CNN model, which achieved an average precision mean of 82.48% for the detection of five types of apple leaf diseases. However, this method is not accurate enough for disease detection and the model detection has certain biases. He et al. [<xref ref-type="bibr" rid="ref_5">5</xref>] used an improved Yolov5 model based on weighted bidirectional feature fusion technology to detect pests in economic forests, with an average precision mean reaching 92.3%. However, the complex background of the dataset limits the extraction of feature targets in this method.</p><p>This paper focuses on whether soybean disease detection can achieve high accuracy and be applied to actual agricultural production environments. It proposes to improve the SPP structure based on the original Yolov5s network model, enhance the model's data feature extraction capabilities, make the model training more efficient, improve the CNN architecture in the backbone network to further enhance the model's detection accuracy, replace the CIoU loss function, improve NMS, and improve the detection of occluded targets. This study investigates the improved Yolov5s model's detection and identification rates for two types of soybean leaf diseases, aiming to improve the accuracy of soybean disease detection and various identification schemes.</p>
    </sec>
    <sec sec-type="">
      <title>2. Yolov5 network model and improvements</title>
      
        <sec>
          
            <title>2.1. Yolov5 network structure</title>
          
          <p>Yolov5 is a one-stage object detection network, which can be further subdivided into several different versions based on the size of the algorithm model and computational complexity: Yolov5s, Yolov5m, Yolov5l, Yolov5x, and Yolov5n. As the depth and width of the network model increase, the model's detection accuracy further improves, but at the cost of slower detection speeds. Therefore, this paper chooses the Yolov5s model, which has lower model complexity. It better meets the real-time requirements of this study, consuming less computing power to maximize recognition speed [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>].</p><p>The Yolov5s model structure primarily consists of the Input, Backbone, Neck, and Prediction segments. The Input part uses the Mosaic data augmentation method, which randomly scales, crops, redistributes, and stitches the input data, adding many small targets and enhancing the robustness of the trained model. The Backbone is the feature extraction part of the Yolov5 network, where the feature extraction capability directly affects the entire network's performance, it includes the Focus, Conv, C3, and SPP modules. The Focus module slices the image, transferring the image's width (W) and height (H) information to the channel space, allowing for 2x downsampling without losing any information. The Conv module performs convolution, batch normalization (BN), and activation function operations on the input feature map. The C3 module is used for part of the feature map extraction, where one part goes through block calculations, and another part through a convolutional shortcut, both parts are then combined using concat. The SPP module is designed to fuse feature maps of different resolutions by reducing the input channels by half with a standard convolutional module, followed by pooling operations with kernel sizes of 5, 9, and 13, and then concatenating the three max pooling results with the unpooled data, finally doubling the channel number [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>], [<xref ref-type="bibr" rid="ref_12">12</xref>], [<xref ref-type="bibr" rid="ref_13">13</xref>], [<xref ref-type="bibr" rid="ref_14">14</xref>], [<xref ref-type="bibr" rid="ref_15">15</xref>]. The Neck is composed of FPN+PAN; the FPN structure downsamples feature maps of different resolutions to obtain a set of feature maps with high semantic content, then the PAN upsamples these feature maps, enlarging their dimensions to detect small targets with large-sized feature maps and large targets with small-sized ones, merging high and low-level feature information to output prediction feature maps. The Prediction part mainly uses the loss function (CIoU) Loss and NMS for post-processing and target prediction box handling [<xref ref-type="bibr" rid="ref_16">16</xref>], [<xref ref-type="bibr" rid="ref_17">17</xref>], [<xref ref-type="bibr" rid="ref_18">18</xref>].</p>
        </sec>
      
      
        <sec>
          
            <title>2.2. Improvements to the yolov5s model</title>
          
          
            <sec>
              
                <title>2.2.1 Simsppf structure</title>
              
              
                <fig id="fig_1">
                  <label>Figure 1</label>
                  <caption>
                    <title>SimSPPF structure</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_8Rh6ozjL20ft_kF5.png"/>
                </fig>
              
              <p>The Yolov5 model uses a SPP structure, and subsequently introduced the SPPF, which replaces the parallel Maxpool of the original SPP with a more efficient, faster serial Maxpool. The SimSPPF further builds on this by replacing the SiLU activation function with ReLU, and uses different sized pooling kernels across various scales to enhance detector performance. In the feature parsing process, the nodes in SimSPPF are divided into different layers by scale, with each layer's node scale being twice that of the previous layer. In each layer, pooling technology is used to reuse already allocated nodes, thus reducing memory usage. This method decreases spatial occupancy and improves parsing performance [<xref ref-type="bibr" rid="ref_19">19</xref>], [<xref ref-type="bibr" rid="ref_20">20</xref>]. The structure of SimSPPF is shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>.</p>
            </sec>
          
          
            <sec>
              
                <title>2.2.2 Botnet structure</title>
              
              <p>In Yolov5, the backbone feature extraction network is a CNN network, which has translational invariance and locality but lacks the capability for global and long-distance modeling. BotNet is a simple yet powerful backbone that, unlike ResNet50, uses Multi-Head Self-Attention (MHSA) to replace the 3×3 spatial convolution in the Bottleneck [<xref ref-type="bibr" rid="ref_21">21</xref>], [<xref ref-type="bibr" rid="ref_22">22</xref>], [<xref ref-type="bibr" rid="ref_23">23</xref>], [<xref ref-type="bibr" rid="ref_24">24</xref>]. The BotNet structure is shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>.</p>
              
                <fig id="fig_2">
                  <label>Figure 2</label>
                  <caption>
                    <title>BotNet structure diagram</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_fbOOHQeml-jjUlI3.png"/>
                </fig>
              
              <p>Similar to traditional attention mechanisms, MHSA can focus more on key information in the input. It runs multiple Self-Attention layers in parallel and synthesizes the learning outcomes of each "head", capturing information from the input sequence across different subspaces, thereby enhancing the model's expressive capacity. The structure of MHSA is shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>.</p>
              
                <fig id="fig_3">
                  <label>Figure 3</label>
                  <caption>
                    <title>MHSA structure</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_r7zce13d-vBRP_Jd.png"/>
                </fig>
              
              <p>MHSA splits the input's query, key, and value matrices into <italic>H</italic> heads, computes attention independently within each head, then concatenates these heads' outputs and applies a linear transformation. This enables simultaneous capture and integration of multiple interaction information across different representational subspaces. The specific formulas are as follows [<xref ref-type="bibr" rid="ref_25">25</xref>], [<xref ref-type="bibr" rid="ref_26">26</xref>]:</p><p style="text-align: center"><italic><inline-formula>
  <mml:math id="m0afyd0846">
    <mml:mi>H</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>Attention</mml:mi>
    <mml:mi>softmax</mml:mi>
    <mml:mi>V</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>H</mml:mi>
    <mml:msub>
      <mml:mi>d</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>∗</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>∈</mml:mo>
    <mml:mo>[</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>Q</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:msub>
      <mml:msub>
        <mml:mi>K</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:msub>
      <mml:msub>
        <mml:mi>V</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:msub>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mfrac>
        <mml:mrow>
          <mml:msub>
            <mml:mi>Q</mml:mi>
            <mml:mi>i</mml:mi>
          </mml:msub>
          <mml:msubsup>
            <mml:mi>K</mml:mi>
            <mml:mi>i</mml:mi>
            <mml:mi>T</mml:mi>
          </mml:msubsup>
        </mml:mrow>
        <mml:msqrt>
          <mml:msub>
            <mml:mi>d</mml:mi>
            <mml:mi>k</mml:mi>
          </mml:msub>
        </mml:msqrt>
      </mml:mfrac>
    </mml:mrow>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula></italic></p><p style="text-align: center"><inline-formula>
  <mml:math id="m5m7o1j87n">
    <mml:mrow>
      <mml:mi>M</mml:mi>
      <mml:mi>H</mml:mi>
      <mml:mi>S</mml:mi>
      <mml:mi>A</mml:mi>
    </mml:mrow>
    <mml:mrow>
      <mml:mi>C</mml:mi>
      <mml:mi>o</mml:mi>
      <mml:mi>n</mml:mi>
      <mml:mi>c</mml:mi>
      <mml:mi>a</mml:mi>
      <mml:mi>t</mml:mi>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>…</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mtext> </mml:mtext>
      <mml:msub>
        <mml:mrow>
          <mml:mi>h</mml:mi>
          <mml:mi>e</mml:mi>
          <mml:mi>a</mml:mi>
          <mml:mi>d</mml:mi>
        </mml:mrow>
        <mml:mn>1</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mrow>
          <mml:mi>h</mml:mi>
          <mml:mi>e</mml:mi>
          <mml:mi>a</mml:mi>
          <mml:mi>d</mml:mi>
        </mml:mrow>
        <mml:mn>2</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mrow>
          <mml:mi>h</mml:mi>
          <mml:mi>e</mml:mi>
          <mml:mi>a</mml:mi>
          <mml:mi>d</mml:mi>
        </mml:mrow>
        <mml:mi>n</mml:mi>
      </mml:msub>
    </mml:mrow>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mi>Q</mml:mi>
    <mml:mi>K</mml:mi>
    <mml:mi>V</mml:mi>
    <mml:msup>
      <mml:mi>W</mml:mi>
      <mml:mi>o</mml:mi>
    </mml:msup>
  </mml:math>
</inline-formula></p><p>In Self-Attention, <inline-formula>
  <mml:math id="mc3fdkei3r">
    <mml:mrow>
      <mml:mi>Q</mml:mi>
    </mml:mrow>
  </mml:math>
</inline-formula>, <inline-formula>
  <mml:math id="mspyh435gy">
    <mml:mrow>
      <mml:mi>K</mml:mi>
    </mml:mrow>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="m1tfg5o28m">
    <mml:mrow>
      <mml:mi>V</mml:mi>
    </mml:mrow>
  </mml:math>
</inline-formula> are matrices obtained from the same input through three different linear transformations, where <inline-formula>
  <mml:math id="mjkhv1ho9y">
    <mml:mi>Q</mml:mi>
    <mml:msup>
      <mml:mi>K</mml:mi>
      <mml:mi>T</mml:mi>
    </mml:msup>
  </mml:math>
</inline-formula> is a s similarity matrix. Applying softmax to this matrix row-wise yields the Attention matrix. The output matrices <inline-formula>
  <mml:math id="m7vlfotcmg">
    <mml:msub>
      <mml:mrow>
        <mml:mi data-mjx-auto-op="false">Head</mml:mi>
      </mml:mrow>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> - are concatenated along the feature dimension (dim) to form  a new matrix, which is then multiplied by the matrix <inline-formula>
  <mml:math id="me186o27mc">
    <mml:msup>
      <mml:mi>W</mml:mi>
      <mml:mi>o</mml:mi>
    </mml:msup>
  </mml:math>
</inline-formula> to produce the output <inline-formula>
  <mml:math id="mel8leinb1">
    <mml:mi>MHSA</mml:mi>
    <mml:mi>Q</mml:mi>
    <mml:mi>K</mml:mi>
    <mml:mi>V</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> [<xref ref-type="bibr" rid="ref_27">27</xref>], [<xref ref-type="bibr" rid="ref_28">28</xref>].</p>
            </sec>
          
          
            <sec>
              
                <title>2.2.3 Eiou loss function</title>
              
              <p>This paper proposes an improved loss function to enhance the model's recognition accuracy. The original Yolov5 model uses the CIoU loss function during training, which should include coverage area, distance between centers, and aspect ratio of the detection data. The CIoU loss, building on the Distance Intersection over Union (DIoU) loss, adds a measure of the aspect ratio <italic>v</italic> between the predicted box and the ground truth (GT) box, which can accelerate the regression speed of the prediction box to some extent. However, there are still significant issues, as the model detection can sometimes be blurry. Based on the gradient formulas for predicted box width <inline-formula>
  <mml:math id="mwd6a58bwt">
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>w</mml:mi>
  </mml:math>
</inline-formula> and height <inline-formula>
  <mml:math id="mg1u6u0ask">
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>h</mml:mi>
  </mml:math>
</inline-formula>, it is evident that 'when one value increases, the other must decrease; they cannot increase or decrease simultaneously. To address this, EIoU proposes direct penalties on the predictions of $w<inline-formula>
  <mml:math id="mpfjxil9be">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>h<inline-formula>
  <mml:math id="mg3qby8e8m">
    <mml:mo>,</mml:mo>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>C_\omega^2<inline-formula>
  <mml:math id="mxsim9cwi2">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>C_h^2<inline-formula>
  <mml:math id="mok5uwk8k3">
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>U</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>[</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mn>29</mml:mn>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>L_{\text {EIoU }}=1-I o U+\frac{\rho^2\left(b, b^{g t}\right)}{c^2}+\frac{\rho^2\left(\omega, \omega^{g t}\right)}{C_\omega^2}+\frac{\rho^2\left(h, h^{g t}\right)}{C_h^2}<inline-formula>
  <mml:math id="m5gbw8z3ae">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>[</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>U</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>U</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mn>30</mml:mn>
  </mml:math>
</inline-formula>L_{\text {EIoU} \text{Loss}}=\mathrm{IoU}^\gamma * L_{E I o U}$</p>
            </sec>
          
          
            <sec>
              
                <title>2.2.4 Nms</title>
              
              <p>In recent years, common object detection algorithms (such as RCNN, SPPNet, Faster-RCNN, etc.) typically identify many potential object bounding boxes from a single image, assigning each a probability of belonging to a certain category [<xref ref-type="bibr" rid="ref_31">31</xref>], [<xref ref-type="bibr" rid="ref_32">32</xref>]. NMS filters out the boxes within a certain area that have the highest score for the same category. Through iterative processing, it continually uses the highest scoring box to perform IoU operations with other boxes, filtering out those with high IoU values to retain the best result. In Yolov5, NMS only considers the overlap between the predicted boxes and true boxes and does not account for distances between centers or aspect ratios. Therefore, this paper proposes EIoU-NMS, which considers the distance between the centers of two boxes, resulting in a model that performs better with EIoU-NMS. The calculation formula for EIoU-NMS is as follows [<xref ref-type="bibr" rid="ref_33">33</xref>]:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mojkf5ysq1">
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mrow>
      <mml:mo>{</mml:mo>
      <mml:mo fence="true"/>
      <mml:mtable columnspacing="1em" rowspacing="4pt">
        <mml:mtr>
          <mml:mtd>
            <mml:msub>
              <mml:mi>S</mml:mi>
              <mml:mi>i</mml:mi>
            </mml:msub>
            <mml:msub>
              <mml:mi>R</mml:mi>
              <mml:mrow>
                <mml:mi>E</mml:mi>
                <mml:mi>I</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>U</mml:mi>
              </mml:mrow>
            </mml:msub>
            <mml:mo>,</mml:mo>
            <mml:mo>−</mml:mo>
            <mml:mo>&amp;lt;</mml:mo>
            <mml:mi>I</mml:mi>
            <mml:mi>o</mml:mi>
            <mml:mi>U</mml:mi>
            <mml:mi>ε</mml:mi>
            <mml:mrow>
              <mml:mo>(</mml:mo>
              <mml:mo>,</mml:mo>
              <mml:mo>)</mml:mo>
              <mml:mi>M</mml:mi>
              <mml:msub>
                <mml:mi>B</mml:mi>
                <mml:mi>i</mml:mi>
              </mml:msub>
            </mml:mrow>
          </mml:mtd>
        </mml:mtr>
        <mml:mtr>
          <mml:mtd>
            <mml:mn>0</mml:mn>
            <mml:mo>,</mml:mo>
            <mml:mo>−</mml:mo>
            <mml:mo>≥</mml:mo>
            <mml:mi>I</mml:mi>
            <mml:mi>o</mml:mi>
            <mml:mi>U</mml:mi>
            <mml:mi>ε</mml:mi>
            <mml:msub>
              <mml:mi>R</mml:mi>
              <mml:mrow>
                <mml:mi>E</mml:mi>
                <mml:mi>I</mml:mi>
                <mml:mi>o</mml:mi>
                <mml:mi>U</mml:mi>
              </mml:mrow>
            </mml:msub>
            <mml:mrow>
              <mml:mo>(</mml:mo>
              <mml:mo>,</mml:mo>
              <mml:mo>)</mml:mo>
              <mml:mi>M</mml:mi>
              <mml:msub>
                <mml:mi>B</mml:mi>
                <mml:mi>i</mml:mi>
              </mml:msub>
            </mml:mrow>
          </mml:mtd>
        </mml:mtr>
      </mml:mtable>
    </mml:mrow>
  </mml:math>
</inline-formula></p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>2.3. Improved network model</title>
          
          <p>This study proposes four improvements based on the Yolov5s model. First, the SPP part of the backbone network is improved by introducing SimSPPF to replace the original SPP layer, which increases the efficiency of model training. Next, the BotNet self-attention mechanism is introduced, enabling the model to locate and identify disease target features more accurately [<xref ref-type="bibr" rid="ref_34">34</xref>]. Lastly, the EIoU Loss function and the NMS (EIoU-NMS) are improved, enhancing the model's prediction accuracy for similar categories. These improvements have enhanced the overall recognition rate of the model. The structure of the improved Yolov5s network model is shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Improved Yolov5s network model structure</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_y_gV15QkBhKogWAw.png"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Experiment</title>
      
        <sec>
          
            <title>3.1. Dataset construction</title>
          
          <p>This study focuses on two soybean diseases: Bacterial Spot disease and Brown Spot disease. The dataset was constructed using two methods: First, by collecting images of soybean leaf diseases in different environments in the field using a smartphone; second, through web scraping, Google searches, and various open-source websites to gather images of soybean leaf diseases. The images collected have complex backgrounds, matching real-world application conditions. The characteristics of the disease images are shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Soybean disease characteristics: (a) Brown spot disease; (b) Bacterial spot disease</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_Su6QmT7laNUNACcl.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_lXQX8ip0p5f2huCi.png"/>
            </fig>
          
          <p>For this experiment, over 600 images of soybean leaf diseases were collected. To avoid image redundancy, 600 images were manually selected. Due to the limited number of original disease images, which could not effectively train the network model, the dataset was augmented to five times the number of original images to enhance model stability and reduce overfitting. The augmentation techniques used included adding Gaussian noise, rotating (at 90° and 180°), mirroring, and adjusting brightness. A total of 3000 effective dataset images were selected, and examples of the augmented images are shown in <xref ref-type="fig" rid="fig_6">Figure 6</xref>. </p>
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Augmented images: (a) Original image; (b) Rotated 90°; (c) Rotated 180°; (d) Mirrored; (e) Adjusted brightness; (f) Gaussian noise</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_dJyNoabAcv1b1viR.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_fg_NpkYqajKFCo0E.png"/>
            </fig>
          
          <p>The dataset was randomly split into a training set of 2100 images, a test set of 600 images, and a validation set of 300 images, following a 7:2:1 ratio. The Labelimg software was used to manually annotate the two types of soybean leaf diseases in the dataset to obtain the coordinates and dimensions of the disease spots on each image, with the annotation information saved into TXT files. An example of image annotation using Labelimg is shown in <xref ref-type="fig" rid="fig_7">Figure 7</xref>. </p>
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>Labelimg annotation</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_qJ94ywVq7cnRKM4n.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Experimental setup</title>
          
          
            <sec>
              
                <title>3.2.1 Experimental environment</title>
              
              <p>All experiments were conducted under the Yolov5s deep learning framework for training and testing the network model. The hardware configuration of the experimental server included: an Intel(R) Core(TM) i5-10400F CPU @ 2.90GHz, NVIDIA GeForce RTX 2060 SUPER graphics card, and a computer with 16GB of memory running on a Windows 10 system. The software environment included Pycharm + Python 3.8, Conda 23.1.0. Images were input at 640×640 pixels, with a batch size of 32, undergoing 300 Epochs, and the best model was saved in the logs.</p>
            </sec>
          
          
            <sec>
              
                <title>3.2.2 Evaluation metrics</title>
              
              <p>To evaluate the performance of the target detection algorithm of the model, two metrics are commonly used: recall and precision. Both metrics, precision (p) and recall (r), simply judge the model's quality from one aspect and range between 0 and 1, where closer to 1 indicates better performance and closer to 0 indicates poorer performance. To comprehensively evaluate the target detection performance, mAP is generally used to further assess the model's quality. By setting different confidence threshold levels, p and r values calculated at different thresholds can be obtained. Generally, p and r values are inversely related. Each target in the target detection model can have an AP value calculated, and averaging all AP values yields the mAP value of the model. The training mAP of the improved Yolov5 model is shown in <xref ref-type="fig" rid="fig_8">Figure 8</xref>. </p>
              
                <fig id="fig_8">
                  <label>Figure 8</label>
                  <caption>
                    <title>mAP curve</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_FkvAhEUxizEdAnRl.png"/>
                </fig>
              
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. Comparative experiments of the improved model</title>
          
          
            <sec>
              
                <title>3.3.1 Comparison of adding botnet to the backbone network</title>
              
              <p>To improve the model's accuracy in detecting disease characteristics, this study explored adding the BotNet self-attention mechanism to the backbone network of the Yolov5s. The experiment involved replacing the last C3 module in the backbone network with the self-attention mechanism BotNet (BOT3 module), which yielded the best model recognition performance. Four comparative experiments were designed, adding currently popular attention mechanisms such as CA, SE, and CBAM under the same basic network and experimental data conditions. The comparative results are shown in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
              
                <table-wrap id="table_1">
                  <label>Table 1</label>
                  <caption>
                    <title>Comparison of different attention mechanisms</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="1"><p>Model Scheme</p></td><td colspan="1" rowspan="1"><p><italic>r </italic>(%)</p></td><td colspan="1" rowspan="1"><p><italic>p </italic>(%)</p></td><td colspan="1" rowspan="1"><p><italic>mAP </italic>(%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov5s + CA</p></td><td colspan="1" rowspan="1"><p>88.5</p></td><td colspan="1" rowspan="1"><p>90.2</p></td><td colspan="1" rowspan="1"><p>91.0</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov5s + SE</p></td><td colspan="1" rowspan="1"><p>88.2</p></td><td colspan="1" rowspan="1"><p>90.0</p></td><td colspan="1" rowspan="1"><p>90.9</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov5s + CBAM</p></td><td colspan="1" rowspan="1"><p>86.7</p></td><td colspan="1" rowspan="1"><p>88.6</p></td><td colspan="1" rowspan="1"><p>89.8</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov5s + BOT3</p></td><td colspan="1" rowspan="1"><p>88.4</p></td><td colspan="1" rowspan="1"><p>90.3</p></td><td colspan="1" rowspan="1"><p>91.9</p></td></tr></tbody></table>
                </table-wrap>
              
              <p>Analysis from <xref ref-type="table" rid="table_1">Table 1</xref> indicates that adding BOT3 to the original Yolov5s network provides the best improvement in recall, precision, and mAP. Compared to adding the CA attention mechanism, mAP improved by 0.9%; compared to SE, it improved by 1.9%; and compared to CBAM, it improved by 2.1%. This demonstrates that adding the BotNet self-attention mechanism can better identify disease characteristics, achieving a higher disease detection rate.</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.2 Ablation study</title>
              
              <p>To further verify the effectiveness of the proposed improvements, an ablation study was conducted by adding only one improvement at a time to the model while keeping training parameters and the dataset the same. The results are shown in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
              
                <table-wrap id="table_2">
                  <label>Table 2</label>
                  <caption>
                    <title>Comparison of ablation study results for the improved model</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="1"><p>Model Scheme</p></td><td colspan="1" rowspan="1"><p><italic>r </italic>(%)</p></td><td colspan="1" rowspan="1"><p><italic>p </italic>(%)</p></td><td colspan="1" rowspan="1"><p><italic>mAP </italic>(%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov5s</p></td><td colspan="1" rowspan="1"><p>84.5</p></td><td colspan="1" rowspan="1"><p>87.7</p></td><td colspan="1" rowspan="1"><p>88.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov5s+SimSPPF</p></td><td colspan="1" rowspan="1"><p>85.8</p></td><td colspan="1" rowspan="1"><p>88.9</p></td><td colspan="1" rowspan="1"><p>89.8</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov5s+ SimSPPF+ BOT3</p></td><td colspan="1" rowspan="1"><p>87.0</p></td><td colspan="1" rowspan="1"><p>90.3</p></td><td colspan="1" rowspan="1"><p>91.9</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov5s+ SimSPPF+ BOT3+ EIoU-Loss</p></td><td colspan="1" rowspan="1"><p>87.2</p></td><td colspan="1" rowspan="1"><p>90.5</p></td><td colspan="1" rowspan="1"><p>92.4</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov5s+ SimSPPF+ BOT3+ EIoU-Loss +EIoU-NMS</p></td><td colspan="1" rowspan="1"><p>87.9</p></td><td colspan="1" rowspan="1"><p>90.9</p></td><td colspan="1" rowspan="1"><p>92.8</p></td></tr></tbody></table>
                </table-wrap>
              
              <p>Analysis from <xref ref-type="table" rid="table_2">Table 2</xref> shows that compared to the original Yolov5s algorithm, the improved Yolov5s model has a 3.4% increase in recall (r), a 3.2% increase in precision (p), and a 4.5% increase in mAP. The experimental results indicate that replacing the SimSPPF module, adding the BotNet attention mechanism, improving the EIoU-Loss function, and the EIoU-NMS have made the improved Yolov5s network model perform better in detecting and identifying the two types of soybean leaf diseases.</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.3 Comparison of different network models</title>
              
              <p>To evaluate the superiority of the improved Yolov5s network model proposed in this study, popular target detection networks such as Faster R-CNN, Yolov4, and MobileNetV2 were selected for comparative experiments. The results are shown in <xref ref-type="table" rid="table_3">Table 3</xref>. </p>
              
                <table-wrap id="table_3">
                  <label>Table 3</label>
                  <caption>
                    <title>Comparison of different network models</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="1"><p>Model Scheme</p></td><td colspan="1" rowspan="1"><p><italic>r </italic>(%)</p></td><td colspan="1" rowspan="1"><p><italic>p </italic>(%)</p></td><td colspan="1" rowspan="1"><p><italic>mAP </italic>(%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>The Proposed Improved Model</p></td><td colspan="1" rowspan="1"><p>87.9</p></td><td colspan="1" rowspan="1"><p>90.9</p></td><td colspan="1" rowspan="1"><p>92.8</p></td></tr><tr><td colspan="1" rowspan="1"><p>Faster R-CNN</p></td><td colspan="1" rowspan="1"><p>77.0</p></td><td colspan="1" rowspan="1"><p>80.3</p></td><td colspan="1" rowspan="1"><p>82.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov4</p></td><td colspan="1" rowspan="1"><p>82.3</p></td><td colspan="1" rowspan="1"><p>85.8</p></td><td colspan="1" rowspan="1"><p>87.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>MobileNetV2</p></td><td colspan="1" rowspan="1"><p>85.1</p></td><td colspan="1" rowspan="1"><p>88.5</p></td><td colspan="1" rowspan="1"><p>90.6</p></td></tr></tbody></table>
                </table-wrap>
              
              <p>As can be seen from <xref ref-type="table" rid="table_3">Table 3</xref>, in terms of the mAP evaluation metric, the improved model shows a 10.3% increase compared to the two-stage target detection algorithm Faster R-CNN, a 5.6% increase compared to the Yolov4 network, and a 2.2% increase compared to the lightweight network MobileNetV2. In terms of recall (r) and average precision (p), it also shows good improvement compared to other network models, indicating that the improved model has superior detection performance.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.4. Model visualization and analysis</title>
          
          <p>In this study, the expanded dataset was imported into the improved Yolov5s model for training, with the training labels set as Bacterial Spot disease and Bean Rust. The model first identifies the type of disease, and each identification result provides a confidence score for the category. The best weight results generated during the training process are shown in <xref ref-type="fig" rid="fig_9">Figure 9</xref>. </p>
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>
                <title>Best weight results during model training</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_VE3W9yVp20vV-5TU.png"/>
            </fig>
          
          <p>The system interface detects and identifies images from the validation set. First, an image is selected for recognition, and after recognition, the type of disease and confidence score are displayed. The average recognition speed for a single image is 0.09 seconds. The system's recognition results are shown in <xref ref-type="fig" rid="fig_10">Figure 10</xref>.</p>
          
            <fig id="fig_10">
              <label>Figure 10</label>
              <caption>
                <title>Soybean disease detection system recognition results</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/5/img_uwcGZE8Pr12J_2Ex.png"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Conclusion and future work</title>
      <p>This paper proposes an improved Yolov5s model for the detection and identification of soybean leaf diseases. The dataset was expanded through data augmentation, and the Yolov5s model was enhanced by using a superior SimSPPF structure, reducing the loss of dataset feature information. The addition of the BotNet structure allows the network to better learn the features of leaf diseases, enhancing the network model's precision in extracting target features. Improvements to the loss function and NMS further optimize the model's detection and identification rates. Final experimental results show that the improved network model has generally increased recall, precision, and mAP by 3.4%, 3.2%, and 4.5%, respectively, compared to the original Yolov5s model. Therefore, the model effectively accomplishes the task of detecting soybean leaf diseases, and the disease detection system studied in this paper has practical reference value for actual agricultural applications. Future research will focus on developing lightweight models and expanding the types of soybean leaf diseases to achieve faster model detection rates and more comprehensive disease system detection.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      <p>This research was supported by the "Three Longitudinal" Foundation Cultivation Plan of Heilongjiang Bayi Agricultural University, a provincial university in Heilongjiang Province (ZRCPY202016).</p>
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p><span style="color: rgb(0, 0, 0); font-family: Times New Roman, sans-serif">The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p><span style="color: rgb(0, 0, 0); font-family: Times New Roman, sans-serif">The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>40</volume>
          <page-range>662-668</page-range>
          <issue>5</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shang</surname>
              <given-names>Z. Q.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>D. F.</given-names>
            </name>
            <name>
              <surname>Z. P. Ma</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.11861/j.issn.1000-9841.2021.05.0662</pub-id>
          <article-title>Automatic identification of soybean leaf diseases based on UAV image and deep convolution neural network</article-title>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>40</volume>
          <page-range>131-134</page-range>
          <issue>4</issue>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shrivastava</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Hooda</surname>
              <given-names>D. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.5923/j.ajis.20140404.01</pub-id>
          <article-title>Automatic brown spot and frog eye detection from the image captured in the field</article-title>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>167</volume>
          <page-range>105060</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Araujo</surname>
              <given-names>J. M. M.</given-names>
            </name>
            <name>
              <surname>Peixoto</surname>
              <given-names>Z. M. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compag.2019.105060</pub-id>
          <article-title>A new proposal for automatic identification of multiple soybean diseases</article-title>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>47</volume>
          <page-range>298-304</page-range>
          <issue>11</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>X. R.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>S. Q.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.19678/j.issn.1000-3428.0059290</pub-id>
          <article-title>Apple leaf diseases detection model based on improved faster R-CNN</article-title>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>43</volume>
          <page-range>106-115</page-range>
          <issue>4</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>He</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Peng</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.13733/j.jcam.issn.2095-5553.2022.04.016</pub-id>
          <article-title>Research on object detection algorithm of economic forestry pests based on improved YOLOv5</article-title>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>44</volume>
          <page-range>124–130</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Z. Ma</surname>
            </name>
            <name>
              <surname>Hao</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Analysis and countermeasures of mechanization development of domestic and foreign facility vegetables</article-title>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>70-73</page-range>
          <issue>10</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shi</surname>
              <given-names>L.Y.</given-names>
            </name>
            <name>
              <surname>Tong</surname>
              <given-names>S.Y.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Feng</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>H.H. Liu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.19850/j.cnki.2096-4706.2023.10.018</pub-id>
          <article-title>Farmland pest image recognition based on improved YOLOv5 attention model</article-title>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>35</volume>
          <page-range>87-94</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.13998/j.cnki.issn1002-1248.21-0188</pub-id>
          <article-title>Recognition and classification of deep learning in soybean leaf image data management</article-title>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>86-95</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zuo</surname>
              <given-names>H. X.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Q. C.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Meng</surname>
              <given-names>F. J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>S. E.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.12133/j.smartag.SA202309004</pub-id>
          <article-title>In situ identification method of maize stalk width based on binocular vision and improved YOLOv8</article-title>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>933-941</page-range>
          <issue>4</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Su</surname>
              <given-names>J.K.</given-names>
            </name>
            <name>
              <surname>Duan</surname>
              <given-names>X.H.</given-names>
            </name>
            <name>
              <surname>Ye</surname>
              <given-names>Z.B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3778/j.issn.1673-9418.2210066</pub-id>
          <article-title>Research on corn disease detection based on improved YOLOv5 algorithm</article-title>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>51</volume>
          <page-range>221-226</page-range>
          <issue>13</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Z. Y.</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>M. Y.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>S. X.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>S. P.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3969/j.issn.0517-6611.2023.13.049</pub-id>
          <article-title>Cherry fruit detection method in natural scene based on improved YOLOv5</article-title>
          <source>Trans. Chin. Soc. Agric. Mach.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Srinivas</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Parmar</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Shlens</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Abbeel</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Vaswani</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Bottleneck transformers for visual recognition</article-title>
          <source>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</source>
          <publisher-name>Nashville, TN, USA</publisher-name>
          <year>2021</year>
          <page-range>16514–16524</page-range>
          <pub-id pub-id-type="doi">10.1109/CVPR46437.2021.01625</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Richey</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Shirvaikar</surname>
              <given-names>M. V.</given-names>
            </name>
          </person-group>
          <article-title>Deep learning based real-time detection of northern corn leaf blight crop disease using YOLOv4</article-title>
          <source>Real-Time Image Processing and Deep Learning</source>
          <publisher-name>2021</publisher-name>
          <page-range>39–45</page-range>
          <pub-id pub-id-type="doi">10.1117/12.2587892</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>44</volume>
          <page-range>190-197</page-range>
          <issue>9</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>L. Zhang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.13733/j.jcam.issn.2095-5553.2023.09.027</pub-id>
          <article-title>Identification and detection of rice leaf diseases by YOLOv5 neural network based on improved SPP-x</article-title>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>51</volume>
          <page-range>212-220</page-range>
          <issue>22</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhou</surname>
              <given-names>S. F.</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>X. L.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Z. Y.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Improved apple leaf disease detection based on YOLOv5s</article-title>
          <source>Jiangsu Agric. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>51</volume>
          <page-range>155-163</page-range>
          <issue>15</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zeng</surname>
              <given-names>Y. L.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>Y. T.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Fei</surname>
              <given-names>J. J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.15889/j.issn.1002-1302.2023.15.022</pub-id>
          <article-title>A detection method for apple leaf diseases based on BCE-YOLOv5</article-title>
          <source>Jiangsu Agric. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>49</volume>
          <page-range>127-132</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Fang</surname>
              <given-names>W. Y.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>Y. G.</given-names>
            </name>
            <name>
              <surname>Guan</surname>
              <given-names>F. C.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Q. Q.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>S.W.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z. C.</given-names>
            </name>
            <name>
              <surname>H. R. Yu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.13331/j.cnki.jhau.2023.01.018</pub-id>
          <article-title>Identification of wormholes in soybean leaves based on improved YOLOv5s algorithm</article-title>
          <source>J. Hunan Agric. Univ. (Nat. Sci. Ed.)</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>54</volume>
          <page-range>267-276</page-range>
          <issue>8</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ma</surname>
              <given-names>H. X.</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>K. B.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y. F.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>S.H.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>W.G.</given-names>
            </name>
            <name>
              <surname>Gou</surname>
              <given-names>J. P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.6041/j.issn.1000-1298.2023.08.026</pub-id>
          <article-title>Research on a lightweight plant recognition model based on improved YOLOv5s</article-title>
          <source>J. Agric. Mach.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>53</volume>
          <page-range>1-4</page-range>
          <issue>16</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hao</surname>
              <given-names>J.J.</given-names>
            </name>
            <name>
              <surname>Fang</surname>
              <given-names>S.Y.</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>Sa</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3969/j.issn.1672-3872.2022.16.001</pub-id>
          <article-title>Research on improving YOLOv5’s leaf black spot detection algorithm</article-title>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>C. Y.</given-names>
            </name>
            <name>
              <surname>Liao</surname>
              <given-names>H. M.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Hsieh</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Yeh</surname>
              <given-names>I. H.</given-names>
            </name>
          </person-group>
          <article-title>CSPNet: A new backbone that can enhance learning capability of CNN</article-title>
          <source>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</source>
          <publisher-name>Seattle, WA, USA</publisher-name>
          <year>2020</year>
          <page-range>1571–1580</page-range>
          <pub-id pub-id-type="doi">10.1109/CVPRW50498.2020.00203</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Y. F.</given-names>
            </name>
            <name>
              <surname>Ren</surname>
              <given-names>W. Q.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Zhen</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Tan</surname>
              <given-names>T. N.</given-names>
            </name>
          </person-group>
          <article-title>Focal and efficient IOU loss for accurate bounding box regression</article-title>
          <source>ArXiv abs/2101.08158</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>52</volume>
          <page-range>624-630</page-range>
          <issue>4</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Ding</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3969/j.issn.1000-2324.2021.04.017</pub-id>
          <article-title>A detection and recognition method for tomato on faster R-CNN algorithm</article-title>
          <source>J. Shandong Agric. Univ.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>7-9</page-range>
          <issue>14</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Guo</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Ge</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.20028/j.zhnydk.2022.14.003</pub-id>
          <article-title>Image recognition method of smart agricultural diseases and insect pests based on transfer learning</article-title>
          <source>J. Smart Agric.</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>44</volume>
          <page-range>151–160</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Luo</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Yin</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.7671/j.issn.1001-411X.202203012</pub-id>
          <article-title>Identification of bergamot pests and diseases using YOLOv5-C algorithm</article-title>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="webpage">
          <article-title>Self-attention with relative position representations</article-title>
          <source>, https://arxiv.org/pdf/1803.02155.pdf</source>
          <year>2018</year>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Hu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Squeeze-and-excitation networks</article-title>
          <source>IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>
          <publisher-name>Salt Lake City, UT, USA</publisher-name>
          <year>2018</year>
          <page-range>7132–7141</page-range>
          <pub-id pub-id-type="doi">10.1109/CVPR.2018.00745</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>33-40</page-range>
          <issue>9</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Du</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.19998/j.cnki.2095-1795.2022.09.007</pub-id>
          <article-title>Identification method of tomato diseases and pests based on SE module and resnet</article-title>
          <source>Agric. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <volume>2023</volume>
          <page-range>165-171</page-range>
          <issue>7</issue>
          <person-group person-group-type="author">
            <name>
              <surname>Cen</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3969/j.issn.1672-9528.2023.07.042</pub-id>
          <article-title>Improved YOLOv5 for rice pest detection</article-title>
          <source>Inf. Technol. Informatization</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <volume>42</volume>
          <page-range>360-366</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>S.Y.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>H.Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z.X.</given-names>
            </name>
            <name>
              <surname>Qiao</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>W.X.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.11861/j.issn.1000-9841.2023.03.0360</pub-id>
          <article-title>Research on soybean disease image detection technology based on python</article-title>
          <source>Soybean Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Haque</surname>
              <given-names>M. E.</given-names>
            </name>
            <name>
              <surname>Rahman</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Junaeid</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Hoque</surname>
              <given-names>S. U.</given-names>
            </name>
            <name>
              <surname>Paul</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2209.01579</pub-id>
          <article-title>Rice leaf disease classification and detection using YOLOv5</article-title>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Woo</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Kweon</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>CBAM: Convolutional block attention module</article-title>
          <source>Computer Vision - ECCV 2018</source>
          <publisher-name>Springer, Cham</publisher-name>
          <year>2018</year>
          <page-range>3-19</page-range>
          <pub-id pub-id-type="doi">10.1007/978-3-030-01234-2_1</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <volume>35</volume>
          <page-range>163–165</page-range>
          <issue>4</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>J.H.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>H.K.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Y.H.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>J.Y.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>X.W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2209.01579</pub-id>
          <article-title>Soybean leaf spot identification method based on YOLOv5 model</article-title>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Hou</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Feng</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Coordinate attention for efficient mobile network design</article-title>
          <source>Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition</source>
          <publisher-name>New York: IEEE</publisher-name>
          <year>2021</year>
          <page-range>303–338</page-range>
          <pub-id pub-id-type="doi">10.1109/CVPR46437.2021.01350</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Cao</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Jiao</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>ViT-YOLO: Transformer-based YOLO for object detection</article-title>
          <source>2021 IEEE/CVF International Conference on Computer VisionWorkshops (ICCVW)</source>
          <publisher-name>Montreal, BC, Canada: IEEE</publisher-name>
          <year>2021</year>
          <page-range>2799–2808</page-range>
          <pub-id pub-id-type="doi">10.1109/ICCVW54120.2021.00314</pub-id>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>