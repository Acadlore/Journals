<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-T7wKjFl9YsHiBHGbR44zbwEBI0OdmBiM</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml020402</article-id>
      <title-group>
        <article-title>Automated Identification of Insect Pests: A Deep Transfer Learning Approach Using ResNet</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Dewi</surname>
            <given-names>Christine</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1284-234X</contrib-id>
          <email>christine.dewi@uksw.edu</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="2">2</xref>
          <name>
            <surname>Christanto</surname>
            <given-names>Henoch Juli</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0276-295X</contrib-id>
          <email>henoch.christanto@atmajaya.ac.id</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="3">3</xref>
          <name>
            <surname>Dai</surname>
            <given-names>Guowei</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0054-0392</contrib-id>
          <email>dgwstyle@foxmail.com</email>
        </contrib>
        <aff id="1">Department of Information Technology, Satya Wacana Christian University, 50715 Salatiga, Indonesia</aff>
        <aff id="2">Department of Information System, Atma Jaya Catholic University of Indonesia, 12930 Jakarta, Indonesia</aff>
        <aff id="3">Agricultural Information Institute of CAAS, National Agriculture Science Data Center, 100081 Beijing, China</aff>
      </contrib-group>
      <year>2023</year>
      <volume>2</volume>
      <issue>4</issue>
      <fpage>194</fpage>
      <lpage>203</lpage>
      <page-range>194-203</page-range>
      <history>
        <date date-type="received">
          <month>09</month>
          <day>01</day>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <month>10</month>
          <day>31</day>
          <year>2023</year>
        </date>
        <date date-type="pub">
          <month>11</month>
          <day>12</day>
          <year>2023</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2023 by the authors</copyright-statement>
        <copyright-year>2023</copyright-year>
        <license>. Licensee Acadlore Publishing Services Limited, Hong Kong. This article can be downloaded for free, and reused and quoted with a citation of the original published version, under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 license</ext-link>.</license>
      </permissions>
      <abstract><p>In the realm of agriculture, crop yields of fundamental cereals such as rice, wheat, maize, soybeans, and sugarcane are adversely impacted by insect pest invasions, leading to significant reductions in agricultural output. Traditional manual identification of these pests is labor-intensive and time-consuming, underscoring the necessity for an automated early detection and classification system. Recent advancements in machine learning, particularly deep learning, have provided robust methodologies for the classification and detection of a diverse array of insect infestations in crop fields. However, inaccuracies in pest classification could inadvertently precipitate the use of inappropriate pesticides, further endangering both agricultural yields and the surrounding ecosystems. In light of this, the efficacy of nine distinct pre-trained deep learning algorithms was evaluated to discern their capability in the accurate detection and classification of insect pests. This assessment utilized two prevalent datasets, comprising ten pest classes of varied sizes. Among the transfer learning techniques scrutinized, adaptations of ResNet-50 and ResNet-101 were deployed. It was observed that ResNet-50, when employed in a transfer learning paradigm, achieved an exemplary classification accuracy of 99.40% in the detection of agricultural pests. Such a high level of precision represents a significant advancement in the field of precision agriculture.</p></abstract>
      <kwd-group>
        <kwd>Convolutional neural networks</kwd>
        <kwd>Insect classification</kwd>
        <kwd>Deep learning</kwd>
        <kwd>Machine learning</kwd>
        <kwd>Insect pests</kwd>
        <kwd>Transfer learning</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors">3</count>
        <fig-count>4</fig-count>
        <table-count>3</table-count>
        <ref-count>43</ref-count>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec disp-level="level1" sec-type="intro">
      <title>1. Introduction</title>
      <p>In response to the projected escalation in global population to an estimated 10 billion by the mid-21<sup>st</sup> century, it has been recognized that a substantial increase in food production is imperative [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>]. Crop afflictions caused by various pests lead to diseases and destruction, which in turn result in diminished yields [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>]. The deployment of chemical pesticides has been the conventional response to pest management, albeit with detrimental consequences for human health and environmental integrity. Consequently, agricultural scientists globally have advocated for integrated pest management (IPM) strategies, a testament to the collaborative effort in seeking alternatives to chemical pesticides [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>]. Pest recognition systems, underpinned by computer vision and machine learning, have emerged as pivotal in the early detection and management of pests within agricultural, forestry, and wider ecological domains. These systems are instrumental in enabling timely interventions, thus ameliorating pest-induced damages. The role of pest recognition systems is crucial in fostering sustainable agricultural practices and in the stewardship of ecosystems, as they facilitate more precise and environmentally considerate pest control measures essential for food security and the reduction of pesticide reliance.</p><p>Research into pest recognition systems is burgeoning, propelled by the imperative for sustainable pest management solutions. This research encompasses the development of sophisticated machine learning algorithms and the implementation of sensor technologies. The traditional method of manual identification is fraught with issues such as human error, subjectivity, inefficiency, limited scalability, bias, inconsistency, high costs, and intensive resource requirements. These challenges underscore the necessity for automated processes, leveraging advancements in machine learning, computer vision, and natural language processing to augment efficacy, reduce error margins, and manage voluminous data sets with enhanced proficiency.</p><p>A myriad of autonomous pest recognition systems leveraging machine learning algorithms have been developed [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>]. In a notable study, Wu et al. [<xref ref-type="bibr" rid="ref_8">8</xref>] compiled an extensive insect dataset, IP102, which encompasses 102 classes and 75,000 images, and their evaluation employed various machine learning and deep learning methodologies. Concurrently, Turkoglu et al. [<xref ref-type="bibr" rid="ref_9">9</xref>] presented a multi-modal approach, both individual and hybrid, tailored for the detection of diseases and pests in apple plants, utilizing an integrated model combining a pre-trained convolutional neural network with Long Short-Term Memory (LSTM) networks [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>].</p><p>In related efforts, Kasinathan et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] introduced a machine learning-based insect identification and classification method, exploiting Support Vector Machine (SVM), Naïve Bayes (NB), K-Nearest Neighbors (KNN), and Artificial Neural Networks (ANN). This approach involved feature extraction from the Wang and Xie datasets and incorporated the GrabCut algorithm for insect detection, thus bypassing the need for manual extraction.</p><p>The utilization of transfer learning methods, where deep learning models pre-trained on extensive datasets are fine-tuned to smaller, target datasets, is well documented [<xref ref-type="bibr" rid="ref_13">13</xref>]. This technique affords several advantages, such as time and resource savings, and improved performance, particularly when labeled data for the new task is scarce [<xref ref-type="bibr" rid="ref_14">14</xref>]. Among the plethora of transfer learning strategies, the selection is contingent upon the specifics of the task and the pre-trained models available.</p><p>This study proposes an automated method for insect classification into ten categories without necessitating manual segmentation or feature extraction from images in the pre-processing phase, thereby enhancing automation compared to some extant methods [<xref ref-type="bibr" rid="ref_15">15</xref>], [<xref ref-type="bibr" rid="ref_16">16</xref>].</p><p>The primary contributions of this study are articulated as follows: Firstly, a novel system utilizing deep learning for the automated identification and classification of insects into ten categories is presented. Secondly, an analysis and validation of the transfer learning concept are provided, specifically for the ResNet model.</p><p>The subsequent structure of this document is as follows: Section 2 elucidates the proposed methodology and the deep neural networks engaged in this study. Section 3 delineates the experimental procedures, datasets employed, and the results thereof. Concluding remarks are presented in Section 4.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>2. Material and methods</title>
      
        <sec disp-level="level2">
          
            <title>2.1. Pest recognition systems and dataset</title>
          
          <p>In the realm of agricultural, forestry, and environmental management, the deployment of pest recognition systems is instrumental. These systems harness advanced technological paradigms to accurately identify and manage pest populations, thus affording a plethora of advantages to agriculturalists, land stewards, and ecosystems. The ensuing are salient benefits derived from the application of pest recognition systems:</p><p>(1) Early detection capabilities facilitate the identification of pests at incipient stages, often antecedent to observable damage to flora or ecosystems. Such preemptive discernment permits timely interventions, mitigating the propagation of pests and curtailing the magnitude of potential harm [<xref ref-type="bibr" rid="ref_17">17</xref>].</p><p>(2) Precision in pest control is augmented through the exactitude provided by these systems, enabling the implementation of pest management measures with greater specificity. Consequently, this obviates the need for indiscriminate pesticide applications, fostering an environmentally sustainable approach to pest control [<xref ref-type="bibr" rid="ref_15">15</xref>].</p><p>(3) The enhanced accuracy of pest detection and subsequent interventions culminate in the reduction of pesticide utilisation [<xref ref-type="bibr" rid="ref_18">18</xref>]. This has a cascade of benefits, encompassing monetary savings for agricultural practitioners, a diminution of environmental contaminants, and the safeguarding of agricultural workers and consumers [<xref ref-type="bibr" rid="ref_19">19</xref>].</p><p>(4) The efficacy of pest recognition systems is instrumental in safeguarding crop yields by mitigating pest-induced detriments. This safeguarding is pivotal for the sustenance of food security and the economic resilience of agricultural pursuits.</p><p>(5) The alignment of pest recognition systems with sustainable agricultural principles is marked, promoting practices that uphold environmental stewardship and economic sustainability over the long term.</p><p>(6) The systems proffer data-driven insights, which serve as a fulcrum for informed decision-making in pest management strategies [<xref ref-type="bibr" rid="ref_20">20</xref>].</p><p>(7) The optimization of pest control measures facilitated by these systems translates into substantial cost efficiencies for agriculturalists and landowners alike [<xref ref-type="bibr" rid="ref_21">21</xref>].</p><p>(8) Resistance to pesticides is a growing concern, and targeted pest control strategies made viable through these systems can play a significant role in managing this resistance [<xref ref-type="bibr" rid="ref_22">22</xref>].</p><p>(9) The health of ecosystems, particularly within the domains of forestry and natural resource management, is bolstered through the mitigation of invasive species and detrimental pests, contributing to ecological resilience.</p><p>(10) Remote monitoring capabilities inherent in some pest recognition systems, through the integration of drones or IoT devices, afford expansive area surveillance with minimal manual intervention [<xref ref-type="bibr" rid="ref_23">23</xref>].</p><p>(11) Adaptability and scalability are intrinsic to these systems, enabling their application across diverse pest types and geographic locales. This flexibility is essential in the face of emergent pest threats or evolutionary changes within extant populations.</p><p>(12) Centralization and dissemination of pest-related data facilitate collaborative pest management endeavours, unifying researchers, agriculturists, and consortia in the pursuit of efficacious control strategies [<xref ref-type="bibr" rid="ref_24">24</xref>].</p><p>Convolutional Neural Networks (CNNs) have emerged as a potent tool in the identification and classification of pests, with proven efficacy across agricultural, forestry, and environmental monitoring applications. The intrinsic design of CNNs renders them particularly adept at discerning and extrapolating salient features from image-based data, thereby serving as a cornerstone technology in pest recognition tasks [<xref ref-type="bibr" rid="ref_25">25</xref>].</p><p>The dataset proposed by Deng et al. [<xref ref-type="bibr" rid="ref_26">26</xref>], comprising ten disparate pest categories depicted through photographic images, has been explored and harnessed within these studies [<xref ref-type="bibr" rid="ref_27">27</xref>], [<xref ref-type="bibr" rid="ref_28">28</xref>]. Visual exemplars from each pest category are depicted in <xref ref-type="fig" rid="fig_1">Figure 1</xref>, while <xref ref-type="table" rid="table_1">Table 1</xref> delineates the categorical data of the 'Small Dataset' as utilised for pest classification.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>Representative images from dataset</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_tvvRV4I_OY-Vmzr6.png"/>
            </fig>
          
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>Summary of ‘Small Dataset’ (Deng et al. [<xref ref-type="bibr" rid="ref_26">26</xref>] data) for pest classification</caption>
              <abstract/>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Class </p></td><td colspan="1" rowspan="1"><p> Number of Images </p></td></tr><tr><td colspan="1" rowspan="1"><p>Locusta migratoria </p></td><td colspan="1" rowspan="1"><p>72</p></td></tr><tr><td colspan="1" rowspan="1"><p> Parasa lepida </p></td><td colspan="1" rowspan="1"><p>59</p></td></tr><tr><td colspan="1" rowspan="1"><p>Chrysochus chinensis </p></td><td colspan="1" rowspan="1"><p>46</p></td></tr><tr><td colspan="1" rowspan="1"><p>Spodoptera exigua </p></td><td colspan="1" rowspan="1"><p>56</p></td></tr><tr><td colspan="1" rowspan="1"><p>Atractomorpha sinensis </p></td><td colspan="1" rowspan="1"><p>62</p></td></tr><tr><td colspan="1" rowspan="1"><p>Empoasca flavescens </p></td><td colspan="1" rowspan="1"><p>41</p></td></tr><tr><td colspan="1" rowspan="1"><p>Laspeyresia pomonella </p></td><td colspan="1" rowspan="1"><p>65</p></td></tr><tr><td colspan="1" rowspan="1"><p> Spodoptera exigua larva </p></td><td colspan="1" rowspan="1"><p>68</p></td></tr><tr><td colspan="1" rowspan="1"><p>Gypsy moth larva </p></td><td colspan="1" rowspan="1"><p>40</p></td></tr><tr><td colspan="1" rowspan="1"><p> Laspeyresia pomonella larva </p></td><td colspan="1" rowspan="1"><p>56</p></td></tr><tr><td colspan="1" rowspan="1"><p>Total number of images </p></td><td colspan="1" rowspan="1"><p>565</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>2.2. Proposed methodology</title>
          
          <p>The application of transfer learning to the classification of entomological imagery is elucidated in <xref ref-type="fig" rid="fig_2">Figure 2</xref>. Within this framework, the latent convolutional layers of pre-existing neural networks are meticulously adjusted to facilitate a categorization into ten distinct classes. It is posited that this nuanced calibration of transfer learning, effected through the modification of terminal layers within the network, augments the network's discriminative power, thereby enhancing performance.</p><p>Transfer learning emerges as a pivotal technique in the development of pest recognition systems, particularly in scenarios marked by a scarcity of labeled data specific to the task at hand. The process is delineated as follows: Initially, the task is defined with precision. Subsequently, a collection of labeled images embodying the pests of interest is compiled, with annotations encompassing both the presence and absence of pests, and, where pertinent, the exact species. This dataset is anticipated to be representative, encompassing various life stages and environmental contexts [<xref ref-type="bibr" rid="ref_29">29</xref>], [<xref ref-type="bibr" rid="ref_30">30</xref>]. A model with prior training on a comprehensive dataset, such as ResNet 50 or ResNet101 utilizing the ImageNet database, is selected. The data are then subjected to preparatory processing, which involves resizing, normalizing, and augmenting the images to conform to the expected input schema of the chosen model. Techniques for data augmentation may include, but are not limited to, random cropping, rotation, and flipping [<xref ref-type="bibr" rid="ref_31">31</xref>].</p><p>Alterations are made to the pre-trained model to tailor it to the specific task of pest recognition. The model is then trained on the labeled pest dataset, applying transfer learning principles by initializing the model with pre-trained weights and refining these on the dataset in question [<xref ref-type="bibr" rid="ref_32">32</xref>]. Monitoring of the training sequence is conducted, employing strategies such as early stopping to mitigate overfitting, with hyperparameters being adjusted accordingly. Model performance is evaluated on a distinct validation set, with hyperparameters fine-tuned to optimize outcomes based on the validation feedback. A comprehensive assessment of the trained model is performed using a dedicated test dataset to determine the model's generalization capability. Metrics of accuracy and reliability are computed to gauge model performance [<xref ref-type="bibr" rid="ref_33">33</xref>], [<xref ref-type="bibr" rid="ref_34">34</xref>]. Should the model satisfy predefined performance thresholds, it is then deployed for real-time or batch inference within the pest recognition framework, potentially integrating with remote sensing technologies such as drones or stationary cameras.</p><p>The principle of transfer learning leverages the knowledge acquired from expansive image datasets by pre-trained models, applying it to the specialized domain of pest detection and classification [<xref ref-type="bibr" rid="ref_35">35</xref>]. This approach holds the potential to markedly diminish the volume of labeled data required to construct an efficacious pest recognition system.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>Application of transfer learning to the classification of insect pests</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_IF3_2kJZPnlCClD1.png"/>
            </fig>
          
          <p>In the study presented, transfer learning has been utilized within deep neural networks to facilitate the classification of insect pests. Transfer learning is characterized as a strategy whereby knowledge harnessed by a pre-established model is repurposed to solve analogous problems. The training of deep neural networks traditionally necessitates extensive datasets, considerable computational time, and processing power.</p><p>It is posited that the integration of transfer learning, particularly with pre-trained deep neural networks, constitutes an advanced, cost-effective alternative for classification challenges in scenarios marked by data and processing resource scarcity. The selection of an appropriate pre-trained neural network model is pivotal within the transfer learning paradigm. This critical choice is contingent upon the similarity of the existing problem to the task for which the model was initially trained. A heightened risk of overfitting is posited when the source dataset employed for training exhibits significant congruence in size and composition with the target dataset.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>2.3. Resnet</title>
          
          <p>Within the domain of convolutional neural network architectures, the ResNet family stands as a prominent contribution, as introduced by He et al. [<xref ref-type="bibr" rid="ref_36">36</xref>]. The moniker "ResNet" is derived from "Residual Network," a term that encapsulates the network's pioneering innovation: the integration of residual blocks. These blocks are instrumental in enabling the training of profoundly deep neural networks. The ResNet architecture is presented in two variants, namely ResNet-50 and ResNet-101, the numerals indicative of the layers' depth [<xref ref-type="bibr" rid="ref_36">36</xref>]. Both models function akin to standard deep networks while additionally featuring identity mapping capabilities. It is through this innovation that ResNet architectures address and mitigate the issue of vanishing gradients, with ResNet-50 boasting an architecture that scales from 34 to an expansive 152 layers [<xref ref-type="bibr" rid="ref_37">37</xref>], [<xref ref-type="bibr" rid="ref_38">38</xref>]. <xref ref-type="fig" rid="fig_3">Figure 3</xref> delineates the ILSVRC 2015 classification task's winning architecture, a testament to the architecture's efficacy [<xref ref-type="bibr" rid="ref_39">39</xref>].</p><p>The deeper variant, ResNet-101, extends to 101 layers, incorporating an increased number of residual blocks to capture more nuanced patterns and features within the dataset. Its application is deemed appropriate when tasks necessitate heightened accuracy or intricate feature extraction, such as image classification and object recognition. Inherently, the added depth translates to a greater parameter count, with ResNet-101 possessing approximately 44.5 million parameters, rendering it a more potent albeit computationally demanding model.</p><p>The schematic of a residual block is depicted in <xref ref-type="fig" rid="fig_3">Figure 3</xref>. It is observed that for applications such as fault diagnosis, ResNet-50, conceptualized as a TCNN, is employed, leveraging its strength as a feature extractor post-training on the ImageNet dataset [<xref ref-type="bibr" rid="ref_40">40</xref>]. The amalgamation of convolutional neural networks with transfer learning is demonstrated as a method for fault diagnostics. Moreover, the predictive capacity of ResNet-50 in the early diagnosis of Alzheimer's disease (AD) has been explored [<xref ref-type="bibr" rid="ref_41">41</xref>]. Utilizing magnetic resonance imaging (MRI) scans, ResNet-50 facilitates multi-class classification to ascertain the presence and intensity of clinical dementia ratings (CDR). This machine learning approach has demonstrated high precision in classifying AD, suggesting potential utility in the preemptive identification of AD patients prior to clinical evaluation.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>Residual block</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_xcFaGZGcD9kEtIRl.png"/>
            </fig>
          
          <p>The ResNet-101 architecture represents an advancement in the field of deep convolutional neural networks, encompassing a comprehensive structure that addresses a multitude of computer vision tasks. Characterised by its depth, with an assemblage of 101 layers, ResNet-101 extends beyond its predecessor, ResNet-50, by facilitating enhanced feature extraction through additional layers. The architecture is delineated as follows: Initially, an input layer receives an image comprising typically three colour channels, such as red, green, and blue (RGB). Subsequent to the input layer, a series of convolutional layers are employed, applying a multitude of convolutional filters to extract low-level and mid-level features, encompassing edges, textures, and shapes. At the heart of ResNet-101 lies the implementation of residual blocks, which are pivotal in mitigating the vanishing gradient problem, thereby enabling the training of deep networks. These blocks consist of convolutional layers, batch normalization, and ReLU activation functions, with the addition of skip connections that perform elementwise addition of the input from a preceding layer to the output of the block. Furthermore, ResNet-101 incorporates two distinct types of residual blocks: identity blocks, which maintain input and output dimensions, and projection blocks, which modify dimensions via a 1×1 convolutional layer. Pooling layers are interspersed following the residual blocks, typically adopting average pooling to diminish spatial dimensions while preserving salient features. As the network progresses towards its conclusion, fully connected layers are utilised for classification, culminating in an output layer where a softmax activation function articulates class probabilities for multi-class classification tasks.</p><p>The ResNet-101 architecture has been demonstrated to be a flexible construct, extensively applied across diverse computer vision domains. Its utility spans several key applications as follows: </p><p>(1) Image classification. ResNet-101 is often selected for image classification endeavors. Its architectural depth enables the intricate capture of patterns within images, a process critical for accurate categorization into predefined classes. </p><p>(2) Object detection. The capacity of ResNet-101 to serve as a potent feature extractor has been validated in object detection frameworks such as Faster R-CNN and Mask R-CNN. Here, it aids in the pinpointing and classification of objects across still images and video sequences. </p><p>(3) Semantic segmentation. The architecture is actively employed in semantic segmentation tasks, with its depth providing the means to execute detailed classification of individual pixels into specified categories. </p><p>(4) Transfer learning: The adaptability of ResNet-101 is particularly evident in transfer learning scenarios. Models pretrained on comprehensive datasets like ImageNet present an invaluable foundation for a multitude of vision-based projects, where fine-tuning on smaller datasets can elicit commendable outcomes despite limited sample volumes. </p><p>(5) Fine-grained classification. Tasks requiring discernment between closely related categories within a larger class benefit from the depth of ResNet-101, which facilitates nuanced differentiation. </p><p>(6) Medical image analysis. Its application has been extended to medical image analysis, addressing critical tasks such as disease detection, organ segmentation, and pathology classification.</p><p>(7) Scene recognition. Employed in the recognition and classification of scenes within images, ResNet-101 supports applications in autonomous navigation and content tagging.</p><p>In sum, the depth and structural ingenuity of ResNet-101, featuring residual blocks and skip connections, render it a robust mechanism for complex feature extraction challenges. This has solidified its status as a cornerstone in the computer vision research community and operational domain.</p>
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>3. Result and discussion</title>
      <p>In the current investigation, the efficacy of transfer learning in classifying insect pest images from the Deng dataset was evaluated across nine distinct deep neural network architectures. Transfer learning, coupled with fine-tuning strategies, was employed to mitigate the risk of overfitting, a frequent challenge in machine learning tasks with limited data. Examination of the classification outcomes, as detailed in <xref ref-type="table" rid="table_2">Table 2</xref>, indicates that with a diminutive dataset, the ResNet-50 model exhibited superior performance, achieving an accuracy of 99.40% and a precision of 99.10%. In comparison, the ResNet-101 model attained an accuracy of 97.63% and a precision of 97.69%.</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>Detailed classification results</caption>
          <abstract/>
          <table><tbody><tr><td colspan="1" rowspan="1"><p> Model </p></td><td colspan="1" rowspan="1"><p> Accuracy </p></td><td colspan="1" rowspan="1"><p>Precision </p></td><td colspan="1" rowspan="1"><p>Recall </p></td><td colspan="1" rowspan="1"><p>Fl-score </p></td><td colspan="1" rowspan="1"><p> Sensitivity </p></td><td colspan="1" rowspan="1"><p> Specificity </p></td></tr><tr><td colspan="1" rowspan="1"><p>ResNet-50 </p></td><td colspan="1" rowspan="1"><p>99.40 %</p></td><td colspan="1" rowspan="1"><p>99.10 %</p></td><td colspan="1" rowspan="1"><p>99.10 %</p></td><td colspan="1" rowspan="1"><p>99.10 %</p></td><td colspan="1" rowspan="1"><p>99 %</p></td><td colspan="1" rowspan="1"><p>99.10 %</p></td></tr><tr><td colspan="1" rowspan="1"><p>ResNet-101 </p></td><td colspan="1" rowspan="1"><p>97.63 %</p></td><td colspan="1" rowspan="1"><p>97.69 %</p></td><td colspan="1" rowspan="1"><p>97.40 %</p></td><td colspan="1" rowspan="1"><p>97.75 %</p></td><td colspan="1" rowspan="1"><p>97 %</p></td><td colspan="1" rowspan="1"><p>97.50 %</p></td></tr></tbody></table>
        </table-wrap>
      
      
        <fig id="fig_4">
          <label>Figure 4</label>
          <caption>Confusion matrix for the highest-performing neural network (ResNet-50)</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/10/img_cHi3LcBJsLSs0BDC.png"/>
        </fig>
      
      <p>The computational environment deployed for model training comprised an Nvidia GTX3060 Super GPU accelerator and an AMD Ryzen 7 3700X CPU, featuring an 8-core processor with 32GB of DDR4-3200 memory. A confusion matrix for the highest-performing neural network, ResNet-50, is illustrated in <xref ref-type="fig" rid="fig_4">Figure 4</xref>. For comprehensive performance evaluation of the deep neural networks, metrics including accuracy, precision, recall, F1-score, sensitivity, and specificity were calculated. These metrics offer a multi-dimensional perspective on model efficacy, facilitating a nuanced analysis of the nine transfer learning models. Eqs. (1)-(4) provide the computational framework for these metrics, which are integral to validating the performance of the aforementioned models [<xref ref-type="bibr" rid="ref_32">32</xref>]. Classification accuracy is defined as the ratio of correct predictions to the total number of datasets.</p>
      
        <disp-formula>
          <label>(1)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mtext> Accuracy </mtext>
            <mo>=</mo>
            <mfrac>
              <mrow>
                <mi>T</mi>
                <mi>P</mi>
                <mi>T</mi>
                <mi>N</mi>
                <mo>+</mo>
              </mrow>
              <mrow>
                <mi>T</mi>
                <mi>P</mi>
                <mi>F</mi>
                <mi>P</mi>
                <mi>T</mi>
                <mi>N</mi>
                <mi>F</mi>
                <mi>N</mi>
                <mo>+</mo>
                <mo>+</mo>
                <mo>+</mo>
              </mrow>
            </mfrac>
          </math>
        </disp-formula>
      
      <p>Precision, denoted as positive predictive values, reflects the proportion of true positive predictions in all positive classes.</p>
      
        <disp-formula>
          <label>(2)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mtext> Precision </mtext>
            <mo>=</mo>
            <mo stretchy="false">(</mo>
            <mo>+</mo>
            <mo stretchy="false">)</mo>
            <mi>T</mi>
            <mi>P</mi>
            <mi>F</mi>
            <mi>P</mi>
            <mi>T</mi>
            <mi>P</mi>
            <mrow data-mjx-texclass="ORD">
              <mo>/</mo>
            </mrow>
          </math>
        </disp-formula>
      
      <p>Recall measures the fraction of true positives identified out of all actual positives. </p>
      
        <disp-formula>
          <label>(3)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mtext> Recall </mtext>
            <mo>=</mo>
            <mo stretchy="false">(</mo>
            <mo>+</mo>
            <mo stretchy="false">)</mo>
            <mi>F</mi>
            <mi>N</mi>
            <mi>F</mi>
            <mi>N</mi>
            <mi>T</mi>
            <mi>P</mi>
            <mrow data-mjx-texclass="ORD">
              <mo>/</mo>
            </mrow>
          </math>
        </disp-formula>
      
      <p>The F1-score represents the harmonic mean of precision and recall.</p>
      
        <disp-formula>
          <label>(4)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>F</mi>
            <mn>1</mn>
            <mn>2</mn>
            <mo>−</mo>
            <mo>=</mo>
            <mo>∗</mo>
            <mtext> Score </mtext>
            <mfrac>
              <mrow>
                <mo stretchy="false">(</mo>
                <mo>.</mo>
                <mo stretchy="false">)</mo>
                <mtext> Precsion </mtext>
                <mtext> Recall </mtext>
              </mrow>
              <mrow>
                <mo stretchy="false">(</mo>
                <mo>+</mo>
                <mo stretchy="false">)</mo>
                <mtext> Precsion </mtext>
                <mtext> Recall </mtext>
              </mrow>
            </mfrac>
          </math>
        </disp-formula>
      
      <p>In this section, sensitivity and specificity were meticulously calculated. Sensitivity, or recall, refers to the proportion of correctly identified positive instances relative to the total number of actual positive instances. A perfect sensitivity score is 1.0, indicating flawless prediction, whereas a score of 0.0 signifies complete misclassification. Specificity, similarly, is determined by the ratio of true negative predictions to the total number of negative instances, with 1.0 being the ideal and 0.0 the antithesis of accurate prediction.</p>
      
        <table-wrap id="table_3">
          <label>Table 3</label>
          <caption>Comparative analysis with preceding state-of-the-art research</caption>
          <abstract/>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Work</p></td><td colspan="1" rowspan="1"><p>Dataset</p></td><td colspan="1" rowspan="1"><p>Method</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td></tr><tr><td colspan="1" rowspan="1"><p>Nanni et al. [<xref ref-type="bibr" rid="ref_42">42</xref>]</p></td><td colspan="1" rowspan="1"><p>Small (Deng et al. [<xref ref-type="bibr" rid="ref_26">26</xref>])</p></td><td colspan="1" rowspan="1"><p>CNN</p></td><td colspan="1" rowspan="1"><p>95.52 %</p></td></tr><tr><td colspan="1" rowspan="1"><p>Deng et al. [<xref ref-type="bibr" rid="ref_26">26</xref>]</p></td><td colspan="1" rowspan="1"><p>Small (Deng et al. [<xref ref-type="bibr" rid="ref_26">26</xref>])</p></td><td colspan="1" rowspan="1"><p>LCP+SVM</p></td><td colspan="1" rowspan="1"><p>85.5 %</p></td></tr><tr><td colspan="1" rowspan="1"><p>Wang et al. [<xref ref-type="bibr" rid="ref_43">43</xref>]</p></td><td colspan="1" rowspan="1"><p>Small (Deng et al. [<xref ref-type="bibr" rid="ref_26">26</xref>])</p></td><td colspan="1" rowspan="1"><p>TL AlexNet</p></td><td colspan="1" rowspan="1"><p>93.84 %</p></td></tr><tr><td colspan="1" rowspan="1"><p>Proposed Method</p></td><td colspan="1" rowspan="1"><p>Small (Deng et al. [<xref ref-type="bibr" rid="ref_26">26</xref>])</p></td><td colspan="1" rowspan="1"><p>ResNet-50</p></td><td colspan="1" rowspan="1"><p>99.40 %</p></td></tr></tbody></table>
        </table-wrap>
      
      <p><xref ref-type="table" rid="table_3">Table 3</xref> describes the comparative analysis with preceding state-of-the-art research. When juxtaposed with preceding benchmarks in the field, the method delineated herein outstripped the performance of the counterparts, achieving an accuracy pinnacle of 99.40%. This is followed by the work of Nanni et al. [<xref ref-type="bibr" rid="ref_42">42</xref>], which attained a commendable 95.52% accuracy. The adoption of ResNet models, particularly ResNet-50, in deep learning and computer vision has been marked by numerous advantages. A synthesis of the merits reveals the following:</p><p>(1) Facilitation of deep network training. ResNet models incorporate skip connections, or residual blocks, which have been shown to alleviate the vanishing gradient issue. This architectural feature permits the training of networks with an extensive number of layers, thus averting performance degradation despite increased depth.</p><p>(2) Enhanced training efficiency. It has been observed that ResNet architectures often exhibit accelerated convergence during the training phase when contrasted with their predecessors. The skip connections facilitate easier learning of identity mappings, culminating in a reduced number of iterations required to reach optimal performance.</p><p>(3) Gradient flow optimisation. The integration of skip connections in ResNet models underpins a robust gradient signal across the network. Such an enhancement in gradient flow contributes to rapid convergence and stabilises the training of deeply layered networks.</p><p>(4) Benchmark performance. Models such as ResNet-101 and ResNet-152 have consistently delivered exemplary performance on a variety of computer vision benchmarks. Their proficiency encompasses tasks ranging from image classification to semantic segmentation.</p><p>(5) Versatile application. The utility of ResNet models spans a broad spectrum of computer vision tasks, evidencing their adaptability to varying challenges including but not limited to classification, detection, and segmentation. This versatility extends to transfer learning applications, where models pre-trained on extensive datasets prove beneficial.</p><p>(6) Potential of transfer learning. The availability of pre-trained ResNet models on datasets such as ImageNet facilitates their use in transfer learning contexts. Such utilization allows for a reduction in training duration while maintaining commendable results in scenarios characterised by limited data.</p><p>(7) Generalisation capability. Notably, ResNet models have demonstrated an impressive capacity to generalise across diverse datasets and contexts, affirming their applicability in a multitude of real-world scenarios.</p><p>(8) Architectural interpretability. The architecture of ResNet models, owing to the presence of skip connections, presents an enhanced level of interpretability. Such a structure permits a more coherent analysis of information flow and feature representation across different layers.</p><p>(9) Community and resource availability. The widespread adoption of ResNet models has fostered a substantial user community and a wealth of resources, ranging from pre-trained models to extensive literature, thereby easing the integration and utilization of these models in both research and practical applications.</p><p>(10) Discriminative feature learning. Deep feature representations learned by ResNet models are distinguished by their discriminative and transferable qualities, proving valuable for downstream tasks irrespective of the initial training objective.</p><p>In the landscape of computer vision, ResNet architectures have been discerned to offer a robust and potent framework, as evidenced by their extensive deployment across varied tasks within the domain. The profundity of their architectural design, coupled with the facilitation of training, has been observed to foster strong gradient flow and perpetuate a consistent performance that aligns with the apex of contemporary standards. These attributes have been pivotal in cementing the role of ResNet models as a foundational element in the advancement of deep learning methodologies and their practical applications within the sphere of computer vision.</p>
    </sec>
    <sec disp-level="level1" sec-type="conclusions">
      <title>4. Conclusions</title>
      <p>The study delineated herein has leveraged the ResNet-50 and ResNet-101 frameworks to evaluate their efficacy in the identification, detection, and classification of insect pests. The application of ResNet in pest recognition has been substantiated as an efficacious methodology, particularly when utilizing pre-trained models in conjunction with limited labeled data sets. The objective pursued was to augment agricultural productivity by facilitating an automated, transfer learning-based mechanism for the prompt and effective detection of insect pests.</p><p>Analyses performed have revealed that the ResNet-50 transfer learning algorithm exhibits remarkable proficiency in the detection of agricultural pests, culminating in an unprecedented accuracy rate of 99.40%, thereby surpassing other transfer learning algorithms in performance metrics. This represents a significant stride in precision agriculture technologies. Prospective studies are advocated to further probe the potential of transfer learning within deep neural networks, with an emphasis on the minimization of computational time complexity inherent in the identification of insect pests. Such inquiries are anticipated to refine the efficiency and applicability of transfer learning algorithms in real-world agricultural settings.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      <p>The author would like to thank all colleagues from Satya Wacana Christian University, Indonesia, and all involved in this research.</p>
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p></p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>71</volume>
          <page-range>552-559</page-range>
          <issue>7</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>V.</given-names>
              <surname>Malathi</surname>
            </name>
            <name>
              <given-names>M. P.</given-names>
              <surname>Gopinath</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/09064710.2021.1874045</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Classification of pest detection in paddy crop based on transfer learning approach</article-title>
          <source>Acta Agric. Scand. Sect. B Soil Plant Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>1-8</page-range>
          <issue>26</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>T. T.</given-names>
              <surname>Nguyen</surname>
            </name>
            <name>
              <given-names>Q. T.</given-names>
              <surname>Vien</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Sellahewa</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.4108/eai.26-1-2021.168227</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>An efficient pest classification in smart agriculture using transfer learning</article-title>
          <source>EAI Endorsed Trans. Ind. Networks Intell. Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <page-range>149</page-range>
          <issue>4</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Zhuang</surname>
            </name>
            <name>
              <given-names>H. J.</given-names>
              <surname>Christanto</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/bdcc6040149</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Yolov5 series algorithm for road marking sign identification</article-title>
          <source>Big Data Cogn. Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="conf-paper">
          <volume>2019</volume>
          <page-range>258–265</page-range>
          <issue/>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Yu</surname>
            </name>
            <name>
              <given-names>Z. X.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>X. J.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Qiao</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>F. G.</given-names>
              <surname>Xie</surname>
            </name>
            <name>
              <given-names>Q.</given-names>
              <surname>Wei</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Yang</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/robio49542.2019.8961714</pub-id>
          <article-title>A DenseNet feature-based loop closure method for visual SLAM system</article-title>
          <source>Proceedings of the IEEE International Conference on Robotics and Biomimetics (ROBIO), Dali, China</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>2022</volume>
          <page-range>7897669</page-range>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>H.</given-names>
              <surname>Kibriya</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Amin</surname>
            </name>
            <name>
              <given-names>A. H.</given-names>
              <surname>Alshehri</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Masood</surname>
            </name>
            <name>
              <given-names>S. S.</given-names>
              <surname>Alshamrani</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Alshehri</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2022/7897669</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A novel and effective brain tumor classification model using deep feature fusion and famous machine learning classifiers</article-title>
          <source>Comput. Intell. Neurosci.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>192</volume>
          <page-range/>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>T.</given-names>
              <surname>Jintasuttisak</surname>
            </name>
            <name>
              <given-names>E.</given-names>
              <surname>Edirisinghe</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Elbattay</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compag.2021.106560</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Deep neural network based date palm tree detection in drone imagery</article-title>
          <source>Comput. Electron. Agric</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="conf-paper">
          <volume>2022</volume>
          <page-range>81-93</page-range>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
            <name>
              <given-names>R. C.</given-names>
              <surname>Chen</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1007/978-3-031-21743-2</pub-id>
          <article-title>Complement naive bayes classifier for sentiment analysis of internet movie database</article-title>
          <source>Asian Conference on Intelligent Information and Database Systems, Ho Chi Minh City, Vietnam, November 28-30</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="conf-paper">
          <volume>2019</volume>
          <page-range>8787-8796</page-range>
          <issue/>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>X. P.</given-names>
              <surname>Wu</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Zhan</surname>
            </name>
            <name>
              <given-names>Y. K.</given-names>
              <surname>Lai</surname>
            </name>
            <name>
              <given-names>M. M.</given-names>
              <surname>Cheng</surname>
            </name>
            <name>
              <given-names>J. F.</given-names>
              <surname>Yang</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/cvpr.2019.00899</pub-id>
          <article-title>IP102: A large-scale benchmark dataset for insect pest recognition</article-title>
          <source>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Long Beach, CA, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>3335-3345</page-range>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Turkoglu</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Hanbay</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Sengur</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s12652-019-01591-w</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Multi-model LSTM-based convolutional neural networks for detection of apple diseases and pests</article-title>
          <source>J. Ambient Intell. Humaniz. Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>1-18</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
            <name>
              <given-names>R. C.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Yu</surname>
            </name>
            <name>
              <given-names>X. Y.</given-names>
              <surname>Jiang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s12652-021-03584-0</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Robust detection method for improving small traffic sign recognition based on spatial pyramid pooling</article-title>
          <source>J. Ambient Intell. Humaniz. Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>52</page-range>
          <issue>1</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>R. C.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
            <name>
              <given-names>S. W.</given-names>
              <surname>Huang</surname>
            </name>
            <name>
              <given-names>R. E.</given-names>
              <surname>Caraka</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s40537-020-00327-4</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Selecting critical features for data classification based on machine learning methods</article-title>
          <source>J. Big Data</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>446-457</page-range>
          <issue>3</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>T.</given-names>
              <surname>Kasinathan</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Singaraju</surname>
            </name>
            <name>
              <given-names>S. R.</given-names>
              <surname>Uyyala</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.inpa.2020.09.006</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Insect classification and detection in field crops using modern machine learning techniques</article-title>
          <source>Inf. Process. Agric.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>17149-17157</page-range>
          <issue/>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A.</given-names>
              <surname>Rehman</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Naz</surname>
            </name>
            <name>
              <given-names>M. I.</given-names>
              <surname>Razzak</surname>
            </name>
            <name>
              <given-names>I. A.</given-names>
              <surname>Hameed</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2018.2890810</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Automatic visual features for writer identification: A deep learning approach</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>104-116</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
            <name>
              <given-names>R. C.</given-names>
              <surname>Chen</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.2478/cait-2022-0007</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Combination of ResNet and spatial pyramid pooling for musical instrument identification</article-title>
          <source>Cybern. Inf. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>79</volume>
          <page-range>32897-32915</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
            <name>
              <given-names>R. C.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Yu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-020-09509-x</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Weight analysis for various prohibitory sign detection and recognition using deep learning</article-title>
          <source>Multimed. Tools Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>39</volume>
          <page-range>5682-5689</page-range>
          <issue>15</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A.</given-names>
              <surname>Jaiswal</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Gianchandani</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Singh</surname>
            </name>
            <name>
              <given-names>V.</given-names>
              <surname>Kumar</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Kaur</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/07391102.2020.1788642</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Classification of the COVID-19 infected patients using DenseNet201 based deep transfer learning</article-title>
          <source>J. Biomol. Struct. Dyn.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>21986-21997</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C. J.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>Y. Y.</given-names>
              <surname>Huang</surname>
            </name>
            <name>
              <given-names>Y. S.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>Y. C.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>C. Y.</given-names>
              <surname>Chang</surname>
            </name>
            <name>
              <given-names>Y. M.</given-names>
              <surname>Huang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2021.3056082</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Identification of fruit tree pests with deep learning on embedded drone to achieve accurate pesticide spraying</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>78</volume>
          <page-range>711-721</page-range>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>R. J.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>T. J.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>C. J.</given-names>
              <surname>Xie</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Qiu</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>J. M.</given-names>
              <surname>Du</surname>
            </name>
            <name>
              <given-names>H. B.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>F. R.</given-names>
              <surname>Shao</surname>
            </name>
            <name>
              <given-names>H. Y.</given-names>
              <surname>Hu</surname>
            </name>
            <name>
              <given-names>H. Y.</given-names>
              <surname>Liu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1002/ps.6684</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>An automatic system for pest recognition and forecasting</article-title>
          <source>Pest Manag. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>60</volume>
          <page-range>4423-4432</page-range>
          <issue>55</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M. E.</given-names>
              <surname>Karar</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Alsunaydi</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Albusaymi</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Alotaibi</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.aej.2021.03.009</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A new mobile application of agricultural pests recognition using deep learning in cloud computing system</article-title>
          <source>Alexandria Eng. J.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>905</page-range>
          <issue>8</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M. Á.</given-names>
              <surname>Rodríguez-García</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>García-Sánchez</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Valencia-García</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/electronics10080905</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Knowledge-based system for crop pests and diseases recognition</article-title>
          <source>Electronics</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>37</volume>
          <page-range>627-638</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
            <name>
              <given-names>H. J.</given-names>
              <surname>Christanto</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/ria.370312</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Automatic medical face mask recognition for COVID-19 mitigation: Utilizing YOLO V5 object detection</article-title>
          <source>Rev. d'Intelligence Artif.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>7589-7598</page-range>
          <issue>11</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>L.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>C. J.</given-names>
              <surname>Xie</surname>
            </name>
            <name>
              <given-names>R. J.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Yang</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Sudirman</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>F. Y.</given-names>
              <surname>Wang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TII.2020.2995208</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Deep learning based automatic multiclass wild pest monitoring approach using hybrid global and local activated features</article-title>
          <source>IEEE Trans. Ind. Informatics</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>39</volume>
          <page-range>711-724</page-range>
          <issue>34</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
            <name>
              <given-names>R. C.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Yu</surname>
            </name>
            <name>
              <given-names>X. Y.</given-names>
              <surname>Jiang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.6688/JISE.202307_39(4).0001</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>XAI for image captioning using SHAP</article-title>
          <source>J. Inf. Sci. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>53</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
            <name>
              <given-names>A. P. S.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>H. J.</given-names>
              <surname>Christanto</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/bdcc7010053</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Deep learning for highly accurate hand recognition based on Yolov7 model</article-title>
          <source>Big Data Cogn. Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range>1-19</page-range>
          <issue/>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>V. S.</given-names>
              <surname>Saravanarajan</surname>
            </name>
            <name>
              <given-names>R. C.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
            <name>
              <given-names>L. S.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Ganesan</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-023-15906-9</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Car crash detection using ensemble deep learning</article-title>
          <source>Multimed. Tools Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>169</volume>
          <page-range>139-148</page-range>
          <issue/>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>L. M.</given-names>
              <surname>Deng</surname>
            </name>
            <name>
              <given-names>Y. J.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>Z. Z.</given-names>
              <surname>Han</surname>
            </name>
            <name>
              <given-names>R. S.</given-names>
              <surname>Yu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.biosystemseng.2018.02.008</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Research on insect pest image detection and recognition based on bio-inspired methods</article-title>
          <source>Biosyst. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>71</volume>
          <page-range>2125-2140</page-range>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>R.</given-names>
              <surname>Sharma</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Singh</surname>
            </name>
            <name>
              <given-names>N. Z.</given-names>
              <surname>Jhanjhi</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Masud</surname>
            </name>
            <name>
              <given-names>E. S.</given-names>
              <surname>Jaha</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Verma</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.32604/cmc.2022.020017</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Plant disease diagnosis and image classification using deep learning</article-title>
          <source>Comput. Mater. Contin.</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <volume>214</volume>
          <page-range/>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>J. F.</given-names>
              <surname>Gao</surname>
            </name>
            <name>
              <given-names>J. C.</given-names>
              <surname>Westergaard</surname>
            </name>
            <name>
              <given-names>E. H. R.</given-names>
              <surname>Sundmark</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Bagge</surname>
            </name>
            <name>
              <given-names>E.</given-names>
              <surname>Liljeroth</surname>
            </name>
            <name>
              <given-names>E.</given-names>
              <surname>Alexandersson</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.knosys.2020.106723</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Automatic late blight lesion recognition and severity quantification based on field imagery of diverse potato genotypes by deep learning</article-title>
          <source>Knowledge-Based Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range/>
          <issue/>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>R. C.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
            <name>
              <given-names>Y. C.</given-names>
              <surname>Zhuang</surname>
            </name>
            <name>
              <given-names>J. K.</given-names>
              <surname>Chen</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2023.3309410</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Contrast limited adaptive histogram equalization for recognizing road marking at night based on YOLO models</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>333-348</page-range>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
            <name>
              <given-names>R. C.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>Y. T.</given-names>
              <surname>Liu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1142/S2196888822500191</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Synthetic traffic sign image generation applying generative adversarial networks</article-title>
          <source>Vietnam J. Comput. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>889</page-range>
          <issue>6</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
            <name>
              <given-names>R. C.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>S. K.</given-names>
              <surname>Tai</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/electronics9060889</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Evaluation of robust spatial pyramid pooling based on convolutional neural network for traffic sign recognition system</article-title>
          <source>Electronics</source>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>2913</page-range>
          <issue>7</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>Y. T.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Yu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app11072913</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Various generative adversarial networks model for synthetic prohibitory sign image generation</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <volume>212</volume>
          <page-range>108129</page-range>
          <issue/>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>G. W.</given-names>
              <surname>Dai</surname>
            </name>
            <name>
              <given-names>J. C.</given-names>
              <surname>Fan</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compag.2023.108129</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>ITF-WPI: Image and text based cross-modal feature fusion model for wolfberry pest recognition</article-title>
          <source>Comput. Electron. Agric.</source>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>478</page-range>
          <issue>9</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Javed</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Sajid</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Kiren</surname>
            </name>
            <name>
              <given-names>I. U.</given-names>
              <surname>Khan</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Cauteruccio</surname>
            </name>
            <name>
              <given-names>H. J.</given-names>
              <surname>Christanto</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/info14090478</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A subjective logical framework-based trust model for wormhole attack detection and mitigation in Low-Power and Lossy (RPL) IoT-Networks</article-title>
          <source>Information</source>
        </element-citation>
      </ref>
      <ref id="ref_35">
        <label>35.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>94</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Dewi</surname>
            </name>
            <name>
              <given-names>A. P. S.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>H. J.</given-names>
              <surname>Christanto</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/bdcc7020094</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Recognizing similar musical instruments with YOLO models</article-title>
          <source>Big Data and Cognitive Computing</source>
        </element-citation>
      </ref>
      <ref id="ref_36">
        <label>36.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range>770-778</page-range>
          <issue/>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <given-names>K. M.</given-names>
              <surname>He</surname>
            </name>
            <name>
              <given-names>X. Y.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>S. Q.</given-names>
              <surname>Ren</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Sun</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/cvpr.2016.90</pub-id>
          <article-title>Deep residual learning for image recognition</article-title>
          <source>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 27-30 June</source>
        </element-citation>
      </ref>
      <ref id="ref_37">
        <label>37.</label>
        <element-citation publication-type="journal">
          <volume>113</volume>
          <page-range>893-903</page-range>
          <issue>5</issue>
          <year>2009</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Gyanesh</given-names>
              <surname>Chander</surname>
            </name>
            <name>
              <given-names>Brian L.</given-names>
              <surname>Markham</surname>
            </name>
            <name>
              <given-names>Dennis L.</given-names>
              <surname>Helder</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.rse.2009.01.007</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Summary of current radiometric calibration coefficients for Landsat MSS, TM, ETM+, and EO-1 ALI sensors</article-title>
          <source>Remote Sens. Environ.</source>
        </element-citation>
      </ref>
      <ref id="ref_38">
        <label>38.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>3168-3177</page-range>
          <issue>9</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>W. Z.</given-names>
              <surname>Fang</surname>
            </name>
            <name>
              <given-names>C. G.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Wan</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>S. Y.</given-names>
              <surname>Zhu</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Fang</surname>
            </name>
            <name>
              <given-names>B. J.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Hong</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/jstars.2019.2929601</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Recognizing global reservoirs from landsat 8 images: A deep learning approach</article-title>
          <source>IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens.</source>
        </element-citation>
      </ref>
      <ref id="ref_39">
        <label>39.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>19490-19503</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Y. N.</given-names>
              <surname>Ibrahim</surname>
            </name>
            <name>
              <given-names>H. B.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Bai</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>J. N.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>Z. M.</given-names>
              <surname>Yang</surname>
            </name>
            <name>
              <given-names>Z.M.</given-names>
              <surname>Chen</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/access.2020.2968129</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Soft Error Resilience of Deep Residual Networks for Object Recognition</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_40">
        <label>40.</label>
        <element-citation publication-type="journal">
          <volume>32</volume>
          <page-range>6111-6124</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>L. Wen</surname>
            </name>
            <name>
              <surname>X. Y. Li</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Gao</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00521-019-04097-w</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A transfer convolutional neural network for fault diagnosis based on ResNet-50</article-title>
          <source>Neural Comput. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_41">
        <label>41.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>212</page-range>
          <issue>9</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Laura V.</given-names>
              <surname>Fulton</surname>
            </name>
            <name>
              <given-names>Diane</given-names>
              <surname>Dolezel</surname>
            </name>
            <name>
              <given-names>James</given-names>
              <surname>Harrop</surname>
            </name>
            <name>
              <given-names>Yiyu</given-names>
              <surname>Yan</surname>
            </name>
            <name>
              <given-names>Charles P.</given-names>
              <surname>Fulton</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/brainsci9090212</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Classification of Alzheimer’s disease with and without imagery using gradient boosted machines and ResNet-50</article-title>
          <source>Brain Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_42">
        <label>42.</label>
        <element-citation publication-type="journal">
          <volume>57</volume>
          <page-range>101089</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Loris</given-names>
              <surname>Nanni</surname>
            </name>
            <name>
              <given-names>Gianluca</given-names>
              <surname>Maguolo</surname>
            </name>
            <name>
              <given-names>Flavio</given-names>
              <surname>Pancino</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ecoinf.2020.101089</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Insect pest image detection and recognition based on bio-inspired methods</article-title>
          <source>Ecol. Inform.</source>
        </element-citation>
      </ref>
      <ref id="ref_43">
        <label>43.</label>
        <element-citation publication-type="journal">
          <volume>99</volume>
          <page-range>4524-4531</page-range>
          <issue>10</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>D. W. Wang</surname>
            </name>
            <name>
              <surname>L. M. Deng</surname>
            </name>
            <name>
              <surname>J. G. Gao</surname>
            </name>
            <name>
              <surname>J. Y. Gao</surname>
            </name>
            <name>
              <given-names>H. F.</given-names>
              <surname>Zhu</surname>
            </name>
            <name>
              <given-names>Z. Z.</given-names>
              <surname>Han</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1002/jsfa.9689</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Recognition pest by image-based transfer learning</article-title>
          <source>J. Sci. Food Agric.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>