<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">JII</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Journal of Industrial Intelligence</journal-title>
        <abbrev-journal-title abbrev-type="issn">J. Ind Intell.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">JII</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-2695</issn>
      <issn publication-format="print">2958-2687</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-Ffe5Zhwalef-rir7far727gw6EwgjSEd</article-id>
      <article-id pub-id-type="doi">10.56578/jii020103</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>An Advanced YOLOv5s Approach for Vehicle Detection Integrating Swin Transformer and SimAM in Dense Traffic Surveillance</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-0165-903X</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Yi</given-names>
          </name>
          <email>yi.zhang-81@postgrad.manchester.ac.uk</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-6054-291X</contrib-id>
          <name>
            <surname>Sun</surname>
            <given-names>Zheng</given-names>
          </name>
          <email>32021210103@cueb.edu.cn</email>
        </contrib>
        <aff id="aff_1">School of Environment, Education and Development, The University of Manchester, M13 9PL Manchester, UK</aff>
        <aff id="aff_2">School of Management and Engineering, Capital University of Economics and Business, 100070 Beijing, China</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>30</day>
        <month>03</month>
        <year>2024</year>
      </pub-date>
      <volume>2</volume>
      <issue>1</issue>
      <fpage>31</fpage>
      <lpage>41</lpage>
      <page-range>31-41</page-range>
      <history>
        <date date-type="received">
          <day>07</day>
          <month>02</month>
          <year>2024</year>
        </date>
        <date date-type="accepted">
          <day>19</day>
          <month>03</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>Â©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>In the realm of high-definition surveillance for dense traffic environments, the accurate detection and classification of vehicles remain paramount challenges, often hindered by missed detections and inaccuracies in vehicle type identification. Addressing these issues, an enhanced version of the You Only Look Once version v5s (YOLOv5s) algorithm is presented, wherein the conventional network structure is optimally modified through the partial integration of the Swin Transformer V2. This innovative approach leverages the convolutional neural networks' (CNNs) proficiency in local feature extraction alongside the Swin Transformer V2's capability in global representation capture, thereby creating a symbiotic system for improved vehicle detection. Furthermore, the introduction of the Similarity-based Attention Module (SimAM) within the CNN framework plays a pivotal role, dynamically refocusing the feature map to accentuate local features critical for accurate detection. An empirical evaluation of this augmented YOLOv5s algorithm demonstrates a significant uplift in performance metrics, evidencing an average detection precision (mAP@0.5:0.95) of 65.7%. Specifically, in the domain of vehicle category identification, a notable increase in the true positive rate by 4.48% is observed, alongside a reduction in the false negative rate by 4.11%. The culmination of these enhancements through the integration of Swin Transformer and SimAM within the YOLOv5s framework marks a substantial advancement in the precision of vehicle type recognition and reduction of target miss detection in densely populated traffic flows. The methodology's success underscores the efficacy of this integrated approach in overcoming the prevalent limitations of existing vehicle detection algorithms under complex surveillance scenarios.</p></abstract>
      <kwd-group>
        <kwd>Improved YOLOv5s algorithm</kwd>
        <kwd>Target detection</kwd>
        <kwd>Deep learning</kwd>
        <kwd>Swin Transformer</kwd>
        <kwd>Similarity-based Attention Module (SimAM)</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="2"/>
        <fig-count count="11"/>
        <table-count count="5"/>
        <ref-count count="28"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>With social development and the acceleration of urbanization, the scale of China's traffic system continues to expand, and its layout is constantly optimized, which puts forward higher requirements for the safety, efficiency, and intelligence of road traffic operations. The traffic system has gradually developed towards intelligent traffic systems, among which roadside object detection technology plays a crucial role. On the "highway" of information technology development, the advancement speed of artificial intelligence has made it an indispensable part. In intelligent traffic systems, video vehicle detection, as one of the foundational technologies, has evolved from simple video recording systems to semi-autonomous systems capable of detecting real-time vehicles accurately, providing effective data support for the implementation of traffic control measures [<xref ref-type="bibr" rid="ref_1">1</xref>]. Recently, Chen and Li [<xref ref-type="bibr" rid="ref_2">2</xref>] proposed an effective vehicle detection method that utilizes deep learning for the study of vehicle detection algorithms, offering new insights for algorithm improvement.</p><p>By setting up supporting sensing facilities on the roadside, relevant departments can timely collect and process current road traffic flow information for a more rational allocation of traffic resources. However, target detection in roadside scenarios still faces many challenges, such as the complexity and variability of the road environment and the severity of vehicle occlusion during road congestion, while overcoming limitations in computational resources [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>]. These challenges have driven the application of deep learning in target detection. For example, Wang and Jia [<xref ref-type="bibr" rid="ref_5">5</xref>] optimized vehicle detection algorithms using deep learning techniques, constructing a vehicle detection model that is both real-time and accurate. Therefore, it is a main task for researchers to improve the detection accuracy of difficult samples, such as small targets and occluded targets, while ensuring the model's lightweight. This task holds high research value and provides more reliable support for the implementation of traffic management measures.</p><p>Common target detection algorithms can be divided into two types: those based on traditional methods and those based on deep learning. Among them, target detection algorithms based on traditional methods mainly rely on manual extraction of image features. Ravichandran et al. [<xref ref-type="bibr" rid="ref_6">6</xref>] utilized SAR image data to extract target features to complete vehicle identification. Dong et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] applied Haar-like and Histogram of Oriented Gradient (HOG) features to extract vehicle features. Although traditional methods basically meet identification needs, they require a large amount of data information and preparation work during the training stage. In addition, traditional classification networks also heavily depend on predefined features and classifiers, making deep learning methods gradually gain wider application in the field of vehicle target detection.</p><p>Vehicle target detection based on deep learning does not require complex data preprocessing. The YOLO algorithm, proposed by Redmon et al. [<xref ref-type="bibr" rid="ref_8">8</xref>], is a milestone in object detection using deep learning, distinguishing itself from classical and traditional object detection methods that rely on manually designed feature extractors with slow algorithm speeds and low accuracy. The algorithm has a wide range of applications, finds detection targets through bounding boxes, and achieves high accuracy with less training resources. With the rapid development of deep learning technology, several excellent vehicle target detection algorithms have emerged, such as YOLOv5, YOLOv4, and YOLOv3 [<xref ref-type="bibr" rid="ref_9">9</xref>]. Among them, YOLOv5 is particularly popular for its easy setup, fast training speed, and user-friendly nature, making it one of the most widely applied detection networks today. For instance, Zhang et al. [<xref ref-type="bibr" rid="ref_3">3</xref>] studied a vehicle millimeter-wave radar target detection method based on deep learning, utilizing the YOLOv5 algorithm to better address the issues found in traditional Constant False Alarm Rate (CFAR) algorithms. Liu [<xref ref-type="bibr" rid="ref_10">10</xref>] replaced the backbone network in YOLOv5 with the basic structure of the MobileNet V3 network to compress the model and speed up detection. Li et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] enhanced the network's feature extraction capability by adding a Convolutional Block Attention Module (CBAM) to the backbone network of YOLOv5 and replacing the neck part of the network with a bi-directional feature pyramid network (BiFPN) structure to improve the algorithm's ability to detect small targets.</p><p>The problem of traffic vehicle target detection presents the following difficulties:</p><p>â¢ Due to the camera angle, the size of the targets in the video images varies, easily missing smaller-sized targets.</p><p>â¢ There are mutual occlusion problems, especially in traffic jam situations, where the occlusion problem is more severe.</p><p>â¢ High-speed moving vehicles may appear blurred and have afterimages in high-definition video surveillance, affecting recognition accuracy [<xref ref-type="bibr" rid="ref_12">12</xref>].</p><p>These factors lead to missed or false detections. The main means to improve algorithm speed and accuracy are to combine different application scenarios to select suitable YOLOv5 model weights, optimize parameter settings and replace the backbone network based on the characteristics of the targets to be detected. By taking into account factors, such as the volume of the target detection model, detection accuracy, and detection speed under high-definition video surveillance, this study selects the YOLOv5s as the basis for related research and improvement work, with the main contributions as follows:</p><p>â¢ Swin Transformer V2 [<xref ref-type="bibr" rid="ref_13">13</xref>] is used to replace part of the original network structure, which optimizes network structure layers to improve the global searching ability of the target detection algorithm, thereby facilitating the early discovery of more targets under traffic road conditions.</p><p>â¢ The SimAM [<xref ref-type="bibr" rid="ref_14">14</xref>] is introduced, enhancing the local discriminative feature extraction ability of the CNN network branch.</p><p>â¢ More common improvement measures are combined, such as introducing a small target detection layer and modifying and replacing the loss function, etc. Target detection and recognition situations are compared to select the optimal improvement measures.</p><p>â¢ The improved YOLOv5s algorithm is validated under actual road detection conditions.</p><p>In summary, this study, based on the YOLOv5s algorithm, ultimately selects the improved algorithm combining Swin Transformer V2 and SimAM, improving the target detection capability and the accuracy of target type recognition.</p>
    </sec>
    <sec sec-type="">
      <title>2. Related work</title>
      
        <sec>
          
            <title>2.1. Replacing part of the network structure with swin transformer v2</title>
          
          <p>The Swin Transformer was introduced in 2021 [<xref ref-type="bibr" rid="ref_15">15</xref>] as a backbone network for computer vision. Its name derives from the concept of a hierarchical vision Transformer using shifted windows. This design utilizes moving windows for each attention mechanism computation within the windows themselves, allowing the computational complexity to grow linearly with the image size when processing higher-resolution data rather than quadratically. Furthermore, the use of moving windows not only brings about greater detection efficiency but also facilitates interaction between adjacent windows. Its design incorporates patch merging, which merges adjacent patches into one for feature value computation, similar to the pooling layer operations covered by the CNN in the original YOLOv5 backbone network. This enlarges the network's receptive field and enhances its multi-scale perception ability. Following this, Swin Transformer V2 [<xref ref-type="bibr" rid="ref_13">13</xref>], with 3 billion parameters, has become the largest dense vision model to date.</p><p>The structure of the Swin Transformer is shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>. Swin Transformer V2 modifies the order of layer norm in each transformer encoder block to facilitate the normalization of outputs. It also replaces inner product similarity with cosine similarity. This compensates for the lack of long-distance modeling YOLOv5s's CNN and its inability to effectively capture global information, thereby achieving better target feature extraction effects. As shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>, the STv2 module, or Swin Transformer V2, replaces some of the convolutional blocks in the original YOLOv5s.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>Structure of the Swin Transformer V2</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_EZovqRcyvDllhN-k.png"/>
            </fig>
          
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Module replaced with Swin Transformer V2</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_gYypBJva21dCN37a.png"/>
            </fig>
          
          <p>This study incorporates several key modules associated with Swin Transformer V2 into the training of YOLOv5s, leveraging the advanced features of Swin Transformer V2. By integrating the Swin Transformer block, window attention mechanism, and Multilayer Perceptron (MLP) module, the model's ability to capture both global and local information when processing high-resolution images is enhanced. The integration of these modules boosts the network's self-attention computation, enabling it to capture distant pixel relationships more accurately while maintaining computational efficiency, especially in densely populated vehicle scenarios. Additionally, introducing the C3STR module into YOLOv5s, with its cross-stage and cross-shape triple receptive field, further optimizes the model's detection capability for vehicles of varying sizes and under occlusion conditions. These improvements enable the model to achieve higher accuracy in identifying and classifying various vehicle targets in complex traffic environments.</p>
        </sec>
      
      
        <sec>
          
            <title>2.2. Simam</title>
          
          <p>SimAM [<xref ref-type="bibr" rid="ref_14">14</xref>] is a new type of attention module in CNNs, characterized by its simplicity and efficiency. Unlike Bottleneck Attention Module (BAM) and CBAM, which focus on channel and spatial attention in parallel and serial manners, respectively [<xref ref-type="bibr" rid="ref_16">16</xref>], SimAM infers 3D attention weights (full 3D weights) for a layer's feature map without adding any parameters to the original network, effectively coordinating channel and spatial attention. SimAM can better process both types of attention and significantly improve the model's detection performance in complex traffic scenes. The energy function of SimAM is shown in Eq. (1):</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="m131v1w8o8">
                <mml:msub>
                  <mml:mi>e</mml:mi>
                  <mml:mi>i</mml:mi>
                </mml:msub>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>w</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>b</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                  <mml:mi>y</mml:mi>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:msup>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>â</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msub>
                      <mml:mi>y</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                    <mml:mrow>
                      <mml:mover>
                        <mml:mi>t</mml:mi>
                        <mml:mo>^</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mn>2</mml:mn>
                </mml:msup>
                <mml:msup>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>â</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msub>
                      <mml:mi>y</mml:mi>
                      <mml:mn>0</mml:mn>
                    </mml:msub>
                    <mml:mrow>
                      <mml:mover>
                        <mml:mi>x</mml:mi>
                        <mml:mo>^</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                  </mml:mrow>
                  <mml:mn>2</mml:mn>
                </mml:msup>
                <mml:mfrac>
                  <mml:mn>1</mml:mn>
                  <mml:mrow>
                    <mml:mi>M</mml:mi>
                    <mml:mo>â</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:mfrac>
                <mml:munderover>
                  <mml:mo>â</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>M</mml:mi>
                    <mml:mo>â</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:munderover>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="my346fgimf">
    <mml:msub>
      <mml:mi>w</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mj3by6jiw0">
    <mml:msub>
      <mml:mi>b</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> represent the weight and bias, respectively, <inline-formula>
  <mml:math id="ml3n5rq2jr">
    <mml:mrow>
      <mml:mover>
        <mml:mi>t</mml:mi>
        <mml:mo>^</mml:mo>
      </mml:mover>
    </mml:mrow>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mm6yvs9h3g">
    <mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> belong to the target neuron and other neurons in the channel, $M<inline-formula>
  <mml:math id="mipbmq6bmo">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>i<inline-formula>
  <mml:math id="me4pn3h5l8">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>t$ and its surrounding neurons, the higher its importance, indicating lower energy. As shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>, SimAM is introduced into the backbone network of YOLOv5s.</p><p>SimAM includes a rapid closed-form solution energy function that can be implemented in less than ten lines of code. One of the advantages of SimAM is that most of the operations are based on solutions to the defined energy function, which avoids extensive structural adjustments. The module proposed by SimAM is flexible and effective, enhancing the feature extraction capability of many CNNs. It is an efficient attention module that is easy to implement and can significantly improve performance across various visual tasks. The SimAM is incorporated into the training of YOLOv5 by modifying the detection head of the backbone network in the yaml configuration file and adding the SimAM at appropriate layers. This enhances the model's performance in identifying individual vehicles while improving the accuracy and reliability of recognizing occluded or distant small-sized vehicles.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Introduction of SimAM</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_OEPQ1gmryK1ObpN5.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>2.3. Small object detection layer</title>
          
          <p>The demand for faster and more accurate detectors for small objects in target detection tasks is increasing. Although the human eye can almost instantly capture contextual information even from a distance, due to limitations in image resolution and computing resources, machine detection of small objects that occupy only a few pixels in an image remains a challenging task [<xref ref-type="bibr" rid="ref_17">17</xref>]. Pretraining results show that the target detection algorithm only recognizes targets when vehicles are relatively close, and this phenomenon is more intuitive in the case of unclear input images. In the COCO dataset used in this study, objects with a pixel count less than 32Ã32 are considered small objects. To enhance the model's accuracy for small-sized targets, a detection head specifically for small targets is added to the original network of YOLOv5s. Based on CNN, the small object detection layer retains the original detection structure and expands with an additional convolutional layer to obtain a 160Ã160 feature layer. This approach increases the computational load to some extent but provides higher detection accuracy. The network structure improved by this method is shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Small object detection head</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_1S2yF_kD-emAqOx8.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>2.4. Eiou loss function</title>
          
          <p>In YOLOv5s, the head (head layer) uses CIoU, which is a bounding box loss function, to measure the consistency of the aspect ratio between the prediction and the target box, as shown in Eq. (2):</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="ma5tfnp3dc">
                <mml:mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd/>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mi>C</mml:mi>
                          <mml:mi>I</mml:mi>
                          <mml:mi>o</mml:mi>
                          <mml:mi>U</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mo>â</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mi>I</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>U</mml:mi>
                      <mml:mi>a</mml:mi>
                      <mml:mi>v</mml:mi>
                      <mml:msup>
                        <mml:mi>Ï</mml:mi>
                        <mml:mn>2</mml:mn>
                      </mml:msup>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>b</mml:mi>
                        <mml:msup>
                          <mml:mi>b</mml:mi>
                          <mml:mrow>
                            <mml:mi>g</mml:mi>
                            <mml:mi>t</mml:mi>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd/>
                    <mml:mtd>
                      <mml:mi>a</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mi>v</mml:mi>
                        <mml:mrow>
                          <mml:mn>1</mml:mn>
                          <mml:mo>â</mml:mo>
                          <mml:mo>+</mml:mo>
                          <mml:mi>I</mml:mi>
                          <mml:mi>o</mml:mi>
                          <mml:mi>U</mml:mi>
                          <mml:mi>v</mml:mi>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd/>
                    <mml:mtd>
                      <mml:mi>v</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mn>2</mml:mn>
                        <mml:mi>Ï</mml:mi>
                      </mml:mfrac>
                      <mml:msup>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>â¡</mml:mo>
                          <mml:mo>â</mml:mo>
                          <mml:mo>â¡</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mi>arctan</mml:mi>
                          <mml:mi>arctan</mml:mi>
                          <mml:mfrac>
                            <mml:msup>
                              <mml:mi>w</mml:mi>
                              <mml:mrow>
                                <mml:mi>h</mml:mi>
                                <mml:mi>t</mml:mi>
                              </mml:mrow>
                            </mml:msup>
                            <mml:msup>
                              <mml:mi>h</mml:mi>
                              <mml:mrow>
                                <mml:mi>g</mml:mi>
                                <mml:mi>t</mml:mi>
                              </mml:mrow>
                            </mml:msup>
                          </mml:mfrac>
                          <mml:mfrac>
                            <mml:mi>w</mml:mi>
                            <mml:mi>h</mml:mi>
                          </mml:mfrac>
                        </mml:mrow>
                        <mml:mn>2</mml:mn>
                      </mml:msup>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mhon62azxu">
    <mml:msup>
      <mml:mi>Ï</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msup>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mi>b</mml:mi>
      <mml:msup>
        <mml:mi>b</mml:mi>
        <mml:mrow>
          <mml:mi>g</mml:mi>
          <mml:mi>t</mml:mi>
        </mml:mrow>
      </mml:msup>
    </mml:mrow>
  </mml:math>
</inline-formula> represents the Euclidean distance between the center points of the prediction and target boxes, <inline-formula>
  <mml:math id="mf96uu5i5t">
    <mml:msup>
      <mml:mi>w</mml:mi>
      <mml:mrow>
        <mml:mi>h</mml:mi>
        <mml:mi>t</mml:mi>
      </mml:mrow>
    </mml:msup>
    <mml:msup>
      <mml:mi>h</mml:mi>
      <mml:mrow>
        <mml:mi>g</mml:mi>
        <mml:mi>t</mml:mi>
      </mml:mrow>
    </mml:msup>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>w</mml:mi>
  </mml:math>
</inline-formula>, and $h$ are the width and height of the prediction and target boxes, respectively. The formula reflects the difference in aspect ratio rather than the actual difference in width, height, and their confidence levels, which could hinder the model's effective optimization of similarity. Addressing this issue, EIoU is proposed on the basis of CIoU by separating the aspect ratio into width and height, as shown in Eq. (3):</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="mg49z322kj">
                <mml:mtable displaystyle="true" columnspacing="1em" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mi>E</mml:mi>
                          <mml:mi>I</mml:mi>
                          <mml:mi>O</mml:mi>
                          <mml:mi>U</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mi>E</mml:mi>
                          <mml:mi>I</mml:mi>
                          <mml:mi>O</mml:mi>
                          <mml:mi>U</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mi>D</mml:mi>
                          <mml:mi>I</mml:mi>
                          <mml:mi>S</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mi>A</mml:mi>
                          <mml:mi>S</mml:mi>
                          <mml:mi>P</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>+</mml:mo>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mo>=</mml:mo>
                      <mml:mo>â</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mn>1</mml:mn>
                      <mml:mi>I</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>U</mml:mi>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:msup>
                            <mml:mi>Ï</mml:mi>
                            <mml:mn>2</mml:mn>
                          </mml:msup>
                          <mml:mrow>
                            <mml:mo>(</mml:mo>
                            <mml:mo>,</mml:mo>
                            <mml:mo>)</mml:mo>
                            <mml:mi>v</mml:mi>
                            <mml:msup>
                              <mml:mi>v</mml:mi>
                              <mml:mrow>
                                <mml:mi>g</mml:mi>
                                <mml:mi>t</mml:mi>
                              </mml:mrow>
                            </mml:msup>
                          </mml:mrow>
                        </mml:mrow>
                        <mml:msup>
                          <mml:mi>c</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msup>
                      </mml:mfrac>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:msup>
                            <mml:mi>Ï</mml:mi>
                            <mml:mn>2</mml:mn>
                          </mml:msup>
                          <mml:mo>+</mml:mo>
                          <mml:mrow>
                            <mml:mo>(</mml:mo>
                            <mml:mo>,</mml:mo>
                            <mml:mo>)</mml:mo>
                            <mml:mi>w</mml:mi>
                            <mml:msup>
                              <mml:mi>w</mml:mi>
                              <mml:mrow>
                                <mml:mi>g</mml:mi>
                                <mml:mi>t</mml:mi>
                              </mml:mrow>
                            </mml:msup>
                          </mml:mrow>
                        </mml:mrow>
                        <mml:msubsup>
                          <mml:mi>c</mml:mi>
                          <mml:mi>z</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                      </mml:mfrac>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:msup>
                            <mml:mi>Ï</mml:mi>
                            <mml:mn>2</mml:mn>
                          </mml:msup>
                          <mml:mo>+</mml:mo>
                          <mml:mrow>
                            <mml:mo>(</mml:mo>
                            <mml:mo>,</mml:mo>
                            <mml:mo>)</mml:mo>
                            <mml:mi>m</mml:mi>
                            <mml:msup>
                              <mml:mi>m</mml:mi>
                              <mml:mrow>
                                <mml:mi>g</mml:mi>
                                <mml:mi>t</mml:mi>
                              </mml:mrow>
                            </mml:msup>
                          </mml:mrow>
                        </mml:mrow>
                        <mml:msubsup>
                          <mml:mi>c</mml:mi>
                          <mml:mi>m</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mhaaiasonp">
    <mml:msubsup>
      <mml:mi>c</mml:mi>
      <mml:mi>z</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msubsup>
    <mml:msubsup>
      <mml:mi>c</mml:mi>
      <mml:mi>m</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msubsup>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="m9kmgltpui">
    <mml:mi>Ï</mml:mi>
  </mml:math>
</inline-formula> are the height and width of the two smallest boxes, and $v<inline-formula>
  <mml:math id="m76gl0d744">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>v^{g t}$ are the Euclidean distances between them. In other words, the original âCIoU = IoU + center point loss + aspect ratio lossâ is changed to âEIoU = IoU + center point loss + width loss + height lossâ. During the experiments, an attempt was made to replace CIoU with EIoU.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Yolov5 target detection algorithm</title>
      <p>The YOLO algorithm is a one-stage target detection algorithm, meaning it uses a single CNN model to achieve end-to-end target detection. The input image of a one-stage algorithm passes through a single network, and the generated results include both detection locations and category information. YOLO utilizes convolutional networks to extract features and fully connected layers to obtain prediction values, dividing the input image into grids, with each grid detecting targets, predicting bounding boxes and their confidence scores. Finally, non-maximum suppression is used for network prediction [<xref ref-type="bibr" rid="ref_18">18</xref>]. The most widely used YOLO algorithm is YOLOv5 [<xref ref-type="bibr" rid="ref_19">19</xref>], which has reached a high level of detection accuracy on general object detection datasets, such as MS COCO [<xref ref-type="bibr" rid="ref_20">20</xref>] and VOC [<xref ref-type="bibr" rid="ref_21">21</xref>] to date. However, vehicle-type target detection under complex traffic flow conditions requires targeted design and training. Through corresponding optimization, a detection model more suited to the target detection task is ultimately obtained.</p><p>YOLOv5, in addition to the basic model, has four different sizes of weight models for different layers of the neural network feature extraction layer, namely, YOLOv5s, YOLOv5m, YOLOv5l, and YOLOv5x. Although many studies suggest enhancing the representation ability of the target detection model in terms of depth and width to increase detection accuracy, this affects the complexity of the model. Since detection tasks require real-time performance, such models are not suitable for autonomous driving systems [<xref ref-type="bibr" rid="ref_22">22</xref>]. As the smallest model, YOLOv5s has the fastest computational speed. According to the given data, it can achieve a computational inference speed of 2 ms per image on the COCO2017 dataset while having the fewest model parameters and the fewest floating-point operations (FLOPs). Although this affects model performance, mAP@0.5 still reaches 56.8%. The YOLOv5s model is highly performant and convenient for deployment, consuming fewer resources [<xref ref-type="bibr" rid="ref_23">23</xref>]. The YOLOv5s algorithm network structure is divided into the head, neck, and backbone networks. The backbone network, consisting of Focus, Center and Scale Prediction (CSP), CBL, and SPP modules, is used for extracting key features from the input image. The SPP module and Path Aggregation Network (PA-NET) are used to integrate feature output from the backbone to the head network. The head network is responsible for the final detection steps, constructing the bounding box positions and recognition types into a neural network to form the final output vector [<xref ref-type="bibr" rid="ref_24">24</xref>], [<xref ref-type="bibr" rid="ref_25">25</xref>]. The network structure of YOLOv5s is shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>.</p><p>In the above figure, Conv represents the convolutional layer, BN stands for batch normalization layer, SiLU is the activation function, MaxPool is the max pooling layer, Concat refers to the concatenation layer, and Resunit is the residual unit [<xref ref-type="bibr" rid="ref_26">26</xref>]. During the prediction process, YOLOv5 takes into account the distance information of the center points of the bounding frames, using the CIoU loss function [<xref ref-type="bibr" rid="ref_27">27</xref>] to calculate the ratio of the intersection and the union of two bounding boxes.</p><p>Beyond the basic model, YOLOv5 provides four different sizes of pretrained weight models for different layers of the neural network feature extraction layer, namely, YOLOv5s, YOLOv5m, YOLOv5l, and YOLOv5x. <xref ref-type="table" rid="table_1">Table 1</xref> compares the performance of YOLO models.</p>
      
        <fig id="fig_5">
          <label>Figure 5</label>
          <caption>
            <title>YOLOv5 network structure</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_yI0ZJy7YsZqZ3j9M.png"/>
        </fig>
      
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>YOLO pre-trained model</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="1" rowspan="1"><p>Size (pixels)</p></td><td colspan="1" rowspan="1"><p>mAPval 50-95</p></td><td colspan="1" rowspan="1"><p>mAPval 50</p></td><td colspan="1" rowspan="1"><p>Speed</p><p>CPU b1 (ms)</p></td><td colspan="1" rowspan="1"><p>Speed</p><p>V100 b1 (ms)</p></td><td colspan="1" rowspan="1"><p>Speed</p><p>V100 b32 (ms)</p></td><td colspan="1" rowspan="1"><p>Params (M)</p></td><td colspan="1" rowspan="1"><p>FLOPs</p><p>@640 (B)</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv5n</p></td><td colspan="1" rowspan="1"><p>640</p></td><td colspan="1" rowspan="1"><p>28.0</p></td><td colspan="1" rowspan="1"><p>45.7</p></td><td colspan="1" rowspan="1"><p>45</p></td><td colspan="1" rowspan="1"><p>6.3</p></td><td colspan="1" rowspan="1"><p>0.6</p></td><td colspan="1" rowspan="1"><p>1.9</p></td><td colspan="1" rowspan="1"><p>4.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv5s</p></td><td colspan="1" rowspan="1"><p>640</p></td><td colspan="1" rowspan="1"><p>37.4</p></td><td colspan="1" rowspan="1"><p>56.8</p></td><td colspan="1" rowspan="1"><p>98</p></td><td colspan="1" rowspan="1"><p>6.4</p></td><td colspan="1" rowspan="1"><p>0.9</p></td><td colspan="1" rowspan="1"><p>7.2</p></td><td colspan="1" rowspan="1"><p>16.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv5m</p></td><td colspan="1" rowspan="1"><p>640</p></td><td colspan="1" rowspan="1"><p>45.4</p></td><td colspan="1" rowspan="1"><p>64.1</p></td><td colspan="1" rowspan="1"><p>224</p></td><td colspan="1" rowspan="1"><p>8.2</p></td><td colspan="1" rowspan="1"><p>1.7</p></td><td colspan="1" rowspan="1"><p>21.2</p></td><td colspan="1" rowspan="1"><p>49.0</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv5I</p></td><td colspan="1" rowspan="1"><p>640</p></td><td colspan="1" rowspan="1"><p>49.0</p></td><td colspan="1" rowspan="1"><p>67.3</p></td><td colspan="1" rowspan="1"><p>430</p></td><td colspan="1" rowspan="1"><p>10.1</p></td><td colspan="1" rowspan="1"><p>2.7</p></td><td colspan="1" rowspan="1"><p>46.5</p></td><td colspan="1" rowspan="1"><p>109.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>YOLOv5x</p></td><td colspan="1" rowspan="1"><p>640</p></td><td colspan="1" rowspan="1"><p>50.7</p></td><td colspan="1" rowspan="1"><p>68.9</p></td><td colspan="1" rowspan="1"><p>766</p></td><td colspan="1" rowspan="1"><p>12.1</p></td><td colspan="1" rowspan="1"><p>4.8</p></td><td colspan="1" rowspan="1"><p>86.7</p></td><td colspan="1" rowspan="1"><p>205.7</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>Among them, the YOLOv5s model performs very well and is convenient for deployment, consuming fewer resources. Taking everything into consideration, it is believed in this study that YOLOv5s is the best choice for the application environment of traffic flow volume statistics. The overall detection process of YOLOv5s can be summarized as follows: First, the image is scaled and padded to meet the input requirements of the YOLOv5 network. Then, the input image is processed with the YOLOv5 algorithm to obtain a series of bounding box location information and type prediction results. Next, the non-maximum suppression module is used to process the output bounding boxes, eliminating redundant duplicate detection results. Finally, the remaining bounding boxes are output as the result of vehicle detection.</p><p>To verify the effectiveness of the YOLOv5s model in complex traffic environments, an analysis predicated on polarity pretraining of the YOLOv5s model was conducted. The initial phase of training culminated in a duration of 1232 seconds, accompanied by an F1-score of 0.84. The pretraining results are shown in <xref ref-type="fig" rid="fig_6">Figure 6</xref>.</p>
      
        <fig id="fig_6">
          <label>Figure 6</label>
          <caption>
            <title>Pre-trained results</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_EG4mJMGtJXwkwD9E.png"/>
        </fig>
      
      <p>In the above figure, <italic>Box</italic> indicates the mean of the GIoU loss function. The smaller the box, the more accurate it is. <italic>Objectness</italic> is the mean of the object detection loss. The smaller, the more accurate the object detection. <italic>Classification</italic> is the mean of the classification loss. The smaller, the more accurate the classification. It can be concluded that during the training process, the means of each metric decrease steadily, and the convergence speed is ideal. Notably, the validation set exhibits significant fluctuations in the classification results, which are speculated to be caused by the small number of images in the validation set and the large difference in the number of different categories of vehicles. Subsequent expansion of the validation set and rationalization of the number of vehicle categories were conducted for repeated pretraining, and the validation results were generally satisfactory. A partial view of the validation conditions is shown in <xref ref-type="fig" rid="fig_7">Figure 7</xref>.</p>
      
        <fig id="fig_7">
          <label>Figure 7</label>
          <caption>
            <title>Pre-trained validation results</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_2fjfOxPrKjvOFB72.png"/>
        </fig>
      
    </sec>
    <sec sec-type="">
      <title>4. Experiments</title>
      
        <sec>
          
            <title>4.1. Experimental environment and traffic flow dataset</title>
          
          <p>The experimental environment includes the Windows 11 operating system, an Intel-Core i7-8700K CPU, an NVIDIA GeForce RTX 2080 GPU, 16GB of RAM, and Pycharm IDE with Python version 3.7.11.</p><p>Traffic flow in China is characterized by high density and complex information. This study selects three of the most representative types of large motor vehicles on roads, namely, cars, buses, and trucks, to construct the initial dataset. The format of the dataset is based on the COCO dataset. The images in the dataset mainly come from VOC2007 and COCO2017. Since the shooting locations of these two official datasets are mostly in Europe and America, photos of Chinese traffic flows were added and annotated, aiming to adapt the algorithm to the application environment of this study while retaining the diversity of the original COCO dataset. This expands the coverage of the dataset and enhances its practicality in local traffic scenarios, providing support for algorithm training and optimization [<xref ref-type="bibr" rid="ref_28">28</xref>].</p>
        </sec>
      
      
        <sec>
          
            <title>4.2. Experimental results and analysis</title>
          
          <p>After implementing the aforementioned four improvements to the YOLOv5s algorithm, training was conducted on the same traffic flow dataset for each modification, with each change trained 10 times, and the average value of the experiment's metrics was taken. <xref ref-type="table" rid="table_2">Table 2</xref> shows the training results.</p><p>Preliminary analysis of the experimental results showed that, after making modifications with Swin Transformer V2 to replace part of the neural network and the SimAM, the overall average accuracy improved by about 5%, and it could even enhance the recognition capability by up to 9% under ideal conditions. The improvement in detection capability with the addition of the small object detection layer to YOLOv5 was not outstanding, with a quite limited enhancement in detection performance. <xref ref-type="fig" rid="fig_8">Figure 8</xref>, <xref ref-type="fig" rid="fig_9">Figure 9</xref> and <xref ref-type="fig" rid="fig_10">Figure 10</xref> show part of the testing process for different models.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Comparison of experimental results</title>
              </caption>
              <table><tr><td >Model</td><td >F1-Score</td><td >mAP@0.5</td><td >mAP@0.5:0.95</td><td >Best mAP@0.5</td></tr><tr><td >YOLOv5s</td><td >0.79</td><td >0.791</td><td >0.608</td><td >0.794</td></tr><tr><td >YOLOv5s+Swin Transformer V2</td><td >0.86</td><td >0.854</td><td >0.685</td><td >0.882</td></tr><tr><td >YOLOv5s+small object detection layer</td><td >0.78</td><td >0.790</td><td >0.633</td><td >0.802</td></tr><tr><td >YOLOv5s+SimAM</td><td >0.83</td><td >0.838</td><td >0.680</td><td >0.859</td></tr><tr><td >YOLOv5s+EIoU</td><td >0.78</td><td >0.785</td><td >0.645</td><td >0.793</td></tr></table>
            </table-wrap>
          
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>Model test A</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_YIMo3TBugSQYlBii.png"/>
            </fig>
          
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>
                <title>Model test B</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_JNPYNQDuc9OvH6lW.png"/>
            </fig>
          
          
            <fig id="fig_10">
              <label>Figure 10</label>
              <caption>
                <title>Model test C</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_2n279Uqs9XCU8g_c.png"/>
            </fig>
          
          <p>The study considers optimizing the small object detection capability of YOLOv5 by combining the small object detection layer with different backbone neural networks. Similarly, the replacement of the original CIoU with EIoU on the traffic flow dataset had a limited improvement effect and even impacted the detection performance, leading to the preliminary conjecture that EIoU might not be entirely suitable for the dataset studied in this study.</p><p>To better understand the impact of various improvement modules in YOLOv5 on training and recognition effects, especially for video detection environments, various improvement modules were combined and experimented with step by step for comparison. <xref ref-type="table" rid="table_3">Table 3</xref> shows the comparison results.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Comparison of experimental results</title>
              </caption>
              <table><tr><td >Model</td><td >FPS</td><td >F1-Score</td><td >mAP@0.5</td><td >mAP@0.5:0.95</td></tr><tr><td >Swin Transformer V2+small object detection layer</td><td >55.8</td><td >0.82</td><td >0.811</td><td >0.653</td></tr><tr><td >SimAM+small object detection layer</td><td >64.2</td><td >0.79</td><td >0.819</td><td >0.638</td></tr><tr><td >Swin Transformer V2+ SimAM+small object detection layer</td><td >57.4</td><td ><mml:math id="m6r8ymaqvz">
  <mml:mtext>0.83</mml:mtext>
</mml:math></td><td >0.816</td><td >0.621</td></tr><tr><td >Swin Transformer V2+SimAM</td><td ><mml:math id="mmclqfoevp">
  <mml:mtext>64.7</mml:mtext>
</mml:math></td><td >0.82</td><td ><mml:math id="mk4cq18pl7">
  <mml:mtext>0.827</mml:mtext>
</mml:math></td><td ><mml:math id="mugzq28hb1">
  <mml:mtext>0.657</mml:mtext>
</mml:math></td></tr><tr><td >Swin Transformer V2+EIoU</td><td >58.9</td><td >0.78</td><td >0.795</td><td >0.616</td></tr><tr><td >SimAM+EIoU</td><td >60.3</td><td >0.76</td><td >0.769</td><td >0.601</td></tr></table>
            </table-wrap>
          
          <p>YOLOv5s, when combined with Swin Transformer V2+SimAM or SimAM with the small object detection layer, could produce quite good detection effects. In addition, the detection frame rate under high-definition video was ideal, basically meeting the requirements. After weighing the pros and cons, the YOLOv5s improvement method combining Swin Transformer V2+SimAM was chosen for actual application testing. <xref ref-type="table" rid="table_4">Table 4</xref> shows the comparison of algorithm evaluation metrics before and after improvement.</p>
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Comparison of algorithm evaluation metrics before and after improvement</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="1" rowspan="1"><p>Precision (%)</p></td><td colspan="1" rowspan="1"><p>Recall (%)</p></td><td colspan="1" rowspan="1"><p>Positive Detection Rate (%)</p></td><td colspan="1" rowspan="1"><p>Miss Rate (%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Initial YOLOv5s algorithm</p></td><td colspan="1" rowspan="1"><p>84.72</p></td><td colspan="1" rowspan="1"><p>74.21</p></td><td colspan="1" rowspan="1"><p>82.91</p></td><td colspan="1" rowspan="1"><p>13.53</p></td></tr><tr><td colspan="1" rowspan="1"><p>Improved YOLOv5s algorithm</p></td><td colspan="1" rowspan="1"><p>86.19</p></td><td colspan="1" rowspan="1"><p>77.56</p></td><td colspan="1" rowspan="1"><p>87.39</p></td><td colspan="1" rowspan="1"><p>9.42</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>From <xref ref-type="table" rid="table_4">Table 4</xref>, it can be seen that compared to the initial YOLOv5s algorithm, the improved algorithm's positive detection rate increases by 4.48%, and the miss rate decreases by 4.11%, indicating an enhancement over the initial algorithm and stronger recognition capability of the improved YOLOv5s algorithm for traffic flow detection. <xref ref-type="fig" rid="fig_11">Figure 11</xref> shows some of the test recognition situations.</p>
          
            <fig id="fig_11">
              <label>Figure 11</label>
              <caption>
                <title>Partial recognition situation</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/3/img_MD4ri-EJebAipHvb.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>4.3. Applying and analyzing video stream detection using different improved models</title>
          
          <p>Based on the results of the ablation study in the previous chapter, combined with DeepSort, enhanced with Swin Transformer V2+SimAM, and modifications for small target detection layers, YOLOv5 was applied to the task of traffic flow counting. The application videos were taken in daylight with normal lighting, featuring a large number of small passenger cars and a few buses. There were instances where the model incorrectly categorized vehicle types. This study addresses these errors by marking incorrect data in the corresponding positions of multiple statistical categories as "statistical result-error quantity." When considering the accuracy of the statistics, the quantity of the corresponding category is also the result of subtraction. <xref ref-type="table" rid="table_5">Table 5</xref> shows the application results.</p>
          
            <table-wrap id="table_5">
              <label>Table 5</label>
              <caption>
                <title>Comparison of application scenario results</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Model</p></td><td colspan="1" rowspan="1"><p>Number of Cars</p></td><td colspan="1" rowspan="1"><p>Number of Buses</p></td><td colspan="1" rowspan="1"><p>Statistical Accuracy</p></td></tr><tr><td colspan="1" rowspan="1"><p>Original video</p></td><td colspan="1" rowspan="1"><p>26</p></td><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p>â</p></td></tr><tr><td colspan="1" rowspan="1"><p>Initial YOLOv5</p></td><td colspan="1" rowspan="1"><p>23-1</p></td><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>79.31%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Swin Transformer V2+SimAM</p></td><td colspan="1" rowspan="1"><p>24-1</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>86.21%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Swin Transformer V2+ SimAM+small target detection layer</p></td><td colspan="1" rowspan="1"><p>25-1</p></td><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>89.66%</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>The statistical results indicate that YOLOv5, with parts of its CNN backbone network replaced by Swin Transformer V2 and combined with SimAM, can significantly enhance the accuracy of target recognition in traffic flow counting application scenarios. Furthermore, adding a small target detection layer on this basis further improves the algorithm's ability to recognize distant traffic flows.</p>
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>Addressing the challenge of detecting numerous small-sized targets with mutual occlusion in traffic flow environments, which often leads to suboptimal detection performance, this study embarks on an exploration of an improved YOLOv5s algorithm. By modifying the network structure of the initial target detection algorithm and comparing several common algorithm modifications, an improved method of YOLOv5s combining Swin Transformer V2 and introducing SimAM was proposed. The experimental results proved that its detection performance was superior to the initial YOLOv5s model. The improved method proposed in this study achieved mAP@0.5 and mAP@0.5:0.95 of 0.827 and 0.657, respectively, with a model volume not significantly different from the initial model. In practical tests, both the positive detection rate and miss rate showed corresponding improvements, offering certain advantages over the initial model and being more suitable for traffic flow detection tasks. In future research, the continuous frame miss rate of the improved algorithm will be further explored, aiming to achieve higher accuracy and fewer omissions while satisfying traffic flow video monitoring requirements.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>113</volume>
          <page-range>109036</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ganapathy</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Ajmera</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compeleceng.2023.109036</pub-id>
          <article-title>An intelligent video surveillance system for detecting the vehicles on road using refined YOLOV4</article-title>
          <source>Comput. Electr. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>2022</volume>
          <page-range>1-9</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2022/2019257</pub-id>
          <article-title>An effective approach of vehicle detection using deep learning</article-title>
          <source>Comput. Intell. Neurosci.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="conf-paper">
          <page-range>175-179</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ccisp59915.2023.10355769</pub-id>
          <article-title>A target detection method of automotive millimeter wave radar based on deep learning</article-title>
          <source>2023 8th International Conference on Communication, Image and Signal Processing (CCISP), Chengdu, China</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>27</volume>
          <page-range>19â24</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.54097/hset.v27i.3716</pub-id>
          <article-title>A review of research on deep learning-based target detection technology for automated vehicle driving systems</article-title>
          <source>Highlights Sci. Eng. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="conf-paper">
          <page-range>412â415</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Jia</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/cvidl58838.2023.10166204</pub-id>
          <article-title>Research on vehicle object detection based on deep learning</article-title>
          <source>2023 4th International Conference on Computer Vision, Image and Deep Learning (CVIDL), Zhuhai, China</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>252-265</page-range>
          <issue>3</issue>
          <year>2007</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ravichandran</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Gandhe</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Mehra</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.inffus.2006.03.001</pub-id>
          <article-title>Robust automatic target recognition using learning classifier systems</article-title>
          <source>Inf. Fusion</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>43</volume>
          <page-range>503â507</page-range>
          <issue>5</issue>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Dong</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Ruan</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>et al.</surname>
            </name>
          </person-group>
          <article-title>Research on traffic video vehicle recognition method combining Haar-like and HOG features</article-title>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="conf-paper">
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Redmon</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Divvala</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Girshick</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Farhadi</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/cvpr.2016.91</pub-id>
          <article-title>You Only Look Once: Unified, real-time object detection</article-title>
          <source>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>347-366</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Pan</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/21642583.2022.2057370</pub-id>
          <article-title>Research on mine vehicle tracking and detection technology based on YOLOv5</article-title>
          <source>Syst. Sci. Control Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>47</volume>
          <page-range>5</page-range>
          <issue>22</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <article-title>Improved road target tracking algorithm based on YOLOv5 and DeepSort</article-title>
          <source>Pract. Automobile Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>31</volume>
          <page-range>242â250</page-range>
          <issue>12</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Tong</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Bao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Ni</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Small object detection based on improved YOLOv5</article-title>
          <source>Comput. Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Doan</surname>
              <given-names>T. N.</given-names>
            </name>
            <name>
              <surname>Truong</surname>
              <given-names>M. T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/kse50997.2020.9287483</pub-id>
          <article-title>Real-time vehicle detection and counting based on YOLO and DeepSORT</article-title>
          <source>2020 12th International Conference on Knowledge and Systems Engineering (KSE), Can Tho, Vietnam</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <page-range>arXiv:2111.09883</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Ning</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Cao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2111.09883</pub-id>
          <article-title>Swin Transformer V2: Scaling up capacity and resolution</article-title>
          <source>arXiv</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="conf-paper">
          <page-range>11863â11874</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>R. Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>SimAM: A simple, parameter-free attention module for convolutional neural networks</article-title>
          <source>Proceedings of the 38th International Conference on Machine Learning, PMLR 139, Virtual</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="conf-paper">
          <page-range>10012â10022</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Cao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/iccv48922.2021.00986</pub-id>
          <article-title>Swin transformer: Hierarchical vision transformer using shifted windows</article-title>
          <source>2021 IEEE/CVF International Conference on Computer Vision (ICCV), Virtual</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>372</volume>
          <page-range>76-81</page-range>
          <issue>6</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.16644/j.cnki.cn33-1094/tp.2023.06.016</pub-id>
          <article-title>Research on safety helmet detection algorithm based on improved YOLOv5</article-title>
          <source>Comput. Era</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <page-range>arXiv:2112.11798</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Benjumea</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Teeti</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Cuzzolin</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Bradley</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2112.11798</pub-id>
          <article-title>YOLO-Z: Improving small object detection in YOLOv5 for autonomous vehicles</article-title>
          <source>arXiv</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>44</volume>
          <page-range>3697-3708</page-range>
          <issue>10</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Chu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Rao</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.11999/JEIT210790</pub-id>
          <article-title>A survey on deep learning-based YOLO object detection</article-title>
          <source>J. Electron. Inf. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>4758</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Malta</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Mendes</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Farinha</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app11114758</pub-id>
          <article-title>Augmented reality maintenance assistant using YOLOv5</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="conf-paper">
          <page-range>740â755</page-range>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lin</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Maire</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Belongie</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Hays</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Perona</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Ramanan</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>DollÃ¡r</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Zitnick</surname>
              <given-names>C. L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-319-10602-1_48</pub-id>
          <article-title>Microsoft COCO: Common objects in context</article-title>
          <source>Proceedings of the European Conference on Computer Vision (ECCV), Zurich, Switzerland</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>88</volume>
          <page-range>303-338</page-range>
          <issue>2</issue>
          <year>2009</year>
          <person-group person-group-type="author">
            <name>
              <surname>Everingham</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Van Gool</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Williams</surname>
              <given-names>C. K. I.</given-names>
            </name>
            <name>
              <surname>Winn</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zisserman</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11263-009-0275-4</pub-id>
          <article-title>The pascal Visual Object Classes (VOC) challenge</article-title>
          <source>Int. J. Comput. Vision</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>168</volume>
          <page-range>115-122</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mahaur</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Mishra</surname>
              <given-names>K.K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.patrec.2023.03.009</pub-id>
          <article-title>Small-object detection based on YOLOv5 in autonomous driving systems</article-title>
          <source>Pattern Recognit. Lett.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>11229</page-range>
          <issue>23</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Park</surname>
              <given-names>S. S.</given-names>
            </name>
            <name>
              <surname>Tran</surname>
              <given-names>V. T.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>D. E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app112311229</pub-id>
          <article-title>Application of various YOLO models for computer vision-based real-time pothole detection</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>51</volume>
          <page-range>357-367</page-range>
          <issue>12</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cai</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Cheng</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Peng</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>et al.</surname>
            </name>
          </person-group>
          <article-title>Research on lightweight infrared weak small vehicle target detection algorithm based on deep learning</article-title>
          <source>Infrared Laser Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Real-time vehicle target detection method of improved YOLOv5s algorithm</article-title>
          <source>J. Harbin Univ. Sci. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1251â1258</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chollet</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/cvpr.2017.195</pub-id>
          <article-title>Xception: Deep Learning with depthwise separable convolutions</article-title>
          <source>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <page-range>arXiv:1911.08287</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zheng</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Ye</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Ren</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.1911.08287</pub-id>
          <article-title>Distance-IoU loss: Faster and better learning for bounding box regression</article-title>
          <source>arXiv</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <volume>46</volume>
          <issue>5</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Durve</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Orsini</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Tiribocchi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Montessori</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Tucny</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lauricella</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Camposeo</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Pisignano</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Succi</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1140/epje/s10189-023-00290-x</pub-id>
          <article-title>Benchmarking YOLOv5 and YOLOv7 models with DeepSORT for droplet tracking applications</article-title>
          <source>Eur. Phys. J. E</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>