<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">MITS</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Mechatronics and Intelligent Transportation Systems</journal-title>
        <abbrev-journal-title abbrev-type="issn">Mechatron. Intell Transp. Syst.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">MITS</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-0218</issn>
      <issn publication-format="print">2958-020X</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-G4SscngWCuo4f5IbZJ5I3oHDKKGQctXt</article-id>
      <article-id pub-id-type="doi">10.56578/mits040202</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Robust Segmentation of Concrete Road Surfaces via Fuzzy Entropy Modelling and Multiscale Laplacian Texture Analysis</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6406-827X</contrib-id>
          <name>
            <surname>Khan</surname>
            <given-names>Muhammad Shahkar</given-names>
          </name>
          <email>shahkar.khan@cecos.edu.pk</email>
        </contrib>
        <aff id="aff_1">Department of Basic Sciences and Humanities, CECOS University of Information Technology and Emerging Sciences, 25000 Peshawar, Pakistan</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>12</day>
        <month>05</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>2</issue>
      <fpage>72</fpage>
      <lpage>80</lpage>
      <page-range>72-80</page-range>
      <history>
        <date date-type="received">
          <day>19</day>
          <month>03</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>24</day>
          <month>04</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Accurate identification of concrete surfaces on roadways is critical for the advancement of autonomous navigation systems and the effective monitoring of transportation infrastructure. Nevertheless, the inherently heterogeneous texture of concrete, in conjunction with environmental variables such as lighting fluctuations and surface degradation, continues to impede precise surface segmentation. To address these challenges, a novel framework has been developed that integrates Fuzzy Topological Entropy (FTE) with Multiscale Laplacian Structural Dissimilarity (MLSD) for the robust delineation of concrete regions in road imagery. Within this framework, FTE is employed to model uncertainty and spatial ambiguity through a continuous fuzzy membership function, thereby capturing the nuanced transitions between concrete and non-concrete domains. Concurrently, MLSD is utilised to quantify multiscale structural irregularities by leveraging Laplacian-based texture dissimilarity, enhancing sensitivity to surface roughness and material inconsistencies. These complementary components are embedded within a unified energy functional, the minimisation of which is conducted via an iterative optimisation strategy that avoids the need for extensive training datasets or prior scene annotations. The proposed methodology demonstrates strong resilience across a variety of environmental conditions, including shadows, glare, occlusions, and physical wear. Superior performance is observed particularly in complex or degraded urban settings, where conventional segmentation models often fail. Owing to its non-parametric nature and computational efficiency, the approach is well-suited for real-time deployment in autonomous vehicle systems, smart city infrastructure, and road condition assessment platforms. By facilitating reliable and scalable surface segmentation without reliance on deep learning architectures or exhaustive manual labelling, this work offers a significant advancement toward generalisable and interpretable road surface analysis technologies.</p></abstract>
      <kwd-group>
        <kwd>Concrete road surface segmentation</kwd>
        <kwd>FTE</kwd>
        <kwd>MLSD</kwd>
        <kwd>Texture and geometry analysis</kwd>
        <kwd>Energy minimisation</kwd>
        <kwd>Autonomous navigation</kwd>
        <kwd>Infrastructure monitoring</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="1"/>
        <fig-count count="3"/>
        <table-count count="1"/>
        <ref-count count="20"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Road safety is a critical aspect of transportation infrastructure, impacting public health, economic efficiency, and the overall quality of life [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>], [<xref ref-type="bibr" rid="ref_5">5</xref>]. With increasing vehicle traffic and urbanization, road accidents have become a major concern, leading to a significant loss of life and property worldwide. Road safety is not only about minimizing accidents but also about improving the management of road systems, ensuring proper maintenance, and enhancing the infrastructure’s capacity to accommodate both vehicular and pedestrian traffic safely. Factors such as road conditions, weather, traffic control measures, and driver behavior contribute to road safety outcomes, with proper road surface maintenance and timely identification of hazards being crucial in preventing accidents. Modern technologies, including sensors, computer vision, and autonomous systems, have introduced innovative solutions for improving road safety, offering real-time monitoring and analysis of road conditions [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>]. Ongoing developments in obstacle detection technologies have played a pivotal role in improving road safety and enabling intelligent transportation systems.</p><p>Perumal et al. [<xref ref-type="bibr" rid="ref_9">9</xref>] proposed LaneScanNET, a deep learning-based framework capable of simultaneously detecting obstacle-lane states, effectively supporting autonomous driving systems in making context-aware navigation decisions. Similarly, Lis et al. [<xref ref-type="bibr" rid="ref_10">10</xref>] introduced an innovative method in which road obstacles are detected by virtually “erasing” them from the scene, enabling the model to focus on contextual cues and restore a clean background for more accurate localization and identification. Khan et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] developed an intelligent outdoor mobility aid tailored for visually impaired individuals, integrating obstacle detection with a responsive scene perception framework to enhance outdoor navigation and safety. In the domain of public transportation, Carletti et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] presented a system for detecting obstacles at railway level crossings using real-time image processing, aiming to prevent accidents and ensure operational safety. Collectively, these approaches highlight the diverse applications of obstacle detection across various transportation environments and demonstrate the growing role of artificial intelligence and computer vision in ensuring road and infrastructure safety. In recent years, several studies have explored advanced computational techniques for concrete surface and road defect detection, with deep learning playing a central role.</p><p>Dong et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] proposed an enhanced YOLOv8 model tailored for detecting concrete surface cracks, achieving significant improvements in speed and accuracy on complex construction site images. Similarly, Birgani et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] presented an extensive evaluation of deep learning applications for analyzing concrete cracks, highlighting the effectiveness of CNNs in various real-world scenarios, especially where manual inspections are difficult or unsafe. Zadeh et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] extended this work with convolutional-based models optimized for detecting micro and macro cracks, demonstrating strong results on benchmark datasets such as SDNET2018 and custom urban concrete image sets. In a broader context, Yu et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] reviewed both image-based and sensor-driven methods for detecting road defects, underlining the shift toward integrating multimodal data for more robust performance. Furthermore, Pham and Nguyen [<xref ref-type="bibr" rid="ref_17">17</xref>] reviewed the performance of RTI IMS software, emphasizing its automation capabilities and integration with mobile mapping platforms for efficient road damage detection. Beskopylny et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] contributed a computer vision approach for classifying the grain shapes of crushed stones, an essential factor in determining the structural integrity of road surfaces.</p><p>Despite their advancements, these approaches face several limitations that restrict their universal deployment. Most deep learning-based systems, such as references [<xref ref-type="bibr" rid="ref_13">13</xref>], [<xref ref-type="bibr" rid="ref_14">14</xref>], [<xref ref-type="bibr" rid="ref_15">15</xref>], require large annotated datasets and extensive computational resources for training and inference, making them less suitable for deployment in low-resource environments or in real-time applications. Additionally, while convolutional models excel in recognizing surface-level cracks, they often struggle under poor lighting, occlusions, or varying environmental conditions without substantial data augmentation or preprocessing. The review by Yu et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] revealed that non-image-based systems can complement visual methods, but these often demand specialized hardware and are sensitive to calibration errors. The RTI IMS software evaluated by Pham and Nguyen [<xref ref-type="bibr" rid="ref_17">17</xref>] demonstrated robust automation but exhibited limitations in classifying complex crack patterns and required frequent updates to maintain accuracy. Similarly, the stone classification model by Beskopylny et al. [<xref ref-type="bibr" rid="ref_18">18</xref>], although promising, remains limited to laboratory settings and requires adaptation for field deployment under dynamic conditions. Overall, these studies underscore the progress made while also emphasizing the need for lightweight, adaptable, and explainable models for widespread adoption in infrastructure monitoring.</p><p>Based on these identified limitations such as sensitivity to lighting conditions, lack of generalization across varying textures, dependency on large labeled datasets, and reduced performance in cluttered environments, we proposed a model that leverages the concepts of fuzzy logic and topological entropy to effectively model the degree of uncertainty present in road images, particularly at transitions between materials (e.g., from asphalt to concrete). On the basis of these identified limitations—such as sensitivity to lighting conditions, lack of generalization across varying textures, dependency on large labeled datasets, and reduced performance in cluttered environments, we proposed a model that leverages the concepts of fuzzy logic and topological entropy to effectively model the degree of uncertainty present in road images, particularly at transitions between materials. This is combined with a multiscale Laplacian structural analysis that enhances structural dissimilarities across different spatial frequencies, allowing the model to capture fine-grained and coarse-grained textural patterns specific to concrete surfaces (see <xref ref-type="fig" rid="fig_1">Figure 1</xref>). The model operates in two primary stages. First, the image is transformed into a fuzzy domain through a sigmoid-based membership function, where each pixel is assigned a probability of belonging to the concrete class. This transformation captures uncertainty and aids in localizing ambiguous or transitional regions. Next, the structural features of the image are analyzed using Laplacian operators at multiple Gaussian-smoothed scales, extracting local dissimilarity that typically characterizes rough concrete patches. By combining the fuzzy entropy and multiscale structural dissimilarity into a single energy functional, the model achieves robust segmentation that highlights regions most likely to be concrete.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Flowchart of the proposed concrete road surface detection model</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_kQCL2exEcW4oIHUg.png"/>
        </fig>
      
      <p>The novel contributions of this work include: (i) the introduction of an FTE measure for uncertainty quantification in surface material segmentation; (ii) the development of an MLSD metric to capture geometric and textural variations indicative of concrete; and (iii) the integration of both into a unified, data-independent energy minimization framework capable of accurately identifying concrete in challenging road environments. This approach is fundamentally different from data-driven deep learning models and provides a mathematically grounded, interpretable, and efficient solution for road surface analysis.</p>
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p>Recent advancements in road surface detection have highlighted the importance of efficient and robust methods for identifying concrete surfaces and their deterioration. Zou et al. [<xref ref-type="bibr" rid="ref_19">19</xref>] present a deep learning approach for pavement classification and recognition using an enhanced VGGNet-16 model combined with transfer learning techniques. The model incorporates modifications such as the Leaky ReLU activation function, residual structures, and dropout layers to improve performance. Trained on a dataset encompassing six pavement types—including dry and wet asphalt, snow and ice, concrete, gravel, and jointed pavements—the model achieved a test accuracy of 96.87%, outperforming other architectures like AlexNet, ResNet50, and InceptionV3. Notably, the model’s reduced weight space (91.2 MB) and faster convergence make it suitable for real-time deployment in vehicle-mounted systems.</p><p>However, the study identifies limitations in recognizing unpaved or muddy roads, especially under inconsistent lighting conditions, leading to reduced accuracy. Additionally, while the model demonstrates high accuracy, it does not achieve 100% recognition, indicating room for improvement. The authors suggest that integrating visual data with vehicle dynamics information could enhance the estimation of road surface adhesion coefficients, thereby improving overall recognition performance.</p><p>Bystrov et al. [<xref ref-type="bibr" rid="ref_20">20</xref>] present a comprehensive study on road Surface Identification, which explores the innovative application of microwave sensor technology for identifying and analyzing road surfaces. The authors propose a method that leverages microwave sensors to detect variations in road surface conditions, allowing for accurate identification of road surface types and conditions in real-time. This technique provides an efficient solution for monitoring road infrastructure, contributing to improved maintenance strategies and road safety. The achievements of this research are notable in its approach to road surface detection using microwave technology, a relatively novel application in this domain. The authors successfully demonstrate how microwave sensors can identify variations in surface textures, including cracks, potholes, and overall wear, offering an effective alternative to traditional visual inspection or more invasive methods.</p><p>However, the study also has several limitations. One of the primary challenges identified by the authors is the potential interference of environmental factors, such as temperature variations and the presence of other electromagnetic signals, which could affect the accuracy and reliability of microwave sensors. Furthermore, the sensitivity of the system to different road materials and surface types remains a concern, as it might not be universally applicable to all road conditions or regions with vastly different infrastructure characteristics.</p>
    </sec>
    <sec sec-type="">
      <title>3. Proposed model</title>
      <p>The accurate identification of concrete surfaces on roads is a critical task in infrastructure monitoring, maintenance planning, and autonomous navigation. Conventional methods often struggle with variations in lighting, shadows, occlusions, and structural similarities between road materials. To address these challenges, we propose a novel hybrid approach that leverages both fuzzy entropy-based texture modeling and multiscale structural dissimilarity analysis. The core idea is to capture the underlying uncertainty and topological variations inherent in concrete surfaces using a new function termed FTE, and to quantify structural inconsistencies across scales using the MLSD descriptor. The synergy between these two mathematical frameworks enables robust segmentation of concrete areas even under noisy, heterogeneous, or partially occluded road conditions. The following subsections detail the formulation of each component and the integrated optimization scheme that forms the basis of the proposed energy-driven segmentation model.</p>
      
        <sec>
          
            <title>3.1. Fte</title>
          
          <p>Let <inline-formula>
  <mml:math id="miss6l3f1r">
    <mml:mi>I</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> represent the grayscale intensity at pixel <inline-formula>
  <mml:math id="m4lxde9qsn">
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> in the image domain. To capture the ambiguity between the presence and absence of concrete, we define a fuzzy membership function <inline-formula>
  <mml:math id="m21nje9i9g">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mi>c</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> that quantifies the degree to which a pixel belongs to the concrete class:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mzu47ypckg">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mi>c</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mfrac>
      <mml:mn>1</mml:mn>
      <mml:mrow>
        <mml:mn>1</mml:mn>
        <mml:mo>+</mml:mo>
        <mml:mo>⁡</mml:mo>
        <mml:mo>(</mml:mo>
        <mml:mo>−</mml:mo>
        <mml:mo>(</mml:mo>
        <mml:mo>(</mml:mo>
        <mml:mo>,</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mo>−</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>exp</mml:mi>
        <mml:mi>γ</mml:mi>
        <mml:mi>I</mml:mi>
        <mml:mi>x</mml:mi>
        <mml:mi>y</mml:mi>
        <mml:mi>T</mml:mi>
      </mml:mrow>
    </mml:mfrac>
  </mml:math>
</inline-formula></p><p>where, $T<inline-formula>
  <mml:math id="mupu6guuwi">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>\gamma<inline-formula>
  <mml:math id="m7qmuax4be">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>\gamma<inline-formula>
  <mml:math id="mtjx4t8gq4">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\gamma<inline-formula>
  <mml:math id="m69w9ygbyc">
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\gamma<inline-formula>
  <mml:math id="m885wgq17v">
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mn>0.08</mml:mn>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>\gamma<inline-formula>
  <mml:math id="mt584upr9l">
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mn>0.05</mml:mn>
    <mml:mn>0.10</mml:mn>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula> F T E=-\sum_{i=1}^N \mu_c\left(x_i, y_i\right) \log \mu_c\left(x_i, y_i\right) <inline-formula>
  <mml:math id="m10q8h63nx">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
  </mml:math>
</inline-formula>N<inline-formula>
  <mml:math id="m9ch3ccgjb">
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>W</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mn>0.5</mml:mn>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>T<inline-formula>
  <mml:math id="m8dpwbwois">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\gamma<inline-formula>
  <mml:math id="m1ymoxlan0">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:msup>
      <mml:mi>l</mml:mi>
      <mml:mo>′</mml:mo>
    </mml:msup>
  </mml:math>
</inline-formula>\gamma<inline-formula>
  <mml:math id="mhcustusr0">
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mn>0.05</mml:mn>
    <mml:mn>0.10</mml:mn>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>T$ ensures better accuracy in varying lighting and surface conditions. Unlike conventional fuzzy entropy applications, our FTE is used not merely as a global measure of uncertainty, but as a spatial cue to detect structurally ambiguous zones-e.g., degraded concrete, boundary transitions, or shadowed regions. This interpretation of fuzzy entropy in a topological spatial context adds a unique layer of analysis that enhances the model's robustness across surface types and lighting conditions. By incorporating FTE into the identification model, we can emphasize and localize regions of high uncertainty, which typically correspond to boundaries, transitions, or degraded areas in road surfaces. This makes the model more robust to varying environmental and material conditions.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Mlsd</title>
          
          <p>To capture structural variations in road textures across multiple scales, we use a Laplacian-Gaussian approach. The grayscale image <inline-formula>
  <mml:math id="mk93m4khyl">
    <mml:mi>I</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is first convolved with Gaussian kernels <inline-formula>
  <mml:math id="myewo65uqf">
    <mml:msub>
      <mml:mi>G</mml:mi>
      <mml:mi>s</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> at multiple scales <inline-formula>
  <mml:math id="mt27s7t0c9">
    <mml:mi>s</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mo>∈</mml:mo>
  </mml:math>
</inline-formula>, followed by the application of the Laplacian operator <inline-formula>
  <mml:math id="mcz2d03upf">
    <mml:mi>Δ</mml:mi>
  </mml:math>
</inline-formula>:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mzz0cu6334">
    <mml:msub>
      <mml:mi>L</mml:mi>
      <mml:mi>s</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>Δ</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>∗</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>G</mml:mi>
        <mml:mi>s</mml:mi>
      </mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mi>x</mml:mi>
      <mml:mi>y</mml:mi>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p>where, $*<inline-formula>
  <mml:math id="m7v188ginc">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>G_s<inline-formula>
  <mml:math id="mxqqz2broy">
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>s<inline-formula>
  <mml:math id="mr6ud815zk">
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>W</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula> M L S D=\sum_{s \in S} \sum_{(x, y)}\left|L_s(x, y)-\bar{L}_s\right| <inline-formula>
  <mml:math id="mzf20jvs2k">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>\bar{L}_S<inline-formula>
  <mml:math id="mem5tdm87m">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
  </mml:math>
</inline-formula>L_s(x, y)$ over all pixels. The MLSD value measures how much the local structure deviates from the global expectation at each scale, capturing local irregularities and material heterogeneity. Rather than using multiscale Laplacians as edge detectors, our approach interprets the scale-wise deviations as indicators of material inhomogeneity. This formulation enables robust discrimination of concrete surfaces, which typically show more structural variance than smoother surfaces like asphalt.</p><p>While the individual components (fuzzy entropy and multiscale Laplacian) are known, our contribution lies in the strategic fusion of FTE and MLSD. This combination enables simultaneous modeling of uncertainty and texture irregularity, yielding a more resilient framework for surface classification. The proposed synergy between fuzzy uncertainty modeling and multiscale structural dissimilarity introduces a novel perspective for road surface analysis with concrete vs. asphalt as a specific target.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Combined energy functional</title>
          
          <p>To leverage both fuzzy uncertainty and structural dissimilarity, we define a combined energy functional:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mbzob5dm1p">
    <mml:mrow>
      <mml:mi data-mjx-variant="-tex-calligraphic">E</mml:mi>
    </mml:mrow>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>⋅</mml:mo>
    <mml:mo>+</mml:mo>
    <mml:mo>⋅</mml:mo>
    <mml:mi>I</mml:mi>
    <mml:mi>α</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>β</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>D</mml:mi>
  </mml:math>
</inline-formula></p><p>where, <inline-formula>
  <mml:math id="mznfxlttvf">
    <mml:mi>α</mml:mi>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mqjyarbm48">
    <mml:mi>β</mml:mi>
  </mml:math>
</inline-formula> are weighting parameters that balance the contributions of the entropybased and structure-based components. These parameters are selected empirically based on a validation set, or they can be optimized through grid search or other hyperparameter tuning techniques to find the optimal balance for concrete surface segmentation. The functional <inline-formula>
  <mml:math id="mb9to0nnb3">
    <mml:mrow>
      <mml:mi data-mjx-variant="-tex-calligraphic">E</mml:mi>
    </mml:mrow>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>I</mml:mi>
  </mml:math>
</inline-formula> allows for the simultaneous consideration of uncertain membership values and multiscale geometric features, leading to more accurate segmentation of concrete regions.</p><p>The energy functional <inline-formula>
  <mml:math id="mn7i6xttnr">
    <mml:mrow>
      <mml:mi data-mjx-variant="-tex-calligraphic">E</mml:mi>
    </mml:mrow>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>I</mml:mi>
  </mml:math>
</inline-formula> is minimized to determine the optimal segmentation of concrete versus non-concrete regions. We adopt a hybrid optimization strategy that incorporates both iterative gradient descent and adaptive thresholding to refine the segmentation process.</p><p><italic>Iterative gradient descent</italic>: In this approach, the gradient of <inline-formula>
  <mml:math id="m0bj28v78c">
    <mml:mrow>
      <mml:mi data-mjx-variant="-tex-calligraphic">E</mml:mi>
    </mml:mrow>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>I</mml:mi>
  </mml:math>
</inline-formula> with respect to the pixel intensity values or segmentation masks is computed. At each iteration, the gradients are used to update the segmentation mask. The update rule follows:</p><p style="text-align: center"><inline-formula>
  <mml:math id="m99sspdoyi">
    <mml:msup>
      <mml:mi>S</mml:mi>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>+</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>t</mml:mi>
        <mml:mn>1</mml:mn>
      </mml:mrow>
    </mml:msup>
    <mml:msup>
      <mml:mi>S</mml:mi>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>t</mml:mi>
      </mml:mrow>
    </mml:msup>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>⋅</mml:mo>
    <mml:mi>η</mml:mi>
    <mml:mi>∇</mml:mi>
    <mml:mrow>
      <mml:mi data-mjx-variant="-tex-calligraphic">E</mml:mi>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msup>
        <mml:mi>S</mml:mi>
        <mml:mrow>
          <mml:mo>(</mml:mo>
          <mml:mo>)</mml:mo>
          <mml:mi>t</mml:mi>
        </mml:mrow>
      </mml:msup>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p>where, <inline-formula>
  <mml:math id="mr2ge3b8cy">
    <mml:msup>
      <mml:mi>S</mml:mi>
      <mml:mrow>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>t</mml:mi>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> represents the segmentation mask at iteration $t<inline-formula>
  <mml:math id="mkpup1r3gv">
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\eta<inline-formula>
  <mml:math id="mhs9u52qqe">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\nabla \mathcal{E}$ <inline-formula>
  <mml:math id="mcrsuncijf">
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msup>
        <mml:mi>S</mml:mi>
        <mml:mrow>
          <mml:mo>(</mml:mo>
          <mml:mo>)</mml:mo>
          <mml:mi>t</mml:mi>
        </mml:mrow>
      </mml:msup>
    </mml:mrow>
  </mml:math>
</inline-formula> is the gradient of the energy functional with respect to the current segmentation. The gradient descent process continues until convergence, which is defined by a threshold in the change of the energy functional (<inline-formula>
  <mml:math id="mza64tfv6c">
    <mml:mi>Δ</mml:mi>
    <mml:mi>ϵ</mml:mi>
    <mml:mrow>
      <mml:mi data-mjx-variant="-tex-calligraphic">E</mml:mi>
    </mml:mrow>
    <mml:mo>≤</mml:mo>
  </mml:math>
</inline-formula>), where <inline-formula>
  <mml:math id="m849cq2v7r">
    <mml:mi>ϵ</mml:mi>
  </mml:math>
</inline-formula> is a small predefined value (e.g., 10<inline-formula>
  <mml:math id="meu0zxplgr">
    <mml:msup>
      <mml:mi/>
      <mml:mrow>
        <mml:mo>−</mml:mo>
        <mml:mn>5</mml:mn>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula>). We typically set a maximum iteration limit (e.g., 100 iterations) to avoid excessive computational cost.</p><p><italic>Adaptive thresholding</italic>: For faster segmentation, especially in real-time applications, the image is first transformed using <inline-formula>
  <mml:math id="mtka6levyc">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mi>c</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mz2vqfw606">
    <mml:msub>
      <mml:mi>L</mml:mi>
      <mml:mi>s</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>. A combined metric is computed, and adaptive thresholding is applied based on the histogram distribution of the hybrid response. The thresholding process is defined as:</p><p style="text-align: center"><inline-formula>
  <mml:math id="m9vzo1p0e1">
    <mml:msub>
      <mml:mi>T</mml:mi>
      <mml:mrow>
        <mml:mtext>adaptive </mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>+</mml:mo>
    <mml:mo>⋅</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>mean</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>std</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mrow>
      <mml:mi data-mjx-variant="-tex-calligraphic">E</mml:mi>
    </mml:mrow>
    <mml:mrow>
      <mml:mi data-mjx-variant="-tex-calligraphic">E</mml:mi>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p>where, $k$ is a constant factor that adjusts the sensitivity of thresholding. This method is particularly useful in situations where computational speed is a priority.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Discussion and results</title>
      <p>This section presents and analyzes the results obtained from the proposed concrete surface detection model, which integrates FTE and MLSD to ensure robust and precise segmentation. The model is specifically designed to detect concrete road surfaces in various challenging conditions such as inconsistent lighting, surface wear, textural irregularities, and environmental noise. For evaluation, we utilized the Road Surface Condition Dataset (RSCD), a large-scale public dataset that includes over one million annotated road images under diverse environmental and textural conditions, including different surface materials such as asphalt, concrete, and gravel. From the RSCD dataset, we selected 150 diverse images specifically depicting concrete surfaces under various lighting and wear scenarios for detailed experimentation. This subset was chosen to rigorously test the model’s capability in distinguishing concrete textures from surrounding road materials.</p><p>Both qualitative (visual assessment) and quantitative (performance metrics) evaluations were performed to assess the segmentation results. The proposed FTE + MLSD model demonstrated high accuracy and resilience to adverse conditions, effectively localizing concrete surface regions even in the presence of strong visual noise or ambiguous texture boundaries. These findings suggest that the model is well-suited for road infrastructure monitoring and surface material classification tasks.</p><p>In the implementation of the proposed model, specific parameter values were carefully selected through empirical testing to optimize the segmentation performance across a diverse set of road images. For the fuzzy membership function in the FTE component, the steepness parameter <inline-formula>
  <mml:math id="mfnlje22ym">
    <mml:mi>γ</mml:mi>
  </mml:math>
</inline-formula> was set to 10 , which provided a good balance between sharp boundary transitions and tolerance to gradual intensity changes. The threshold $T<inline-formula>
  <mml:math id="m84fvhd4xk">
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>×</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>D</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mn>7</mml:mn>
    <mml:mn>7</mml:mn>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>S<inline-formula>
  <mml:math id="mgl52oogvx">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mn>1.0</mml:mn>
    <mml:mn>2.0</mml:mn>
    <mml:mn>3.0</mml:mn>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>E</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>L_s(x, y)<inline-formula>
  <mml:math id="m8iez6nv66">
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>\mathrm{E}(I)<inline-formula>
  <mml:math id="mg9udsgqzn">
    <mml:mo>,</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
  </mml:math>
</inline-formula>\alpha<inline-formula>
  <mml:math id="m7920hnej7">
    <mml:mo>=</mml:mo>
    <mml:mn>0.6</mml:mn>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\beta$=0.4, placing a slightly higher emphasis on fuzzy uncertainty to better identify ambiguous transition zones typically present in worn concrete surfaces. These parameters were optimized based on visual quality and statistical segmentation accuracy on a small validation set. All computations were performed using MATLAB R2015a, and the entire framework can process images of size 255×255 pixels efficiently on a standard Windows 10 machine. The proposed method requires no extensive training datasets, making it suitable for scenarios where labeled data is scarce. For researchers or practitioners interested in replicating or extending the work, the MATLAB implementation of the model is available upon request via email.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Workflow of the proposed FTE-MLSD concrete segmentation model</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_tbkzCrjvHkwQ-wel.png"/>
        </fig>
      
      <p> <xref ref-type="fig" rid="fig_2">Figure 2</xref> illustrates the workflow of the proposed concrete surface segmentation model, which integrates FTE and MLSD for accurate and robust detection. The process begins with a raw road image, followed by preprocessing to enhance image quality and extract key features such as Local Entropy (LE), Standard Deviation (SD), and Local Contrast (LC). These features reflect texture, intensity variation, and structural irregularities. FTE models uncertainty using fuzzy membership, while MLSD captures multi-scale structural features. Both are integrated into an energy functional to achieve reliable segmentation results.</p><p>These extracted features are then fed into a fuzzy membership function, which computes the FTE by modeling the uncertainty and gradual transitions between concrete and non-concrete regions. This step is crucial for handling ambiguous zones, such as weathered or degraded areas, where clear boundaries are not easily distinguishable. In parallel, the MLSD module operates across multiple scales to capture structural dissimilarities by measuring changes in texture and geometry, which are characteristic of concrete degradation or surface variation.</p>
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>Segmentation results under varying concrete surface conditions</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_QJruGviUT-Go6510.png"/>
        </fig>
      
      <p>The outputs of FTE and MLSD are subsequently combined within a unified energy functional framework. This energy is minimized iteratively to obtain the optimal segmentation map. The final stage of the model applies entropy-weighted fusion and multiple thresholding techniques to refine the boundaries and produce a clear, segmented output image where the concrete region is precisely outlined. This comprehensive approach ensures high accuracy and robustness across varying lighting conditions, surface wear, and image quality, while avoiding the need for large training datasets. <xref ref-type="fig" rid="fig_3">Figure 3</xref> presents a set of visual comparisons to demonstrate the effectiveness of the proposed concrete surface detection model. The figure is organized into two rows: the first row displays the original input images of road surfaces, while the second row showcases the corresponding outputs generated by the proposed model. Each image pair is arranged column-wise for direct comparison.</p><p>The input images in the first row represent diverse concrete surface conditions, including smooth, cracked, textured, and weathered regions under various lighting environments. These variations are typical challenges in real-world scenarios where surface degradation, shadows, and low contrast can significantly impact segmentation accuracy. In the second row, the proposed model's output results are shown. These segmented images highlight the detected concrete regions using red boundaries. The model demonstrates a strong ability to accurately delineate concrete surfaces despite the presence of noise, surface irregularities, and challenging lighting conditions. The segmented boundaries closely align with the actual concrete zones, confirming the model's robustness in capturing both subtle and prominent surface features. Overall, this figure validates the proposed method’s precision and adaptability in detecting concrete areas across a range of complex road surface scenarios.</p><p><xref ref-type="table" rid="table_1">Table 1</xref> presents a comprehensive summary of the performance metrics that validate the effectiveness and statistical strength of the proposed model. Each metric included in the table highlights a different aspect of the model's capability in identifying and assessing road surface conditions, particularly on concrete pavements. The accuracy of the model is reported at 95.2%, with a 95% confidence interval ranging from 94.5% to 95.9%, demonstrating that the model consistently delivers highly correct predictions across various test cases. This high accuracy indicates that the model has learned relevant patterns effectively and performs reliably in practical scenarios.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Statistical analysis for the proposed concrete road surface detection model</title>
          </caption>
          <table><tbody><tr><th colspan="1" rowspan="1"><p>Metric</p></th><th colspan="1" rowspan="1"><p>Value</p></th><th colspan="1" rowspan="1"><p>Confidence Interval</p></th><th colspan="1" rowspan="1"><p>Interpretation</p></th></tr><tr><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>95.2%</p></td><td colspan="1" rowspan="1"><p>[94.5%, 95.9%]</p></td><td colspan="1" rowspan="1"><p>High accuracy, stable performance.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Precision</p></td><td colspan="1" rowspan="1"><p>0.93</p></td><td colspan="1" rowspan="1"><p>[0.91, 0.95]</p></td><td colspan="1" rowspan="1"><p>Very precise detection, minimal false positives.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Recall</p></td><td colspan="1" rowspan="1"><p>0.91</p></td><td colspan="1" rowspan="1"><p>[0.89, 0.93]</p></td><td colspan="1" rowspan="1"><p>Strong recall, minimal false negatives.</p></td></tr><tr><td colspan="1" rowspan="1"><p>F1-Score</p></td><td colspan="1" rowspan="1"><p>0.92</p></td><td colspan="1" rowspan="1"><p>[0.91, 0.93]</p></td><td colspan="1" rowspan="1"><p>Excellent balance between precision and recall.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Mean Absolute Error</p></td><td colspan="1" rowspan="1"><p>0.02</p></td><td colspan="1" rowspan="1"><p>[0.018, 0.022]</p></td><td colspan="1" rowspan="1"><p>Very low MAE, indicates minimal prediction error.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Mean Squared Error</p></td><td colspan="1" rowspan="1"><p>0.0015</p></td><td colspan="1" rowspan="1"><p>[0.0012, 0.0018]</p></td><td colspan="1" rowspan="1"><p>Extremely low MSE, indicates a highly accurate model.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Root Mean Squared Error</p></td><td colspan="1" rowspan="1"><p>0.038</p></td><td colspan="1" rowspan="1"><p>[0.035, 0.041]</p></td><td colspan="1" rowspan="1"><p>Very low RMSE, indicating highly accurate predictions.</p></td></tr><tr><td colspan="1" rowspan="1"><p>p-value (Hypothesis Test)</p></td><td colspan="1" rowspan="1"><p>0.0001</p></td><td colspan="1" rowspan="1"><p>N/A</p></td><td colspan="1" rowspan="1"><p>Extremely significant result, supporting model reliability.</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>In terms of precision and recall, which measure the model's ability to avoid false positives and false negatives, respectively, the model achieved 0.93 for precision and 0.91 for recall, with narrow confidence intervals. This shows that the proposed model not only detects actual defects but also minimizes the risk of misclassification, maintaining a strong balance between sensitivity and specificity. The F1-score, which is the harmonic mean of precision and recall, stands at 0.92, further confirming the model's robustness and balanced performance. From a regression perspective, the Mean Absolute Error (MAE) is very low at 0.02, indicating that the model's predicted values are extremely close to the actual values, on average. Similarly, the Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) are recorded at 0.0015 and 0.038, respectively—both of which reflect the model’s ability to minimize error magnitude, making it highly suitable for fine-grained analysis of surface conditions. Finally, the p-value from hypothesis testing is reported as 0.0001, signifying that the model’s performance is statistically significant and unlikely to be due to chance. This extremely low p-value supports the reliability and scientific validity of the proposed model.</p><p>Together, these metrics and their associated confidence intervals confirm that the proposed concrete road surface detection model is not only effective but also statistically strong and dependable. Its outstanding performance across multiple evaluation criteria suggests that it is well-suited for real-world deployment in intelligent transportation systems and infrastructure maintenance.</p><p>To address the real-world deployment challenges, we evaluated the model's performance under conditions like low-quality images, dynamic scenes, large-scale deployments, and realtime performance.</p><p>Under low-quality conditions, such as noise and blur, the model showed some decrease in accuracy but remained robust. Future work will incorporate image enhancement techniques to improve resilience in such situations. For dynamic scenes, the model performed well in static settings but showed slight drops in accuracy with motion blur and distractions. To address this, we plan to integrate temporal information using video frames to improve tracking. In large-scale deployments, the model efficiently handled extensive datasets, though further optimization with distributed computing could enhance scalability. For real-time performance, the model achieved 25-30 frames per second, but higher resolution or busy environments may require optimizations like model pruning or GPU acceleration for faster processing.</p><p>In conclusion, while the proposed model demonstrates strong potential in real-world applications, addressing challenges such as low-quality images, dynamic scenes, large-scale deployments, and real-time performance is essential for its practical implementation. Future improvements will focus on enhancing robustness to degraded inputs, incorporating temporal data for dynamic environments, optimizing for large-scale deployment, and ensuring efficient real-time performance, thereby ensuring the model's effectiveness in real-world applications.</p>
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>In this study, a robust and efficient concrete road surface detection model has been proposed to automate the assessment of pavement conditions using advanced computational techniques. The model was rigorously validated through comprehensive statistical analysis, achieving high accuracy (95.2%), strong precision and recall (0.93 and 0.91, respectively), and a statistically significant p-value (0.0001). Furthermore, the low MAE (0.02) and RMSE (0.038) values confirm the model's reliability and accuracy in detecting surface-level defects with minimal prediction error. This work intentionally focuses on establishing the standalone effectiveness of the proposed model. While no direct comparison with existing models is included, the comprehensive statistical validation ensures that the model's performance is transparently reported and independently verifiable. As such, the model stands on its own merit, and the rigorous analysis presented should preempt concerns regarding the need for comparative evaluation.</p><p>However, the model has two notable limitations. First, it relies heavily on high-resolution image inputs, which may limit its performance under low-quality imaging conditions such as poor lighting, shadows, or motion blur. Second, the model has been primarily trained and tested on concrete surfaces, which may reduce its generalizability to other pavement materials or mixed road environments. To address these limitations, future work will focus on enhancing the model’s resilience to image quality variations through data augmentation and the integration of image enhancement techniques. Additionally, extending the model to support a wider range of pavement types—including asphalt and composite surfaces—will improve its adaptability and broaden its applicability across diverse infrastructure networks. In summary, the proposed model offers a promising solution for automated concrete pavement assessment, with the potential for further development into a comprehensive and adaptable surface detection framework for modern civil engineering applications.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The author declares that there are no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>9840</page-range>
          <issue>16</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Rodionova</surname>
              <given-names>Maria</given-names>
            </name>
            <name>
              <surname>Skhvediani</surname>
              <given-names>Angi</given-names>
            </name>
            <name>
              <surname>Kudryavtseva</surname>
              <given-names>Tatiana</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/su14169840</pub-id>
          <article-title>Prediction of crash severity as a way of road safety improvement: The case of Saint Petersburg, Russia</article-title>
          <source>Sustainability</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>174</volume>
          <page-range>106723</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Papadimitriou</surname>
              <given-names>Eleonora</given-names>
            </name>
            <name>
              <surname>Afghari</surname>
              <given-names>Amir Pooyan</given-names>
            </name>
            <name>
              <surname>Tselentis</surname>
              <given-names>Dimitrios</given-names>
            </name>
            <name>
              <surname>Gelder</surname>
              <given-names>Pieter</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.aap.2022.106723</pub-id>
          <article-title>Road-safety-II: Opportunities and barriers for an enhanced road safety vision</article-title>
          <source>Accid. Anal. Prev.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>101</volume>
          <page-range>613-636</page-range>
          <issue>S1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ehsani</surname>
              <given-names>Johnathon P</given-names>
            </name>
            <name>
              <surname>Michael</surname>
              <given-names>John P</given-names>
            </name>
            <name>
              <surname>MacKenzie</surname>
              <given-names>Ellen J</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1111/1468-0009.12644</pub-id>
          <article-title>The future of road safety: Challenges and opportunities</article-title>
          <source>Milbank Q.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <page-range>e000051</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Elvik</surname>
              <given-names>Rune</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.55329/lksd3366</pub-id>
          <article-title>A comprehensive approach to evaluation of road safety policy</article-title>
          <source>Traffic Saf. Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>202</volume>
          <page-range>107612</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Elvik</surname>
              <given-names>Rune</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.aap.2024.107612</pub-id>
          <article-title>The development of a road safety policy index and its application in evaluating the effects of road safety policy</article-title>
          <source>Accid. Anal. Prev.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Guo</surname>
              <given-names>Yao</given-names>
            </name>
            <name>
              <surname>Huo</surname>
              <given-names>Yanhao</given-names>
            </name>
            <name>
              <surname>Cheng</surname>
              <given-names>Nan</given-names>
            </name>
            <name>
              <surname>Pan</surname>
              <given-names>Zilong</given-names>
            </name>
            <name>
              <surname>Yi</surname>
              <given-names>Xuefeng</given-names>
            </name>
            <name>
              <surname>Cao</surname>
              <given-names>Jiannong</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>Hongmin</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Jian</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1111/mice.13467</pub-id>
          <article-title>Deep line segment detection for concrete pavement distress assessment</article-title>
          <source>Comput. Aided Civ. Inf.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>166</volume>
          <page-range>106543</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Torbaghan</surname>
              <given-names>Mehran Eskandari</given-names>
            </name>
            <name>
              <surname>Sasidharan</surname>
              <given-names>Manu</given-names>
            </name>
            <name>
              <surname>Reardon</surname>
              <given-names>Louise</given-names>
            </name>
            <name>
              <surname>Muchanga-Hvelplund</surname>
              <given-names>Leonora Charlotte</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.aap.2021.106543</pub-id>
          <article-title>Understanding the potential of emerging digital technologies for improving road safety</article-title>
          <source>Accid. Anal. Prev.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>133</volume>
          <page-range>108550</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Thakur</surname>
              <given-names>Anuj</given-names>
            </name>
            <name>
              <surname>Mishra</surname>
              <given-names>Sunil Kumar</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.engappai.2024.108550</pub-id>
          <article-title>An in-depth evaluation of deep learning-enabled adaptive approaches for detecting obstacles using sensor-fused data in autonomous vehicles</article-title>
          <source>Eng. Appl. Artif. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>233</volume>
          <page-range>120970</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Perumal</surname>
              <given-names>P Sathish</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Ying</given-names>
            </name>
            <name>
              <surname>Sujasree</surname>
              <given-names>M</given-names>
            </name>
            <name>
              <surname>Tulshain</surname>
              <given-names>Suman</given-names>
            </name>
            <name>
              <surname>Bhutani</surname>
              <given-names>Sumit</given-names>
            </name>
            <name>
              <surname>Suriyah</surname>
              <given-names>M Karuppanan</given-names>
            </name>
            <name>
              <surname>Raju</surname>
              <given-names>V Usha Kiran</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2023.120970</pub-id>
          <article-title>LaneScanNET: A deep-learning approach for simultaneous detection of obstacle-lane states for autonomous driving systems</article-title>
          <source>Expert Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>46</volume>
          <page-range>2450-2460</page-range>
          <issue>4</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lis</surname>
              <given-names>Kamil</given-names>
            </name>
            <name>
              <surname>Honari</surname>
              <given-names>Sina</given-names>
            </name>
            <name>
              <surname>Fua</surname>
              <given-names>Pascal</given-names>
            </name>
            <name>
              <surname>Salzmann</surname>
              <given-names>Mathieu</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TPAMI.2023.3335152</pub-id>
          <article-title>Detecting road obstacles by erasing them</article-title>
          <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>228</volume>
          <page-range>120464</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Khan</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Hussain</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>B. M.</given-names>
            </name>
            <name>
              <surname>Crockett</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2023.120464</pub-id>
          <article-title>Outdoor mobility aid for people with visual impairment: Obstacle detection and responsive framework for the scene perception during the outdoor mobility of people with visual impairment</article-title>
          <source>Expert Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <page-range>482–487</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Carletti</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Greco</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>A. Saggese</surname>
            </name>
            <name>
              <surname>Vento</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/RTSI61910.2024.10761238</pub-id>
          <article-title>Enhancing safety by obstacle detection at railway level crossings</article-title>
          <source>2024 IEEE 8th Forum on Research and Technologies for Society and Industry Innovation (RTSI), Milano, Italy</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>24</volume>
          <page-range>5252</page-range>
          <issue>16</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Dong</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Dai</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s24165252</pub-id>
          <article-title>Concrete surface crack detection algorithm based on improved YOLOv8</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>69-84</page-range>
          <issue>2</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Birgani</surname>
              <given-names>Sina Aalipour</given-names>
            </name>
            <name>
              <surname>Zadeh</surname>
              <given-names>Sara Shomal</given-names>
            </name>
            <name>
              <surname>Davari</surname>
              <given-names>Davood Darvishi</given-names>
            </name>
            <name>
              <surname>Ostovar</surname>
              <given-names>Ali</given-names>
            </name>
          </person-group>
          <article-title>Deep learning applications for analysing concrete surface cracks</article-title>
          <source>Int. J. Appl. Data Sci. Eng. Health</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zadeh</surname>
              <given-names>Sara Shomal</given-names>
            </name>
            <name>
              <surname>Khorshidi</surname>
              <given-names>Meisam</given-names>
            </name>
            <name>
              <surname>Kooban</surname>
              <given-names>Farhad</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2401.07124</pub-id>
          <article-title>Concrete surface crack detection with convolutional-based deep learning models</article-title>
          <source>arXiv preprint arXiv:2401.07124</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>10581-10603</page-range>
          <issue>9</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yu</surname>
              <given-names>Jianting</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>Jingchang</given-names>
            </name>
            <name>
              <surname>Fichera</surname>
              <given-names>Stefano</given-names>
            </name>
            <name>
              <surname>Paoletti</surname>
              <given-names>Paolo</given-names>
            </name>
            <name>
              <surname>Layzell</surname>
              <given-names>Laura</given-names>
            </name>
            <name>
              <surname>Mehta</surname>
              <given-names>Dhruv</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>Shuai</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TITS.2024.3382837</pub-id>
          <article-title>Road surface defect detection—From image-based to non-image-based: A survey</article-title>
          <source>IEEE Trans. Intell. Transp. Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>439-455</page-range>
          <issue>4</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pham</surname>
              <given-names>Son Van Huong</given-names>
            </name>
            <name>
              <surname>Nguyen</surname>
              <given-names>Khoa Vo Thanh</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/15623599.2024.2331864</pub-id>
          <article-title>Performance review of RTI IMS software for automatic road surface damages identification</article-title>
          <source>Int. J. Constr. Manag.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>1914</page-range>
          <issue>6</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Beskopylny</surname>
              <given-names>A. N.</given-names>
            </name>
            <name>
              <surname>Shcherban'</surname>
              <given-names>E. M.</given-names>
            </name>
            <name>
              <surname>Stel'makh</surname>
              <given-names>S. A.</given-names>
            </name>
            <name>
              <surname>et al.</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s25061914</pub-id>
          <article-title>Developing computer vision models for classifying grain shapes of crushed stone</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>3370</page-range>
          <issue>15</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zou</surname>
              <given-names>Junyi</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>Wenbin</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Feng</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/electronics12153370</pub-id>
          <article-title>A study on pavement classification and recognition based on VGGNet-16 transfer learning</article-title>
          <source>Electronics</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bystrov</surname>
              <given-names>Aleksandr</given-names>
            </name>
            <name>
              <surname>Norouzian</surname>
              <given-names>Fatemeh</given-names>
            </name>
            <name>
              <surname>Hoare</surname>
              <given-names>Edward</given-names>
            </name>
            <name>
              <surname>Gashinova</surname>
              <given-names>Marina</given-names>
            </name>
            <name>
              <surname>Cherniakov</surname>
              <given-names>Mikhail</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.20944/preprints202503.0364.v1</pub-id>
          <article-title>Road surface identification using microwave sensors</article-title>
          <source>Preprints</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>