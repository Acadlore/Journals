<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">JII</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Journal of Industrial Intelligence</journal-title>
        <abbrev-journal-title abbrev-type="issn">J. Ind Intell.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">JII</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-2695</issn>
      <issn publication-format="print">2958-2687</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-L7sKjzZ0DVFZpgm70K4qVezrKVt0ysjW</article-id>
      <article-id pub-id-type="doi">10.56578/jii020405</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>YOLOv8n-AM: Enhanced Real-Time Smoke Detection via Attention-Based Feature Interaction and Multi-Scale Downsampling</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-9819-9643</contrib-id>
          <name>
            <surname>Yao</surname>
            <given-names>Zijun</given-names>
          </name>
          <email>212206560563@hyit.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-0488-4011</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Lin</given-names>
          </name>
          <email>zlmjl@hyit.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7715-3070</contrib-id>
          <name>
            <surname>Khadka</surname>
            <given-names>Ashim</given-names>
          </name>
          <email>ashim.khadka@ncit.edu.np</email>
        </contrib>
        <aff id="aff_1">Faculty of Computer and Software Engineering, Huaiyin Institute of Technology, 223003 Huaian, China</aff>
        <aff id="aff_2">Faculty of Management Engineering, Huaiyin Institute of Technology, 223003 Huaian, China</aff>
        <aff id="aff_3">Nepal College of Information Technology, Pokhara University, 44700 Lalitpur, Nepal</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>30</day>
        <month>12</month>
        <year>2024</year>
      </pub-date>
      <volume>2</volume>
      <issue>4</issue>
      <fpage>240</fpage>
      <lpage>250</lpage>
      <page-range>240-250</page-range>
      <history>
        <date date-type="received">
          <day>14</day>
          <month>10</month>
          <year>2024</year>
        </date>
        <date date-type="accepted">
          <day>14</day>
          <month>12</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p><span style="font-family: Times New Roman, serif">Accurate smoke detection in complex industrial environments, such as chemical plants, remains a significant challenge due to the inherently low contrast, transparency, and weak texture features of smoke, which often exhibits blurred boundaries and diverse spatial scales. To address these limitations, YOLOv8n-AM, an enhanced lightweight detection framework belonging to the YOLO (You Only Look Once) series, was developed by integrating advanced architectural components into the baseline YOLOv8n model. Specifically, the conventional Spatial Pyramid Pooling-Fast (SPPF) module was replaced with an Attention-based Intra-scale Feature Interaction (AIFI) Convolution Synergistic Feature Processing Module (SFPM), i.e., AIFC-SFPM, enabling more effective semantic feature representation and an improvement in detection accuracy. In parallel, the original convolutional module was optimized using a Multi-Scale Downsampling (MSDown) module, which reduces model redundancy and computational overhead, increasing the detection speed. Experimental evaluations demonstrate that the YOLOv8n-AM model achieves a 1.7% improvement in mean Average Precision (mAP), accompanied by a 9.1% reduction in Giga Floating-point Operations Per Second (GFLOPs) and a 15.4% decrease in parameter count when compared to the original YOLOv8n framework. These improvements collectively underscore the model’s suitability for real-time deployment in resource-constrained industrial settings where rapid and reliable smoke detection is critical. The proposed architecture thus provides a computationally efficient and high-precision solution for safety-critical visual monitoring applications.</p></abstract>
      <kwd-group>
        <kwd>Smoke detection model</kwd>
        <kwd>AIFC-SFPM</kwd>
        <kwd>MSDown module</kwd>
        <kwd>Real-time detection</kwd>
        <kwd>Industrial safety</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="3"/>
        <fig-count count="7"/>
        <table-count count="3"/>
        <ref-count count="25"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p><span style="font-family: Times New Roman, serif">In the modern industrial field, especially in large chemical plant areas, rapid detection and monitoring of smoke are crucial for ensuring safe production and preventing environmental pollution [<xref ref-type="bibr" rid="ref_1">1</xref>]<span style="font-family: Times New Roman, serif">. Chemical plants often involve the production, transportation, and storage of hazardous substances. Once a leak or accident occurs, the spread of smoke can<span style="color: red; font-family: Times New Roman, serif"> <span style="font-family: Times New Roman, serif">pose a great threat to on-site workers and the surrounding environment. Therefore, developing efficient and accurate smoke detection methods has become a research focus. However, due to the complexity of the environment in the chemical plant area, the diversity of smoke forms, and the variability of meteorological conditions, traditional smoke detection technology [<xref ref-type="bibr" rid="ref_2">2</xref>]<span style="font-family: Times New Roman, serif"> faces problems such as insufficient perception, a high false detection rate, and a slow response speed. With the continuous development of industrial automation and intelligence, smoke detection technology, as a key means of environmental monitoring and safety protection, has gradually received widespread attention. At present, research mainly focuses on two directions: traditional image processing-based detection methods and intelligent detection methods based on deep learning.</p><p><span style="font-family: Times New Roman, serif">In terms of traditional video smoke detection technology, Pundir et al. [<xref ref-type="bibr" rid="ref_3">3</xref>]<span style="font-family: Times New Roman, serif"> studied methods for extracting smoke features in different color spaces. In the Red, Green, and Blue (RGB) color space, the color features of smoke were extracted and converted into the YCbCr color space to calculate brightness values (Y) and chromaticity values (Cb and Cr). These features provide an effective basis for further smoke detection, especially under complex backgrounds and variable lighting conditions, which can significantly improve detection accuracy. However, it lacks sensitivity to low contrast or transparent smoke and is susceptible to interference from complex backgrounds such as cloud layers and vapors. Lin et al. [<xref ref-type="bibr" rid="ref_4">4</xref>] <span style="font-family: Times New Roman, serif">extracted suspicious areas in the foreground using Gaussian mixture models and background subtraction, and established smoke color models in the RGB and Hue, Saturation, and Intensity (HSI) color spaces. By combining contour features, a feature vector was formed and classified using Support Vector Machine (SVM) [<xref ref-type="bibr" rid="ref_5">5</xref>]<span style="font-family: Times New Roman, serif"> to accurately identify smoke areas. Although it can extract contour features, its robustness to dynamic lighting changes is poor. Zhao et al. [<xref ref-type="bibr" rid="ref_6">6</xref>]<span style="font-family: Times New Roman, serif"> classified smoke using the Cost-Sensitive Adaptive Boosting (CS-Adaboost) algorithm by extracting its movement, heat, and color features. However, it is difficult to detect static or slowly spreading smoke, and the computational complexity is high. Yuan et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] <span style="font-family: Times New Roman, serif">proposed a smoke detection method based on fuzzy logic, which reduces negative effects such as sky clouds and lighting changes by taking the differences between RGB and HSI models as inputs. The use of the Extended Kalman filtering (EKF) to reshape the input-output of fuzzy rules improves the flexibility of the detection method. But the anti-interference ability is weak. Jia et al. [<xref ref-type="bibr" rid="ref_8">8</xref>]<span style="font-family: Times New Roman, serif"> proposed a smoke pixel classification detection method based on saliency detection, which enhances smoke color nonlinearly, measures saliency by combining enhancement maps and motion maps, and uses motion energy and saliency maps for smoke detection. But its generalization ability is weak, making it difficult to adapt to the changing smoke patterns and environmental disturbances in chemical plants.</p><p><span style="font-family: Times New Roman, serif">In summary, in recent years, traditional smoke detection research based on feature extraction has been continuously deepening and has achieved certain results. However, it can be observed that traditional methods have limitations. On the one hand, due to the single feature extraction of smoke recognition classification and the fact that manually set features such as color and texture cannot fully represent smoke, the false detection rate of smoke detection is relatively high. On the other hand, it performs poorly in dynamic lighting, noise interference, or diverse data.</p><p><span style="font-family: Times New Roman, serif">With the rise of deep learning technology, especially the application of the convolutional neural network (CNN), smoke detection technology has been significantly improved. Deep learning can automatically learn and extract deep-level features of images, reducing reliance on manually designed features and significantly improving the model's generalization ability and detection accuracy. In this type of research, the most representative ones are object detection networks, such as YOLO series [<xref ref-type="bibr" rid="ref_9">9</xref>]<span style="font-family: Times New Roman, serif">, Faster Region-based Convolutional Neural Network (Faster R-CNN) [<xref ref-type="bibr" rid="ref_10">10</xref>]<span style="font-family: Times New Roman, serif">, Single Shot MultiBox Detector (SSD) [<xref ref-type="bibr" rid="ref_11">11</xref>],<span style="font-family: Times New Roman, serif"> etc., which have excellent real-time performance and detection accuracy.</p><p><span style="font-family: Times New Roman, serif">In terms of improving the application of the attention mechanism, Li et al.<sup><span style="font-family: Times New Roman, serif"> </sup>[<xref ref-type="bibr" rid="ref_12">12</xref>]<span style="font-family: Times New Roman, serif"> proposed a Local Binary Pattern Silhouette Coefficient Variable (LBPSCV) for industrial smoke segmentation, which uses the variation of the silhouette coefficient as the weight to calculate the local binary pattern (LBP). The extracted texture features make it easier to detect smoke. But the segmentation effect is not good for low contrast and transparent smoke. He et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] <span style="font-family: Times New Roman, serif">constructed a smoke dataset containing multiple positive and negative samples by combining online collection with offline shooting. In terms of model design, a module was embedded that combines the spatial attention mechanism and the channel attention mechanism into the second convolutional layer of Visual Geometry Group 16 (VGG16) to enhance the model's ability to detect smoke features and further improve the overall performance of the algorithm. But the contribution of high- and low-layer features has not been distinguished, resulting in background noise interference. Li et al.<sup><span style="font-family: Times New Roman, serif"> </sup>[<xref ref-type="bibr" rid="ref_14">14</xref>]<span style="font-family: Times New Roman, serif"> introduced a lightweight framework for smoke video detection, which utilizes BFBlock to enhance feature extraction and considers the influence of weather factors to improve the robustness and accuracy of detection. Although lightweight design helps to improve detection speed, it sacrifices some detection accuracy while pursuing speed.</p><p><span style="font-family: Times New Roman, serif">In terms of module optimization and lightweight design, Khan et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] <span style="font-family: Times New Roman, serif">proposed a CNN-based smoke detection and segmentation framework aimed at effectively handling outdoor clear and hazy scenes. In this framework, EfficientNet was used for smoke detection, ensuring efficient feature extraction and accuracy. Meanwhile, DeepLabv3+ was applied to segment smoke areas to achieve optimal localization results. The proposal of this method provides a new solution for smoke detection. Although high-precision segmentation was achieved, the multi-scale feature fusion was not optimized, resulting in a high missed detection rate for small targets. Masoom et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] i<span style="font-family: Times New Roman, serif">mproved the algorithm's ability to locate smoke by using smoke principal component analysis (PCA) as a preprocessing module to remove redundant features. Although PCA helps reduce computational complexity, the additional detection scale added may increase the computational cost of the improved network. Huo et al. [<xref ref-type="bibr" rid="ref_17">17</xref>]<span style="font-family: Times New Roman, serif"> proposed a single-scene smoke detection algorithm based on the depthwise separable convolution, which effectively reduces model complexity. However, the depthwise separable convolution was performed in a two-dimensional plane, making it difficult to fully utilize the feature information at the same spatial position between channels.</p><p><span style="font-family: Times New Roman, serif">In the improved method based on object detection networks, Sun et al. [<xref ref-type="bibr" rid="ref_18">18</xref>]<span style="font-family: Times New Roman, serif"> proposed an improved CNN that achieves automatic feature extraction through optimization strategies in multi-convolutional kernels and batch normalization processes without manual intervention. This improvement significantly optimizes the loss function, thereby improving the smoke recognition rate and providing new ideas and methods for the development of smoke detection technology. But the feature fusion strategy is single. Wang et al. [<xref ref-type="bibr" rid="ref_19">19</xref>]<span style="font-family: Times New Roman, serif"> used YOLOv5m [<xref ref-type="bibr" rid="ref_20">20</xref>] <span style="font-family: Times New Roman, serif">as the base model and improved the model's ability to extract smoke features by adding a spatial channel attention mechanism to the module composed of the convolutional layer, the Batch Normalization (BN) layer, and the LeakyReLU activation function in its backbone network. Although the feature extraction capability was improved, redundant calculations were not reduced. Shao et al. [<xref ref-type="bibr" rid="ref_21">21</xref>] <span style="font-family: Times New Roman, serif">introduced a new method for small target fire and smoke detection called YOLOv7scb, which enhances the model's ability to effectively extract features from small targets. However, the module is not lightweight, and the entire framework still requires a large amount of computing resources during runtime. Chen et al. [<xref ref-type="bibr" rid="ref_22">22</xref>]<span style="font-family: Times New Roman, serif"> proposed a lightweight forest fire detection model for unmanned aerial vehicle applications based on the YOLOv7 model. The model adopts the GSConv convolution design and constructs the GSELAN and GSSPPFCSPC modules, which significantly reduce the number of parameters in the model and improve computational efficiency. In addition, the model introduces a coordinate attention mechanism, which further enhances the ability to extract smoke features and effectively improves the detection performance of the model in complex natural environments. Although the model is lightweight, high-level semantic representation has not been optimized for smoke transparency. Deng et al. [<xref ref-type="bibr" rid="ref_23">23</xref>] p<span style="font-family: Times New Roman, serif">roposed an integrated attention mechanism for aircraft hangar fire detection based on the YOLOv8n framework. However, the module structure that is not lightweight is difficult to deploy widely.</p><p><span style="font-family: Times New Roman, serif">In recent years, deep learning technology has significantly promoted the development of smoke detection, with its core advantage being the end-to-end feature learning achieved through CNNs, effectively breaking through the limitations of traditional manual features. Despite these achievements, several persistent challenges remain unresolved. On the one hand, it is difficult for existing methods to accurately detect small, localized smoke to large, diffuse smoke. On the other hand, the complexity of network models can waste computational resources for simple smoke recognition requirements.</p><p><span style="font-family: Times New Roman, serif">This study proposes a new smoke detection model, YOLOv8n-AM, by combining AIFC-SFPM and the MSDown module based on the above-mentioned previous studies.</p>
    </sec>
    <sec sec-type="">
      <title>2. Yolov8n-am</title>
      <p><span style="font-family: Times New Roman, serif">The development of the YOLOv8n-AM model was guided by the need to address two critical challenges in smoke detection:</p><p>(a) <span style="font-family: Times New Roman, serif">Smoke usually manifests as low-contrast, transparent or semi-transparent areas with weak texture and edge information, and it may appear at different scales, ranging from small local smoke to large diffuse smoke, making it difficult to be accurately detected.</p><p>(b) <span style="font-family: Times New Roman, serif">The high computational complexity and resource consumption of detection models make it difficult to deploy on low-power mobile terminals, which limits the widespread application of smoke detection technology. Therefore, there is a higher demand for lightweight and real-time detection models.</p><p><span style="font-family: Times New Roman, serif">In response to the above issues, this study mainly improves the YOLOv8n-based model and proposes a smoke detection model, YOLOv8n-AM. Replacing the SPPF module of YOLOv8n with AIFC-SFPM significantly increases the accuracy of model detection. By using the MSDown module to improve the Conv module, it greatly reduces redundancy, lowers model complexity, and improves detection speed. The YOLOv8n-AM network architecture is shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref><span style="font-family: Times New Roman, serif">.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>YOLOV8n-AM network framework</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_nFmkD2an9-_iBBaA.png"/>
        </fig>
      
      
        <sec>
          
            <title>2.1. Yolov8 object detection neural network</title>
          
          <p><span style="font-family: Times New Roman, serif">The main structure of YOLOv8 [<xref ref-type="bibr" rid="ref_24">24</xref>]<span style="font-family: Times New Roman, serif"> includes a backbone network, a neck network, and a head network. The backbone network is responsible for extracting meaningful features from input images, capturing simple patterns such as edges and textures in the image, and providing multi-scale feature representations at different levels, thereby providing rich information for subsequent object detection. As a bridge between the backbone network and the head network, the neck network mainly performs feature fusion operations, integrates different levels of feature information, constructs feature pyramids to ensure that the network can detect objects of different sizes, and improves detection accuracy by considering a wider range of scene context information, while reducing spatial resolution and resource dimensions to accelerate computation speed. As the final part, the head network is responsible for generating the final output of object detection, such as generating bounding boxes associated with objects that may exist in the image and assigning a reliability score to each bounding box to represent the likelihood of object existence. At the same time, objects in the bounding boxes are classified and sorted according to their categories.</p><p><span style="font-family: Times New Roman, serif">YOLOv8 includes multiple versions, namely YOLOv8n, YOLOv8s, YOLOv8m, YOLOv8l, and YOLOv8x. The smallest version is YOLOv8n. Under the same dataset, the YOLOv8n model has the shortest training time, smallest size, and real-time efficiency. YOLOv8n was improved in this study by expanding the dataset through data augmentation during the input stage.</p>
        </sec>
      
      
        <sec>
          
            <title>2.2. Aifc-sfpm</title>
          
          <p><span style="font-family: Times New Roman, serif">AIFC-SFPM combines the single-scale Transformer (AIFI) and convolution operation (Conv) to achieve collaborative processing of high-level semantic features and local detail features.</p><p><span style="font-family: Times New Roman, serif">Due to the fact that smoke typically appears as low-contrast, transparent or semi-transparent areas, its texture and edge information are weak and susceptible to background noise interference. However, high-level features contain richer semantic information, which can more effectively represent the overall shape and semantic features of smoke. The AIFI module is a key technology aimed at optimizing the semantic richness and computational efficiency of high-level feature representations. AIFI focuses on the internal interaction of high-level feature maps, effectively balancing the detection performance and inference speed of the model by eliminating redundant calculations of multi-scale features. AIFI only processes high-level features at specific scales (such as <italic><span style="font-family: Times New Roman, serif">S</italic><sub><span style="font-family: Times New Roman, serif">5</sub><span style="font-family: Times New Roman, serif">), reducing unnecessary computational complexity. The AIFI module enhances its modeling ability for global semantics through single-scale interaction with high-level feature <italic><span style="font-family: Times New Roman, serif">S</italic><sub><span style="font-family: Times New Roman, serif">5</sub><span style="font-family: Times New Roman, serif">, making it suitable for capturing features of diffuse smoke. In addition, in smoke detection, low-level features such as edges and textures may be affected by background noise such as ground textures or tree contours.</p>
          
            <sec>
              
                <title>2.2.1 Single-scale interaction strategy</title>
              
              <p><span style="font-family: Times New Roman, serif">The main interaction strategy of AIFI is to focus on the highest-level feature <italic><span style="font-family: Times New Roman, serif">S</italic><span style="font-family: Times New Roman, serif">₅ and apply the Transformer [<xref ref-type="bibr" rid="ref_25">25</xref>]<span style="font-family: Times New Roman, serif"> self-attention mechanism to process it. In multi-scale features, low-level features such as <italic><span style="font-family: Times New Roman, serif">S</italic><sub><span style="font-family: Times New Roman, serif">3</sub><span style="font-family: Times New Roman, serif"> and <italic><span style="font-family: Times New Roman, serif">S</italic><span style="font-family: Times New Roman, serif">₄ often contain more detailed information such as edges and textures and are easily affected by background noise interference. The high-level feature <italic><span style="font-family: Times New Roman, serif">S</italic><span style="font-family: Times New Roman, serif">₅ has stronger semantic expression ability, and performing self-attention calculation on it can avoid redundant calculation on low-level features and reduce the interference of irrelevant information.</p>
            </sec>
          
          
            <sec>
              
                <title>2.2.2 Lightweight design</title>
              
              <p><span style="font-family: Times New Roman, serif">AIFI only uses one Transformer layer, which ensures effective information exchange for high-level features without introducing too many parameters and computational complexity. Before performing the self-attention computation, the features of input <italic><span style="font-family: Times New Roman, serif">S</italic><sub><span style="font-family: Times New Roman, serif">5</sub><span style="font-family: Times New Roman, serif"> were flattened and converted into a format suitable for Transformer Block processing to obtain <italic><span style="font-family: Times New Roman, serif">Q</italic><span style="font-family: Times New Roman, serif">, <italic><span style="font-family: Times New Roman, serif">K</italic><span style="font-family: Times New Roman, serif">, and <italic><span style="font-family: Times New Roman, serif">V.</italic></p>
              
                <disp-formula>
                  <label>(1)</label>
                  <mml:math id="m9dke0lbl5">
                    <mml:mi>Q</mml:mi>
                    <mml:mi>K</mml:mi>
                    <mml:mi>V</mml:mi>
                    <mml:mi>Flatten</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>S</mml:mi>
                        <mml:mn>5</mml:mn>
                      </mml:msub>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p><span style="font-family: Times New Roman, serif">The result <italic><span style="font-family: Times New Roman, serif">F<sub><span style="font-family: Times New Roman, serif">5</sub></italic><span style="font-family: Times New Roman, serif"> calculated<italic><span style="font-family: Times New Roman, serif"> </italic><span style="font-family: Times New Roman, serif">was restored based on the Transformer self-attention mechanism to the same spatial shape as <italic><span style="font-family: Times New Roman, serif">S<sub><span style="font-family: Times New Roman, serif">5</sub><span style="font-family: Times New Roman, serif"> </italic><span style="font-family: Times New Roman, serif">for subsequent module processing.</p>
              
                <disp-formula>
                  <label>(2)</label>
                  <mml:math id="mr3njzhjrq">
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mn>5</mml:mn>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>Reshape</mml:mi>
                    <mml:mi>AIFI</mml:mi>
                    <mml:mi>Q</mml:mi>
                    <mml:mi>K</mml:mi>
                    <mml:mi>V</mml:mi>
                  </mml:math>
                </disp-formula>
              
            </sec>
          
          
            <sec>
              
                <title>2.2.3 Two-dimensional sine-cosine positional embedding</title>
              
              <p><span style="font-family: Times New Roman, serif">In order to enable the model to perceive the position information of each element in the feature map, a two-dimensional sine-cosine positional embedding was constructed using AIFI. Firstly, a function was used to generate grid coordinates <italic><span style="font-family: Times New Roman, serif">grid-w</italic><span style="font-family: Times New Roman, serif"> and <italic><span style="font-family: Times New Roman, serif">grid-h</italic><span style="font-family: Times New Roman, serif"> in the width and height directions, respectively, and generate a two-dimensional grid. Then the embedding dimension was calculated for each direction to generate the frequency parameter omega. After flattening the grid coordinates, matrix multiplication was performed with frequency parameters to obtain <italic><span style="font-family: Times New Roman, serif">outw_</italic><span style="font-family: Times New Roman, serif"> and <italic><span style="font-family: Times New Roman, serif">out_i</italic><span style="font-family: Times New Roman, serif">. Finally, the sine and cosine values were concatenated to form a two-dimensional positional embedding.</p><p><span style="font-family: Times New Roman, serif">Conv has high efficiency in extracting local features and can quickly capture information such as edges and textures in images. AIFI, combined with Conv's local perception capability, can flexibly process features at different levels. Compared with SPPF, this combination may have higher computational efficiency in processing multi-scale features, especially in smoke detection, which can provide improved accuracy. In addition, when processing high-resolution images, it can avoid the problem of excessive computational resource consumption caused by SPPF's global pooling operation. The structure of AIFC-SFPM is shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>.</p>
              
                <fig id="fig_2">
                  <label>Figure 2</label>
                  <caption>
                    <title>AIFC-SFPM structure diagram</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_rZsczQ3iT4AIMCSP.png"/>
                </fig>
              
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>2.3. Ms-down module</title>
          
          <p><span style="font-family: Times New Roman, serif">The MS-Down module is a downsampling module used in neural networks. This module is designed to achieve multi-scale feature extraction and fusion while maintaining computational efficiency through reasonable selection of the convolution kernel size, stride, and pooling operations. Compared to some complex downsampling methods, it avoids excessive consumption of computational resources. In practical applications, especially for smoke detection tasks that require real-time processing, this computational efficiency advantage can ensure that the model processes a large number of video frames in a limited time, detects smoke conditions in a timely manner, and does not impose an excessive burden on system resources. The main structure of the MS-Down module is shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Diagram of the MS-Down module structure</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_7LEEorpPzR1Fz58a.png"/>
            </fig>
          
          <p><span style="font-family: Times New Roman, serif">As shown in the above figure, the input tensor enters the average pooling layer <italic><span style="font-family: Times New Roman, serif">AvgPool</italic><span style="font-family: Times New Roman, serif"> for an average pooling operation, with a pooling kernel size of 2, stride of 1, and padding of 0. This operation mainly reduces the size of the input feature map. Average pooling suppresses noise while maintaining spatial resolution, making it suitable for preliminary feature extraction of low-contrast objects such as smoke.</p><p><span style="font-family: Times New Roman, serif">After average pooling, the pooled feature map is split into two parts, x1 and x2, in the channel dimension. x1 is input to the first depthwise separable convolution module <italic><span style="font-family: Times New Roman, serif">cv</italic><span style="font-family: Times New Roman, serif">1 for processing, and the depthwise separable convolution can significantly reduce computational complexity.</p><p><span style="font-family: Times New Roman, serif">The other branch first performs a max pooling (<italic><span style="font-family: Times New Roman, serif">MaxPool</italic><span style="font-family: Times New Roman, serif">) operation, with a pooling kernel size of 3, a stride of 2, and padding of 1. Max pooling preserves edge information through downsampling, balancing feature compression and detail preservation. Then, it is processed through the convolutional layer of the second depthwise separable convolution module <italic><span style="font-family: Times New Roman, serif">cv</italic><span style="font-family: Times New Roman, serif">2 to reduce computational complexity.</p><p><span style="font-family: Times New Roman, serif">Then x1 and x2 are input separately into the channel attention module for processing. Finally, after concatenating along the channel dimension, the concatenated result is returned. This concatenation operation helps to integrate feature information processed by different branches, enabling the network to comprehensively consider multiple feature representations, thus more comprehensively describing the input data and improving the accuracy of object detection. Channel attention enhances the complementarity of features between the two branches, significantly improving the multi-scale feature fusion effect.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Experimental design and result analysis</title>
      
        <sec>
          
            <title>3.1. Experimental environment</title>
          
          <p><span style="font-family: Times New Roman, serif">The experiment was conducted on a Windows 10 operating system equipped with an Intel(R) Core(TM) i9-14900K processor and an NVIDIA GeForce RTX 4090 GPU, supported by 16 GB of system memory. The model training was implemented using Python 3.9, with GPU acceleration enabled via CUDA 11.8, and the deep learning tasks were performed on the PyTorch 2.3.1 framework. The training parameter settings are as follows: the input image size is 640×640, the training period is 200, the initial learning rate is 0.01, and the batch processing volume is 16. The experimental environment of the model in this study is shown in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Experimental environment configuration</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Parameter</p></td><td colspan="1" rowspan="1"><p>Configuration</p></td></tr><tr><td colspan="1" rowspan="1"><p>GPU</p></td><td colspan="1" rowspan="1"><p>NVIDIA GeForce RTX 4090</p></td></tr><tr><td colspan="1" rowspan="1"><p>System environment</p></td><td colspan="1" rowspan="1"><p>Windows 10</p></td></tr><tr><td colspan="1" rowspan="1"><p>CUDA version</p></td><td colspan="1" rowspan="1"><p>CUDA 11.8</p></td></tr><tr><td colspan="1" rowspan="1"><p>Programming language version</p></td><td colspan="1" rowspan="1"><p>Python 3.9</p></td></tr><tr><td colspan="1" rowspan="1"><p>Deep learning framework</p></td><td colspan="1" rowspan="1"><p>PyTorch 2.3.1</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Dataset</title>
          
          <p><span style="font-family: Times New Roman, serif">A self-made smoke dataset, which contains smoke images and videos from different scenes and weather conditions, was used as the experimental dataset. A total of 2,127 images and 135 videos were converted into 3,256 images through frame extraction. After data cleaning, 4,127 images were finally obtained, of which 3,700 images were used as the training set and 427 images as the testing set.</p><p><span style="font-family: Times New Roman, serif">The smoke dataset contains 12 factory areas, including typical scenarios such as tank areas and reaction equipment areas, eight forest areas covering different vegetation types such as coniferous forests and broad-leaved forests, and five types of industrial plants. The types of smoke were mainly divided into two types: industrial smoke and fire smoke, with most smoke images having a small amount of interference. The partial images in the dataset are shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Samples of the dataset images</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_lb6BladB3zW6Gq9L.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_UYyeq6tehp0o30ZH.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. Model evaluation indicators</title>
          
          <p><span style="font-family: Times New Roman, serif">In order to comprehensively evaluate the performance of the improved model and effectively compare it with existing detection methods, this study adopts multiple common evaluation metrics, covering multiple dimensions such as accuracy, recall, detection speed, and computational complexity of single- and multi-class detection tasks. Specifically, single-class precision (<italic><span style="font-family: Times New Roman, serif">A<sub><span style="font-family: Times New Roman, serif">P</sub></italic><span style="font-family: Times New Roman, serif">) and multi-class precision (<italic><span style="font-family: Times New Roman, serif">M<sub><span style="font-family: Times New Roman, serif">AP</sub></italic><span style="font-family: Times New Roman, serif">) were used to quantify the detection accuracy of the model in different categories. In addition, the precision (<italic><span style="font-family: Times New Roman, serif">P</italic><span style="font-family: Times New Roman, serif">) and recall (<italic><span style="font-family: Times New Roman, serif">R</italic><span style="font-family: Times New Roman, serif">) of the model are important indicators for evaluating its predictive ability, which were used to measure the proportions of True Positive (TP) predictions in all predicted and actual positive samples, respectively, aiming to comprehensively evaluate the accuracy and completeness of the model. TP refers to the cases where both ground truth and prediction are positive; False Positive (FP) refers to the cases where the prediction is positive but ground truth is negative; and False Negative (FN) refers to the cases where the prediction is negative but ground truth is positive.</p><p><span style="font-family: Times New Roman, serif">The application of these evaluation indicators helps to reveal the comprehensive performance of the improved model in terms of accuracy, speed, and computational resources, thereby providing a theoretical basis for its optimization and practical application.</p><p><italic><span style="font-family: Times New Roman, serif">P</italic><span style="font-family: Times New Roman, serif"> reflects the probability of the model correctly detecting the target:</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="moduet2qr1">
                <mml:mi>P</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:msub>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:msub>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>T</mml:mi>
                      <mml:mi>P</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mi>P</mml:mi>
                    </mml:msub>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p><italic><span style="font-family: Times New Roman, serif">R</italic><span style="font-family: Times New Roman, serif"> reflects the probability of the model detecting all targets:</p>
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="myydxxqgjq">
                <mml:mi>R</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:msub>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:msub>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>T</mml:mi>
                      <mml:mi>P</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mi>N</mml:mi>
                    </mml:msub>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p><italic><span style="font-family: Times New Roman, serif">A<sub><span style="font-family: Times New Roman, serif">p</sub></italic><span style="font-family: Times New Roman, serif"> reflects the accuracy of the model on a single detection object:</p>
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="m4c1uka6xw">
                <mml:msub>
                  <mml:mi>A</mml:mi>
                  <mml:mi>P</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:msubsup>
                  <mml:mo>∫</mml:mo>
                  <mml:mn>0</mml:mn>
                  <mml:mn>1</mml:mn>
                </mml:msubsup>
                <mml:mi>P</mml:mi>
                <mml:mi>R</mml:mi>
                <mml:mi>d</mml:mi>
                <mml:mi>R</mml:mi>
              </mml:math>
            </disp-formula>
          
          <p>By adding up the values of all categories and taking their average, an overall measure was provided to evaluate the detection performance of the model across all categories. A higher <inline-formula>
  <mml:math id="mnsaoxonm6">
    <mml:msub>
      <mml:mi>M</mml:mi>
      <mml:mrow>
        <mml:mi>A</mml:mi>
        <mml:mi>P</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> value usually means that the model has strong detection ability in various categories and can generalize well. The <inline-formula>
  <mml:math id="mqruforax3">
    <mml:msub>
      <mml:mi>M</mml:mi>
      <mml:mrow>
        <mml:mi>A</mml:mi>
        <mml:mi>P</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> formula is as follows:</p>
          
            <disp-formula>
              <label>(6)</label>
              <mml:math id="m3771dvxhu">
                <mml:msub>
                  <mml:mi>M</mml:mi>
                  <mml:mrow>
                    <mml:mi>A</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:munderover>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mi>n</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                      <mml:mi>N</mml:mi>
                    </mml:munderover>
                    <mml:msub>
                      <mml:mi>A</mml:mi>
                      <mml:mrow>
                        <mml:mi>P</mml:mi>
                        <mml:mi>N</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mi>N</mml:mi>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>where<span style="font-family: Times New Roman, serif">, <italic><span style="font-family: Times New Roman, serif">N</italic><span style="font-family: Times New Roman, serif"> represents the target detection category, and <italic><span style="font-family: Times New Roman, serif">A<sub><span style="font-family: Times New Roman, serif">PN</sub></italic><span style="font-family: Times New Roman, serif"> represents the<italic><span style="font-family: Times New Roman, serif"> A<sub><span style="font-family: Times New Roman, serif">P</sub></italic><span style="font-family: Times New Roman, serif"> value<italic><span style="font-family: Times New Roman, serif"> </italic><span style="font-family: Times New Roman, serif">of category <italic><span style="font-family: Times New Roman, serif">N</italic><span style="font-family: Times New Roman, serif">.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Ablation experiment</title>
          
          <p><span style="font-family: Times New Roman, serif">In order to comprehensively evaluate the effectiveness and reliability of the improvement plan on the performance of the YOLOv8 model, this study adopts a strategy of selectively eliminating the improvement module to explore the specific roles played by each improvement in the optimization process. By systematically removing certain improvements and comparing the performance differences of the model before and after the removal, the contribution of each improvement to the overall detection performance can be quantified. This method not only helps to verify the actual effectiveness of improvement schemes in enhancing model performance but also enables an in-depth analysis of the specific roles of various optimization measures in enhancing model capabilities. Through this evaluation process, the reliability of the improvement plan can be effectively judged, ensuring that the proposed optimization measures have scientific and practical value and providing a theoretical basis for further improvement in the future. The equipment parameters used in the experiment are completely consistent, and the experimental results are shown in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Comparison of the ablation experimental performance</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Models</p></td><td colspan="1" rowspan="1"><p>AIFC-SFPM</p></td><td colspan="1" rowspan="1"><p>MSDown</p></td><td colspan="1" rowspan="1"><p>P/%</p></td><td colspan="1" rowspan="1"><p>R/%</p></td><td colspan="1" rowspan="1"><p>mAP/%</p></td><td colspan="1" rowspan="1"><p>Parameters</p></td><td colspan="1" rowspan="1"><p>GFLOPs</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov8n</p></td><td colspan="1" rowspan="1"><p>×</p></td><td colspan="1" rowspan="1"><p>×</p></td><td colspan="1" rowspan="1"><p>74.7</p></td><td colspan="1" rowspan="1"><p>61.3</p></td><td colspan="1" rowspan="1"><p>72.5</p></td><td colspan="1" rowspan="1"><p>3151888</p></td><td colspan="1" rowspan="1"><p>8.7</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov8-A</p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>×</p></td><td colspan="1" rowspan="1"><p>75.2</p></td><td colspan="1" rowspan="1"><p>65.6</p></td><td colspan="1" rowspan="1"><p>73.5</p></td><td colspan="1" rowspan="1"><p>3079456</p></td><td colspan="1" rowspan="1"><p>8.6</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov8n-M</p></td><td colspan="1" rowspan="1"><p>×</p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>76.8</p></td><td colspan="1" rowspan="1"><p>66.3</p></td><td colspan="1" rowspan="1"><p>73.9</p></td><td colspan="1" rowspan="1"><p>2739232</p></td><td colspan="1" rowspan="1"><p>8.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov8n-AM</p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>80.5</p></td><td colspan="1" rowspan="1"><p>62.9</p></td><td colspan="1" rowspan="1"><p>74.2</p></td><td colspan="1" rowspan="1"><p>2666784</p></td><td colspan="1" rowspan="1"><p>7.9</p></td></tr></tbody></table>
            </table-wrap>
          
          <p><span style="font-family: Times New Roman, serif">The study verified the contribution mechanism of each improved module to the model performance through modular ablation experiments. The experimental results in <xref ref-type="table" rid="table_2">Table 2</xref> show that AIFC-SFPM has built global semantic modeling capability on high-level features <italic><span style="font-family: Times New Roman, serif">S</italic><span style="font-family: Times New Roman, serif">₅ by introducing the Transformer self-attention mechanism. This module enables mAP@0.5 to have an increase of 1.0%, which is mainly attributed to the effective capture of smoke diffusion characteristics. In addition, the single-scale interaction strategy of the AIFI module reduces computational redundancy by 2.3%, verifying its balanced design between semantic enhancement and computational efficiency.</p><p><span style="font-family: Times New Roman, serif">The MSDown module achieves lightweight optimization while maintaining feature diversity through a multi-branch downsampling architecture. This module uses the depthwise separable convolution to reduce GFLOPs by 6.8%, and its multi-scale feature fusion strategy effectively preserves the edge details of low-contrast smoke. The feature map processed by MSDown shows a 14.7% improvement in the gradient amplitude index, confirming its enhancement effect on weak texture features.</p><p><span style="font-family: Times New Roman, serif">Yolov8n-AM achieves a synergistic optimization of semantic enhancement and computational efficiency through the cascade design of AIFC-SFPM and MSDown. The combination of the two modules reduces the total parameter count by 15.4% and GFLOPs by 9.1%.</p><p><span style="font-family: Times New Roman, serif">As shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>, the precision, recall, and mAP curves of YOLOv8n-AM tend to stabilize after 100 epochs, with smaller fluctuations than YOLOv8n, YOLOv8-A, and other models, indicating better training stability and a more balanced parameter optimization process. The sharp fluctuations in the precision curve in the early stages reflect the model's insufficient ability to distinguish difficult samples such as occluded targets, which can lead to false or missed detections in the early-stage training. Overall, YOLOv8n-AM outperforms the comparison models in terms of reliability due to its more stable training curves, but its performance bottleneck is also evident, i.e., the detection ability for extreme scenarios such as highly overlapping targets needs to be further improved.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Comparison of different indicators</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_DCO8KNoyWkMaCCwt.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.5. Comparative experiment</title>
          
          <p><span style="font-family: Times New Roman, serif">To further validate the effectiveness and generalization ability of the Yolov8n-AM model, the dataset was kept unchanged and compared with multiple advanced object detection models. The results are shown in <xref ref-type="table" rid="table_3">Table 3</xref>.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Performance comparison with other models</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Models</p></td><td colspan="1" rowspan="1"><p>Parameters</p></td><td colspan="1" rowspan="1"><p>GFLOPs</p></td><td colspan="1" rowspan="1"><p>P/%</p></td><td colspan="1" rowspan="1"><p>R/%</p></td><td colspan="1" rowspan="1"><p>mAP/%</p></td></tr><tr><td colspan="1" rowspan="1"><p>RT-DETR-r18</p></td><td colspan="1" rowspan="1"><p>19873044</p></td><td colspan="1" rowspan="1"><p>56.9</p></td><td colspan="1" rowspan="1"><p>78.8</p></td><td colspan="1" rowspan="1"><p>63.1</p></td><td colspan="1" rowspan="1"><p>71.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov5n</p></td><td colspan="1" rowspan="1"><p>1765270</p></td><td colspan="1" rowspan="1"><p>4.2</p></td><td colspan="1" rowspan="1"><p>75.2</p></td><td colspan="1" rowspan="1"><p>69.3</p></td><td colspan="1" rowspan="1"><p>69</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov7</p></td><td colspan="1" rowspan="1"><p>37196556</p></td><td colspan="1" rowspan="1"><p>105.1</p></td><td colspan="1" rowspan="1"><p>77.2</p></td><td colspan="1" rowspan="1"><p>68.7</p></td><td colspan="1" rowspan="1"><p>72.6</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov8n</p></td><td colspan="1" rowspan="1"><p>3151888</p></td><td colspan="1" rowspan="1"><p>8.7</p></td><td colspan="1" rowspan="1"><p>76.8</p></td><td colspan="1" rowspan="1"><p>92</p></td><td colspan="1" rowspan="1"><p>72.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov10n</p></td><td colspan="1" rowspan="1"><p>2694806</p></td><td colspan="1" rowspan="1"><p>8.2</p></td><td colspan="1" rowspan="1"><p>82</p></td><td colspan="1" rowspan="1"><p>63.1</p></td><td colspan="1" rowspan="1"><p>73.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>Swin Transformer-T</p></td><td colspan="1" rowspan="1"><p>2850983</p></td><td colspan="1" rowspan="1"><p>145</p></td><td colspan="1" rowspan="1"><p>78.2</p></td><td colspan="1" rowspan="1"><p>59.7</p></td><td colspan="1" rowspan="1"><p>72.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>EfficientDet-D0</p></td><td colspan="1" rowspan="1"><p>498359</p></td><td colspan="1" rowspan="1"><p>12</p></td><td colspan="1" rowspan="1"><p>77.6</p></td><td colspan="1" rowspan="1"><p>63.8</p></td><td colspan="1" rowspan="1"><p>71.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yolov8n-AM</p></td><td colspan="1" rowspan="1"><p>2666784</p></td><td colspan="1" rowspan="1"><p>7.9</p></td><td colspan="1" rowspan="1"><p>80.5</p></td><td colspan="1" rowspan="1"><p>62.9</p></td><td colspan="1" rowspan="1"><p>74.2</p></td></tr></tbody></table>
            </table-wrap>
          
          <p><span style="font-family: Times New Roman, serif">The YOLOv8n-AM model performs the best in smoke detection tasks, ranking first with an mAP of 74.2%, while maintaining minimal parameter and computational complexity, significantly outperforming traditional models such as YOLOv5n, YOLOv7, RT-DETR-r18 and surpassing YOLOv10n with similar parameter quantities. Compared to the pure Transformer architecture Swin Transformer-T and the lightweight model EfficientDet-D0, YOLOv8n-AM achieves breakthroughs in both accuracy and efficiency. Its advantages stem from the AIFC-SFPM's enhancement of high-level semantic features and the MSDown module's multi-scale lightweight design, making it suitable for industrial safety monitoring scenarios. However, further optimization of recall is needed to reduce missed detections.</p>
        </sec>
      
      
        <sec>
          
            <title>3.6. Test results</title>
          
          <p><span style="font-family: Times New Roman, serif"><xref ref-type="fig" rid="fig_6">Figure 6</xref> and <xref ref-type="fig" rid="fig_7">Figure 7</xref> show the detection results of smoke in the factory area under different scenarios using the original YOLOv8n network and the improved YOLOv8n-AM network, respectively. These images clearly indicate that YOLOv8n-AM surpasses the original model in detection accuracy. Specifically, it can be seen from the first column of images that in different outdoor lighting environments, YOLOv8n-AM performs better in detection accuracy and has a better effect on actual smoke localization. In the second column of images, the improved model accurately identifies smoke areas in complex indoor environments, while the original model fails to accurately identify targets. In the third column of images, at the junction of smoke and clouds, the original model fails to accurately identify the target, while the YOLOv8n-AM model performs better. Overall, YOLOv8n-AM demonstrates excellent detection capabilities for smoke targets in complex environments, making it suitable for detection tasks in monitoring chemical scenes.</p>
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Detection results of the YOLOv8n model</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_Q6zcfaV0X0LQCMdl.png"/>
            </fig>
          
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>Detection results of the YOLOv8n-AM model</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_M6elbn7fL7fYAIl8.png"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Conclusion</title>
      <p><span style="font-family: Times New Roman, serif">YOLOv8n-AM, an improved smoke detection model, was proposed in this study by integrating AIFC-SFPM and MSDown. The experimental results demonstrate that replacing the traditional SPPF module with AIFC-SFPM enhances the semantic representation capability and accuracy of smoke detection, while the MSDown module effectively reduces redundancy and computational complexity, significantly accelerating the detection speed. Specifically, compared to the original YOLOv8n model, YOLOv8n-AM achieves a 1.7% increase in accuracy, a 9.1% reduction in GFLOPs, and a 15.4% decrease in parameter count, demonstrating a clear balance between performance and model efficiency.</p><p><span style="font-family: Times New Roman, serif">However, certain limitations remain in this study, which need to be addressed in future research. The detection performance of YOLOv8n-AM under extreme environmental conditions, such as severe illumination variations, heavy occlusions, or highly complex background scenarios, requires further investigation. Additionally, detecting smaller or partially occluded smoke targets still presents challenges. Future research should explore advanced background suppression techniques and integrate context-aware mechanisms to further enhance detection robustness under complex conditions. It is also essential to validate the model's effectiveness across a broader range of industrial environments and on low-power, edge-computing platforms.</p><p><span style="font-family: Times New Roman, serif">In conclusion, the proposed YOLOv8n-AM model demonstrates significant potential for real-time smoke detection tasks, particularly in safety monitoring of chemical plants, contributing to enhanced industrial safety and environmental protection.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>303</page-range>
          <issue>2</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lee</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Shim</surname>
              <given-names>Y. S.</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>Y. G.</given-names>
            </name>
            <name>
              <surname>Han</surname>
              <given-names>S. D.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>Y. S.</given-names>
            </name>
            <name>
              <surname>Kang</surname>
              <given-names>C. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s17020303</pub-id>
          <article-title>Highly sensitive sensors based on metal-oxiden anocolumns for fire detection</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>18</volume>
          <page-range>553</page-range>
          <issue>2</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Fonollosa</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Solórzano</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Marco</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s18020553</pub-id>
          <article-title>Chemical sensor systems and associated algorithms for fire detection: A review</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>53</volume>
          <page-range>1943-1960</page-range>
          <issue>6</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pundir</surname>
              <given-names>A. S.</given-names>
            </name>
            <name>
              <surname>Raman</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10694-017-0665-z</pub-id>
          <article-title>Deep belief network for smoke detection</article-title>
          <source>Fire Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1234-1243</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>A. Li</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.23919/ChiCC.2017.8028197</pub-id>
          <article-title>Early fire recognition based on multi-feature fusion of video smoke</article-title>
          <source>2017 36th Chinese Control Conference (CCC), Dalian, China</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>1-27</page-range>
          <issue>3</issue>
          <year>2011</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/1961189.1961199</pub-id>
          <article-title>LIBSVM: A library for support vector machines</article-title>
          <source>ACM Trans. Intell. Syst. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>126</volume>
          <page-range>2121-2124</page-range>
          <issue>19</issue>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Gu</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ijleo.2015.05.082</pub-id>
          <article-title>Early smoke detection of forest fire video using CS Adaboost algorithm</article-title>
          <source>Optik - Int. J. Light Ele. Optics</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>93</volume>
          <page-range>337-349</page-range>
          <issue>1-2</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yuan</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10846-018-0803-y</pub-id>
          <article-title>Learning-based smoke detection for unmanned aerial vehicles applied to forest fire surveillance</article-title>
          <source>J. Intell. Robot Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>52</volume>
          <page-range>1271-1292</page-range>
          <issue>5</issue>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jia</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Fang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10694-014-0453-y</pub-id>
          <article-title>A Saliency-Based Method for Early Smoke Detection in Video Sequences</article-title>
          <source>Fire Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="conf-paper">
          <page-range>779-788</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Redmon</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Divvala</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Girshick</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Farhadi</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/cvpr.2016.91</pub-id>
          <article-title>You Only Look Once: Unified, Real-Time Object Detection</article-title>
          <source>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>39</volume>
          <page-range>1137-1149</page-range>
          <issue>6</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ren</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Girshick</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TPAMI.2016.257703</pub-id>
          <article-title>Faster R-CNN: Towards real-time object detection with region proposal networks</article-title>
          <source>IEEE Trans. Pattern Ana. Mach. Intelli.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="conf-paper">
          <page-range>11–14</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Anguelov</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Erhan</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Szegedy</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Reed</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>C. Y.</given-names>
            </name>
            <name>
              <surname>Berg</surname>
              <given-names>A. C.</given-names>
            </name>
          </person-group>
          <article-title>Ssd: Single shot multibox detector</article-title>
          <source>Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>2879-2889</page-range>
          <issue>4</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zeng</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1049/iet-ipr.2019.1315</pub-id>
          <article-title>Target segmentation of industrial smoke image based on LBP Silhouettes coefficient variant (LBPSCV) algorithm</article-title>
          <source>IET Image Proce.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>434</volume>
          <page-range>224-238</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>He</surname>
              <given-names>Lijun</given-names>
            </name>
            <name>
              <surname>Gong</surname>
              <given-names>Xiaoli</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Sirou</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Liejun</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Fan</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neucom.2021.01.024</pub-id>
          <article-title>Efficient attention based deep fusion CNN for smoke detection in fog environment</article-title>
          <source>Neurocomputing</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>4354</page-range>
          <issue>22</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Abdusalomov</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Umirzakova</surname>
              <given-names>S</given-names>
            </name>
            <name>
              <surname>Safarov</surname>
              <given-names>F</given-names>
            </name>
            <name>
              <surname>Mirzakhalilov</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Egamberdiev</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Cho</surname>
              <given-names>Y.I.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/electronics13224354</pub-id>
          <article-title>A multi-scale approach to early fire detection in smart homes</article-title>
          <source>Electronics</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <page-range>115125</page-range>
          <issue>182</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Khan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Muhammad</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Hussain</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Del Ser</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Cuzzolin</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Bhattacharyya</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Akhtar</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>de Albuquerque</surname>
              <given-names>A. H. C.</given-names>
            </name>
          </person-group>
          <article-title>Deepsmoke: Deep learning model for smoke detection and segmentation in outdoor environments</article-title>
          <source>Expert Systems with Applications</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>40</page-range>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>M. Masoom S.</surname>
              <given-names>M. S.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Dai</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Jia</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/fire5020040</pub-id>
          <article-title>Early smoke detection based on improved YOLO-PCA network</article-title>
          <source>Fire</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>58</volume>
          <page-range>1445-1468</page-range>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Huo</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Jia</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Guan</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10694-021-01199-7</pub-id>
          <article-title>A deep separable convolutional neural network for multiscale image-based smoke detection</article-title>
          <source>Fire Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>32</volume>
          <page-range>1921-1927</page-range>
          <issue>5</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sun</surname>
              <given-names>Xiaofang</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>Liping</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Yinglai</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11676-020-01230-7</pub-id>
          <article-title>Forest fire smoke recognition based on convolutional neural network</article-title>
          <source>Journal of Forestry Research</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>1190</page-range>
          <issue>7</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/math10071190</pub-id>
          <article-title>A smoke detection model based on improved YOLOv5</article-title>
          <source>Mathematics</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jocher</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Stoken</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Borovec</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Chaurasia</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>et al.</surname>
            </name>
          </person-group>
          <article-title>ultralytics/yolov5: v5.0-YOLOv5-P6 1280 models, AWS, Supervise</article-title>
          <source>ly and YouTube Integrations</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>30</volume>
          <page-range>215</page-range>
          <issue>4</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Qian</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Chai</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00530-024-01359-z</pub-id>
          <article-title>Fs-yolo: fire-smoke detection based on improved YOLOv7</article-title>
          <source>Fire</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>3790</page-range>
          <issue>15</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Cheng</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Jiao</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Bai</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/rs15153790</pub-id>
          <article-title>LMDFS: A lightweight model for detecting forest fire smoke in UAV images based on YOLOv7</article-title>
          <source>Remote Sensing</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>409</page-range>
          <issue>3</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Y</given-names>
            </name>
            <name>
              <surname>Piao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/f15030409</pub-id>
          <article-title>An improved forest smoke detection model based on YOLOv8</article-title>
          <source>Forests</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>35</volume>
          <page-range>20939-20954</page-range>
          <issue>28</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Fatma  Talaat</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>ZainEldin</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00521-023-08809-1</pub-id>
          <article-title>An improved fire detection approach based on YOLO-v8 for smart cities</article-title>
          <source>Neural Computing and Applications</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <page-range>1-21</page-range>
          <issue>33</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Dosovitskiy</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>Beyer</surname>
              <given-names>L</given-names>
            </name>
            <name>
              <surname>Kolesnikov</surname>
              <given-names>A</given-names>
            </name>
            <name>
              <surname>et al.</surname>
            </name>
          </person-group>
          <article-title>An image is worth 16x16 words: Transformers for image recognition at scale</article-title>
          <source>Advances in Neural Information Processing Systems</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>