<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-QhGeyUrvm35KPlFE5o6btTz6xmsZcFcq</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml050104</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Transformer-Driven Feature Fusion for Robust Diagnosis of Lung Cancer Brain Metastasis Under Missing-Modality Scenarios</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0006-0987-2579</contrib-id>
          <name>
            <surname>Ding</surname>
            <given-names>Yue</given-names>
          </name>
          <email>yueding399@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-1410-169X</contrib-id>
          <name>
            <surname>Ma</surname>
            <given-names>Yunqi</given-names>
          </name>
          <email>mayunqi2025@163.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0002-7606-1152</contrib-id>
          <name>
            <surname>Jing</surname>
            <given-names>Kuo</given-names>
          </name>
          <email>on1ywork@foxmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-0497-3975</contrib-id>
          <name>
            <surname>Shang</surname>
            <given-names>Zhansong</given-names>
          </name>
          <email>484698977@qq.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-1210-2054</contrib-id>
          <name>
            <surname>Gao</surname>
            <given-names>Feiyang</given-names>
          </name>
          <email>2146881346@qq.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-1379-9613</contrib-id>
          <name>
            <surname>Cui</surname>
            <given-names>Zhengwei</given-names>
          </name>
          <email>13721403327@163.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1,2,3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5753-0120</contrib-id>
          <name>
            <surname>Xue</surname>
            <given-names>Linyan</given-names>
          </name>
          <email>lyxue@hbu.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1,2,3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9552-1364</contrib-id>
          <name>
            <surname>Liu</surname>
            <given-names>Shuang</given-names>
          </name>
          <email>wsg6813@163.com</email>
        </contrib>
        <aff id="aff_1">College of Quality and Technical Supervision, Hebei University, 071002 Baoding, China</aff>
        <aff id="aff_2">Hebei Technology Innovation Center for Lightweight of New Energy Vehicle Power System, 071002 Baoding, China</aff>
        <aff id="aff_3">National &amp; Local Joint Engineering Research Center of Metrology Instrument and System, Hebei University, 071002 Baoding, China</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>05</day>
        <month>02</month>
        <year>2026</year>
      </pub-date>
      <volume>5</volume>
      <issue>1</issue>
      <fpage>32</fpage>
      <lpage>43</lpage>
      <page-range>32-43</page-range>
      <history>
        <date date-type="received">
          <day>19</day>
          <month>11</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>26</day>
          <month>01</month>
          <year>2026</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2026 by the author(s)</copyright-statement>
        <copyright-year>2026</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Accurate diagnosis of lung cancer brain metastasis is often hindered by incomplete magnetic resonance imaging (MRI) modalities, resulting in suboptimal utilization of complementary radiological information. To address the challenge of ineffective feature integration in missing-modality scenarios, a Transformer-based multi-modal feature fusion framework, referred to as Missing Modality Transformer (MMT), was introduced. In this study, multi-modal MRI data from 279 individuals diagnosed with lung cancer brain metastasis, including both small cell lung cancer (SCLC) and non-small cell lung cancer (NSCLC), were acquired and processed through a standardized radiomics pipeline encompassing feature extraction, feature selection, and controlled data augmentation. The proposed MMT framework was trained and evaluated under various single-modality and combined-modality configurations to assess its robustness to modality absence. A maximum diagnostic accuracy of 0.905 was achieved under single-modality missing conditions, exceeding the performance of the full-modality baseline by 0.017. Interpretability was further strengthened through systematic analysis of loss-function hyperparameters and quantitative assessments of modality-specific importance. The experimental findings collectively indicate that the MMT framework provides a reliable and clinically meaningful solution for diagnostic environments in which imaging acquisition is limited by patient conditions, equipment availability, or time constraints. These results highlight the potential of Transformer-based radiomics fusion to advance computational neuro-oncology by improving diagnostic performance, enhancing robustness to real-world imaging variability, and offering transparent interpretability that aligns with clinical decision-support requirements.</p></abstract>
      <kwd-group>
        <kwd>Lung cancer brain metastasis</kwd>
        <kwd>Radiomics</kwd>
        <kwd>Multi-modal</kwd>
        <kwd>Modality missing</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="8"/>
        <fig-count count="8"/>
        <table-count count="3"/>
        <ref-count count="22"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Lung cancer is one of the most prevalent malignancies globally and a leading cause of cancer-related mortality [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>]. This malignant tumor originates from cancerous cells in lung tissue. Pathologically and therapeutically, it is classified into two main categories: small cell lung cancer (SCLC) and non-small cell lung cancer (NSCLC). The latter encompasses various histological subtypes, including adenocarcinoma and squamous cell carcinoma, while the former constitutes the remaining category [<xref ref-type="bibr" rid="ref_3">3</xref>]. SCLC accounts for approximately 15% of lung cancer cases, whereas NSCLC makes up 85% [<xref ref-type="bibr" rid="ref_4">4</xref>], [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>]. Over half of newly diagnosed lung cancer patients present with advanced or metastatic disease [<xref ref-type="bibr" rid="ref_7">7</xref>], with 10%–26% exhibiting brain metastases at diagnosis and another 30% developing them during disease progression [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>]. Due to the absence of distinct early symptoms, lung cancer is often detected in advanced stages when metastasis has already spread, making treatment and management particularly challenging.</p><p>With the rapid advancement of artificial intelligence, machine learning methods have become crucial in diagnosing brain metastases from lung cancer. Radiomics is widely employed to extract high-throughput features from medical images, significantly enhancing the interpretability of image-assisted diagnosis [<xref ref-type="bibr" rid="ref_11">11</xref>], [<xref ref-type="bibr" rid="ref_12">12</xref>], [<xref ref-type="bibr" rid="ref_13">13</xref>]. For instance, Xu et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] utilized radiomics to extract features from preoperative chest CT scans to predict anaplastic lymphoma kinase (ALK) gene molecular signatures. However, while traditional machine learning offers strong interpretability, it often struggles with complex nonlinear problems inherent in multi-modal MRI data. Deep learning, a subset of machine learning, excels at processing such intricate relationships [<xref ref-type="bibr" rid="ref_15">15</xref>], [<xref ref-type="bibr" rid="ref_16">16</xref>]. For example, Guo et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] developed a multi-modal MRI decision fusion network for glioma classification, and Li et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] proposed a Hybrid Graph Convolutional Network (HGCN) for cancer survival prediction. Despite these breakthroughs, existing methods often lack robust diagnostic capabilities when specific modalities are missing.</p><p>A dataset of lung cancer brain metastasis patients from the Affiliated Hospital of Hebei University was utilized in this study. Using radiomics, high-throughput feature information was extracted from MRI images [<xref ref-type="bibr" rid="ref_19">19</xref>]. Based on the Transformer architecture, a network was designed for missing-modality scenarios. It employs a modal interaction self-attention mechanism to learn inter-modal correlations and generate multi-level feature representations for missing modalities. This approach enables effective reconstruction and maintains diagnostic accuracy even when modalities are incomplete.</p>
    </sec>
    <sec sec-type="">
      <title>2. Methodology</title>
      <p>Aiming to address the critical issue of ineffective feature fusion in the diagnosis of brain metastases from lung cancer when MRI image modalities are missing, a Transformer-based feature fusion network called Missing Modality Transformer (MMT) was proposed in this study to achieve effective feature fusion.</p>
      
        <sec>
          
            <title>2.1. Multimodal fusion network—variational autoencoder model</title>
          
          <p>Multimodal Fusion Network—Variational Autoencoder (MFN-VAE) is a Variational Autoencoder (VAE) model designed to integrate multi-modal features. It processes multi-modal data by mapping them into a unified latent space. By learning correlations between different modalities, MFN-VAE achieves deep feature integration. The latent representations learned by MFN-VAE serve as robust features for downstream tasks, significantly improving performance. The model consists of three components: an encoder, a decoder, and a predictor. In the lung cancer brain metastasis prediction task, the dataset includes three modalities: T1-weighted imaging (T1WI), fluid-attenuated inversion recovery (FLAIR), and diffusion-weighted imaging (DWI). Each modality is processed through the VAE to learn latent representations, which are then fed into the predictor to forecast brain metastasis outcomes.</p>
        </sec>
      
      
        <sec>
          
            <title>2.2. Channel attention module</title>
          
          <p>Self-attention mechanisms are a cornerstone of Transformer models and a key factor behind their outstanding performance. These mechanisms determine element importance by comparing the similarity between positions in a sequence. Specifically, through self-attention calculations, each position generates an attention weight vector reflecting its relative importance. This adaptive allocation of attention enables the model to process sequence data efficiently. The theory revolves around three core components: Query (Q), Key (K), and Value (V). In Transformer models, input sequences are encoded and mapped to these three embeddings. A classic implementation is the Scaled Dot-Product Attention (SDPA) mechanism, as illustrated in <xref ref-type="fig" rid="fig_1">Figure 1</xref>.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>Scaled Dot-Product Attention (SDPA) mechanism</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_GvCPars7a4cM9vMO.png"/>
            </fig>
          
          <p>The SDPA mechanism involves performing dot-product operations on $Q<inline-formula>
  <mml:math id="md318ppp1a">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>K<inline-formula>
  <mml:math id="m5x4o5wap7">
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>0</mml:mn>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula>V$ vectors to generate the context representation for each position. This process effectively integrates information from other positions into the current position’s representation. The specific calculation formula is as follows:</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="myhu4j2w33">
                <mml:mi>Attention</mml:mi>
                <mml:mi>Q</mml:mi>
                <mml:mi>K</mml:mi>
                <mml:mi>V</mml:mi>
                <mml:mi>SoftMax</mml:mi>
                <mml:mi>V</mml:mi>
                <mml:mo>(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mi>Q</mml:mi>
                      <mml:msup>
                        <mml:mi>K</mml:mi>
                        <mml:mi>T</mml:mi>
                      </mml:msup>
                    </mml:mrow>
                    <mml:msqrt>
                      <mml:msub>
                        <mml:mi>d</mml:mi>
                        <mml:mi>k</mml:mi>
                      </mml:msub>
                    </mml:msqrt>
                  </mml:mfrac>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mgnxhv90ce">
    <mml:msub>
      <mml:mi>d</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> denotes the dimension of the $K<inline-formula>
  <mml:math id="m08obkzz4p">
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>Q<inline-formula>
  <mml:math id="msbgtbz5kr">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>K$ pairs, facilitating efficient batch processing. The SDPA mechanism is favored for its computational simplicity and efficiency, allowing it to effectively capture both local and global dependencies within sequences, which explains its widespread adoption in Transformer models.</p>
        </sec>
      
      
        <sec>
          
            <title>2.3. Modal interaction self-attention</title>
          
          <p>Modal interaction self-attention is the core of MMT’s design, enhancing the interaction between different modalities. This enables MMT to maintain high performance even when certain modalities are missing by leveraging existing modalities to generate feature information related to the missing ones. Taking MMT-2M as an example, the four inputs to the modal interaction self-attention mechanism consist of Modal 1 features, Modal 2 features, the Missing Modality Token, and the Classification Token. These inputs first undergo standardization through a Layer Normalization (LayerNorm) layer. Subsequently, the features of Modal 1, Modal 2, and the Missing Modality Token are mapped to a larger feature space and decomposed into <inline-formula>
  <mml:math id="m1eu5gny43">
    <mml:mi>Q</mml:mi>
    <mml:mi>K</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>, and $V<inline-formula>
  <mml:math id="msxbtr1di0">
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>Q$ component is retained.</p>
          <p>Regarding the strategy for cross-modal interaction, the proposed design principle focuses on enhancing interactions between the tokens of Modal 1, Modal 2, and the missing modality, while the Classification Token only interacts with the missing modality. Technically, $K<inline-formula>
  <mml:math id="mlvgfx46zh">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>V<inline-formula>
  <mml:math id="me2x8mfw0s">
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>Q<inline-formula>
  <mml:math id="memezlhdsd">
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>K<inline-formula>
  <mml:math id="muz6zhh5w3">
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>H</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
  </mml:math>
</inline-formula>Q<inline-formula>
  <mml:math id="myd0ln7rbp">
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>H</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>V<inline-formula>
  <mml:math id="m0fuq7gpjm">
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
  </mml:math>
</inline-formula>K<inline-formula>
  <mml:math id="mbij62az07">
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>Q<inline-formula>
  <mml:math id="mkh72bzz44">
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>K<inline-formula>
  <mml:math id="m0khfhujvm">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>V<inline-formula>
  <mml:math id="mdxoybxy45">
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>Q<inline-formula>
  <mml:math id="m87iv8yoxx">
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>Q<inline-formula>
  <mml:math id="mn795b9946">
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>Q$ vectors are ultimately combined with the pre-differentiated vectors for output. After passing through two modal interaction attention layers, the four tensors are concatenated and fed into the Multi-Layer Perceptron (MLP). <xref ref-type="fig" rid="fig_2">Figure 2</xref> and <xref ref-type="fig" rid="fig_3">Figure 3</xref> illustrate the architectures of the FNN and the MLP.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Feedforward Neural Network (FNN) structure</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_O5BBiDPOkATfXsr6.png"/>
            </fig>
          
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Multi-Layer Perceptron (MLP) network structure</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_JFN8Pc9cBdzXSsWv.png"/>
            </fig>
          
          <p>An FNN consists of two fully connected layers, each followed by a LayerNorm layer and a Dropout layer. As a widely adopted regularization technique, the Dropout layer randomly discards a portion of neurons during training to mitigate overfitting risks. The MLP consists of three fully connected layers. The first layer is preceded by a LayerNorm layer, followed by another LayerNorm layer and a Gaussian Error Linear Unit (GELU) activation function. The second fully connected layer also includes a LayerNorm layer and a GELU activation function. The third fully connected layer is terminated by a SoftMax layer.</p>
        </sec>
      
      
        <sec>
          
            <title>2.4. Loss function</title>
          
          <p>The loss function of MMT consists of two components: reconstruction loss and classification loss. The reconstruction loss employs the Mean Absolute Error (MAE) loss. This function calculates the absolute difference between reconstructed and original features, averaging these differences to determine the final loss value. Compared to Mean Squared Error (MSE) loss, MAE demonstrates greater robustness by being less sensitive to outliers. Minimizing MAE helps the model generate reconstructed features that closely resemble the original data. </p><p>The formula is as follows:</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="mbwfq7sf2e">
                <mml:msub>
                  <mml:mi>L</mml:mi>
                  <mml:mrow>
                    <mml:mtext>mae </mml:mtext>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mn>1</mml:mn>
                  <mml:mi>m</mml:mi>
                </mml:mfrac>
                <mml:munderover>
                  <mml:mo>∑</mml:mo>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                  <mml:mi>m</mml:mi>
                </mml:munderover>
                <mml:mrow>
                  <mml:mo>|</mml:mo>
                  <mml:mo>−</mml:mo>
                  <mml:mo>|</mml:mo>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mover>
                        <mml:mi>x</mml:mi>
                        <mml:mo>^</mml:mo>
                      </mml:mover>
                    </mml:mrow>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mzatot506k">
    <mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mlhjyyxg4q">
    <mml:msub>
      <mml:mrow>
        <mml:mover>
          <mml:mi>x</mml:mi>
          <mml:mo>^</mml:mo>
        </mml:mover>
      </mml:mrow>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> represent the radiomics feature and reconstructed feature of the $i<inline-formula>
  <mml:math id="mb7xuoocda">
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
  </mml:math>
</inline-formula>m$ being the number of samples.</p><p>The classification loss employs the cross entropy loss function, which quantifies the discrepancy between predicted probabilities and actual labels. When predictions perfectly match the labels, the loss is zero; otherwise, it increases. Minimizing this loss aligns the model’s predictions with the true labels. The formula is:</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="mingccjw9l">
                <mml:msub>
                  <mml:mi>L</mml:mi>
                  <mml:mrow>
                    <mml:mi>C</mml:mi>
                    <mml:mi>E</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>∑</mml:mo>
                <mml:mrow>
                  <mml:mo>[</mml:mo>
                  <mml:mo>⁡</mml:mo>
                  <mml:mo>⁡</mml:mo>
                  <mml:mo>]</mml:mo>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mover>
                            <mml:mi>y</mml:mi>
                            <mml:mo>^</mml:mo>
                          </mml:mover>
                        </mml:mrow>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>+</mml:mo>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mi>log</mml:mi>
                  <mml:mi>log</mml:mi>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mn>1</mml:mn>
                    <mml:msub>
                      <mml:mi>y</mml:mi>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>(</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mn>1</mml:mn>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mover>
                          <mml:mi>y</mml:mi>
                          <mml:mo>^</mml:mo>
                        </mml:mover>
                      </mml:mrow>
                      <mml:mi>i</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="m1ccmmlvfn">
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="moidr13fdp">
    <mml:msub>
      <mml:mrow>
        <mml:mover>
          <mml:mi>y</mml:mi>
          <mml:mo>^</mml:mo>
        </mml:mover>
      </mml:mrow>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> represent the true label and predicted label of the $i$-th sample.</p><p>The reconstruction loss and classification loss are then weighted and summed. Due to structural differences between the MMT-2M and MMT-1M models, their loss functions vary slightly, defined by Eq. (4) and Eq. (5):</p>
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="mg58o6kus5">
                <mml:msub>
                  <mml:mi>L</mml:mi>
                  <mml:mrow>
                    <mml:mi>M</mml:mi>
                    <mml:mi>M</mml:mi>
                    <mml:mi>T</mml:mi>
                    <mml:mi>M</mml:mi>
                    <mml:mo>−</mml:mo>
                    <mml:mn>2</mml:mn>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>L</mml:mi>
                  <mml:mrow>
                    <mml:mi>m</mml:mi>
                    <mml:mi>a</mml:mi>
                    <mml:mi>e</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>L</mml:mi>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mi>e</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mi>α</mml:mi>
                <mml:mi>β</mml:mi>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="mzn8jrreis">
                <mml:msub>
                  <mml:mi>L</mml:mi>
                  <mml:mrow>
                    <mml:mi>M</mml:mi>
                    <mml:mi>M</mml:mi>
                    <mml:mi>T</mml:mi>
                    <mml:mi>M</mml:mi>
                    <mml:mo>−</mml:mo>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>L</mml:mi>
                  <mml:mrow>
                    <mml:mi>m</mml:mi>
                    <mml:mi>a</mml:mi>
                    <mml:mi>e</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>L</mml:mi>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mi>e</mml:mi>
                    <mml:mn>1</mml:mn>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>L</mml:mi>
                  <mml:mrow>
                    <mml:mi>c</mml:mi>
                    <mml:mi>e</mml:mi>
                    <mml:mn>2</mml:mn>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mi>α</mml:mi>
                <mml:mi>β</mml:mi>
                <mml:mi>γ</mml:mi>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mvhw1i2mj5">
    <mml:mi>α</mml:mi>
    <mml:mi>β</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="mts6kttoqi">
    <mml:mi>γ</mml:mi>
  </mml:math>
</inline-formula> are the respective weight parameters.</p>
        </sec>
      
      
        <sec>
          
            <title>2.5. Missing modality transformer model</title>
          
          <p>The MMT model is a Transformer-based classification network designed for missing-modality scenarios. As shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>, two MMT architectures were developed: MMT-2M for single-modality missing and MMT-1M for dual-modality missing. Its core principle is to enhance the model’s inter-modal interaction capabilities, enabling the reconstruction of missing modalities.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Missing Modality Transformer (MMT) network structure</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_8w9NNQ868fgFiBPC.png"/>
            </fig>
          
          <p>Taking MMT-2M as an example, it features four inputs: Modal 1 features, Modal 2 features, a Missing Modality Token, and a Classification Token. The Classification Token is a trainable neural network weight parameter updated through backpropagation, facilitating feature reconstruction and classification. The features from Modal 1 and Modal 2 are linearly mapped to a new space via a fully connected layer, then fed into the modal interaction self-attention mechanism. This layer enhances the network’s feature learning capacity. Since tokens are weight parameters following a normal distribution and contain no prior feature information, they are directly input into the self-attention mechanism.</p><p>After two modal interactions, the missing modal features regenerated by the tokens are output. These are then input into the fully connected neural network alongside Modal 1, Modal 2, and the features generated by the Classification Token to output classification results. At its core, MMT is a classification model that utilizes modal interaction self-attention and reconstruction loss to generate missing modal features via inter-modal connections. This capability not only allows MMT to reconstruct missing modalities but, more importantly, enables it to learn deeper-level feature representations. The reconstruction process serves solely as a means to enhance the network’s classification performance rather than focusing on reconstruction quality itself.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Experimental research</title>
      
        <sec>
          
            <title>3.1. Experiment environment and workflow</title>
          
          <p>This study constructed a multi-modal MRI dataset encompassing both SCLC and NSCLC brain metastases, which was trained and tested using the MMT network model. The MMT experiments were conducted on an Ubuntu workstation equipped with an NVIDIA GeForce RTX 2080 Ti GPU and Intel® Xeon® Gold 6240 CPU. Training parameters were configured as follows: batch size set to 32, loss weight range between 0.1 and 1, and learning rate range between $1 \times 10^{-6}<inline-formula>
  <mml:math id="mfs38afka7">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>1 \times 10^{-5}$. The AdamW optimizer was employed during training. As the MMT network is built on the Transformer architecture, it required substantial memory and computational resources due to its large parameter scale. A total of 200 training epochs were completed with hyperparameter tuning strategies to ensure model convergence and prevent underfitting.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Data collection</title>
          
          <p>The dataset, sourced from the Affiliated Hospital of Hebei University, comprises multi-modal MRI images of 279 lung cancer patients with brain metastases. It includes six subtypes: SCLC (100 cases), adenocarcinoma (153 cases), squamous cell carcinoma (17 cases), adenosquamous carcinoma (2 cases), large cell lung cancer (6 cases), and other brain metastases (1 case). Each patient received T1WI, FLAIR, and DWI scans. In clinical practice, specialists classify these subtypes into SCLC (100 cases) and NSCLC (179 cases), as shown in <xref ref-type="table" rid="table_1">Table 1</xref>. To ensure accurate tumor localization, MRI images were annotated by a senior clinical expert, enabling comprehensive analysis of multi-modal data.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Detection results of ResNet50 with different attention modules</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Disease Type</p></th><th colspan="1" rowspan="1"><p>Number of Patients</p></th><th colspan="1" rowspan="1"><p>Age (Years)</p></th><th colspan="1" rowspan="1"><p>Number of Lesions</p></th><th colspan="1" rowspan="1"><p>Gender</p></th></tr><tr><td colspan="1" rowspan="1"><p>SCLC</p></td><td colspan="1" rowspan="1"><p>100</p></td><td colspan="1" rowspan="1"><p>62.4 <mml:math id="mivdfvzzux">
  <mml:mo>±</mml:mo>
</mml:math> 9.7</p></td><td colspan="1" rowspan="1"><p>439</p></td><td colspan="1" rowspan="1"><p>72/28</p></td></tr><tr><td colspan="1" rowspan="1"><p>NSCLC</p></td><td colspan="1" rowspan="1"><p>179</p></td><td colspan="1" rowspan="1"><p>60.6 <mml:math id="mh5rficfpk">
  <mml:mo>±</mml:mo>
</mml:math> 9.5</p></td><td colspan="1" rowspan="1"><p>679</p></td><td colspan="1" rowspan="1"><p>109/180</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. Radiomics feature extraction</title>
          
          <p>Medical imaging contains not only visible structures but also latent information. This study employed radiomics technology to extract and analyze such hidden data. The Python-based open-source package PyRadiomics [<xref ref-type="bibr" rid="ref_20">20</xref>] was utilized to extract morphological, textural, and statistical features from the images. In MRI image processing, feature extraction requires preprocessing filters. Using filters prior to extraction significantly improves the accuracy and robustness of subsequent analyses. The “original” filter refers to unprocessed MRI images retaining complete original information. Wavelet filters are used for noise removal and edge detection. The Laplacian of Gaussian (LoG) filter combines Gaussian filtering with Laplacian operations to enhance edges. Other filters used include square, square root, logarithmic, exponential, gradient, and Local Binary Pattern (LBP), each serving to highlight specific intensity regions or texture details.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Data augmentation</title>
          
          <p>Data augmentation expands training datasets by generating new samples through transformations of existing data. To address data imbalance, the Borderline Synthetic Minority Oversampling Technique (Borderline SMOTE) method was employed for oversampling [<xref ref-type="bibr" rid="ref_21">21</xref>]. As an enhanced version of traditional SMOTE, Borderline SMOTE focuses on creating synthetic samples near category boundaries while avoiding the interpolation of noisy points. This approach effectively prevents the generation of inaccurate synthetic samples, thereby minimizing adverse impacts on model performance. <xref ref-type="fig" rid="fig_5">Figure 5</xref> shows the distribution of data before and after using Borderline SMOTE for data augmentation.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Distribution of data before and after using Borderline Synthetic Minority Oversampling Technique (SMOTE) for data augmentation: (a) Distribution of the original dataset; (b) Borderline minority samples (solid squares); (c) Borderline synthetic minority samples (hollow squares).</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_yODkJPO8FiriJhnO.png"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="results">
      <title>4. Results</title>
      
        <sec>
          
            <title>4.1. Evaluation indicators</title>
          
          <p>Experimental results were evaluated using five metrics: accuracy, Area Under the Curve (AUC), precision, sensitivity, and specificity. Accuracy measures the proportion of correct classifications. AUC quantifies the model’s classification performance across different thresholds. Precision indicates the accuracy of positive predictions. Sensitivity reflects the model’s ability to identify true positives, while specificity measures the capacity to distinguish true negatives. The formulas are as follows:</p>
          
            <disp-formula>
              <label>(6)</label>
              <mml:math id="mgniuncvh8">
                <mml:mtext> Precision </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(7)</label>
              <mml:math id="mjnn8w3u6i">
                <mml:mtext> Sensitivity </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(8)</label>
              <mml:math id="mrh5uxphs7">
                <mml:mtext> Specificity </mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>N</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>T</mml:mi>
                    <mml:mi>N</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>P</mml:mi>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mrz2bd5wv9">
    <mml:mi>T</mml:mi>
    <mml:mi>P</mml:mi>
  </mml:math>
</inline-formula> (True Positives) refers to correctly predicted positive samples, <inline-formula>
  <mml:math id="meyjtav6sm">
    <mml:mi>T</mml:mi>
    <mml:mi>N</mml:mi>
  </mml:math>
</inline-formula> (True Negatives) refers to correctly predicted negative samples, <inline-formula>
  <mml:math id="myfbstv5ah">
    <mml:mi>F</mml:mi>
    <mml:mi>P</mml:mi>
  </mml:math>
</inline-formula> (False Positives) refers to incorrectly predicted positive samples, and <inline-formula>
  <mml:math id="muvn9z9e4h">
    <mml:mi>F</mml:mi>
    <mml:mi>N</mml:mi>
  </mml:math>
</inline-formula> (False Negatives) refers to incorrectly predicted negative samples.</p>
        </sec>
      
      
        <sec>
          
            <title>4.2. Experimental result</title>
          
          <p>To validate MMT’s superior performance under missing modality conditions, a comparative experiment with MFN-VAE was conducted. The results are presented in <xref ref-type="table" rid="table_2">Table 2</xref> and <xref ref-type="fig" rid="fig_6">Figure 6</xref>.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Comparison between Missing Modality Transformer (MMT) under missing-modality scenarios and Multimodal Fusion Network—Variational Autoencoder (MFN-VAE) under different modality combination settings</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="2"><p>Model</p></td><td colspan="3" rowspan="1"><p>Mode</p></td><td colspan="1" rowspan="2"><p>Accuracy</p></td><td colspan="1" rowspan="2"><p>AUC</p></td><td colspan="1" rowspan="2"><p>Precision</p></td><td colspan="1" rowspan="2"><p>Sensitivity</p></td><td colspan="1" rowspan="2"><p>Specificity</p></td></tr><tr><td colspan="1" rowspan="1"><p>T</p></td><td colspan="1" rowspan="1"><p>F</p></td><td colspan="1" rowspan="1"><p>D</p></td></tr><tr><td colspan="1" rowspan="6"><p>MMT</p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>0.877 ± 0.033</p></td><td colspan="1" rowspan="1"><p>0.894 ± 0.026</p></td><td colspan="1" rowspan="1"><p>0.860 ± 0.046</p></td><td colspan="1" rowspan="1"><p>0.897 ± 0.047</p></td><td colspan="1" rowspan="1"><p>0.850 ± 0.068</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>0.863 ± 0.047</p></td><td colspan="1" rowspan="1"><p>0.867 ± 0.077</p></td><td colspan="1" rowspan="1"><p>0.858 ± 0.075</p></td><td colspan="1" rowspan="1"><p>0.883 ± 0.056</p></td><td colspan="1" rowspan="1"><p>0.837 ± 0.133</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>0.863 ± 0.037</p></td><td colspan="1" rowspan="1"><p>0.846 ± 0.070</p></td><td colspan="1" rowspan="1"><p>0.885 ± 0.038</p></td><td colspan="1" rowspan="1"><p>0.836 ± 0.044</p></td><td colspan="1" rowspan="1"><p>0.892 ± 0.033</p></td></tr><tr><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>0.905 ± 0.037</p></td><td colspan="1" rowspan="1"><p>0.899 ± 0.016</p></td><td colspan="1" rowspan="1"><p>0.926 ± 0.043</p></td><td colspan="1" rowspan="1"><p>0.883 ± 0.026</p></td><td colspan="1" rowspan="1"><p>0.923 ± 0.057</p></td></tr><tr><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>0.897 ± 0.032</p></td><td colspan="1" rowspan="1"><p>0.867 ± 0.048</p></td><td colspan="1" rowspan="1"><p>0.924 ± 0.029</p></td><td colspan="1" rowspan="1"><p>0.867 ± 0.022</p></td><td colspan="1" rowspan="1"><p>0.924 ± 0.048</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>0.897 ± 0.029</p></td><td colspan="1" rowspan="1"><p>0.874 ± 0.089</p></td><td colspan="1" rowspan="1"><p>0.940 ± 0.054</p></td><td colspan="1" rowspan="1"><p>0.847 ± 0.016</p></td><td colspan="1" rowspan="1"><p>0.942 ± 0.055</p></td></tr><tr><td colspan="1" rowspan="7"><p>MFN-VAE</p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>0.849 ± 0.044</p></td><td colspan="1" rowspan="1"><p>0.866 ± 0.075</p></td><td colspan="1" rowspan="1"><p>0.819 ± 0.076</p></td><td colspan="1" rowspan="1"><p>0.897 ± 0.038</p></td><td colspan="1" rowspan="1"><p>0.806 ± 0.064</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>0.852 ± 0.040</p></td><td colspan="1" rowspan="1"><p>0.869 ± 0.063</p></td><td colspan="1" rowspan="1"><p>0.833 ± 0.060</p></td><td colspan="1" rowspan="1"><p>0.888 ± 0.046</p></td><td colspan="1" rowspan="1"><p>0.816 ± 0.085</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>0.838 ± 0.040</p></td><td colspan="1" rowspan="1"><p>0.855 ± 0.064</p></td><td colspan="1" rowspan="1"><p>0.816 ± 0.076</p></td><td colspan="1" rowspan="1"><p>0.877 ± 0.063</p></td><td colspan="1" rowspan="1"><p>0.806 ± 0.068</p></td></tr><tr><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>0.877 ± 0.031</p></td><td colspan="1" rowspan="1"><p>0.902 ± 0.049</p></td><td colspan="1" rowspan="1"><p>0.879 ± 0.066</p></td><td colspan="1" rowspan="1"><p>0.883 ± 0.038</p></td><td colspan="1" rowspan="1"><p>0.878 ± 0.069</p></td></tr><tr><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>0.880 ± 0.032</p></td><td colspan="1" rowspan="1"><p>0.879 ± 0.051</p></td><td colspan="1" rowspan="1"><p>0.865 ± 0.051</p></td><td colspan="1" rowspan="1"><p>0.900 ± 0.022</p></td><td colspan="1" rowspan="1"><p>0.859 ± 0.050</p></td></tr><tr><td colspan="1" rowspan="1"><p> </p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>0.849 ± 0.022</p></td><td colspan="1" rowspan="1"><p>0.872 ± 0.037</p></td><td colspan="1" rowspan="1"><p>0.825 ± 0.049</p></td><td colspan="1" rowspan="1"><p>0.882 ± 0.031</p></td><td colspan="1" rowspan="1"><p>0.815 ± 0.031</p></td></tr><tr><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>0.888 ±0.026</p></td><td colspan="1" rowspan="1"><p>0.920 ± 0.026</p></td><td colspan="1" rowspan="1"><p>0.884 ± 0.038</p></td><td colspan="1" rowspan="1"><p>0.924 ± 0.022</p></td><td colspan="1" rowspan="1"><p>0.850 ± 0.044</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Visual comparison of Missing Modality Transformer (MMT) under missing-modality conditions and Multimodal Fusion Network—Variational Autoencoder (MFN-VAE) across different modality combinations</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_tjcOnw-skEPxN8MG.png"/>
            </fig>
          
          <p><xref ref-type="table" rid="table_2">Table 2</xref> results indicate that MMT achieves an accuracy of 0.863–0.877 in single-modality scenarios, surpassing MFN-VAE’s accuracy of 0.838–0.852 in the same setting. This performance level is comparable to MFN-VAE’s results across dual-modal conditions (0.849–0.880). When operating in dual-modal environments (missing one modality), MMT demonstrates an accuracy range of 0.897 to 0.905, outperforming MFN-VAE’s accuracy of 0.888 in full-modal scenarios. These findings demonstrate MMT’s capability to effectively leverage multi-modal feature information. By integrating existing modalities and learning inter-modal relationships through its self-attention mechanism, MMT generates contextual features for missing modalities. This approach enables MMT to achieve performance parity with full-modal models even when data is incomplete.</p>
          <p>The Receiver Operating Characteristic (ROC) performance of the best-performing MFN-VAE and MMT models is illustrated in <xref ref-type="fig" rid="fig_7">Figure 7</xref>.</p><p>The loss function is a crucial component of MMT optimization. As shown in <xref ref-type="fig" rid="fig_8">Figure 8</xref>, while the overall MMT loss gradually decreases during epoch iterations, the validation loss consistently remains higher than the training loss, indicating a certain degree of overfitting. The model demonstrates good convergence of reconstruction loss during training, but shows significant fluctuations in validation loss with almost no convergence trend.</p><p> In contrast, the classification loss performs well with only slight overfitting. This discrepancy may stem from MMT directly utilizing the output of the modal interaction self-attention mechanism as the reconstructed feature. This architectural choice prioritizes classification performance over reconstruction fidelity, which explains why the reconstruction loss on the validation set fails to converge.</p>
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>Receiver Operating Characteristic (ROC) curves of the best-performing model weights for Multimodal Fusion Network—Variational Autoencoder (MFN-VAE) and Missing Modality Transformer (MMT)</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_d3FotPZv_frodTdw.png"/>
            </fig>
          
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>Loss function curve for the optimal modality combination in Missing Modality Transformer (MMT)</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_aWGImscq-S5qLy1U.png"/>
            </fig>
          
          <p>Based on the training weights of models under multi-modal combinations, the optimal combination of T1WI and FLAIR was selected, as shown in <xref ref-type="table" rid="table_3">Table 3</xref>. The MMT loss function is a weighted sum of classification loss and reconstruction loss. In this study, the loss function hyperparameters from the study by Hutter et al. [<xref ref-type="bibr" rid="ref_22">22</xref>] were used for evaluation. Overall, the highest score was assigned to the classification loss weight, followed by the reconstruction loss weights for T1WI and DWI. For the FLAIR modality, the reconstruction loss weight consistently scored 0.1, significantly lower than other parameters. When using a single modality, only FLAIR showed a deviation from the overall results, with its classification weight score being only 0.117. Meanwhile, the feature reconstruction scores for both T1WI and DWI exceeded the classification loss score. Overall, it was found that in MMT, the classification loss, T1WI reconstruction loss, and DWI reconstruction loss carry relatively greater weight, while the FLAIR loss holds a secondary position. Therefore, during model optimization, the optimization of these three loss components was prioritized.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Importance scores of classification and reconstruction loss weights across modality combinations</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="2"><p>Model</p></td><td colspan="3" rowspan="1"><p>Mode</p></td><td colspan="1" rowspan="2"><p>Classification Loss</p></td><td colspan="3" rowspan="1"><p>Reconstruction Loss</p></td></tr><tr><td colspan="1" rowspan="1"><p>T</p></td><td colspan="1" rowspan="1"><p>F</p></td><td colspan="1" rowspan="1"><p>D</p></td><td colspan="1" rowspan="1"><p>T</p></td><td colspan="1" rowspan="1"><p>F</p></td><td colspan="1" rowspan="1"><p>D</p></td></tr><tr><td colspan="1" rowspan="3"><p>MMT-1M</p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>0.351</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>0.119</p></td><td colspan="1" rowspan="1"><p>0.248</p></td></tr><tr><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>0.117</p></td><td colspan="1" rowspan="1"><p>0.216</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>0.160</p></td></tr><tr><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>0.298</p></td><td colspan="1" rowspan="1"><p>0.298</p></td><td colspan="1" rowspan="1"><p>0.126</p></td><td colspan="1" rowspan="1"><p>-</p></td></tr><tr><td colspan="1" rowspan="3"><p>MMT-2M</p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>0.262</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>0.378</p></td></tr><tr><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>0.341</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>0.138</p></td><td colspan="1" rowspan="1"><p>-</p></td></tr><tr><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>√</p></td><td colspan="1" rowspan="1"><p>0.292</p></td><td colspan="1" rowspan="1"><p>0.303</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>-</p></td></tr><tr><td colspan="1" rowspan="1"><p>Average</p></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>0.277</p></td><td colspan="1" rowspan="1"><p>0.272</p></td><td colspan="1" rowspan="1"><p>0.126</p></td><td colspan="1" rowspan="1"><p>0.262</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>This study proposed an MMT network to address the common issue of missing modalities in medical diagnosis. Through comparative validation with the full-modality MFN-VAE network, MMT demonstrates superior performance. MMT’s mechanisms were analyzed via loss function visualization and importance assessment, enhancing interpretability. MMT achieves a maximum accuracy of 0.905 with a single missing modality, surpassing the full-modality MFN-VAE by 0.017. This provides a reliable solution for clinical diagnosis in scenarios where data modalities are incomplete.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>Conceptualization, Y.D., Y.Q.M., and S.L.; methodology, Y.D. and Y.Q.M.; software, Y.D. and Y.Q.M.; validation, K.J., Z.S.S., F.Y.G., and Z.W.C.; formal analysis, Y.D. and Y.Q.M.; investigation, Y.D., Y.Q.M., K.J., and Z.S.S.; resources, S.L. and L.Y.X.; data curation, Y.D., Y.Q.M., F.Y.G., and Z.W.C.; writing—original draft preparation, Y.D. and Y.Q.M.; writing—review and editing, S.L., L.Y.X., and Y.D.; visualization, Y.D. and Y.Q.M.; supervision, S.L. and L.Y.X.; project administration, S.L. and L.Y.X.; funding acquisition, S.L. All authors have read and agreed to the published version of the manuscript.</p>
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data (multi-modal MRI images and extracted radiomics features) supporting our research results are under privacy or ethical restrictions. The data used to support the research findings were sourced from the Affiliated Hospital of Hebei University and are available from the corresponding author upon request for researchers who meet the criteria for accessing confidential medical data.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>American Cancer Society</surname>
            </name>
          </person-group>
          <source>Global Cancer Facts &amp; Figures 4th Edition</source>
          <publisher-name>Atlanta, American Cancer Society</publisher-name>
          <year>2018</year>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>68</volume>
          <page-range>394-424</page-range>
          <issue>6</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bray</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Ferlay</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Soerjomataram</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Siegel</surname>
              <given-names>R. L.</given-names>
            </name>
            <name>
              <surname>Torre</surname>
              <given-names>L. A.</given-names>
            </name>
            <name>
              <surname>Jemal</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3322/caac.21492</pub-id>
          <article-title>Global cancer statistics 2018: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries</article-title>
          <source>CA Cancer J. Clin.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>2026</volume>
          <page-range>500758</page-range>
          <year>2026</year>
          <person-group person-group-type="author">
            <name>
              <surname>Solmaz</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Gulsever</surname>
              <given-names>C. I.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neucie.2026.500758</pub-id>
          <article-title>Atypical non-enhancing brain metastases from ALK-positive non-small cell lung carcinoma</article-title>
          <source>Neurocirugía (Engl. Ed.)</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>549-570</page-range>
          <issue>4</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>General Office of National Health Commission of the People's Republic of China</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.12290/xhyxzz.2022-0352</pub-id>
          <article-title>Clinical practice guideline for primary lung cancer (2022 Version)</article-title>
          <source>Med. J. Peking Union Med. Coll. Hosp.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>3</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Rudin</surname>
              <given-names>M. C.</given-names>
            </name>
            <name>
              <surname>Brambilla</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Faivre-Finn</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Sage</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41572-020-00235-0</pub-id>
          <article-title>Small-cell lung cancer</article-title>
          <source>Nat. Rev. Dis. Primers</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>378</volume>
          <page-range>1727-1740</page-range>
          <issue>9804</issue>
          <year>2011</year>
          <person-group person-group-type="author">
            <name>
              <surname>Goldstraw</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Ball</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Jett</surname>
              <given-names>J. R.</given-names>
            </name>
            <name>
              <surname>Le Chevalier</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Lim</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Nicholson</surname>
              <given-names>A. G.</given-names>
            </name>
            <name>
              <surname>Shepherd</surname>
              <given-names>F. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/s0140-6736(10)62101-0</pub-id>
          <article-title>Non-small-cell lung cancer</article-title>
          <source>Lancet</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="webpage">
          <article-title>SEER cancer stat facts: Lung and bronchus cancer</article-title>
          <source>, https://seer.cancer.gov/statfacts/html/lungb.html</source>
          <year>2023</year>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>94</volume>
          <page-range>1623-1640</page-range>
          <issue>8</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Duma</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Santana-Davila</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Molina</surname>
              <given-names>J. R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.mayocp.2019.01.013</pub-id>
          <article-title>Non–small cell lung cancer: Epidemiology, screening, diagnosis, and treatment</article-title>
          <source>Mayo Clin. Proc.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>2865-2872</page-range>
          <issue>14</issue>
          <year>2004</year>
          <person-group person-group-type="author">
            <name>
              <surname>Barnholtz-Sloan</surname>
              <given-names>J. S.</given-names>
            </name>
            <name>
              <surname>Sloan</surname>
              <given-names>A. E.</given-names>
            </name>
            <name>
              <surname>Davis</surname>
              <given-names>F. G.</given-names>
            </name>
            <name>
              <surname>Vigneau</surname>
              <given-names>F. D.</given-names>
            </name>
            <name>
              <surname>Lai</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Sawaya</surname>
              <given-names>R. E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1200/jco.2004.12.149</pub-id>
          <article-title>Incidence proportions of brain metastases in patients diagnosed (1973 to 2001) in the Metropolitan Detroit Cancer Surveillance System</article-title>
          <source>J. Clin. Oncol.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>19</volume>
          <page-range>e373-e379</page-range>
          <issue>4</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Waqar</surname>
              <given-names>S. N.</given-names>
            </name>
            <name>
              <surname>Samson</surname>
              <given-names>P. P.</given-names>
            </name>
            <name>
              <surname>Robinson</surname>
              <given-names>C. G.</given-names>
            </name>
            <name>
              <surname>Bradley</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Devarakonda</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Du</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Govindan</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Puri</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Morgensztern</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.cllc.2018.01.007</pub-id>
          <article-title>Non-small-cell lung cancer with brain metastasis at presentation</article-title>
          <source>Clin. Lung Cancer</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>443</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cao</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Jia</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Fan</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wen</surname>
              <given-names>Q.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s12885-025-13823-8</pub-id>
          <article-title>Deep learning radiomics for the prediction of epidermal growth factor receptor mutation status based on MRI in brain metastasis from lung adenocarcinoma patients</article-title>
          <source>BMC Cancer</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>44</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tabnak</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Kargar</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Ebrahimnezhad</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>HajiEsmailPoor</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s12880-025-01566-8</pub-id>
          <article-title>A Bayesian meta‑analysis on MRI-based radiomics for predicting EGFR mutation in brain metastasis of lung cancer</article-title>
          <source>BMC Med. Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>24</volume>
          <page-range>65</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Y. R.</given-names>
            </name>
            <name>
              <surname>Jin</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y. L.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>W. Y.</given-names>
            </name>
            <name>
              <surname>Jia</surname>
              <given-names>W. X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s40644-024-00709-4</pub-id>
          <article-title>MR-based radiomics predictive modelling of EGFR mutation and HER2 overexpression in metastatic brain adenocarcinoma: A two-centre study</article-title>
          <source>Cancer Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>4516-4528</page-range>
          <issue>11</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Xu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wen</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Cao</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Fan</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.21037/jtd.2019.11.01</pub-id>
          <article-title>Application of radiomics signature captured from pretreatment thoracic CT to predict brain metastases in stage III/IV ALK-positive non-small cell lung cancer patients</article-title>
          <source>J. Thorac. Dis.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>1133</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Niu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Jia</surname>
              <given-names>H. B.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>X. M.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>W. J.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>P. P.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Z. Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Q. J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y. Z.</given-names>
            </name>
            <name>
              <surname>Miao</surname>
              <given-names>S. D.</given-names>
            </name>
            <name>
              <surname>and et al.</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s12885-025-14466-5</pub-id>
          <article-title>Deep learning radiomics and mediastinal adipose tissue-based nomogram for preoperative prediction of postoperative brain metastasis risk in non-small cell lung cancer</article-title>
          <source>BMC Cancer</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>24</volume>
          <page-range>1</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gong</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z. Z.</given-names>
            </name>
            <name>
              <surname>Chu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>T. D.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>M. L.</given-names>
            </name>
            <name>
              <surname>Peng</surname>
              <given-names>W. J.</given-names>
            </name>
            <name>
              <surname>Feng</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Tong</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Gu</surname>
              <given-names>Y. J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s40644-023-00623-1</pub-id>
          <article-title>Enhancing brain metastasis prediction in non-small cell lung cancer: A deep learning-based segmentation and CT radiomics-based ensemble learning model</article-title>
          <source>Cancer Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>8196878</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Guo</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fonc.2022.819673</pub-id>
          <article-title>Multimodal MRI image decision fusion-based network for glioma classification</article-title>
          <source>Front. Oncol.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="conf-paper">
          <page-range>39-44</page-range>
          <publisher-place>New York, United States</publisher-place>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Xiang</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Wan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3468945.3468952</pub-id>
          <article-title>Prediction of IDH mutation status of glioma based on multimodal MRI images</article-title>
          <source>Proceedings of the 2021 3rd International Conference on Intelligent Medicine and Image Processing</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>224</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhu</surname>
              <given-names>F. Y.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>Y. F.</given-names>
            </name>
            <name>
              <surname>Yin</surname>
              <given-names>X. P.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Xing</surname>
              <given-names>L. H.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>Z. P.</given-names>
            </name>
            <name>
              <surname>Xue</surname>
              <given-names>L. Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J. N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s12672-023-00837-6</pub-id>
          <article-title>Using machine learning-based radiomics to differentiate between glioma and solitary brain metastasis from lung cancer and its subtypes</article-title>
          <source>Discover Oncol.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>77</volume>
          <page-range>e104-e107</page-range>
          <issue>21</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Griethuysen</surname>
              <given-names>J. J. M.</given-names>
            </name>
            <name>
              <surname>Fedorov</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Parmar</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Hosny</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Aucoin</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Narayan</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Beets-Tan</surname>
              <given-names>R. G. H.</given-names>
            </name>
            <name>
              <surname>Fillion-Robin</surname>
              <given-names>J. C.</given-names>
            </name>
            <name>
              <surname>Pieper</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Aerts</surname>
              <given-names>H. J. W. L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1158/0008-5472.CAN-17-0339</pub-id>
          <article-title>Computational radiomics system to decode the radiographic phenotype</article-title>
          <source>Cancer Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Han</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>W. Y.</given-names>
            </name>
            <name>
              <surname>Mao</surname>
              <given-names>B. H.</given-names>
            </name>
          </person-group>
          <article-title>Borderline-SMOTE: A new over-sampling method in imbalanced data sets learning</article-title>
          <source>Lecture Notes in Computer Science</source>
          <publisher-name>Springer, Berlin, Heidelberg</publisher-name>
          <year>2005</year>
          <page-range>878-887</page-range>
          <pub-id pub-id-type="doi">10.1007/11538059_91</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="conf-paper">
          <page-range>754-762</page-range>
          <publisher-place>Beijing, China</publisher-place>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hutter</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Hoos</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Leyton-Brown</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>An efficient approach for assessing hyperparameter importance</article-title>
          <source>Proceedings of the 31st International Conference on Machine Learning</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>