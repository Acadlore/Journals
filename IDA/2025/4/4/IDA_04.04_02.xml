<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IDA</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Information Dynamics and Applications</journal-title>
        <abbrev-journal-title abbrev-type="issn">Inf. Dyn. Appl.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IDA</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-1494</issn>
      <issn publication-format="print">2958-1486</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-X3c6qUREfpmubouduLltJgnCVpWPvi3h</article-id>
      <article-id pub-id-type="doi">10.56578/ida040402</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>A Hybrid Vision Transformer–Driven Feature Extraction and Machine Learning Framework for Automated Skin Burn Detection</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-3483-5962</contrib-id>
          <name>
            <surname>Duman</surname>
            <given-names>Ahmet Yasir</given-names>
          </name>
          <email>ahmetyasirduman@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-8087-4044</contrib-id>
          <name>
            <surname>Karaduman</surname>
            <given-names>Mucahit</given-names>
          </name>
          <email>mucahit.karaduman@ozal.edu.tr</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-1866-4721</contrib-id>
          <name>
            <surname>Yildirim</surname>
            <given-names>Muhammed</given-names>
          </name>
          <email>muhammed.yildirim@ozal.edu.tr</email>
        </contrib>
        <aff id="aff_1">Department of Computer Engineering, Malatya Turgut Ozal University, 44200 Malatya, Turkey</aff>
        <aff id="aff_2">Department of Software Engineering, Malatya Turgut Ozal University, 44200 Malatya, Turkey</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>11</day>
        <month>12</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>4</issue>
      <fpage>201</fpage>
      <lpage>211</lpage>
      <page-range>201-211</page-range>
      <history>
        <date date-type="received">
          <day>02</day>
          <month>11</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>06</day>
          <month>12</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Skin burns represent a major clinical concern due to their association with pain, functional impairment, sensory damage, and even life-threatening complications. Early and accurate assessment is critical for first aid, clinical intervention, and the prevention of secondary complications. However, conventional burn diagnosis remains highly dependent on visual inspection and clinical expertise, which can introduce subjectivity and delay timely decision-making. To address these limitations, a hybrid automated skin burn detection framework was proposed, integrating transformer-based feature extraction with classical machine learning classification. In this framework, discriminative visual features were extracted using multiple Vision Transformer (ViT) architectures, including ViT-B/16, ViT-L/16, ViT-B/32, and DINOv2 (a self-supervised Vision Transformer model). The extracted features were subsequently fused. Given the resulting high-dimensional feature space, dimensionality reduction was performed using the Chi-square (Chi$^2$) algorithm, through which 500 features were retained, reducing computational complexity and mitigating the risk of model overfitting. The reduced feature set was then employed for burn classification using six classifiers. Model effectiveness was assessed using accuracy, precision, sensitivity, and F1-score metrics. Experimental results demonstrated that the Support Vector Machine (SVM) classifier achieved the highest classification performance, yielding an accuracy of 82.29%. Comparable yet slightly lower accuracies were observed for the Light Gradient Boosting Machine (LGBM) (80.51%) and Extreme Gradient Boosting (XGBoost) (80.17%) classifiers. Overall, the proposed hybrid model consistently outperformed baseline models, highlighting its superior discriminative capability. These findings indicate that the proposed framework holds strong potential for integration into clinical decision support systems, offering a reliable and objective tool for automated skin burn detection.</p></abstract>
      <kwd-group>
        <kwd>Artificial intelligence</kwd>
        <kwd>Chi-square</kwd>
        <kwd>Classifiers</kwd>
        <kwd>Skin burn</kwd>
        <kwd>Vision Transformer</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="3"/>
        <fig-count count="14"/>
        <table-count count="5"/>
        <ref-count count="30"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Burn injuries are one of the most common types of trauma. A burn is an injury caused by various factors, including hot objects, flames, chemicals, electric currents, radiation, and friction, damaging tissue. Burns affect people both physically and psychologically. If not treated appropriately, they can have fatal consequences [<xref ref-type="bibr" rid="ref_1">1</xref>]. Skin burns not only cause tissue and organ damage but also profoundly impact an individual’s mental health [<xref ref-type="bibr" rid="ref_2">2</xref>]. Because burns can occur for a variety of reasons, the severity of the injury can vary. To plan the burn treatment process appropriately, it is essential to quickly identify the cause and extent.</p><p>Early and correct intervention in burn treatment alleviates patient suffering and significantly reduces the risk of death. Inappropriate or delayed intervention can endanger human life. The integration of developing computer-aided systems into medical diagnostic processes has increased the possibilities for accurate and timely intervention in the healthcare field. In addition to other health studies, there are studies that have used artificial intelligence (AI) to detect burns and demonstrated successful results in treatment processes [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>], [<xref ref-type="bibr" rid="ref_5">5</xref>]. Skin burns consist of superficial first-degree, partial-thickness second-degree, and full-thickness third-degree burns. Automatic recognition of these graded burns with computer systems is important for clinical treatment processes. Pabitha and Vanathi [<xref ref-type="bibr" rid="ref_6">6</xref>] graded skin burns using the Region-Based Convolutional Neural Network (R-CNN) method and applied masking to these burns. Şevik et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] achieved automatic classification of skin burn images using tissue-based feature extraction. It was found that the best combination was a multilayer feedforward Artificial Neural Network (ANN) trained with a backpropagation algorithm for classification. With this method, an F-score of 74.28% was obtained.</p><p>In addition, Abazari et al. [<xref ref-type="bibr" rid="ref_8">8</xref>] conducted a classification study to select the appropriate treatment for burn wounds based on diagnosis and wound type. Similarly, Elsarta et al. [<xref ref-type="bibr" rid="ref_9">9</xref>] used deep learning methods to classify skin burns. Multi-source data were integrated to develop an AI model. Acha et al. [<xref ref-type="bibr" rid="ref_10">10</xref>] developed a segmentation and classification model, utilizing color and texture information from burn images. An 82.26% success rate was achieved with this model. Kuan et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] conducted a comparative study of 20 classification algorithms for classifying burn depths using an image mining approach. The dataset was divided into both test and training sections and evaluated using 10-fold cross-validation methods. The results showed that the best classification algorithm achieved an average accuracy of 68.9%. In the 10-fold cross-validation evaluation, the best result was determined to be an average of 73.2%. Rahman et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] focused on image-based diagnosis of burn type and depth. Various data augmentation processes were applied to a dataset of 29 patients. The proposed AI model was evaluated using a 5-fold cross-validation method. It was found that the average success rate of the three classes was 79%. Khan et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] developed a model using a Deep Convolutional Neural Network (DCNN) for burn detection. During model development, 65% of the dataset, comprising a total of 450 images, was used for training, while the remaining 35% was used for testing. The results showed that the proposed model achieved a success rate of 79.4%.</p><p>In this study, a hybrid deep learning model was developed to classify skin burns with high accuracy. The model combines the powerful representation capabilities of different transformer-based architectures. In the first stage, features were extracted from four different architectures and then combined, aiming to use different features of the same image together. At this stage, these models were not trained and were only used for feature extraction. Feature selection was then performed on the resulting feature map using Chi<inline-formula>
  <mml:math id="mh0dnfes8s">
    <mml:msup>
      <mml:mi/>
      <mml:mn>2</mml:mn>
    </mml:msup>
  </mml:math>
</inline-formula>. The aim of this step is to make the model run faster and produce more successful results. Finally, the optimized feature map was classified using six different classifiers. In the remainder of this study, the dataset used, the deep learning models, and the proposed model are detailed in Section 2. Experimental results are presented in Section 3. The conclusion is presented in Section 4.</p>
    </sec>
    <sec sec-type="">
      <title>2. Methodology</title>
      <p>In this section, the burn dataset used in the study is examined. The applied methods are also explained in detail.</p>
      
        <sec>
          
            <title>2.1. Dataset</title>
          
          <p>The dataset used was obtained from the publicly available Kaggle website [<xref ref-type="bibr" rid="ref_14">14</xref>]. It contains a total of 6,099 images. The 200 unlabeled images within the dataset were included solely for testing purposes; therefore, the accuracy of the proposed model’s predicted values cannot be determined. Because they are unlabeled, they were excluded from the training model. Therefore, the total dataset consists of 5,899 records, of which 4,719 were used as training data and 1,180 as new test data to determine the model’s accuracy. The classes and sample images of the dataset are shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>.</p><p>Burn severities in the dataset are labeled as first-degree burns, second-degree burns, and third-degree burns. First-degree burns are ranked as the mildest, and third-degree burns are ranked as the most severe. Examples of each degree in the dataset are shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>Dataset distribution</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_y2M2FjQ6Q2OcqyZc.png"/>
            </fig>
          
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Images taken from the dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_Xnt3hdYYfxXXF-ph.png"/>
            </fig>
          
          <p>In <xref ref-type="fig" rid="fig_2">Figure 2</xref>, first-degree burns are shown as the lightest, while third-degree burns are listed as the most severe.</p>
        </sec>
      
      
        <sec>
          
            <title>2.2. Transformers and classifiers</title>
          
          <p>Transformer-based deep learning models, which have achieved significant success in Natural Language Processing (NLP), have demonstrated remarkable performance due to their feature extraction capabilities [<xref ref-type="bibr" rid="ref_15">15</xref>]. Among the key advantages of transformers are their ability to model long-term dependencies between input sequence elements and their capacity to support parallel processing of sequences, which is superior to recurrent networks. These strengths have led to exciting advances in several vision tasks using transformer networks [<xref ref-type="bibr" rid="ref_16">16</xref>]. The Vision Transformer (ViT) models used in this study—ViT-B/16, ViT-L/16, ViT-B/32, and DINOv2 (a self-supervised Vision Transformer model) networks—were employed for feature extraction.</p><p>ViT-B/16 is a transformer encoder model pre-trained in a supervised manner on an extensive collection of images called ImageNet-21k [<xref ref-type="bibr" rid="ref_17">17</xref>]. A standard classifier can be trained by placing a linear layer on top of the pre-trained encoder. However, in this study, in addition to being trained as a classifier, it was also used for feature extraction. The ViT-B/32 model differs from ViT-B/16 in that the images are presented to the model as a series of linearly embedded, fixed-size patches, with a resolution of 32 × 32 [<xref ref-type="bibr" rid="ref_18">18</xref>]. This model was utilized to enhance efficiency in the feature extraction portion of this study. In the ViT-L/16 model, although both models belong to the ViT family, the model’s capabilities are more comprehensive than the other base models [<xref ref-type="bibr" rid="ref_19">19</xref>]. The DINOv2 model was also used for feature extraction in the ViT models used in this study. The quality of the DINOv2 model was superior at both the image and pixel levels compared to various computer vision models [<xref ref-type="bibr" rid="ref_20">20</xref>]. The DINOv2 model has also been used particularly in medical imaging studies [<xref ref-type="bibr" rid="ref_21">21</xref>]. These features make it a valuable model for feature extraction in this study.</p><p>For each model, features obtained using ViTs were compared with those from six different classifiers. Three of these classifiers are XGBoost, Categorical Boosting (CatBoost), and Light Gradient Boosting Machine (LGBM), all of which are gradient boosting methods. The learning procedure in gradient boosting machines sequentially applies new models to ensure a more accurate prediction of the response variable. The basic idea is to create a new learner with maximum correlation with the negative gradient of the loss function [<xref ref-type="bibr" rid="ref_22">22</xref>]. However, the high computational cost of traditional Gradient Boosting Decision Tree (GBDT) algorithms has limited their use in large datasets. In this context, classifiers were created using the LGBM method, the CatBoost method, and the XGBoost method, which offer the advantages of gradient boosting-based algorithms in a faster and more scalable form [<xref ref-type="bibr" rid="ref_23">23</xref>], [<xref ref-type="bibr" rid="ref_24">24</xref>], [<xref ref-type="bibr" rid="ref_25">25</xref>]. In this study, in addition to gradient boosting-based methods, k-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Random Forest (RF) classifiers were also used. Despite its simple structure, the KNN classifier is one of the methods that can produce effective results in classification problems [<xref ref-type="bibr" rid="ref_26">26</xref>]. Furthermore, SVM is a powerful classification method that aims to maximize accuracy while reducing the risk of overfitting [<xref ref-type="bibr" rid="ref_27">27</xref>]. RF creates multiple random decision trees and combines their results to obtain more balanced predictions [<xref ref-type="bibr" rid="ref_28">28</xref>].</p>
        </sec>
      
      
        <sec>
          
            <title>2.3. Proposed model for skin burn detection</title>
          
          <p>In the proposed hybrid method for skin burn detection, the ViT-B/16, ViT-L/16, ViT-B/32, and DINOv2 models were used for feature extraction. These obtained features were combined and classified using XGBoost, LGBM, CatBoost, KNN, SVM, and RF. In the feature selection phase, the Chi<inline-formula>
  <mml:math id="mjeznsiwin">
    <mml:msup>
      <mml:mi/>
      <mml:mn>2</mml:mn>
    </mml:msup>
  </mml:math>
</inline-formula> feature selection method was used to identify the 500 most significant features. The Chi<inline-formula>
  <mml:math id="mct8dq9yvp">
    <mml:msup>
      <mml:mi/>
      <mml:mn>2</mml:mn>
    </mml:msup>
  </mml:math>
</inline-formula> algorithm tests the significance of relationships between features using the Chi<inline-formula>
  <mml:math id="m7c968x89m">
    <mml:msup>
      <mml:mi/>
      <mml:mn>2</mml:mn>
    </mml:msup>
  </mml:math>
</inline-formula> statistic and infers feature size [<xref ref-type="bibr" rid="ref_29">29</xref>]. This aims to save time by reducing training time and improving classification performance. The feature vectors, initially obtained with size 5899 × 3328, were reduced to 5899 × 500 using the Chi<inline-formula>
  <mml:math id="mf231s7ytu">
    <mml:msup>
      <mml:mi/>
      <mml:mn>2</mml:mn>
    </mml:msup>
  </mml:math>
</inline-formula> algorithm. <xref ref-type="fig" rid="fig_3">Figure 3</xref> visually illustrates the general architecture of the proposed hybrid model.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Proposed model</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_ihM7FU9x1-O3BZox.png"/>
            </fig>
          
          <p>The last step of the proposed model was to evaluate the performance with different metrics after skin burns were detected.</p>
        </sec>
      
    </sec>
    <sec sec-type="results">
      <title>3. Results</title>
      <p>In this section, the performance of the proposed hybrid model and the other models used for comparison is examined. First, the confusion matrices of the models are presented. The performance of the classification model is measured using the values found in the confusion matrix. Accuracy, recall, precision, and F1-score metrics are used to evaluate the performance of the models [<xref ref-type="bibr" rid="ref_30">30</xref>].</p>
      
        <sec>
          
            <title>3.1. Evaluation of the vit-b/16 model results for burn detection</title>
          
          <p>The features extracted by the ViT-B/16 model used in this study were fed to six different classifiers. The resulting confusion matrices for each of these classifiers are presented in <xref ref-type="fig" rid="fig_4">Figure 4</xref>.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Confusion matrices of features extracted with ViT-B/16 in different classifiers</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_d-rWKoNNCNYmgk9a.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_HYjiwU8zyeBgUVZl.png"/>
            </fig>
          
          <p><xref ref-type="fig" rid="fig_4">Figure 4</xref> shows the most successful metrics in the SVM’s confusion matrix; however, it’s particularly noticeable that the model struggles to classify third-degree burns. Similarly, the results reveal that third-degree burns are generally the most challenging class to predict across all classifiers because some second-degree and third-degree burns are very similar in appearance. The distribution of success scores across classifiers is shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Comparison of classifier performance using ViT-B/16 features</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_r0s0IVYTQbWineld.png"/>
            </fig>
          
          <p>The evaluation of the classifiers was conducted not only in terms of accuracy but also in terms of time and performance metrics, as shown in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
          <p>It can be seen from <xref ref-type="fig" rid="fig_5">Figure 5</xref> and <xref ref-type="table" rid="table_1">Table 1</xref> that the highest accuracy value was achieved with the SVM classifier using features extracted from the ViT-B/16 model. This can be attributed to the short training time and its success.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Comparison of accuracy, training time, and prediction speed of classifiers using ViT-B/16 features</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p>Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Prediction Speed (images/s)</p></th><th colspan="1" rowspan="1"><p>Training Time (s)</p></th></tr><tr><td colspan="1" rowspan="1"><p>SVM</p></td><td colspan="1" rowspan="1"><p>79.58</p></td><td colspan="1" rowspan="1"><p>616.9</p></td><td colspan="1" rowspan="1"><p>4.41</p></td></tr><tr><td colspan="1" rowspan="1"><p>CatBoost</p></td><td colspan="1" rowspan="1"><p>78.98</p></td><td colspan="1" rowspan="1"><p>32712.55</p></td><td colspan="1" rowspan="1"><p>519.33</p></td></tr><tr><td colspan="1" rowspan="1"><p>XGBoost</p></td><td colspan="1" rowspan="1"><p>77.37</p></td><td colspan="1" rowspan="1"><p>79480.95</p></td><td colspan="1" rowspan="1"><p>72.38</p></td></tr><tr><td colspan="1" rowspan="1"><p>LGBM</p></td><td colspan="1" rowspan="1"><p>77.37</p></td><td colspan="1" rowspan="1"><p>33229.57</p></td><td colspan="1" rowspan="1"><p>40.49</p></td></tr><tr><td colspan="1" rowspan="1"><p>RF</p></td><td colspan="1" rowspan="1"><p>71.86</p></td><td colspan="1" rowspan="1"><p>35962.58</p></td><td colspan="1" rowspan="1"><p>14.87</p></td></tr><tr><td colspan="1" rowspan="1"><p>KNN</p></td><td colspan="1" rowspan="1"><p>71.69</p></td><td colspan="1" rowspan="1"><p>3858.05</p></td><td colspan="1" rowspan="1"><p>0.02</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Evaluation of the vit-l/16 model results for burn detection</title>
          
          <p>Feature extraction was also performed using the ViT-L/16 model for burn detection. This model provides a more comprehensive feature representation compared to ViT-B/16, and, thanks to its broad structure, can capture detailed information. The extracted features were fed to different classifiers, and the results are presented as confusion matrices for each classifier in <xref ref-type="fig" rid="fig_6">Figure 6</xref>. </p>
          
            <fig id="fig_6">
              <label>Figure 6</label>
              <caption>
                <title>Confusion matrices of features extracted with DINOv2 in different classifiers</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_SqKOXZMHV_HHIf5M.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_GPCq-DooZHstzcBR.png"/>
            </fig>
          
          <p>As shown in the classification results using features obtained from the ViT-L/16 model, the highest accuracy value is achieved with the XGBoost algorithm. However, this success is quite close to that of the SVM classifier. The comparison of performance metrics of classifiers using ViT-L/16 features is presented in <xref ref-type="fig" rid="fig_7">Figure 7</xref>.</p><p>The impact of the models on system performance and the success metrics obtained accordingly are shown in <xref ref-type="table" rid="table_2">Table 2</xref>.</p><p>The classification results using features obtained from the ViT-L/16 model show that the XGBoost algorithm achieved the highest accuracy. However, this success is quite similar to that of the SVM classifier. When comparing training time and prediction speed, it is evident that SVM is more efficient. However, as with the ViT-B/16 model, accurately classifying third-degree burns remains the most challenging task for all classifiers.</p>
          
            <fig id="fig_7">
              <label>Figure 7</label>
              <caption>
                <title>Performance comparison of classifiers using ViT-L/16 features</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_KkYsgamqvrOEh6g2.png"/>
            </fig>
          
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Comparison of accuracy, training time, and prediction speed of classifiers using ViT-L/16 features</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p>Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Prediction Speed (images/s)</p></th><th colspan="1" rowspan="1"><p>Training Time (s)</p></th></tr><tr><td colspan="1" rowspan="1"><p>XGBoost</p></td><td colspan="1" rowspan="1"><p>78.98</p></td><td colspan="1" rowspan="1"><p>65080.13</p></td><td colspan="1" rowspan="1"><p>87.24</p></td></tr><tr><td colspan="1" rowspan="1"><p>SVM</p></td><td colspan="1" rowspan="1"><p>78.73</p></td><td colspan="1" rowspan="1"><p>456.84</p></td><td colspan="1" rowspan="1"><p>8.46</p></td></tr><tr><td colspan="1" rowspan="1"><p>CatBoost</p></td><td colspan="1" rowspan="1"><p>77.97</p></td><td colspan="1" rowspan="1"><p>70115.02</p></td><td colspan="1" rowspan="1"><p>639.38</p></td></tr><tr><td colspan="1" rowspan="1"><p>LGBM</p></td><td colspan="1" rowspan="1"><p>77.03</p></td><td colspan="1" rowspan="1"><p>29944.45</p></td><td colspan="1" rowspan="1"><p>55.83</p></td></tr><tr><td colspan="1" rowspan="1"><p>KNN</p></td><td colspan="1" rowspan="1"><p>74.66</p></td><td colspan="1" rowspan="1"><p>3376.54</p></td><td colspan="1" rowspan="1"><p>0.03</p></td></tr><tr><td colspan="1" rowspan="1"><p>RF</p></td><td colspan="1" rowspan="1"><p>74.07</p></td><td colspan="1" rowspan="1"><p>35745.70</p></td><td colspan="1" rowspan="1"><p>46.76</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. Evaluation of the vit-b/32 model results for burn detection</title>
          
          <p>The ViT-B/32 model used for burn detection works with a 32 × 32 patch size, allowing for investigating how resolution-based differences in feature extraction impact model performance. The resulting features were fed to six different classifiers, and the confusion matrices of these classifiers are presented in <xref ref-type="fig" rid="fig_8">Figure 8</xref>. </p>
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>
                <title>Confusion matrices of features extracted with ViT-B32 in different classifiers</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_K7TRW1iyq4ilj7dC.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_bgCXUDUoYU3HG3z7.png"/>
            </fig>
          
          <p>As shown in the results obtained with the ViT-B/32 model, the SVM classifier achieved the highest accuracy. However, as with previous models (ViT-B/16 and ViT-L/16), correctly classifying third-degree burns proved to be the most challenging task for all classifiers. The comparison of performance metrics of classifiers using ViT-B/32 features is presented in <xref ref-type="fig" rid="fig_9">Figure 9</xref>.</p><p>The system performance of the models was monitored and the resulting values are shown in <xref ref-type="table" rid="table_3">Table 3</xref>.</p>
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>
                <title>Performance comparison of classifiers using ViT-B/32 features</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_MzPbS8Rtes_nWPS0.png"/>
            </fig>
          
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Comparison of accuracy, training time, and prediction speed of classifiers using ViT-B/32 features</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p>Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Prediction Speed (images/s)</p></th><th colspan="1" rowspan="1"><p>Training Time (s)</p></th></tr><tr><td colspan="1" rowspan="1"><p>SVM</p></td><td colspan="1" rowspan="1"><p>78.64</p></td><td colspan="1" rowspan="1"><p>473.44</p></td><td colspan="1" rowspan="1"><p>5.09</p></td></tr><tr><td colspan="1" rowspan="1"><p>CatBoost</p></td><td colspan="1" rowspan="1"><p>77.97</p></td><td colspan="1" rowspan="1"><p>91468.68</p></td><td colspan="1" rowspan="1"><p>487.57</p></td></tr><tr><td colspan="1" rowspan="1"><p>LGBM</p></td><td colspan="1" rowspan="1"><p>77.71</p></td><td colspan="1" rowspan="1"><p>28088.98</p></td><td colspan="1" rowspan="1"><p>37.85</p></td></tr><tr><td colspan="1" rowspan="1"><p>XGBoost</p></td><td colspan="1" rowspan="1"><p>77.46</p></td><td colspan="1" rowspan="1"><p>79301</p></td><td colspan="1" rowspan="1"><p>65.22</p></td></tr><tr><td colspan="1" rowspan="1"><p>KNN</p></td><td colspan="1" rowspan="1"><p>74.75</p></td><td colspan="1" rowspan="1"><p>3786.16</p></td><td colspan="1" rowspan="1"><p>0.02</p></td></tr><tr><td colspan="1" rowspan="1"><p>RF</p></td><td colspan="1" rowspan="1"><p>73.14</p></td><td colspan="1" rowspan="1"><p>35175.86</p></td><td colspan="1" rowspan="1"><p>14.26</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>When <xref ref-type="table" rid="table_3">Table 3</xref> is examined and evaluated in terms of computational performance, it is determined that the KNN classifier achieves the fastest training time, and the SVM classifier achieves the highest prediction speed.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Evaluation of the dinov2 model results for burn detection</title>
          
          <p>In addition to ViT-based models, the DINOv2 model was also included in this study to facilitate a comparative evaluation of model performance. The features obtained with DINOv2 were transferred to different classifiers, and the confusion matrices of these classifiers are presented in <xref ref-type="fig" rid="fig_10">Figure 10</xref>. </p>
          
            <fig id="fig_10">
              <label>Figure 10</label>
              <caption>
                <title>Confusion matrices of features extracted with DINOv2 in different classifiers</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_8GD1HNu1XiZEvlof.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_pDQN6rXQXYW_nRhb.png"/>
            </fig>
          
          <p>The highest accuracy was achieved with the SVM algorithm in classifications using features extracted from the DINOv2 network. However, as with other classifiers, the model struggles with third-degree burns. The metrics calculated based on the obtained results are presented in <xref ref-type="fig" rid="fig_11">Figure 11</xref> for a comparative analysis of the classifiers.</p>
          
            <fig id="fig_11">
              <label>Figure 11</label>
              <caption>
                <title>Performance comparison of classifiers using DINOv2 features</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_AMTErku8YUsOaAuv.png"/>
            </fig>
          
          <p>The performance of the models trained with features extracted from the DINOv2 network within the dataset used is given in <xref ref-type="table" rid="table_4">Table 4</xref>.</p>
          
            <table-wrap id="table_4">
              <label>Table 4</label>
              <caption>
                <title>Comparison of accuracy, training time, and prediction speed of classifiers using DINOv2 features</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p>Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Prediction Speed (images/s)</p></th><th colspan="1" rowspan="1"><p>Training Time (s)</p></th></tr><tr><td colspan="1" rowspan="1"><p>SVM</p></td><td colspan="1" rowspan="1"><p>81.27</p></td><td colspan="1" rowspan="1"><p>547.42</p></td><td colspan="1" rowspan="1"><p>5.06</p></td></tr><tr><td colspan="1" rowspan="1"><p>XGBoost</p></td><td colspan="1" rowspan="1"><p>81.10</p></td><td colspan="1" rowspan="1"><p>79364.97</p></td><td colspan="1" rowspan="1"><p>65.78</p></td></tr><tr><td colspan="1" rowspan="1"><p>LGBM</p></td><td colspan="1" rowspan="1"><p>80.25</p></td><td colspan="1" rowspan="1"><p>30819.92</p></td><td colspan="1" rowspan="1"><p>37.49</p></td></tr><tr><td colspan="1" rowspan="1"><p>CatBoost</p></td><td colspan="1" rowspan="1"><p>79.41</p></td><td colspan="1" rowspan="1"><p>41994.64</p></td><td colspan="1" rowspan="1"><p>485.02</p></td></tr><tr><td colspan="1" rowspan="1"><p>KNN</p></td><td colspan="1" rowspan="1"><p>78.22</p></td><td colspan="1" rowspan="1"><p>4167.18</p></td><td colspan="1" rowspan="1"><p>0.02</p></td></tr><tr><td colspan="1" rowspan="1"><p>RF</p></td><td colspan="1" rowspan="1"><p>76.78</p></td><td colspan="1" rowspan="1"><p>34803.83</p></td><td colspan="1" rowspan="1"><p>14.10</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>Although KNN stands out as the fastest algorithm in terms of training time, it lags behind SVM in prediction speed. When accuracy and speed metrics are evaluated together, it is concluded that SVM is the most balanced and successful classifier for this model.</p>
        </sec>
      
      
        <sec>
          
            <title>3.5. Experimental result of the proposed model</title>
          
          <p>In the proposed hybrid model for determining the degree of skin burns, features obtained from four different ViT-based architectures were combined and evaluated using the Chi<inline-formula>
  <mml:math id="mnm2e5s952">
    <mml:msup>
      <mml:mi/>
      <mml:mn>2</mml:mn>
    </mml:msup>
  </mml:math>
</inline-formula> method, selecting the 500 most significant features. These features from the burn dataset were fed to six different classifiers—XGBoost, LGBM, CatBoost, KNN, SVM, and RF—and trained. The resulting confusion matrices from the training process are presented in <xref ref-type="fig" rid="fig_12">Figure 12</xref>.</p>
          
            <fig id="fig_12">
              <label>Figure 12</label>
              <caption>
                <title>Confusion matrices of the proposed model</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_h4ZKfcp2Q3unUX8Z.png"/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_VDGP65HGInA30dq8.png"/>
            </fig>
          
          <p>An examination of the confusion matrices obtained from the proposed hybrid model reveals that most classifiers can predict first- and second-degree burns with high accuracy. The SVM and LGBM models, in particular, demonstrated quite successful performance in distinguishing these two classes. However, there was a common trend that all classifiers have difficulty in classifying third-degree burns. This is also clinically significant because third-degree burns cause severe tissue integrity disruption, and their visual similarity makes them more difficult for algorithms to distinguish.</p><p>Furthermore, while third-degree burns were more frequently confused with second-degree burns in the CatBoost and RF models, the SVM classifier performed relatively more evenly in this class. Despite the KNN model’s advantage in lower training time, the misclassification rate for third-degree burns was exceptionally high. The success metrics calculated from these matrices for each classifier are detailed in <xref ref-type="fig" rid="fig_13">Figure 13</xref>.</p>
          
            <fig id="fig_13">
              <label>Figure 13</label>
              <caption>
                <title>Performance comparison of classifiers using features of the proposed model</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_NF8haVv8E9qlK-uJ.png"/>
            </fig>
          
          
            <table-wrap id="table_5">
              <label>Table 5</label>
              <caption>
                <title>Comparison of accuracy, training time, and prediction speed of classifiers using features of the proposed model</title>
              </caption>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p>Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Prediction Speed (images/s)</p></th><th colspan="1" rowspan="1"><p>Training Time (s)</p></th></tr><tr><td colspan="1" rowspan="1"><p>SVM</p></td><td colspan="1" rowspan="1"><p>82.29</p></td><td colspan="1" rowspan="1"><p>835.61</p></td><td colspan="1" rowspan="1"><p>3.42</p></td></tr><tr><td colspan="1" rowspan="1"><p>LGBM</p></td><td colspan="1" rowspan="1"><p>80.51</p></td><td colspan="1" rowspan="1"><p>12813.90</p></td><td colspan="1" rowspan="1"><p>56.91</p></td></tr><tr><td colspan="1" rowspan="1"><p>XGBoost</p></td><td colspan="1" rowspan="1"><p>80.17</p></td><td colspan="1" rowspan="1"><p>41513.48</p></td><td colspan="1" rowspan="1"><p>119.67</p></td></tr><tr><td colspan="1" rowspan="1"><p>CatBoost</p></td><td colspan="1" rowspan="1"><p>78.56</p></td><td colspan="1" rowspan="1"><p>151636.96</p></td><td colspan="1" rowspan="1"><p>92.85</p></td></tr><tr><td colspan="1" rowspan="1"><p>KNN</p></td><td colspan="1" rowspan="1"><p>77.88</p></td><td colspan="1" rowspan="1"><p>5681.70</p></td><td colspan="1" rowspan="1"><p>0.03</p></td></tr><tr><td colspan="1" rowspan="1"><p>RF</p></td><td colspan="1" rowspan="1"><p>76.10</p></td><td colspan="1" rowspan="1"><p>7501.33</p></td><td colspan="1" rowspan="1"><p>42.25</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>The effect of the hybrid model in terms of accuracy and system performance is shown in <xref ref-type="table" rid="table_5">Table 5</xref>.</p><p>The results in <xref ref-type="fig" rid="fig_12">Figure 12</xref> and <xref ref-type="table" rid="table_5">Table 5</xref> show that the highest accuracy rate among the classifiers was achieved with the SVM classifier at 82.29%. LGBM and XGBoost achieved accuracies of 80.51% and 80.17%, values close to that of SVM. CatBoost and KNN achieved accuracies of 78.56% and 77.88%. The lowest performance was observed with the RF classifier with 76.10% accuracy. In terms of computational performance, the shortest training time was achieved with the KNN classifier with 0.03 seconds. Despite its slower prediction speed, the highest accuracy value was achieved with the SVM classifier. As shown in the performance metrics of the proposed model, all classifiers can distinguish first- and second-degree burns with high accuracy. However, the classifiers achieved lower performance in classifying third-degree burns.</p><p>The proposed model had particular difficulty distinguishing between second- and third-degree burns. <xref ref-type="fig" rid="fig_14">Figure 14</xref> shows some images of second- and third-degree burns.</p><p>In creating the dataset shown in <xref ref-type="fig" rid="fig_14">Figure 14</xref>, some images were taken very close up, or small sections were obtained by cropping different images specifically focusing on burns. This resulted in the second-degree and third-degree burns appearing similar. These images, and similar ones, resemble each other due to their blurriness and low-pixel quality. This is considered the primary reason why the model developed in this study confuses second-degree and third-degree burns.</p>
          
            <fig id="fig_14">
              <label>Figure 14</label>
              <caption>
                <title>Similar burn samples in the dataset</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2026/1/img_U9hzpoG0wO1tMDFd.jpeg"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Conclusion</title>
      <p>A burn is tissue damage caused by exposure of the skin or underlying tissues to external factors such as heat, chemicals, electricity, sunlight, or radiation. Burns are classified according to the depth and extent of the damage. The dataset used in this study categorized burns into three classes. In this study, feature extraction was performed using different transformer-based architectures to detect skin burns. These extracted feature maps were combined to bring together different features of the same image. The proposed model was then further optimized by feature selection, aiming to perform faster. The proposed model achieved an accuracy of 82.29%. However, the study has some limitations. One of the limitations is the use of a single-center public dataset. In future studies, a system that operates in real time with more data from different centers will be developed.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>Conceptualization, A.Y.D. and M.K.; methodology, A.Y.D. and M.K.; software, A.Y.D. and M.K.; validation, A.Y.D., M.K., and M.Y.; formal analysis, A.Y.D.; investigation, A.Y.D. and M.K.; resources, A.Y.D. and M.K.; data curation, A.Y.D. and M.K.; writing—original draft preparation, A.Y.D., M.K., and M.Y.; writing—review and editing, A.Y.D., M.K., and M.Y.; visualization, A.Y.D., M.K., and M.Y.; supervision, M.K. and M.Y.; project administration, M.K. and M.Y. All authors have read and agreed to the published version of the manuscript.</p>
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>100371</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Suha</surname>
              <given-names>S. A.</given-names>
            </name>
            <name>
              <surname>Sanam</surname>
              <given-names>T. F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.mlwa.2022.100371</pub-id>
          <article-title>A deep convolutional neural network-based approach for detecting burn severity from skin burn images</article-title>
          <source>Mach. Learn. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>33</volume>
          <page-range>946-957</page-range>
          <issue>8</issue>
          <year>2007</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pham</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Greenwood</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Cleland</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Woodruff</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Maddern</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.burns.2007.03.020</pub-id>
          <article-title>Bioengineered skin substitutes for the management of burns: A systematic review</article-title>
          <source>Burns</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>46</volume>
          <page-range>6-13</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Justin  Lee</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Abdolahnejad</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Morzycki</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Freeman</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Chan</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Hong</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Joshi</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Joshua  Wong</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1093/jbcr/irae121</pub-id>
          <article-title>Comparing artificial intelligence guided Image assessment to current methods of burn assessment</article-title>
          <source>J. Burn Care Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>7604</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Rangaiah</surname>
              <given-names>P. K.</given-names>
            </name>
            <name>
              <surname>Kumar</surname>
              <given-names>B. P.</given-names>
            </name>
            <name>
              <surname>Huss</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Augustine</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-025-92096-4</pub-id>
          <article-title>Precision diagnosis of burn injuries using imaging and predictive modeling for clinical applications</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>e14681</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Bu</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1111/iwj.14681</pub-id>
          <article-title>Non-invasive medical imaging technology for the diagnosis of burn depth</article-title>
          <source>Int. Wound J.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>53</volume>
          <page-range>319-337</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pabitha</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Vanathi</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11063-020-10387-5</pub-id>
          <article-title>DenseMask RCNN: A hybrid model for skin burn image classification and severity grading</article-title>
          <source>Neural Process. Lett.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>2018-2028</page-range>
          <issue>11</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Şevik</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Karakullukçu</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Berber</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Akbaş</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Türkyılmaz</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1049/iet-ipr.2018.5899</pub-id>
          <article-title>Automatic classification of skin burn colour images using texture-based feature extraction</article-title>
          <source>IET Image Process.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>18-30</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Abazari</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Ghaffari</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Rashidzadeh</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Badeleh</surname>
              <given-names>S. M.</given-names>
            </name>
            <name>
              <surname>Maleki</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1177/1534734620924857</pub-id>
          <article-title>A systematic review on classification, identification, and healing process of burn wound healing</article-title>
          <source>Int. J. Low. Extrem. Wounds</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>195</volume>
          <page-range>110556</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Elsarta</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Fathalla</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Nasser</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Elwatany</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Fekry</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Ebaid</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Mahmoud</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Anwar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Gaber</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compbiomed.2025.110556</pub-id>
          <article-title>Integrating multi-source data for skin burn classification using deep learning</article-title>
          <source>Comput. Biol. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>034014</page-range>
          <issue>3</issue>
          <year>2005</year>
          <person-group person-group-type="author">
            <name>
              <surname>Acha</surname>
              <given-names>B. A.</given-names>
            </name>
            <name>
              <surname>Serrano</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Acha</surname>
              <given-names>J. I.</given-names>
            </name>
            <name>
              <surname>Roa</surname>
              <given-names>L. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1117/1.1921227</pub-id>
          <article-title>Segmentation and classification of burn images by color and texture information</article-title>
          <source>J. Biomed. Opt.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>15-23</page-range>
          <issue>2–10</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kuan</surname>
              <given-names>P. N.</given-names>
            </name>
            <name>
              <surname>Chua</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Safawi</surname>
              <given-names>E. B.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H. H.</given-names>
            </name>
            <name>
              <surname>Tiong</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>A comparative study of the classification of skin burn depth in human</article-title>
          <source>J. Telecommun. Electron. Comput. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>190</volume>
          <page-range>387-393</page-range>
          <issue>Supplement_2</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Rahman</surname>
              <given-names>M. M.</given-names>
            </name>
            <name>
              <surname>Masry</surname>
              <given-names>M. E. L.</given-names>
            </name>
            <name>
              <surname>Gnyawali</surname>
              <given-names>S. C.</given-names>
            </name>
            <name>
              <surname>Xue</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Gordillo</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Wachs</surname>
              <given-names>J. P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1093/milmed/usaf198</pub-id>
          <article-title>A framework for advancing burn assessment with artificial intelligence</article-title>
          <source>Mil. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>79</volume>
          <page-range>34545-34568</page-range>
          <issue>45</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Khan</surname>
              <given-names>F. A.</given-names>
            </name>
            <name>
              <surname>Butt</surname>
              <given-names>A. U. R.</given-names>
            </name>
            <name>
              <surname>Asif</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Ahmad</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Nawaz</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Jamjoom</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Alabdulkreem</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-020-08768-y</pub-id>
          <article-title>Computer-aided diagnosis for burnt skin images using deep convolutional neural network</article-title>
          <source>Multimed. Tools Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="webpage">
          <article-title>Skin Burn Dataset</article-title>
          <source>, https://www.kaggle.com/datasets/faresabbasai2022/skin-burn-dataset/data</source>
          <year>2022</year>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>45</volume>
          <page-range>87-110</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Han</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TPAMI.2022.3152247</pub-id>
          <article-title>A survey on Vision Transformer</article-title>
          <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>54</volume>
          <page-range>Article 200</page-range>
          <issue>10s</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Khan</surname>
              <given-names>Salman</given-names>
            </name>
            <name>
              <surname>Naseer</surname>
              <given-names>Muzammal</given-names>
            </name>
            <name>
              <surname>Hayat</surname>
              <given-names>Munawar</given-names>
            </name>
            <name>
              <surname>Zamir</surname>
              <given-names>Syed Waqas</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>Fahad Shahbaz</given-names>
            </name>
            <name>
              <surname>Shah</surname>
              <given-names>Mubarak</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3505244</pub-id>
          <article-title>Transformers in Vision: A Survey</article-title>
          <source>ACM Comput. Surv.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="webpage">
          <article-title>ViT-Base Patch16 224 Model</article-title>
          <source>, https://huggingface.co/google/vit-base-patch16-224</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="webpage">
          <article-title>ViT-Base Patch32 224 In21k Model</article-title>
          <source>, https://huggingface.co/google/vit-base-patch32-224-in21k</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="webpage">
          <article-title>ViT-Large Patch16 224 AugReg In21k Model</article-title>
          <source>, https://huggingface.co/timm/vit_large_patch16_224.augreg_in21k</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Oquab</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Darcet</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Moutakanni</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Vo</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Szafraniec</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Khalidov</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Bojanowski</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2304.07193</pub-id>
          <article-title>DINOv2: Learning robust visual features without supervision</article-title>
          <source>arXiv preprint arXiv:2304.07193</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Song</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2402.15687</pub-id>
          <article-title>General purpose image encoder DINOv2 for medical image registration</article-title>
          <source>arXiv preprint arXiv:2402.15687</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>21</page-range>
          <year>2013</year>
          <person-group person-group-type="author">
            <name>
              <surname>Natekin</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Knoll</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fnbot.2013.00021</pub-id>
          <article-title>Gradient boosting machines, a tutorial</article-title>
          <source>Front. Neurorobot.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="conf-paper">
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ke</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Qi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Finley</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Ye</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>T.-Y.</given-names>
            </name>
          </person-group>
          <article-title>LightGBM: A highly efficient gradient boosting decision tree</article-title>
          <source>Advances in Neural Information Processing Systems</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Benesty</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Khotilovich</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Cho</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Mitchell</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Cano</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>XGBoost: Extreme Gradient Boosting</article-title>
          <source>R package version 0.4-2</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="conf-paper">
          <publisher-place>Montréal, Canada</publisher-place>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Prokhorenkova</surname>
              <given-names>P. L.</given-names>
            </name>
            <name>
              <surname>Gusev</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Vorobev</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Dorogush</surname>
              <given-names>A. V.</given-names>
            </name>
            <name>
              <surname>Gulin</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>CatBoost: Unbiased boosting with categorical features</article-title>
          <source>Advances in Neural Information Processing Systems</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="conf-paper">
          <page-range>986-996</page-range>
          <publisher-place>Berlin, Heidelberg</publisher-place>
          <year>2003</year>
          <person-group person-group-type="author">
            <name>
              <surname>Guo</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Bell</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Bi</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Greer</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-540-39964-3_62</pub-id>
          <article-title>KNN model-based approach in classification</article-title>
          <source>OTM Confederated International Conferences "On the Move to Meaningful Internet Systems"</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="report">
          <article-title>Tutorial on support vector machine (SVM)</article-title>
          <year>2006</year>
          <publisher-name>School of EECS, Washington State University, Technical Report</publisher-name>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="conf-paper">
          <publisher-place>Coimbatore, India</publisher-place>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Parmar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Katariya</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Patel</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-030-03146-6_86</pub-id>
          <article-title>A review on random forest: An ensemble classifier</article-title>
          <source>International Conference on Intelligent Data Communication Technologies and Internet of Things (ICICI 2018)</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>642-645</page-range>
          <issue>4</issue>
          <year>1997</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Setiono</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/69.617056</pub-id>
          <article-title>Feature selection via discretization</article-title>
          <source>IEEE Trans. Knowl. Data Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="conf-paper">
          <page-range>79-91</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yacouby</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Axman</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Probabilistic extension of precision, recall, and F1 score for more thorough evaluation of classification models</article-title>
          <source>Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>