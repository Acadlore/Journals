<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-EISNgt48kNLCJ-EX_cq9KxQJkpPosnjV</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml040105</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Advanced Image Restoration Through CIPFS-Integrated Mathematical Transformations</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-4826-6065</contrib-id>
          <name>
            <surname>Husain</surname>
            <given-names>Zakir</given-names>
          </name>
          <email>zakir1995@uop.edu.pk</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2526-6927</contrib-id>
          <name>
            <surname>Yow</surname>
            <given-names>Kai Siong</given-names>
          </name>
          <email>ksyow@upm.edu.my</email>
        </contrib>
        <aff id="aff_1">Department of Mathematics, University of Peshawar, 25120 Peshawar, Pakistan</aff>
        <aff id="aff_2">Department of Mathematics and Statistics, Faculty of Science, Universiti Putra Malaysia, 43400 Serdang, Malaysia</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>27</day>
        <month>03</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>1</issue>
      <fpage>50</fpage>
      <lpage>61</lpage>
      <page-range>50-61</page-range>
      <history>
        <date date-type="received">
          <day>02</day>
          <month>02</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>21</day>
          <month>03</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>The restoration of blurred images remains a critical challenge in computational image processing, necessitating advanced methodologies capable of reconstructing fine details while mitigating structural degradation. In this study, an innovative image restoration framework was introduced, employing Complex Interval Pythagorean Fuzzy Sets (CIPFSs) integrated with mathematically structured transformations to achieve enhanced deblurring performance. The proposed methodology initiates with the geometric correction of pixel-level distortions induced by blurring. A key innovation lies in the incorporation of CIPFS-based entropy, which is synergistically combined with local statistical energy to enable robust blur estimation and adaptive correction. Unlike traditional fuzzy logic-based approaches, CIPFS facilitates a more expressive modeling of uncertainty by leveraging complex interval-valued membership functions, thereby enabling nuanced differentiation of blur intensity across image regions. A fuzzy inference mechanism was utilized to guide the refinement process, ensuring that localized corrections are adaptively applied to degraded regions while leaving undistorted areas unaffected. To preserve edge integrity, a geometric step function was applied to reinforce structural boundaries and suppress over-smoothing artifacts. In the final restoration phase, structural consistency is enforced through normalization and regularization techniques to ensure coherence with the original image context. Experimental validations demonstrate that the proposed model delivers superior image clarity, improved edge sharpness, and reduced visual artifacts compared to state-of-the-art deblurring methods. Enhanced robustness against varying blur patterns and noise intensities was also confirmed, indicating strong generalization potential. By unifying the expressive power of CIPFS with analytically driven restoration strategies, this approach contributes a significant advancement to the domain of image deblurring and restoration under uncertainty.</p></abstract>
      <kwd-group>
        <kwd>Blurred image restoration</kwd>
        <kwd>CIPFSs</kwd>
        <kwd>Fuzzy logic</kwd>
        <kwd>Mathematical transformations</kwd>
        <kwd>Image clarity enhancement</kwd>
        <kwd>Fuzzy edge enhancement</kwd>
        <kwd>Image processing</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="2"/>
        <fig-count count="3"/>
        <table-count count="1"/>
        <ref-count count="22"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Image processing plays a crucial role in various computer vision applications, including medical diagnostics, environmental monitoring, and industrial automation [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>]. One of the key tasks in these domains is defect image restoration, which is essential for ensuring accuracy and reliability in subsequent analyses. In real-world scenarios, however, images often suffer from degradations such as noise, blur, occlusions, and distortions, which can significantly affect the performance of image processing systems [<xref ref-type="bibr" rid="ref_5">5</xref>].</p><p>When it comes to defect detection, image blur caused by factors like motion, incorrect focus, or environmental conditions can obscure critical details, making it challenging to identify surface defects, scratches, or misalignments. Atmospheric phenomena such as fog and haze introduce additional difficulties in fields like satellite imaging and autonomous driving, where visibility is of utmost importance [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>]. Poor image quality can lead to serious issues, such as missed defects during industrial inspections or incorrect diagnoses in medical imaging due to low-resolution scans [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>]. In satellite-based Earth monitoring, distortions caused by atmospheric factors, including fog and clouds, can block vital environmental information, impeding disaster response and climate research [<xref ref-type="bibr" rid="ref_9">9</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>]. As such, enhancing image restoration techniques is essential for improving the accuracy of these applications.</p><p>Traditional image processing techniques, such as median filtering, Gaussian smoothing, and edge detection, have been extensively used to address issues of image degradation [<xref ref-type="bibr" rid="ref_11">11</xref>], [<xref ref-type="bibr" rid="ref_12">12</xref>]. While effective for some specific cases, these methods often struggle when faced with complex distortions that don’t follow a uniform pattern. For instance, when an image is affected by multiple degradations, such as mixed Gaussian blur and salt-and-pepper noise, conventional methods often fail to adapt [<xref ref-type="bibr" rid="ref_13">13</xref>]. Moreover, these techniques assume a uniform noise distribution, which is rarely the case in real-world scenarios. These limitations emphasize the need for more advanced methods that can handle uncertainty and variations in image defects.</p><p>The field of image deblurring has seen significant progress with the introduction of deep learning models that show promising results. For instance, Jiang et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] developed a Convolutional Neural Network (CNN)-based method that effectively reduces motion blur. However, the model’s reliance on large-scale annotated datasets limits its performance on unseen blur types. Similarly, Xiang et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] evaluated deep learning models for motion deblurring and found that while these models perform well on synthetic datasets, their real-world performance is hindered by unpredictable variations in lighting, noise, and blur patterns. Aoi and Goto [<xref ref-type="bibr" rid="ref_16">16</xref>] improved structural detail restoration by incorporating self-attention mechanisms into a ConvNeXt-V2-based model. However, the high computational demands of this approach make it unsuitable for real-time applications. Chen and Liu [<xref ref-type="bibr" rid="ref_17">17</xref>] proposed a diffusion-based deblurring method that iteratively refines image details, producing high-quality results but with increased computational cost. Varghese et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] developed a real-time deblurring model for gimbal-based imaging systems, which performs well in structured motion blur cases but struggles with non-uniform blur patterns.</p><p>In recent years, advanced techniques like wavelet transforms, non-local means filtering, and deep learning-based methods have been explored for defect restoration. Zhai et al. [<xref ref-type="bibr" rid="ref_19">19</xref>] reviewed deep learning-based image restoration methods, categorizing them into CNNs, Generative Adversarial Networks (GANs), Transformers, and Multilayer Perceptrons (MLPs). These methods effectively restore degraded images by learning complex features, with CNNs excelling in spatial feature extraction and GANs improving image realism through adversarial training. While GANs and CNNs offer significant efficiency in image restoration and enhanced visual quality, they have limitations. These models require large, high-quality datasets, which may not always be available for real-world degradation types. Furthermore, CNNs struggle with handling multiple types of degradation, and GANs face challenges such as training instability and high computational requirements. Real-time applications also need to balance the quality of restoration with computational efficiency.</p><p>A common challenge across deep learning-based methods is their reliance on vast annotated datasets for supervised training. Many models depend on paired blurred and sharp images, which are difficult to acquire for real-world applications. Synthetic datasets often fail to capture the full range of real-world blur patterns, resulting in diminished performance on unseen data. Another limitation is the interpretability of these models, as their deblurring processes function as “black boxes,” making it difficult to control or understand their decision-making mechanisms.</p><p>To address these limitations, this study proposes a novel CIPFS-based blurred image processing model, which integrates the principles of fuzzy logic and mathematical image processing with CIPFS for enhanced image restoration. The proposed model offers a flexible framework that adapts to varying degrees of blur and uncertainty without relying on extensive training datasets. The key contributions of this study include the formulation of fuzzy membership functions for blur classification, a rule-based fuzzy correction mechanism, and a mathematical fusion strategy for edge-aware restoration. By utilizing CIPFS, the model’s ability to handle different types of uncertainty was enhanced and multiple fuzzy sets were effectively combined for a more accurate restoration. This approach improves adaptability to various blur types and ensures better generalization to real-world images. This research holds significant potential for applications in surveillance, autonomous navigation, and remote sensing, where clear and accurate image reconstruction is essential.</p><p>The proposed model integrates CIPFS, mathematical transformations, and edge enhancement to achieve superior image restoration performance. The selection of these specific techniques is driven by their ability to effectively handle uncertainty, preserve structural details, and enhance image clarity, which conventional approaches often fail to achieve. CIPFS provides a robust mathematical framework for modeling complex uncertainties in image data, outperforming traditional fuzzy sets in capturing intricate variations. Mathematical transformations, particularly in pixel coordinate correction and CIPFS-weighted filtering, ensure precise spatial alignment and noise suppression. Edge enhancement using CIPFS-based techniques prevents boundary leakage and maintains structural integrity, which is critical for high-quality image restoration. Other alternatives, such as conventional wavelet-based deblurring or histogram equalization, were not considered due to their limitations in handling non-uniform noise and intensity variations.</p><p>The rest of this study is organized as follows: Section 2 presents related work, discussing previous methods in image restoration and their limitations. Section 3 introduces the proposed mathematical framework, detailing the integration of fuzzy logic with mathematical image processing techniques. Section 4 presents the experimental results, including the evaluation metrics and performance comparison of the proposed model with existing methods. Finally, Section 5 concludes this study, summarizing the key findings and suggesting future directions for research.</p>
    </sec>
    <sec sec-type="">
      <title>2. Related work</title>
      <p>Image defect restoration has been the subject of considerable research, with various techniques proposed to address the challenges posed by image degradation. Among these, fuzzy logic has emerged as a promising tool due to its ability to effectively manage uncertainty in image restoration tasks. Zhao et al. [<xref ref-type="bibr" rid="ref_20">20</xref>] introduced a framework using fuzzy radial basis function (RBF) neural networks for motion-blurred image restoration. This approach estimates the blur parameters and adapts to dynamic environments, making it more flexible than traditional methods. The model uses fuzzy logic to handle the uncertainty inherent in motion blur, allowing it to adjust its restoration process to different blur conditions. Although this framework offers significant advantages over conventional methods, the accuracy of blur parameter estimation is crucial to its performance. Inaccurate determination of these parameters can result in degraded restoration results, undermining the overall effectiveness of the model. Additionally, the fuzzy-RBF network requires large amounts of labeled training data, which can be a limitation in real-world applications where such datasets are not readily available. This constraint reduces the model’s generalizability and its ability to handle unseen types of blur or real-world variations in image quality. </p><p>Similarly, the Blind Image Deblurring Framework (BLIF) proposed by Peng et al. [<xref ref-type="bibr" rid="ref_21">21</xref>] addresses the challenge of blind image deblurring through a novel approach based on minimizing the similarity between fuzzy sets on image pixels. By utilizing fuzzy set theory, this method models uncertainty in the image and improves restoration accuracy by distinguishing between sharp and blurred regions. BLIF is effective in handling multiple blur types, including motion blur and defocus blur, and excels in restoring fine image details. The method is particularly effective for images with varying blur conditions, showcasing its versatility compared to traditional deblurring methods. However, one limitation of the BLIF model is its reliance on similarity metrics within fuzzy sets, which can result in significant computational overhead when processing high-resolution images or images with complex, non-uniform blur patterns. As the method processes pixel-wise fuzzy sets, it becomes computationally expensive, particularly in the case of large images. Furthermore, while BLIF shows promising results, its generalization capability to real-world scenarios remains uncertain. The model was primarily tested on synthetic datasets, and its performance under real-world conditions—where images are often subject to noise, varying illumination, and more complex distortions—remains a challenge. This gap in generalization underscores the need for more adaptable models that can handle diverse image quality variations in practical applications.</p><p>To overcome these limitations, a CIPFS-based image restoration model was proposed in this study, which integrates the power of fuzzy logic with mathematical image processing techniques using CIPFS. The CIPFS-based model is designed to address the uncertainty in both blur and noise by combining multiple fuzzy sets in a way that enhances the image restoration process while reducing computational overhead. Unlike the fuzzy-RBF network or BLIF, the CIPFS model can effectively manage the complexity of high-resolution images and handle real-world scenarios with varying illumination and noise. It offers a flexible framework that adapts to different types of blur and uncertainty without the need for extensive labeled datasets, making it more practical for real-world applications. Moreover, the CIPFS-based approach enhances computational efficiency by minimizing unnecessary computations, providing an effective balance between performance and speed. </p><p>While existing models like fuzzy-RBF and BLIF have demonstrated effectiveness in their specific domains, the proposed model introduces a comprehensive framework that enhances accuracy and robustness in image segmentation and deblurring through the integration of CIPFS. Unlike conventional methods that struggle with noise, intensity variations, and boundary preservation, the proposed approach systematically transforms and corrects pixel coordinates using CIPFS, ensuring precise spatial representation. The incorporation of CIPFS in fuzzy logic-based deblurring, particularly through CIPFS-weighted filtering and mathematical integration for blur restoration, significantly improves clarity while reducing artifacts. Furthermore, the proposed model employs CIPFS-based edge enhancement for blur correction, leading to sharper boundaries and better structural preservation. The normalization and structural refinement process ensures consistency in segmentation results across different imaging conditions. Through these advancements, the proposed model consistently outperforms traditional approaches in terms of segmentation accuracy, robustness against intensity fluctuations, and resilience to boundary leakage, establishing a new benchmark in image restoration and medical image analysis.</p>
    </sec>
    <sec sec-type="">
      <title>3. Mathematical framework of the proposed model</title>
      <p>This section presents the CIPFS-based image restoration model, a novel approach that integrates CIPFS with advanced mathematical image processing techniques to address the challenges of image degradation. The proposed model leverages the flexibility of fuzzy logic to effectively handle uncertainty in image quality. The model is designed to restore images affected by a wide range of degradations, including motion blur, defocus, noise, and varying illumination conditions, without the need for large-scale annotated datasets. This approach combines the benefits of fuzzy set theory with efficient mathematical fusion strategies, offering a robust framework that adapts to diverse types of blur and noise while maintaining high computational efficiency. By employing CIPFS, the model captures complex relationships between blurred and sharp regions in an image, ensuring improved restoration performance and generalization to real-world scenarios.</p>
      
        <sec>
          
            <title>3.1. Transformation of pixel coordinates using cipfs</title>
          
          <p>Considering a blurred image with a resolution of <inline-formula>
  <mml:math id="mwsgaqnjif">
    <mml:msub>
      <mml:mi>B</mml:mi>
      <mml:mi>m</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>B</mml:mi>
      <mml:mi>n</mml:mi>
    </mml:msub>
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula>, where, <inline-formula>
  <mml:math id="m36qno24y8">
    <mml:msub>
      <mml:mi>B</mml:mi>
      <mml:mi>m</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mwbtit5284">
    <mml:msub>
      <mml:mi>B</mml:mi>
      <mml:mi>n</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> indicate the height and width of the pixel grid, the Cartesian coordinate transformation was extended to accommodate uncertainty using CIPFS. The transformation is defined as:</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="mic2lqgefl">
                <mml:msub>
                  <mml:mi>B</mml:mi>
                  <mml:mi>m</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>x</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>x</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>z</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>μ</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>B</mml:mi>
                  <mml:mi>n</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>y</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>y</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>z</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>ν</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:mi>j</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>⋅</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mrow>
                  <mml:msup>
                    <mml:mi>e</mml:mi>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:msub>
                        <mml:mi>θ</mml:mi>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                  </mml:msup>
                </mml:mrow>
                <mml:mrow>
                  <mml:msup>
                    <mml:mi>e</mml:mi>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:msub>
                        <mml:mi>ϕ</mml:mi>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                  </mml:msup>
                </mml:mrow>
                <mml:mstyle scriptlevel="0">
                  <mml:mspace width="1em"/>
                </mml:mstyle>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="ms0u4kk4xx">
    <mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="mujm9nymzk">
    <mml:msub>
      <mml:mi>z</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> correspond to pixel locations along the $x<inline-formula>
  <mml:math id="m7ujq8ofqx">
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>y<inline-formula>
  <mml:math id="mtg0cp25e1">
    <mml:mo>,</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>z<inline-formula>
  <mml:math id="mq3zfoc2za">
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\mu_{i j}<inline-formula>
  <mml:math id="mg39bxutvl">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\nu_{i j}<inline-formula>
  <mml:math id="miv8cpov4u">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>P</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>−</mml:mo>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>\theta_{i j}<inline-formula>
  <mml:math id="mjpqrqvhnn">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\phi_{i j}$ model phase uncertainties in the coordinate transformations.</p><p>The CIPFS-based Euclidean distance of a pixel from the center of the image is given by:</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="m27fgb3pft">
                <mml:msub>
                  <mml:mi>d</mml:mi>
                  <mml:mrow>
                    <mml:mi>x</mml:mi>
                    <mml:mi>y</mml:mi>
                    <mml:mi>z</mml:mi>
                    <mml:mo>,</mml:mo>
                    <mml:mo>,</mml:mo>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:msqrt>
                  <mml:msubsup>
                    <mml:mi>x</mml:mi>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:msubsup>
                  <mml:msubsup>
                    <mml:mi>y</mml:mi>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:msubsup>
                  <mml:msubsup>
                    <mml:mi>z</mml:mi>
                    <mml:mrow>
                      <mml:mi>i</mml:mi>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:msubsup>
                  <mml:mo>+</mml:mo>
                  <mml:mo>+</mml:mo>
                  <mml:mo>+</mml:mo>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>π</mml:mi>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msup>
                        <mml:mi>e</mml:mi>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:msub>
                            <mml:mi>γ</mml:mi>
                            <mml:mrow>
                              <mml:mi>i</mml:mi>
                              <mml:mi>j</mml:mi>
                            </mml:mrow>
                          </mml:msub>
                        </mml:mrow>
                      </mml:msup>
                    </mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:msup>
                </mml:msqrt>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mkjiozn456">
    <mml:msub>
      <mml:mi>π</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> is the hesitation degree in the CIPFS model, representing additional uncertainty in blur estimation.</p><p>To model the projection process for image restoration, the modified relationship is given by:</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="mgtqhejd65">
                <mml:mi>tan</mml:mi>
                <mml:mo>⁡</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:msub>
                  <mml:mi>Θ</mml:mi>
                  <mml:mi>k</mml:mi>
                </mml:msub>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>q</mml:mi>
                      <mml:mi>k</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>μ</mml:mi>
                      <mml:mi>k</mml:mi>
                    </mml:msub>
                    <mml:mo>+</mml:mo>
                    <mml:msup>
                      <mml:mi>e</mml:mi>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:msub>
                          <mml:mi>δ</mml:mi>
                          <mml:mi>k</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                    </mml:msup>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>Z</mml:mi>
                      <mml:mi>k</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>ν</mml:mi>
                      <mml:mi>k</mml:mi>
                    </mml:msub>
                    <mml:mo>+</mml:mo>
                    <mml:msup>
                      <mml:mi>e</mml:mi>
                      <mml:mrow>
                        <mml:mi>i</mml:mi>
                        <mml:msub>
                          <mml:mi>η</mml:mi>
                          <mml:mi>k</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                    </mml:msup>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="m0q8tjdyrl">
    <mml:msub>
      <mml:mi>q</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> denotes the linear separation between a pixel in the blurred image and its restored location, and <inline-formula>
  <mml:math id="madid7sytj">
    <mml:msub>
      <mml:mi>Z</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is the total pixel count in the blurred image, both adjusted by their respective complex interval fuzzy parameters.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Correction of pixel coordinates using cipfs</title>
          
          <p>To mitigate distortions introduced by blurring effects, pixel coordinate corrections incorporate CIPFS elements as follows:</p>
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="mh4kealjez">
                <mml:mi>X</mml:mi>
                <mml:mi>Y</mml:mi>
                <mml:mi>Z</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:msub>
                  <mml:mi>λ</mml:mi>
                  <mml:mn>0</mml:mn>
                </mml:msub>
                <mml:msub>
                  <mml:mi>R</mml:mi>
                  <mml:mn>1</mml:mn>
                </mml:msub>
                <mml:msub>
                  <mml:mi>μ</mml:mi>
                  <mml:mi>X</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>λ</mml:mi>
                  <mml:mn>1</mml:mn>
                </mml:msub>
                <mml:msub>
                  <mml:mi>R</mml:mi>
                  <mml:mn>2</mml:mn>
                </mml:msub>
                <mml:msub>
                  <mml:mi>ν</mml:mi>
                  <mml:mi>Y</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>λ</mml:mi>
                  <mml:mn>2</mml:mn>
                </mml:msub>
                <mml:msub>
                  <mml:mi>R</mml:mi>
                  <mml:mn>3</mml:mn>
                </mml:msub>
                <mml:msub>
                  <mml:mi>π</mml:mi>
                  <mml:mi>Z</mml:mi>
                </mml:msub>
                <mml:msup>
                  <mml:mi>e</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:msub>
                      <mml:mi>θ</mml:mi>
                      <mml:mi>X</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:msup>
                <mml:msup>
                  <mml:mi>e</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:msub>
                      <mml:mi>ϕ</mml:mi>
                      <mml:mi>Y</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:msup>
                <mml:msup>
                  <mml:mi>e</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:msub>
                      <mml:mi>γ</mml:mi>
                      <mml:mi>Z</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:msup>
                <mml:mstyle scriptlevel="0">
                  <mml:mspace width="1em"/>
                </mml:mstyle>
                <mml:mstyle scriptlevel="0">
                  <mml:mspace width="1em"/>
                </mml:mstyle>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mbp7pz8euy">
    <mml:msub>
      <mml:mi>λ</mml:mi>
      <mml:mn>0</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>λ</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="m4btieng6m">
    <mml:msub>
      <mml:mi>λ</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> are correction parameters derived through CIPFS-based membership functions, and <inline-formula>
  <mml:math id="m17f3bkgil">
    <mml:msub>
      <mml:mi>R</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>R</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="mzlnhm0w58">
    <mml:msub>
      <mml:mi>R</mml:mi>
      <mml:mn>3</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> are the radii of angles formed by light interaction with the imaging lens, adjusted by their complex interval uncertainties. </p><p>By incorporating CIPFS-based corrections, pixel displacements induced by the blur kernel were adjusted more effectively, ensuring robust image reconstruction.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Integration of cipfs in fuzzy logic-based deblurring</title>
          
          <p>To address uncertainty in blur estimation, complex interval Pythagorean fuzzy logic was applied. Adaptive membership functions were designed to incorporate complex interval values, preserving sharp edges while minimizing distortions.</p><p>Fuzzy membership functions quantify the degree of blurriness at each pixel. The blur levels were classified into high, medium, and low using the following CIPFS-based function:</p>
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="m4zirz9leq">
                <mml:msub>
                  <mml:mi>μ</mml:mi>
                  <mml:mrow>
                    <mml:mtext>blur </mml:mtext>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mi>x</mml:mi>
                <mml:mrow>
                  <mml:mo>{</mml:mo>
                  <mml:mo fence="true"/>
                  <mml:mtable columnalign="left left" columnspacing="1em" rowspacing=".2em">
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mn>1</mml:mn>
                        <mml:mo>+</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:msub>
                          <mml:mi>μ</mml:mi>
                          <mml:mi>x</mml:mi>
                        </mml:msub>
                        <mml:msup>
                          <mml:mi>e</mml:mi>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                            <mml:msub>
                              <mml:mi>θ</mml:mi>
                              <mml:mi>x</mml:mi>
                            </mml:msub>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mtd>
                      <mml:mtd>
                        <mml:mtext> if </mml:mtext>
                        <mml:mi>x</mml:mi>
                        <mml:mo>≥</mml:mo>
                        <mml:msub>
                          <mml:mi>T</mml:mi>
                          <mml:mi>h</mml:mi>
                        </mml:msub>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mfrac>
                          <mml:mrow>
                            <mml:mi>x</mml:mi>
                            <mml:mo>−</mml:mo>
                            <mml:msub>
                              <mml:mi>T</mml:mi>
                              <mml:mi>l</mml:mi>
                            </mml:msub>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mi>T</mml:mi>
                              <mml:mi>h</mml:mi>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>T</mml:mi>
                              <mml:mi>l</mml:mi>
                            </mml:msub>
                            <mml:mo>−</mml:mo>
                          </mml:mrow>
                        </mml:mfrac>
                        <mml:mo>+</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:msub>
                          <mml:mi>ν</mml:mi>
                          <mml:mi>x</mml:mi>
                        </mml:msub>
                        <mml:msup>
                          <mml:mi>e</mml:mi>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                            <mml:msub>
                              <mml:mi>ϕ</mml:mi>
                              <mml:mi>x</mml:mi>
                            </mml:msub>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mtd>
                      <mml:mtd>
                        <mml:mtext> if </mml:mtext>
                        <mml:msub>
                          <mml:mi>T</mml:mi>
                          <mml:mi>l</mml:mi>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>T</mml:mi>
                          <mml:mi>h</mml:mi>
                        </mml:msub>
                        <mml:mo>&lt;</mml:mo>
                        <mml:mo>&lt;</mml:mo>
                        <mml:mi>x</mml:mi>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mn>0</mml:mn>
                        <mml:mo>+</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:msub>
                          <mml:mi>π</mml:mi>
                          <mml:mi>x</mml:mi>
                        </mml:msub>
                        <mml:msup>
                          <mml:mi>e</mml:mi>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                            <mml:msub>
                              <mml:mi>γ</mml:mi>
                              <mml:mi>x</mml:mi>
                            </mml:msub>
                          </mml:mrow>
                        </mml:msup>
                      </mml:mtd>
                      <mml:mtd>
                        <mml:mtext> if </mml:mtext>
                        <mml:mi>x</mml:mi>
                        <mml:mo>≤</mml:mo>
                        <mml:msub>
                          <mml:mi>T</mml:mi>
                          <mml:mi>l</mml:mi>
                        </mml:msub>
                      </mml:mtd>
                    </mml:mtr>
                  </mml:mtable>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mqdwadcqlw">
    <mml:msub>
      <mml:mi>T</mml:mi>
      <mml:mi>h</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="m7nl260cnt">
    <mml:msub>
      <mml:mi>T</mml:mi>
      <mml:mi>l</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> are intensity thresholds defining the blur range, and <inline-formula>
  <mml:math id="mfblqgy72y">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mi>x</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>, <inline-formula>
  <mml:math id="mx9ngs7y9i">
    <mml:msub>
      <mml:mi>ν</mml:mi>
      <mml:mi>x</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="mui83kq8xe">
    <mml:msub>
      <mml:mi>π</mml:mi>
      <mml:mi>x</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> represent the CIPFS-based membership, non-membership, and hesitation degrees, respectively, each carrying an associated phase component <inline-formula>
  <mml:math id="mt4mrm1d5c">
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>θ</mml:mi>
        <mml:mi>x</mml:mi>
      </mml:msub>
      <mml:msub>
        <mml:mi>ϕ</mml:mi>
        <mml:mi>x</mml:mi>
      </mml:msub>
      <mml:msub>
        <mml:mi>γ</mml:mi>
        <mml:mi>x</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula>. This enhances uncertainty handling in blur estimation and correction. </p><p>By leveraging CIPFS, this refined fuzzy deblurring approach yields several benefits: a) Enhanced uncertainty modeling. Complex interval values improve robustness against variations in blur intensity. b) Edge preservation. Complex hesitancy components refine pixel-wise adjustments, reducing over-smoothing. c) Higher image quality, i.e., enhanced restoration accuracy with adaptive correction parameters. </p><p>This transformation and correction framework improves deblurring precision, making it particularly effective in handling complex image distortions.</p>
          
            <sec>
              
                <title>3.3.1 Fuzzy rule-based mechanism with cipfs</title>
              
              <p>A rule-based fuzzy system determines how to correct blur at each pixel by associating input blur levels and spatial relationships with corrective measures. The proposed model leverages CIPFS to represent the degrees of blur correction with enhanced flexibility and robustness. Some example rules include:</p><p>• Rule 1: If the blur level is high and the pixel is close to an edge, a strong correction is applied based on CIPFS-based similarity measures.</p><p>• Rule 2: If the blur level is moderate and the pixel is at a medium distance from an edge, a moderate correction is applied using CIPFS-modulated weights.</p><p>• Rule 3: If the blur level is low and the pixel is far from an edge, a weak correction governed by CIPFS confidence values is applied.</p><p>These rules integrate expert knowledge with the CIPFS principles to dynamically adapt to variations in blur intensity and spatial image characteristics.</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.2 Cipfs aggregation and defuzzification</title>
              
              <p>Following the application of rules, the fuzzy outputs are aggregated into a composite complex interval Pythagorean fuzzy set representing the corrective modifications needed to restore the image. Each rule contributes based on its corresponding complex-valued membership function and weight. </p><p>Defuzzification translates the CIPFS-derived fuzzy set into a definitive correction value, utilizing a modified centroid method:</p>
              
                <disp-formula>
                  <label>(6)</label>
                  <mml:math id="m61zah47m1">
                    <mml:mi>C</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mo>∫</mml:mo>
                          <mml:mi>x</mml:mi>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>μ</mml:mi>
                          <mml:mi>C</mml:mi>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>ν</mml:mi>
                          <mml:mi>C</mml:mi>
                        </mml:msub>
                        <mml:mi>x</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:mi>d</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:mo>⋅</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mo>⋅</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:msub>
                          <mml:mo>∫</mml:mo>
                          <mml:mi>x</mml:mi>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>μ</mml:mi>
                          <mml:mi>C</mml:mi>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>ν</mml:mi>
                          <mml:mi>C</mml:mi>
                        </mml:msub>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mo>⋅</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>x</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:mi>d</mml:mi>
                        <mml:mi>x</mml:mi>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="ms93vkixvz">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mi>C</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mqau83avpg">
    <mml:msub>
      <mml:mi>ν</mml:mi>
      <mml:mi>C</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
  </mml:math>
</inline-formula> represent the complex-valued membership and non-membership functions of the aggregated CIPFS, respectively.</p><p>This process ensures that the computed correction value maintains a balance between correction robustness and preservation of fine details. By integrating multiple corrective actions and their associated fuzzy memberships, the centroid method in CIPFS produces an adjusted pixel value that adheres to the extended Pythagorean fuzzy logic principles while restoring image clarity.</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.3 Cipfs-weighted filtering</title>
              
              <p>A CIPFS-weighted filtering technique was utilized to determine correction intensity based on the blur level at each pixel. This approach blends information from both the original and CIPFS-corrected images, effectively restoring blurred sections while preserving unaltered regions. The corrected pixel intensity, <inline-formula>
  <mml:math id="mkxh3ctls8">
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mrow>
        <mml:mtext>corrected</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>, is computed as:</p>
              
                <disp-formula>
                  <label>(7)</label>
                  <mml:math id="mxc6ratmlj">
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mrow>
                        <mml:mtext>corrected</mml:mtext>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>μ</mml:mi>
                      <mml:mi>C</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>ν</mml:mi>
                      <mml:mi>C</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mrow>
                        <mml:mtext>CIPFS</mml:mtext>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mrow>
                        <mml:mtext>original</mml:mtext>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>⋅</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mi>y</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>y</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>y</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>y</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>y</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>⋅</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mn>1</mml:mn>
                      <mml:msub>
                        <mml:mi>μ</mml:mi>
                        <mml:mi>C</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>ν</mml:mi>
                        <mml:mi>C</mml:mi>
                      </mml:msub>
                      <mml:mi>x</mml:mi>
                      <mml:mi>y</mml:mi>
                      <mml:mi>x</mml:mi>
                      <mml:mi>y</mml:mi>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="m6aq4h78qh">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mi>C</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="m558wk6c2p">
    <mml:msub>
      <mml:mi>ν</mml:mi>
      <mml:mi>C</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> represent the membership and non-membership degrees of CIPFS for the blur degree at pixel <inline-formula>
  <mml:math id="mgr40a8s9a">
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>; <inline-formula>
  <mml:math id="mpt7un50u0">
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mrow>
        <mml:mrow>
          <mml:mi data-mjx-auto-op="false">CIPFS</mml:mi>
        </mml:mrow>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> denotes the intensity after applying CIPFS-based fuzzy corrections; and <inline-formula>
  <mml:math id="m3ej2vhqjn">
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mrow>
        <mml:mtext>original</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> corresponds to the unprocessed intensity.</p><p>For pixels in heavily blurred regions where <inline-formula>
  <mml:math id="mmwlx5xgq0">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mi>C</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>ν</mml:mi>
      <mml:mi>C</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>⋅</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>≈</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mn>0.8</mml:mn>
  </mml:math>
</inline-formula>, the restoration depends primarily on <inline-formula>
  <mml:math id="mir53q1osz">
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mrow>
        <mml:mrow>
          <mml:mi data-mjx-auto-op="false">CIPFS</mml:mi>
        </mml:mrow>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>. Conversely, for less blurred areas where <inline-formula>
  <mml:math id="m0yvuf1iqo">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mi>C</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>ν</mml:mi>
      <mml:mi>C</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>⋅</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>≈</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mn>0.2</mml:mn>
  </mml:math>
</inline-formula>, the correction mainly retains <inline-formula>
  <mml:math id="mhvp2wnkyt">
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mrow>
        <mml:mtext>original</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>. </p><p>By employing this approach, the restoration process remains localized, preserving clarity in sharp regions while effectively correcting blurred areas. The CIPFS framework ensures adaptive and robust image restoration, mitigating diverse blurring artifacts without over-processing unblurred regions.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.4. Cipfs-mathematical transformation integration for blur restoration</title>
          
          <p>The proposed methodology leverages a combination of CIPFS and mathematical transformations to improve the restoration of blurred images. This hybrid approach capitalizes on the extended flexibility of CIPFS in handling uncertainty and noise while utilizing precise mathematical adjustments to refine image clarity. The result is an effective restoration technique tailored to address varying levels of blur. </p><p>During the restoration process, pixel coordinates were corrected based on a combination of CIPFS-based membership functions and mathematical transformation rules, formulated as follows:</p>
          
            <disp-formula>
              <label>(8)</label>
              <mml:math id="mnuxh3249m">
                <mml:mi>X</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mi>Y</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>+</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>λ</mml:mi>
                    <mml:mn>0</mml:mn>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>R</mml:mi>
                    <mml:mn>1</mml:mn>
                  </mml:msub>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>+</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>λ</mml:mi>
                    <mml:mn>1</mml:mn>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>R</mml:mi>
                    <mml:mn>2</mml:mn>
                  </mml:msub>
                </mml:mrow>
                <mml:msub>
                  <mml:mi>μ</mml:mi>
                  <mml:mi>C</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>ν</mml:mi>
                  <mml:mi>C</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>μ</mml:mi>
                  <mml:mi>C</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>ν</mml:mi>
                  <mml:mi>C</mml:mi>
                </mml:msub>
                <mml:mstyle scriptlevel="0">
                  <mml:mspace width="1em"/>
                </mml:mstyle>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mzgg1uma2e">
    <mml:msub>
      <mml:mi>λ</mml:mi>
      <mml:mn>0</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mtbdue1q9y">
    <mml:msub>
      <mml:mi>λ</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> represent correction factors for the $X<inline-formula>
  <mml:math id="mid9pqekgj">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>Y<inline-formula>
  <mml:math id="m15p00lz3i">
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>R_1<inline-formula>
  <mml:math id="m451tsgaah">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>R_2<inline-formula>
  <mml:math id="m32q6gbsgg">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>\mu_C(x, y) \cdot<inline-formula>
  <mml:math id="m61bme3e0x">
    <mml:msub>
      <mml:mi>ν</mml:mi>
      <mml:mi>C</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> determines the complex-valued blur intensity at pixel <inline-formula>
  <mml:math id="m4ybbw3aa1">
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>, with values ranging from 0 (no blur) to 1 (severe blur).</p><p>For areas with significant blur <inline-formula>
  <mml:math id="moz0a1rn6a">
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>⋅</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>≈</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>μ</mml:mi>
        <mml:mi>C</mml:mi>
      </mml:msub>
      <mml:msub>
        <mml:mi>ν</mml:mi>
        <mml:mi>C</mml:mi>
      </mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mi>y</mml:mi>
      <mml:mi>x</mml:mi>
      <mml:mi>y</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:mrow>
  </mml:math>
</inline-formula>, the correction terms <inline-formula>
  <mml:math id="mgo2f434ul">
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>+</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>λ</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:msub>
      <mml:msub>
        <mml:mi>R</mml:mi>
        <mml:mi>i</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> are applied extensively to enhance restoration. Conversely, for regions with minimal blur <inline-formula>
  <mml:math id="mqsy7e1ekp">
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>⋅</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>≈</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>μ</mml:mi>
        <mml:mi>C</mml:mi>
      </mml:msub>
      <mml:msub>
        <mml:mi>ν</mml:mi>
        <mml:mi>C</mml:mi>
      </mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mi>y</mml:mi>
      <mml:mi>x</mml:mi>
      <mml:mi>y</mml:mi>
      <mml:mn>0</mml:mn>
    </mml:mrow>
  </mml:math>
</inline-formula>, these adjustments remain limited to preserve the original image structure.</p><p>This adaptive fusion technique ensures that restoration corrections are applied selectively, refining sharpness in highly degraded regions while maintaining the integrity of less-affected parts of the image.</p>
        </sec>
      
      
        <sec>
          
            <title>3.5. Cipfs-based edge enhancement for blur correction</title>
          
          <p>To restore edges compromised due to blurring, a CIPFS-based edge enhancement approach was employed. This method utilizes gradient computations and fuzzy classification within the complex interval Pythagorean fuzzy framework to highlight edges while suppressing noise. The process follows these key steps:</p><p>Step 1: Computation of gradient magnitudes</p><p>The intensity gradient of the corrected image was derived using Sobel operators:</p>
          
            <disp-formula>
              <label>(9)</label>
              <mml:math id="mtqjjgplyl">
                <mml:mi>G</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mo>(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:msqrt>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>∂</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:msub>
                            <mml:mi>I</mml:mi>
                            <mml:mrow>
                              <mml:mtext>corrected</mml:mtext>
                            </mml:mrow>
                          </mml:msub>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>∂</mml:mi>
                          <mml:mi>x</mml:mi>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:msup>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>∂</mml:mi>
                          <mml:mi>x</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:msub>
                            <mml:mi>I</mml:mi>
                            <mml:mrow>
                              <mml:mtext>corrected</mml:mtext>
                            </mml:mrow>
                          </mml:msub>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mi>∂</mml:mi>
                          <mml:mi>y</mml:mi>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:msup>
                  <mml:mo>+</mml:mo>
                </mml:msqrt>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="md4ovdtb9q">
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mrow>
        <mml:mtext>corrected</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> represents the pixel intensity after the initial CIPFS-based correction.</p><p>Step 2: CIPFS membership for edge classification</p><p>The gradient values were categorized into different edge strengths (weak, moderate, and strong) using a complex interval Pythagorean fuzzy membership function:</p>
          
            <disp-formula>
              <label>(10)</label>
              <mml:math id="mr8525td1k">
                <mml:msub>
                  <mml:mi>μ</mml:mi>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">edge</mml:mi>
                    </mml:mrow>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mi>G</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>+</mml:mo>
                  <mml:mo>⋅</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mi>G</mml:mi>
                      <mml:msub>
                        <mml:mi>G</mml:mi>
                        <mml:mi>l</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>G</mml:mi>
                        <mml:mi>h</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>G</mml:mi>
                        <mml:mi>l</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                  </mml:mfrac>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mi>G</mml:mi>
                      <mml:msub>
                        <mml:mi>G</mml:mi>
                        <mml:mi>m</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>G</mml:mi>
                        <mml:mi>h</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>G</mml:mi>
                        <mml:mi>l</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                  </mml:mfrac>
                  <mml:mi>j</mml:mi>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mwosnuifim">
    <mml:msub>
      <mml:mi>G</mml:mi>
      <mml:mi>h</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>G</mml:mi>
      <mml:mi>l</mml:mi>
    </mml:msub>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="m0mlr2rm0z">
    <mml:msub>
      <mml:mi>G</mml:mi>
      <mml:mi>m</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> define the upper, lower, and mid thresholds for classifying edge intensities. The imaginary component introduces additional adaptability, capturing variations in edge intensity more effectively than traditional fuzzy methods.</p><p>Step 3: Edge enhancement via CIPFS rules</p><p>A CIPFS rule-based enhancement was applied to refine edge details:</p>
          
            <disp-formula>
              <label>(11)</label>
              <mml:math id="mhowfw1ypx">
                <mml:msub>
                  <mml:mi>I</mml:mi>
                  <mml:mrow>
                    <mml:mtext>enhanced</mml:mtext>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>I</mml:mi>
                  <mml:mrow>
                    <mml:mtext>corrected</mml:mtext>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mi>x</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mi>α</mml:mi>
                <mml:mi>ℜ</mml:mi>
                <mml:mi>G</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mrow>
                  <mml:mo>{</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mo>}</mml:mo>
                  <mml:msub>
                    <mml:mi>μ</mml:mi>
                    <mml:mrow>
                      <mml:mtext>edge </mml:mtext>
                    </mml:mrow>
                  </mml:msub>
                  <mml:mi>G</mml:mi>
                  <mml:mi>x</mml:mi>
                  <mml:mi>y</mml:mi>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mf1em04eap">
    <mml:mi>α</mml:mi>
  </mml:math>
</inline-formula> determines the intensity of enhancement, and <inline-formula>
  <mml:math id="mghpzjc02d">
    <mml:mi>ℜ</mml:mi>
    <mml:mrow>
      <mml:mo>{</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>}</mml:mo>
      <mml:msub>
        <mml:mi>μ</mml:mi>
        <mml:mrow>
          <mml:mtext>edge</mml:mtext>
        </mml:mrow>
      </mml:msub>
      <mml:mi>G</mml:mi>
      <mml:mi>x</mml:mi>
      <mml:mi>y</mml:mi>
    </mml:mrow>
  </mml:math>
</inline-formula> extracts the real part of the CIPFS membership function, ensuring robust edge reinforcement while minimizing artifacts.</p>
        </sec>
      
      
        <sec>
          
            <title>3.6. Normalization and structural refinement using cipfs</title>
          
          <p>To maintain consistency with the original image, the restored output was normalized to fit within the intensity range of the unblurred reference image:</p>
          
            <disp-formula>
              <label>(12)</label>
              <mml:math id="mn19438td1">
                <mml:msub>
                  <mml:mi>I</mml:mi>
                  <mml:mrow>
                    <mml:mtext>restored</mml:mtext>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>I</mml:mi>
                  <mml:mrow>
                    <mml:mtext>ground_truth</mml:mtext>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>(</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mo>max</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mi>x</mml:mi>
                <mml:mi>y</mml:mi>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mrow>
                        <mml:mtext>enhanced</mml:mtext>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mrow>
                        <mml:mtext>enhanced</mml:mtext>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>min</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>x</mml:mi>
                    <mml:mi>y</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mo>max</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>min</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mrow>
                        <mml:mtext>enhanced</mml:mtext>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mrow>
                        <mml:mtext>enhanced</mml:mtext>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>where, min(<inline-formula>
  <mml:math id="mxuvnp7x2b">
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mtext>enhanced</mml:mtext>
    </mml:msub>
  </mml:math>
</inline-formula>) and max(<inline-formula>
  <mml:math id="m4ybojsbme">
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mtext>enhanced</mml:mtext>
    </mml:msub>
  </mml:math>
</inline-formula>) represent the minimum and maximum intensities of the enhanced image, and max(<inline-formula>
  <mml:math id="mqoaj68cm4">
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mrow>
        <mml:mtext>ground_truth</mml:mtext>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula>) is the peak intensity of the reference image.</p><p>A CIPFS-based structural similarity constraint was incorporated to ensure the restored image remains faithful to the original:</p>
          
            <disp-formula>
              <label>(13)</label>
              <mml:math id="mg69zx11j6">
                <mml:msub>
                  <mml:mrow>
                    <mml:mo>‖</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>‖</mml:mo>
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mrow>
                        <mml:mtext>restored </mml:mtext>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>I</mml:mi>
                      <mml:mrow>
                        <mml:mtext>ground_truth</mml:mtext>
                      </mml:mrow>
                    </mml:msub>
                  </mml:mrow>
                  <mml:mn>2</mml:mn>
                </mml:msub>
                <mml:mo>→</mml:mo>
                <mml:mo>min</mml:mo>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mi>ℑ</mml:mi>
                  <mml:mrow>
                    <mml:mo>{</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>}</mml:mo>
                    <mml:msub>
                      <mml:mi>μ</mml:mi>
                      <mml:mrow>
                        <mml:mtext>blur </mml:mtext>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mi>y</mml:mi>
                  </mml:mrow>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mb7doj6kxt">
    <mml:mi>ℑ</mml:mi>
    <mml:mrow>
      <mml:mo>{</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>}</mml:mo>
      <mml:msub>
        <mml:mi>μ</mml:mi>
        <mml:mrow>
          <mml:mtext>blur </mml:mtext>
        </mml:mrow>
      </mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mi>y</mml:mi>
    </mml:mrow>
  </mml:math>
</inline-formula> represents the imaginary component of the CIPFS membership function, ensuring that the similarity measure dynamically adapts to varying blur levels. </p><p>This approach effectively restores sharpness in blurred regions while preserving finer details in areas with minimal degradation. By dynamically adjusting corrections based on the CIPFS-based blur intensity, the method minimizes artifacts and ensures a visually coherent and structurally accurate image restoration. </p><p>The proposed model, which integrates fuzzy logic with multiple mathematical transformations, demonstrates significant potential for restoring blurred images by effectively addressing uncertainties and variations in blur intensity. However, the proposed model relies on handcrafted fuzzy rules, which restricts its adaptability and scalability to diverse or novel image degradation scenarios. The predefined rules are based on specific blur level classifications and expert experience, making it challenging to generalize the model to complex or unknown degradation forms that may arise in practical applications. Additionally, the model’s computational complexity, particularly in high-resolution image processing, further compounds this limitation, as the rigid rule-based framework may not efficiently scale to handle large datasets or real-time processing demands. This limitation stems from the complexity of the operations involved, including pixel coordinate transformations, fuzzy membership calculations, rule-based corrections, and edge enhancement processes. Addressing these limitations in future work will be critical to enhancing the model’s robustness and applicability across a broader range of image restoration tasks. To validate the efficacy of the proposed approach, it was compared against conventional image restoration methods using Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), Natural Image Quality Evaluator (NIQE), Blind/Referenceless Image Spatial Quality Evaluator (BRISQUE), and Mean Opinion Score (MOS) metrics, with statistical tests confirming significant performance improvements. The results demonstrate that the proposed model not only enhances visual clarity but also provides greater robustness against noise and intensity variations, making it a reliable solution for high-precision image restoration tasks.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Experimental results</title>
      <p>To evaluate the performance of the proposed fuzzy-mathematical fusion model in restoring blurred images, a series of experiments were conducted. The primary objective was to assess the model’s ability to handle different types of blur, including Gaussian blur, motion blur, and uniform blur, under various real-world conditions. To ensure a comprehensive analysis of robustness, test images were intentionally blurred, simulating real-world degradation scenarios. </p><p>A dataset of 50 images, each with a resolution of 255×255, was carefully curated from publicly accessible sources such as the Berkeley Dataset and the Google Open Images Dataset. The dataset was designed to include a diverse selection of real-world images rather than entirely synthetic ones, thereby improving the study’s practical applicability. The selected images encompass natural landscapes, urban environments, medical scans, remote sensing images, and architectural structures, ensuring a well-rounded evaluation of the model’s effectiveness in different contexts. The dataset was curated by prioritizing images that naturally exhibit a range of textures, edges, and intensity variations, which are critical factors in evaluating image restoration models.</p><p>To further enhance the study’s validity, real-world distortions were simulated by applying Gaussian, motion, and uniform blur at different intensity levels, rather than relying on artificial or uniform degradation. This ensures that the test conditions closely resemble practical scenarios encountered in fields such as medical imaging, surveillance, and remote sensing. Additionally, the rationale for selecting this dataset over others is based on its diversity, availability, and relevance to the problem domain. By incorporating a broad spectrum of image types and degradation levels, the proposed model’s capability to restore blurred images was evaluated under challenging and varied conditions, reinforcing the credibility of the study.</p><p>The parameter setup for the proposed CIPFS-based image restoration model involves essential components for optimal performance. The membership functions for blur estimation were defined by <inline-formula>
  <mml:math id="m7cc2lcrxe">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:msub>
      <mml:mi>ν</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mn>0.8</mml:mn>
    <mml:mn>0.2</mml:mn>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="mlmvmvw8a8">
    <mml:msub>
      <mml:mi>π</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mn>0.1</mml:mn>
  </mml:math>
</inline-formula>, with phase components <inline-formula>
  <mml:math id="met8t7nnwm">
    <mml:msub>
      <mml:mi>θ</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mn>0.3</mml:mn>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="m663ltkm8m">
    <mml:msub>
      <mml:mi>ϕ</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mn>0.5</mml:mn>
  </mml:math>
</inline-formula>. The correction parameters for pixel displacement were <inline-formula>
  <mml:math id="mbg5jo807w">
    <mml:msub>
      <mml:mi>λ</mml:mi>
      <mml:mn>0</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>λ</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mn>0.5</mml:mn>
    <mml:mn>0.7</mml:mn>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="mw8r2mnfog">
    <mml:msub>
      <mml:mi>λ</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mn>0.6</mml:mn>
  </mml:math>
</inline-formula>, with correction radii <inline-formula>
  <mml:math id="mzi4sxuui1">
    <mml:msub>
      <mml:mi>R</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>R</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mn>1.2</mml:mn>
    <mml:mn>1.5</mml:mn>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="mup31vwxxj">
    <mml:msub>
      <mml:mi>R</mml:mi>
      <mml:mn>3</mml:mn>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mn>1.3</mml:mn>
  </mml:math>
</inline-formula>, which contribute to effective blur correction, ensuring high-quality restoration. These values were chosen for their role in enhancing the model’s ability to restore sharpness while managing uncertainty in blurred regions. </p><p>The experiments were conducted on a system running MATLAB R2019, configured with 8 GB RAM and Windows 10. Image processing for the 255×255 dataset was performed under this setup, ensuring smooth execution of the proposed restoration model. To provide a fair and objective evaluation, the model’s performance was benchmarked against two well-established image restoration methods, RBF [<xref ref-type="bibr" rid="ref_20">20</xref>] and BLIF [<xref ref-type="bibr" rid="ref_21">21</xref>], both of which are widely utilized in image enhancement tasks. The effectiveness of the proposed model was assessed using multiple quantitative metrics, including PSNR, SSIM, NIQE, BRISQUE, and MOS [<xref ref-type="bibr" rid="ref_22">22</xref>]. PSNR measures image reconstruction accuracy by comparing the restored image with the original, where higher values indicate superior quality. SSIM evaluates structural integrity preservation, with higher scores reflecting improved detail retention. NIQE and BRISQUE, which are no-reference quality assessment metrics, estimate perceptual quality, where lower scores correspond to clearer images. MOS, a subjective evaluation metric, represents human perception of image quality, with higher scores denoting greater visual appeal. The performance of the proposed approach was analyzed by comparing the restored images with ground truth across various blur conditions, validating its overall efficacy.</p><p> <xref ref-type="fig" rid="fig_1">Figure 1</xref> provides a comprehensive visual comparison of the image deblurring capabilities of the proposed approach against the competing models, RBF and BLIF. The first column presents the original blurred images, which suffer from significant loss of detail and sharpness. The second and third columns depict the deblurred outputs generated by RBF and BLIF, respectively. Although these models improve image clarity to some extent, they struggle to recover intricate details, particularly in regions with complex textures and edges. In contrast, the fourth column showcases the results produced by the proposed model, which effectively restores fine details, preserves texture consistency, and enhances overall sharpness. As summarized in <xref ref-type="table" rid="table_1">Table 1</xref>, the proposed method achieves a higher PSNR of 34.2±0.7 dB, outperforming RBF (30.5±1.1 dB) and BLIF (29.1±1.3 dB). These results validate the robustness and effectiveness of the fuzzy logic-based approach in mitigating image blurring.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Visual comparison of image deblurring results</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_igYr-DpJtQwYCfeL.png"/>
        </fig>
      
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Performance comparison of the proposed model and competing approaches</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Metric</p></td><td colspan="1" rowspan="1"><p>Proposed Model</p></td><td colspan="1" rowspan="1"><p>RBF</p></td><td colspan="1" rowspan="1"><p>BLIF</p></td><td colspan="1" rowspan="1"><p>p-value</p></td></tr><tr><td colspan="1" rowspan="1"><p>PSNR</p></td><td colspan="1" rowspan="1"><p><mml:math id="mqd9sp8tsp">
  <mml:mrow>
    <mml:mn>34.</mml:mn>
    <mml:mn>2</mml:mn>
  </mml:mrow>
  <mml:mrow>
    <mml:mn>0</mml:mn>
    <mml:mn>7</mml:mn>
    <mml:mo>.</mml:mo>
  </mml:mrow>
  <mml:mo>±</mml:mo>
</mml:math></p></td><td colspan="1" rowspan="1"><p>$30.5 \pm 1.1<mml:math id="mgdibx3f2k">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>29.1 \pm 1.3<mml:math id="mc9nkaqozl">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>p&lt;0.01<mml:math id="m34a7kije7">
  <mml:mo>(</mml:mo>
  <mml:mo>)</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mi>A</mml:mi>
  <mml:mi>N</mml:mi>
  <mml:mi>O</mml:mi>
  <mml:mi>V</mml:mi>
  <mml:mi>A</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>S</mml:mi>
  <mml:mi>S</mml:mi>
  <mml:mi>I</mml:mi>
  <mml:mi>M</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>{0 . 9 7} \pm {0 . 0 1}<mml:math id="mtbxgsnmcu">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>0.90 \pm 0.03<mml:math id="mq3lej4mi8">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>0.86 \pm 0.04<mml:math id="mch2mntimo">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>p&lt;0.01<mml:math id="mc3uafameg">
  <mml:mo>(</mml:mo>
  <mml:mo>)</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mi>A</mml:mi>
  <mml:mi>N</mml:mi>
  <mml:mi>O</mml:mi>
  <mml:mi>V</mml:mi>
  <mml:mi>A</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>N</mml:mi>
  <mml:mi>I</mml:mi>
  <mml:mi>Q</mml:mi>
  <mml:mi>E</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>{2 . 7} \pm {0 . 4}<mml:math id="mfgdzrloej">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>4.0 \pm 0.6<mml:math id="m2naepvsas">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>4.5 \pm 0.7<mml:math id="mrzidumrf3">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>p&lt;0.01<mml:math id="ma1oklgp7p">
  <mml:mo>(</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>)</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mi>T</mml:mi>
  <mml:mi>T</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>B</mml:mi>
  <mml:mi>R</mml:mi>
  <mml:mi>I</mml:mi>
  <mml:mi>S</mml:mi>
  <mml:mi>Q</mml:mi>
  <mml:mi>U</mml:mi>
  <mml:mi>E</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>{1 9 . 5} \pm {2 . 0}<mml:math id="m5ls3vo1vz">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>28.7 \pm 2.6<mml:math id="m9nbbi9wpt">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>31.2 \pm 2.9<mml:math id="m6j22umrd6">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>p&lt;0.01<mml:math id="mw7ynnuyyl">
  <mml:mo>(</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>)</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mi>T</mml:mi>
  <mml:mi>T</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>M</mml:mi>
  <mml:mi>O</mml:mi>
  <mml:mi>S</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>{5 . 4} \pm {0 . 2}<mml:math id="mwfdp11hub">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>4.2 \pm 0.3<mml:math id="mav7b3e3no">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>3.8 \pm 0.4<mml:math id="mqup5o18jk">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>p&lt;0.05<mml:math id="mh2ktnl52g">
  <mml:mo>(</mml:mo>
  <mml:mo>)</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mi>W</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>C</mml:mi>
  <mml:mi>P</mml:mi>
  <mml:mi>U</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>{4 . 3} \pm {0 . 5}<mml:math id="mbr4k0f79t">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>3.4 \pm 0.4<mml:math id="mit797z8h4">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>2.9 \pm 0.3$</p></td><td colspan="1" rowspan="1"><p>-</p></td></tr></tbody></table>
        </table-wrap>
      
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Visual comparison of combined image deblurring and denoising results with Gaussian noise (variance = 0.01)</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_WBVrCVkDH7wO-cLe.png"/>
        </fig>
      
      <p> <xref ref-type="fig" rid="fig_2">Figure 2</xref> evaluates the combined performance of deblurring and denoising under the presence of Gaussian noise with a variance of 0.02. The first column displays images affected by both blurring and noise, leading to a noticeable degradation in visual quality. The second and third columns present the outputs from RBF and BLIF, respectively. Although these methods attempt to reduce noise and enhance clarity, they introduce artifacts and fail to fully restore fine image details. In contrast, the fourth column illustrates the output of the proposed model, which efficiently removes noise while recovering details and sharpness, producing a visually more natural result. A quantitative assessment, as shown in <xref ref-type="table" rid="table_1">Table 1</xref>, indicates that the proposed approach achieves the highest SSIM score of 0.97±0.02, surpassing RBF (0.90±0.03) and BLIF (0.86±0.04). This demonstrates the model’s effectiveness in handling complex tasks that require simultaneous deblurring and denoising. However, this improved performance comes at a computational cost, with the proposed model requiring 4.3±0.5 seconds per image due to the inclusion of fuzzy constraints, compared to RBF and BLIF, which process images in 3.4±0.4 seconds and 2.9±0.3 seconds, respectively.</p><p>PSNR is a commonly used metric for assessing image reconstruction quality, measuring the difference between an enhanced image and its original counterpart. It is computed using the following formula:</p>
      
        <disp-formula>
          <label>(14)</label>
          <mml:math id="mziruniix3">
            <mml:mtext>PSNR</mml:mtext>
            <mml:mo>=</mml:mo>
            <mml:mo>⋅</mml:mo>
            <mml:mo>⁡</mml:mo>
            <mml:mn>10</mml:mn>
            <mml:msub>
              <mml:mi>log</mml:mi>
              <mml:mrow>
                <mml:mn>10</mml:mn>
              </mml:mrow>
            </mml:msub>
            <mml:mrow>
              <mml:mo>(</mml:mo>
              <mml:mo>)</mml:mo>
              <mml:mfrac>
                <mml:msup>
                  <mml:mtext>MAX</mml:mtext>
                  <mml:mn>2</mml:mn>
                </mml:msup>
                <mml:mtext>MSE</mml:mtext>
              </mml:mfrac>
            </mml:mrow>
          </mml:math>
        </disp-formula>
      
      <p>where, MSE represents the mean squared error, and MAX is the maximum possible pixel intensity. A higher PSNR value indicates a better-quality restoration. The proposed method achieved an average PSNR of 34.2±0.7, outperforming RBF (30.5±1.1) and BLIF (29.1±1.3). Analysis of Variance (ANOVA) testing confirmed statistical significance with a p-value below 0.01 (<xref ref-type="table" rid="table_1">Table 1</xref>).</p><p>SSIM evaluates image quality by comparing structural integrity, contrast, and brightness between an enhanced image and its reference. A higher SSIM value suggests better detail preservation. The proposed approach achieved an SSIM score of 0.97±0.01, outperforming RBF (0.90±0.03) and BLIF (0.86±0.04). Statistical analysis using ANOVA indicated a significant difference, with a p-value below 0.01. NIQE is a no-reference metric that quantifies image quality based on statistical modeling of natural images. A lower NIQE value corresponds to better perceptual quality. The proposed method obtained a NIQE score of 2.7±0.4, notably lower than RBF (4.0±0.6) and BLIF (4.5±0.7). A paired t-test confirmed this improvement as statistically significant, with a p-value below 0.01. BRISQUE evaluates image quality by analyzing deviations from expected natural image characteristics. A lower BRISQUE value signifies better quality. The proposed model achieved the lowest BRISQUE score of 19.5±2.0, outperforming RBF (28.7±2.6) and BLIF (31.2±2.9). The improvement was statistically significant, as confirmed by a paired t-test with a p-value below 0.01. MOS is a subjective image quality evaluation metric based on human perception, where higher scores indicate better visual quality. The proposed model achieved the highest MOS rating of 5.4±0.2, surpassing RBF (4.2±0.3) and BLIF (3.8±0.4). A Wilcoxon signed-rank test yielded a p-value below 0.05, confirming that the perceived improvement in quality is statistically significant (<xref ref-type="table" rid="table_1">Table 1</xref>).</p>
      <p>As shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>, the proposed model consistently outperforms the competing approaches, demonstrating superior image quality restoration while maintaining computational efficiency. Specifically, the proposed model outperforms RBF and BLIF across multiple image quality metrics, achieving the highest PSNR (34.2 dB) and SSIM (0.97) values, indicating superior reconstruction quality and structural preservation. It also records the lowest NIQE (2.7) and BRISQUE (19.5) scores, reflecting enhanced perceptual quality. Additionally, the highest MOS (5.4) confirms its superiority in subjective evaluation. Despite its improved performance, the proposed method maintains a reasonable processing time of 4.3 seconds, compared to 3.4 seconds (RBF) and 2.9 seconds (BLIF). Statistical validation through ANOVA, t-tests, and the Wilcoxon signed-rank test confirms the significance of these improvements, establishing the proposed approach as the most effective for image enhancement.</p><p>To ensure the scalability and applicability of the proposed CIPFS-based model in real-time imaging environments, its computational efficiency was analyzed and potential optimization strategies were explored in this study. While the model demonstrates superior restoration quality by effectively handling uncertainty and preserving structural details, its computational complexity results in an average processing time of 4.3 seconds, which is higher than RBF (3.4 seconds) and BLIF (2.9 seconds). This additional processing time is attributed to the robust uncertainty modeling and precise structural preservation facilitated by the CIPFS framework. To mitigate this latency and enhance real-time applicability, future optimizations could focus on parallel processing, Graphics Processing Unit (GPU) acceleration, and memory-efficient algorithmic refinements. Additionally, adaptive fuzzy rule pruning could be explored to dynamically adjust the complexity of fuzzy inference based on image characteristics, further reducing execution time. Hardware-based acceleration, such as Field-Programmable Gate Array (FPGA) implementation, could also be considered to improve the model’s efficiency in time-sensitive applications. These enhancements will ensure that the proposed approach maintains its accuracy and robustness while achieving the necessary computational efficiency for real-time and large-scale imaging scenarios.</p>
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>Performance comparison of the proposed model with RBF and BLIF across multiple evaluation metrics</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_7wjMOcewmYvVE01A.png"/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/2/img_tqWlbbaln7OLUnsp.png"/>
        </fig>
      
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>An innovative complex interval Pythagorean fuzzy-mathematical fusion model for image restoration was proposed in this study, which effectively addresses the challenges of deblurring and noise reduction. The model demonstrated exceptional performance in handling various blur types such as Gaussian, motion, and uniform blur, as well as combinations of blur and noise. When tested on a dataset of 50 grayscale images, the proposed model consistently outperformed existing methods, including RBF and BLIF, in terms of key metrics. Specifically, it achieved an average PSNR of 34.2±0.7 dB, SSIM of 0.97±0.01, NIQE of 2.7±0.4, BRISQUE of 19.5±2.0, and MOS of 5.4±0.2. These outcomes highlight the model’s ability to preserve fine details, reduce distortions, and enhance perceptual quality compared to competing techniques. The use of fuzzy logic within the framework enabled adaptive adjustments that preserved edge structures while minimizing overcorrection and artifacts. However, the model has certain limitations. Its computational demand remains a concern, especially for high-resolution or real-time applications, as the average processing time is 4.3 seconds, which is longer than the 3.4 seconds for RBF and 2.9 seconds for BLIF. This additional processing time is primarily due to the enhanced robustness and precision provided by the CIPFS-based framework. While this trade-off ensures superior segmentation quality, further optimization is necessary to improve computational efficiency. Future efforts could explore parallel processing techniques, memory-efficient algorithms, and hardware acceleration using GPUs or FPGAs to reduce execution time without compromising performance. Moreover, the model makes certain assumptions regarding blur types and noise distributions, which may limit its performance in more complex or diverse scenarios. Specifically, its effectiveness in handling mixed degradations, varying illumination conditions, and occlusions has not been extensively tested. </p><p>Beyond image deblurring, the proposed fuzzy-mathematical fusion model has significant potential for a wide range of real-world applications requiring adaptive noise reduction and structural preservation. It could be highly beneficial in remote sensing, where satellite images are often degraded by motion blur and atmospheric distortions. Similarly, in surveillance systems, enhancing blurred footage could improve object recognition and event analysis. The model also holds promise for forensic imaging, where restoring tampered or degraded images is crucial, and industrial quality control, where enhancing images captured under dynamic conditions can improve defect detection. </p><p>Future work could focus on further optimizing computational efficiency, incorporating data-driven learning for enhanced adaptability, extending the model’s application to higher-resolution color images, and validating its performance on diverse real-world datasets to strengthen its practical implementation. By expanding the applications of this fuzzy logic-based restoration method, the proposed model can significantly contribute to effective and adaptive image enhancement across various fields.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>171</volume>
          <page-range>108238</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Yi Chi</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>Zhen Rong</given-names>
            </name>
            <name>
              <surname>Jiao</surname>
              <given-names>Ru Shi</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compbiomed.2024.108238</pub-id>
          <article-title>Segment anything model for medical image segmentation: Current applications and future directions</article-title>
          <source>Comput. Biol. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>1034</page-range>
          <issue>10</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Xu</surname>
              <given-names>Yan</given-names>
            </name>
            <name>
              <surname>Quan</surname>
              <given-names>Rixiang</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Weiting</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Yi</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Xiaolong</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Fengyuan</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/bioengineering11101034</pub-id>
          <article-title>Advances in medical image segmentation: A comprehensive review of traditional, deep learning and hybrid approaches</article-title>
          <source>Bioengineering</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <page-range>1113-1136</page-range>
          <issue>5</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hussain</surname>
              <given-names>Ibrar</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>Haider</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>Muhammad Shahkar</given-names>
            </name>
            <name>
              <surname>Niu</surname>
              <given-names>Sijie</given-names>
            </name>
            <name>
              <surname>Rada</surname>
              <given-names>Lavdie</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3934/ipi.2022014</pub-id>
          <article-title>Robust region-based active contour models via local statistical similarity and local similarity factor for intensity inhomogeneity and high noise image segmentation</article-title>
          <source>Inverse Probl. Imag.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>44</volume>
          <page-range>8-19</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Panhwar</surname>
              <given-names>Ali Orangzeb</given-names>
            </name>
            <name>
              <surname>Sathio</surname>
              <given-names>Anwar Ali</given-names>
            </name>
            <name>
              <surname>Shah</surname>
              <given-names>Nadeem Manzoor</given-names>
            </name>
            <name>
              <surname>Memon</surname>
              <given-names>Sumaira</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.22581/muet1982.2742</pub-id>
          <article-title>A scheme based on deep learning for fruit classification</article-title>
          <source>Mehran Univ. Res. J. Eng. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>116-126</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hussain</surname>
              <given-names>Ibrar</given-names>
            </name>
            <name>
              <surname>Muhammad</surname>
              <given-names>Jan</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>Rifaqat</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/ijkis010204</pub-id>
          <article-title>Enhanced global image segmentation: Addressing pixel inhomogeneity and noise with average convolution and entropy-based local factor</article-title>
          <source>Int. J. Knowl. Innov. Stud.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>212-222</page-range>
          <issue>4</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Khan</surname>
              <given-names>Muhammad Shahkar</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/mits030402</pub-id>
          <article-title>A region-based fuzzy logic approach for enhancing road image visibility in foggy conditions</article-title>
          <source>Mechatron. Intell. Transp. Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>1916</volume>
          <page-range>012165</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Prabha</surname>
              <given-names>P. A.</given-names>
            </name>
            <name>
              <surname>Bharathwaj</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Dinesh</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Prashath</surname>
              <given-names>G. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1088/1742-6596/1916/1/012165</pub-id>
          <article-title>Defect detection of industrial products using image segmentation and saliency</article-title>
          <source>J. Phys. Conf. Ser.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>63</volume>
          <page-range>159-165</page-range>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Funama</surname>
              <given-names>Yoshinori</given-names>
            </name>
            <name>
              <surname>Oda</surname>
              <given-names>Seitaro</given-names>
            </name>
            <name>
              <surname>Kidoh</surname>
              <given-names>Masafumi</given-names>
            </name>
            <name>
              <surname>Sakabe</surname>
              <given-names>Daisuke</given-names>
            </name>
            <name>
              <surname>Nakaura</surname>
              <given-names>Takeshi</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1177/0284185120986938</pub-id>
          <article-title>Effect of image quality on myocardial extracellular volume quantification using cardiac computed tomography: A phantom study</article-title>
          <source>Acta Radiol.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>100-110</page-range>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Meng</surname>
              <given-names>Jun</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Yuanyuan</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>Huahua</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>You</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.37965/jait.2022.0110</pub-id>
          <article-title>Single-image dehazing based on two-stream convolutional neural network</article-title>
          <source>J. Artif. Intell. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>40612-40622</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>B. Y.</given-names>
            </name>
            <name>
              <surname>Yanghua</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2021.3064346</pub-id>
          <article-title>IIE-SegNet: Deep semantic segmentation network with enhanced boundary based on image information entropy</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>29</volume>
          <page-range>583-607</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Qi</surname>
              <given-names>Yun Liang</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Zhen</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>Wen Hao</given-names>
            </name>
            <name>
              <surname>Lou</surname>
              <given-names>Meng</given-names>
            </name>
            <name>
              <surname>Lian</surname>
              <given-names>Jing</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Wen Wei</given-names>
            </name>
            <name>
              <surname>Deng</surname>
              <given-names>Xiang Yu</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>Yi De</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11831-021-09587-6</pub-id>
          <article-title>A comprehensive overview of image enhancement techniques</article-title>
          <source>Arch. Comput. Methods Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>1926-1939</page-range>
          <issue>9</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Premnath</surname>
              <given-names>S. P.</given-names>
            </name>
            <name>
              <surname>Renjit</surname>
              <given-names>J. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1049/ipr2.12162</pub-id>
          <article-title>Image restoration model using Jaya-Bat optimization-enabled noise prediction map</article-title>
          <source>IET Image Process.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>30</volume>
          <page-range>134-149</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ghulyani</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Arigovindan</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/tip.2020.3032036</pub-id>
          <article-title>Fast roughness minimizing image restoration under mixed Poisson–Gaussian noise</article-title>
          <source>IEEE Transactions on Image Processing</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1075-1079</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jiang</surname>
              <given-names>Cheng Chao</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>Lei</given-names>
            </name>
            <name>
              <surname>Jia</surname>
              <given-names>Guo Tao</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3690407.3690586</pub-id>
          <article-title>The application of deep learning in image deblurring</article-title>
          <source>CAIBDA' 24: Proceedings of the 2024 4th International Conference on Artificial Intelligence, Big Data and Algorithms, New York, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Xiang</surname>
              <given-names>Ya Wen</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Heng</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Cheng Yang</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>Fang Wei</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Zhong Bo</given-names>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names>Yong Qiang</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00371-024-03632-8</pub-id>
          <article-title>Deep learning in motion deblurring: Current status, benchmarks and future prospects</article-title>
          <source>Vis. Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="conf-paper">
          <page-range>62-66</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Aoi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Goto</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1145/3705927.3705938</pub-id>
          <article-title>Deblurred image quality improvement by learning based deblurring method utilizing ConvNeXt-V2</article-title>
          <source>DMIP' 24: Proceedings of the 2024 7th International Conference on Digital Medicine and Image Processing, Osaka, Japan</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>Kang</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Yuan Jie</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2401.05907</pub-id>
          <article-title>Efficient image deblurring networks based on diffusion models</article-title>
          <source>arXiv preprint, arXiv:2401.05907</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>18</volume>
          <page-range>346-357</page-range>
          <issue>3</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Varghese</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Rajagopalan</surname>
              <given-names>A. N.</given-names>
            </name>
            <name>
              <surname>Ansari</surname>
              <given-names>Z. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/JSTSP.2024.3386056</pub-id>
          <article-title>Real-time large-motion deblurring for gimbal-based imaging systems</article-title>
          <source>IEEE J. Sel. Top. Signal Process.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>21049-21067</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhai</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Cui</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2023.3250616</pub-id>
          <article-title>A comprehensive review of deep learning-based real-world image restoration</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>132</volume>
          <page-range>108983</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>Sheng Min</given-names>
            </name>
            <name>
              <surname>Oh</surname>
              <given-names>Sung Kwun</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>Jin Yul</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>Zun Wei</given-names>
            </name>
            <name>
              <surname>Pedrycz</surname>
              <given-names>Witold</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.patcog.2022.108983</pub-id>
          <article-title>Motion blurred image restoration framework based on parameter estimation and fuzzy radial basis function neural networks</article-title>
          <source>Pattern Recognit.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>34</volume>
          <page-range>11851-11873</page-range>
          <issue>11</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Peng</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Pei</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TCSVT.2024.3424675</pub-id>
          <article-title>Blind image deblurring via minimizing similarity between fuzzy sets on image pixels</article-title>
          <source>IEEE Trans. Circuits Syst. Video Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Bovik</surname>
              <given-names>A. C.</given-names>
            </name>
          </person-group>
          <source>The Essential Guide to Image Processing</source>
          <publisher-name>Academic Press</publisher-name>
          <year>2009</year>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>