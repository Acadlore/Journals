<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IJCMEM</journal-id>
      <journal-id journal-id-type="doi">10.18280</journal-id>
      <journal-title-group>
        <journal-title>International Journal of Computational Methods and Experimental Measurements</journal-title>
        <abbrev-journal-title abbrev-type="issn">Int. J. Comput. Methods Exp. Meas.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IJCMEM</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2046-0554</issn>
      <issn publication-format="print">2046-0546</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-r3Z3hvmUlVOOIs2c-pSUtitW9YTc594F</article-id>
      <article-id pub-id-type="doi">10.18280/ijcmem.110103</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Eye-Tracking Calibration to Control a Cobot</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <name>
            <surname>Faura-Pujol</surname>
            <given-names>Anna</given-names>
          </name>
          <email/>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-0605-1282</contrib-id>
          <name>
            <surname>Faundez-Zanuy</surname>
            <given-names>Marcos</given-names>
          </name>
          <email>faundez@tecnocampus.cat</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <name>
            <surname>Moral-ViñAls</surname>
            <given-names>Aleix</given-names>
          </name>
          <email/>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5138-8947</contrib-id>
          <name>
            <surname>LóPez-Xarbau</surname>
            <given-names>Josep</given-names>
          </name>
          <email/>
        </contrib>
        <aff id="aff_1">Tecnocampus, Universitat Pompeu Fabra, Mataró 08302, Spain</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub"/>
      <volume>11</volume>
      <issue>1</issue>
      <fpage>17</fpage>
      <lpage>25</lpage>
      <page-range>17-25</page-range>
      <history>
        <date date-type="received"/>
        <date date-type="accepted"/>
      </history>
      <permissions>
        <copyright-statement>©2023 by the author(s)</copyright-statement>
        <copyright-year>2023</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p><span style="font-family: Times New Roman, serif">The present study pursues to determine the optimal operation range of a specific screen-based eye-tracker, the Tobii X2-30, regarding the variation of precision and accuracy in measures. Furthermore, a connection setup to operate a collaborative robot (cobot) Omron TM5-700 by means of this eye-tracker will be presented. The possibility to operate a collaborative robot by gaze can be used as a third arm, which allows human beings to do more sophisticated activities, as well as making the manipulation of dangerous or perilous substance easier and safer. When developing new technological tools, we have mainly two options. The first one consists on a specifically designed hardware. While in this option, the engineer has full control over the device and can fit it to the specific requirements; in general, it will be a time consuming and expensive development. A second drawback is the limited possibility of researchers from other countries to construct an exactly equal device and replicate the experiments. The second option consists on adopting an existing commercial hardware, which probably has not been designed for the specific application in mind. The main advantage is the easy adaptation of this solution by other researchers, who only need to purchase the same commercial device and follow the recommendations. However, the main drawback of this approach is that the developers must test the device and check that it can be used for the new application. The goal of this paper is to test a commercial device and provide usability recommendations for a new application such is the movement of a robotic arm using eye-tracking. This paper includes the results from three experiments, which assess the final conclusion on the best performance positioning of the user regarding the Tobii X2-30 eye-tracker, in <italic><span style="font-family: Times New Roman, serif">x, y</italic><span style="font-family: Times New Roman, serif">, and <italic><span style="font-family: Times New Roman, serif">z </italic><span style="font-family: Times New Roman, serif">coordinates. When it comes to its implementation with the cobot, the outcome of a practical demo and experimental setup is also presented. This last one consists of accuracy measurements, where the control of the position of the cobot is defined by means of gaze, which defines a set of points in (<italic><span style="font-family: Times New Roman, serif">x</italic><span style="font-family: Times New Roman, serif">,y) plane. Later on, the robot picks up an ink-pen and draws a graph in a piece of paper. This drawing involves connecting these pre-defined dots by straights lines. To this end, a set of figures (parallelogram, pentagon, etc.) have been acquired and compared with the desired printed images on the PC screen.</p></abstract>
      <kwd-group>
        <kwd>Accuracy</kwd>
        <kwd>Collaborative robot</kwd>
        <kwd>Eye-tracker</kwd>
        <kwd>Gaze</kwd>
        <kwd>Precision</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="4"/>
        <fig-count count="17"/>
        <table-count count="1"/>
        <ref-count count="27"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Eye-tracking is a technology with applications in a large set of life areas. Soon after the COVID-19 arose, in 2020, the eye-tracking market was valued at USD 664.9 million. Moreover, it is expected to reach USD 4.86 billion by 2030, which implies growths at a Compound Annual Growth Rate (CAGR) of 22.4% during the forecast period [<xref ref-type="bibr" rid="ref_1">1</xref>].</p><p>Eye-tracking technology is growing quickly in the very recent years. A Google Scholar search of the terms ‘eye-tracking’ and ‘safety’ yields 38,000 results. Of these, 17,600 were published in the 2017-2021 period. Nowadays, health and retail are the two areas where the penetration of this technology’ use is the highest. However, the tendency is that, in the forthcoming years, automotive, neuromarketing, and industry 4.0 will stand out.</p><p>Eye-tracking technology can be very useful for a wide variety of safety and security applications. In the impending years, predictions suggest that augmented reality and virtual reality will be the most recurring applications of this technology, as well as its incorporation into mobile devices, entertainment, and gaming or contactless biometric solutions. Furthermore, in the context of the COVID-19 pandemic, new insights have emerged. Touch-free machines, with which the user can operate without the need to touch a surface, are more desirable, as the risk of contagion decreases very significantly.</p><p>In the light of the above, some specific uses of eye-tracking technologies are described below.</p><p>•<span style="font-family: Times New Roman"> Safety for construction workers. Several studies that have managed to capture the viewing patterns of this group assert that there is a real correlation between these and the workers’ hazard recognition performance. Experts claim that the viewing patterns analysis undoubtedly leads to an improvement of the understanding of their hazard recognition performance. Some examples of research using eye-tracking in this field can be found in [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>].</p><p>•<span style="font-family: Times New Roman"> Automotive is one of the most important application areas of eye-tracking. The most direct application in this sector is in driver monitoring systems, where eye-tracking technology is becoming paramount. With the aim to create safer and more advanced cars, the combination of facial recognition and eye-tracking enables information on the driver’s attention, alertness or concentration on the driving task, therefore enabling the creation of different warnings and notifications. Some examples can be found in [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>]. In this area, in which there is an irrefutable link to pedestrians, this groups’ mobility safety has also been put in the focus of the analysis [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>].</p><p>•<span style="font-family: Times New Roman"> Neuromarketing is probably the first commercial field where eye-tracking technology entered. Its determining application in the identification of the customers’ behavior when visiting a point of sale has made this sector very interested in eye-tracking technologies since their first appearances. Advertising and marketing rely on the knowledge of the customer’s behavior, on their preferences and decisions, always aiming to foretell their purchasing behavior. Consequently, the applications are endless, from software and hardware usability, advertising testing and product in stores, to analysis of the conception of a company’s corporate image. See, for instance, the study about neuroergonomics [<xref ref-type="bibr" rid="ref_12">12</xref>].</p><p>•<span style="font-family: Times New Roman"> Robotics and industry, in which it is more and more important to pursue safe environments, is a promising area too. An eye-tracker broadens the possibilities of robots in many tasks. To start with, it allows people to operate with machinery for which they would need both hands, only with one single hand. This is made possible thanks to a collaborative robot that ought to act as a ‘third arm’ to the worker, controlled by the worker’s eyes. Related to this same issue we find applications in camera inspection, where eye-directed cameras could approach the place where a certain picture is to be taken only obeying orders coming from the worker’s sight. What is more, by winking one eye the camera would shoot the photo, for example. In addition, eye-tracking is bound to minimize the risks for human beings when dealing with dangerous substances using a robot. See for instance [<xref ref-type="bibr" rid="ref_13">13</xref>]. Not to forget an application concerning handicapped people, who could overcome their limitations by controlling a robot as it has been explained, enabling them to be more autonomous [<xref ref-type="bibr" rid="ref_14">14</xref>].</p><p>Several studies have recently focused on how eye-tracking devices can help a user perform different tasks when it comes to its interaction with a robot. An example of this can be found in [<xref ref-type="bibr" rid="ref_15">15</xref>], where an eye-tracker collaborates with a robotic arm to allow individuals with Severe Speech and Motor Impairment (SSMI) to manipulate objects. In addition, this robot is able to perform some tasks that help them in their rehabilitation process.</p><p>Regarding a similar field of application, an air pressure actuator can be regulated using eye-tracking techniques. This has currently been implemented as a physiotherapy device among users that present weakness in their forearm [<xref ref-type="bibr" rid="ref_16">16</xref>].</p><p>Other examinations have concluded that eye-tracking techniques are able to control telepresence robots [<xref ref-type="bibr" rid="ref_17">17</xref>], but this still remains an area that requires future work and investigation.</p><p>•<span style="font-family: Times New Roman"> The health sector is bound to be one of the areas in which eye-tracking technologies take root in an unprecedented way, with an important potential role in diversified departments and applications. Some case studies have revealed that these technologies are an invaluable technique to analyze hidden aspects of aging, as well as one’s reasoning process, all of which remain unknown using any other non-invasive tool [<xref ref-type="bibr" rid="ref_18">18</xref>], [<xref ref-type="bibr" rid="ref_19">19</xref>]. Moreover, eye-tracking has proved to contribute to the diagnose of dementias such as Alzheimer [<xref ref-type="bibr" rid="ref_20">20</xref>], and has many more on the horizon.</p><p>The aim of this paper is to determine the optimal operation range of a specific screen-based eye-tracker, the Tobii X2-30, regarding the variation of precision and accuracy in measures. Further applications of this analysis in the health area include the eyesight monitorization of patients suffering from Parkinson Disease (PD). Researchers have studied the impact of visual feedback on the writing size of this group [<xref ref-type="bibr" rid="ref_21">21</xref>], which has led to the idea that there could be something in common when it comes to the vision of PD patients, worthy of study.</p><p><span style="font-family: Times, serif">In addition, we setup the communication between this eye-tracker and a cobot, or collaborative robot, and we perform a set of experiments regarding drawing performance based on gaze. This is a simple environment test that has already been explored by different authors using different setups [<xref ref-type="bibr" rid="ref_22">22</xref>]. Handwriting analysis has a wide range of applications in e-security and e-health [<xref ref-type="bibr" rid="ref_23">23</xref>], and a large set of tasks can be performed [<xref ref-type="bibr" rid="ref_24">24</xref>]. There are great possibilities when it comes to combining handwriting analysis and eye-tracking in e-health and e-security, which ought to be developed in the forthcoming years. Therefore, we consider interesting to describe the setup of the system due to the fact that a large number of robots and eye-trackers exist and its interconnection is not trivial and must be ad hoc designed.</p>
    </sec>
    <sec sec-type="">
      <title>2. Determination of the user’s optimal position regarding the eye-tracker</title>
      <p>In this paper, we want to perform several experiments in order to be able to determine the user’s optimal exact position (in <italic>x</italic>, <italic>y</italic>, and <italic>z </italic>coordinates) regarding the eye-tracker. Moreover, we want to define mobility margins within which the user can move without compromising the validity of the obtained results.</p>
      
        <sec>
          
            <title>2.1. Experimental setup description</title>
          
          <p>The Tobii X2-30 is a screen-based eye-tracker that has been designed to be connected to a PC, below its screen. In our case, it has been attached to a laptop as shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>.</p><p>The data gathering from the different experiments have been carried out with the Tobii Pro Lab software running in the laptop, which offers a complete toolset for this purpose.</p><p>When it comes to the user’s position regarding the eye-tracker, we have established the coordinate system in <xref ref-type="fig" rid="fig_2">Figure 2</xref>. As it can be seen in the ground plan view, it is eye-tracker based.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>Tobii X2-30 attached to a laptop</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_C188OukTDer0f0NJ.jpeg"/>
            </fig>
          
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Coordinate system for experiments</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_-nArI7hy08HE-v8d.jpeg"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>2.2. Experiments and results</title>
          
          <p>We have designed a set of three simple experiments in order to determine the optimal <italic>y</italic>, <italic>x</italic>, and <italic>z </italic>position, respectively. All experiments have been carried out ensuring that the gaze angle (α) never exceeded 36º, according to manufacturer (see <xref ref-type="fig" rid="fig_3">Figure 3</xref>). Before the start of each of the experiments that will be presented, a process of calibration has been made. Although with the Tobii Pro Lab software it is simple and quick, if not done, the accuracy of results could be compromised.</p>
          <p>Data recorded by the Tobii Pro Lab software can be either monocular, if it is based on data from the subject’s dominant eye only, or binocular, when the data shown are the average of both eyes. In this paper, all data are binocular.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Gaze angle</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_XF90h6JpwIrKrHMM.jpeg"/>
            </fig>
          
          
            <sec>
              
                <title>2.2.1 Accuracy and precision</title>
              
              <p>The results given by the Tobii Pro Lab software are the accuracy and precision degrees between the exact position of several points that appear on the screen, which ought to be followed by the user’s sight and the gaze point that the eye-tracker captures in each case.</p>
              <p>By accuracy we understand the closeness of the measurements to a specific value, normally referred to the closeness of the measurements to the real or accepted value. Precision stands for closeness of the measurements to each other. <xref ref-type="fig" rid="fig_4">Figure 4</xref> is bound to illustrate this. Here, the actual value is represented with a green circle on the origin <italic>x </italic>axis, while 30 experimental measures are represented by blue circles in the <italic>x </italic>axis.</p><p>Both accuracy and precision are error measures: the lower the accuracy and precision of a measurement are, the better. In order to clarify both concepts, some conceptual graphs coming from normal distributions are shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>, where the real value is that in green.</p>
              
                <fig id="fig_4">
                  <label>Figure 4</label>
                  <caption>
                    <title>Concepts of accuracy and precision</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_sEkUSAeGH1dQy7It.jpeg"/>
                </fig>
              
              
                <fig id="fig_5">
                  <label>Figure 5</label>
                  <caption>
                    <title>Accuracy and precision example</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_Dyfyd_9vswBp_UXp.jpeg"/>
                </fig>
              
            </sec>
          
          
            <sec>
              
                <title>2.2.2 Experiment 1: $y$ axis position</title>
              
              <p>First, we wanted to see the impact in precision and accuracy measurements when the user moves forward and back. In other words, when he/she varies his/her <italic>y </italic>distance to the eyetracker, according to the established coordinate system.</p><p>The Tobii manufacturer affirms that this eye-tracker can record data in a range of 40-90 cm between the eye-tracker and the subject. Nevertheless, the experiments’ results have proven that <italic>y </italic>distances equal or greater than 76.0 cm, as well as distances equal or less than 46.8 cm, are out of this device’s range.</p><p>In order to determine the optimal <italic>y </italic>position of the user towards the eye-tracker, for each <italic>y </italic>distance, three iterations have been made. The average accuracy and precision results of these have been plotted as shown in the following graphs, using the maximum, minimum, and average value in each case.</p><p>The eye-tracker incorporates several cameras. The purpose of these cameras is to permit the eye detection and measurement of relevant information for eye-tracking. When developing an eye-tracker, the cameras are selected for a specific feature. Normally, they are fixed cameras, with no possibility to automatically focus an object at a large set of focal distances. Thus, in <xref ref-type="fig" rid="fig_6">Figure 6</xref> and <xref ref-type="fig" rid="fig_7">Figure 7</xref>, we detect the optimal operational range of the cameras.</p><p>The manufacturer affirms that the best performance of this eye-tracker is in the <italic>y </italic>range [60, 65] cm. In the light of the results, one can see that the lowest accuracy and precision are achieved from 55 to 65 cm. All in all, we confirm that the best performance of the eye-tracker is carried out from a relative distance of 55 to 65 cm between the user and the eye-tracker. With greater distances, the accuracy and precision both increase, and the same happens with very short distances.</p>
              
                <fig id="fig_6">
                  <label>Figure 6</label>
                  <caption>
                    <title>Accuracy results when varying the $y$ distance (Experiment 1)</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_xgqXtSKpUWj-dk9q.jpeg"/>
                </fig>
              
              
                <fig id="fig_7">
                  <label>Figure 7</label>
                  <caption>
                    <title>Precision results when varying the $y$ distance (Experiment 1)</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_t6CnyS75-zOXGbBB.jpeg"/>
                </fig>
              
            </sec>
          
          
            <sec>
              
                <title>2.2.3 Experiment 2: $x$ axis position</title>
              
              <p>Second, once the best performance area in the <italic>y </italic>axis has been determined, the aim of this experiment is to quantify the real difference in the eye-tracker measurements when the user moves in the <italic>x </italic>axis, so if he/she moves right to left.</p><p>Having seen the best performance range in the <italic>y </italic>axis, in this second experiment we have set the user to be seated at <italic>y</italic>=60/63/65 cm. For each of these three distances, we have registered data from the eye-tracker when the subject moved from <italic>x</italic>=15-cm to <italic>x</italic>=15 cm.</p><p>Carrying on with the methodology in Experiment 1, three iterations have been done in each case. The average accuracy and precision results of these have been plotted as shown in the following graphs, using the maximum, minimum, and average value in each case. The average value is represented by a dot, while the maximum and minimum values are indicated by the top and the bottom of the straight line that passes through the dot.</p><p>If the user moves in the <italic>y </italic>direction within a 10-cm eye-tracker-centered margin, the accuracy goes from 0.37º to, at most, 0.67º. The average accuracy within the best performance area is 0.6º. According to Tobii manufacturer, <italic>one degree accuracy corresponds to an average error of 12 mm on a screen at a distance of 65 cm </italic>[<xref ref-type="bibr" rid="ref_25">25</xref>]<italic>. </italic>Therefore, the worst accuracy will be of, approximately, 8 mm. This can also be calculated with trigonometry, considering the gaze angle and <italic>y </italic>distance (both known parameters).</p><p>If we now take a bigger <italic>x </italic>range, from <italic>x </italic>=-15 cm to <italic>x</italic>=15 cm, this impinges on the accuracy, as expected. At a distance of 60 cm this is not significant, but the accuracy gets worse as the <italic>x </italic>distance increases, achieving 0.75º, at most, at 63 cm. This will be, approximately, 9 mm.</p><p>If the user moves in the <italic>y </italic>direction within a 10-cm eye-tracker-centered margin, precision goes from 0.39 to 0.52, at most, which is still lower than the maximum accuracy in these conditions. The worst accuracy will be of, approximately, 6 mm. In average within this range, it is exactly 0.47º.</p><p>Similarly to what happened with accuracy, taking a bigger range impinges on the precision. At a distance of 60 cm this is not significant, but precision gets worse as the <italic>x </italic>distance increases, achieving 0.64º, at most, at 63 cm, which is, approximately, 7.7 mm.</p><p>In the light of the results, both precision and accuracy get worse if the user can move within a 30-cm centered margin in reference to the eye-tracker (so with <italic>x </italic>distances of 15 to 15 cm) (as shown in <xref ref-type="fig" rid="fig_8">Figure 8</xref>). Whenever possible, the eye-tracker’s results will be better if the user does not move, or if he/ she does not move a lot (so if he/she is kept in a 10-cm centered margin in reference to the eye-tracker). However, it would not be realistic to think that the user will be seating still all the time due to the fact that he/she can easily move 10 cm around the centered position (5 cm to the right or left) without even noticing it. That is why there should be no significant change in the obtained results, and we have proved this true. Even so, the least the user moves, the better.</p>
              
                <fig id="fig_8">
                  <label>Figure 8</label>
                  <caption>
                    <title>Accuracy results when varying the $x$ and $y$ distance (Experiment 2)</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_ecL3SASYsSaNxmng.jpeg"/>
                </fig>
              
            </sec>
          
          
            <sec>
              
                <title>2.2.4 Experiment 3: $z$ axis position</title>
              
              <p>The last experiment pursued the determination of the optimal height at which the subject should be seated in order to achieve better accuracy and precision results. In this case, a new variable had to be factored in: the type of chair. Up to this point, the previous experiments had been carried out while the user was seated in a normal, four-legged static chair. But for this third experiment, the subject has been seating on an office chair on wheels, so the height could be modified throughout the data gathering process. Particularly, the chosen chair and its height parameters that are relevant for this experiment can be seen in <xref ref-type="fig" rid="fig_9">Figure 9</xref>.</p><p>In addition, the eye-tracker’s <italic>z </italic>position, so the distance from this device to the floor during Experiment 3, was 77 cm.</p><p>It must be said that, in Experiment 3, the data recording took place in a different physical environment than the one used for Experiments 1 and 2. Although it was also an indoor office environment, it had less natural light and a little more artificial light. This is important because, according to Tobii manufacturer, when <italic>the illumination in the lab changes, the size and shape of the pupil is affected. Unless compensated for, this may cause a significantly reduced accuracy </italic>[<xref ref-type="bibr" rid="ref_26">26</xref>]. This comes to say that the accuracy or precision values obtained in this experiment should not be compared with the ones obtained in the previous experiments, as they are bound to differ. Nevertheless, they are still valuable because the aim of this experiment is to see the tendency of the accuracy and precision variation as height increases, in the same conditions (as shown in <xref ref-type="fig" rid="fig_10">Figure 10</xref>).</p><p>The results have been plotted and are shown in <xref ref-type="fig" rid="fig_11">Figure 11</xref> and <xref ref-type="fig" rid="fig_12">Figure 12</xref>. Note that <italic>z </italic>distance is the distance from the floor.</p>
              <p>The best height for optimal performance of the eye-tracker is 52 cm. When the user is sat on a chair that is elevated 52 cm above the floor, the accuracy and precision are at its best (their values are small). If this altitude over the floor cannot be guaranteed exactly, it is better to position the user between <italic>z </italic>= 46 cm, and <italic>z </italic>= 48 cm than to position him/her at <italic>z </italic>= 50 cm. At 50 cm, both the accuracy and precision are the worst.</p>
              
                <fig id="fig_9">
                  <label>Figure 9</label>
                  <caption>
                    <title>Precision results when varying the $x$ and $y$ distance (Experiment 2)</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_l4ctoIfS28PLUzPp.jpeg"/>
                </fig>
              
              
                <fig id="fig_10">
                  <label>Figure 10</label>
                  <caption>
                    <title>Height parameters of the office chair on wheels used for Experiment 3</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_zFPdaxMNteA31aHQ.jpeg"/>
                </fig>
              
              
                <fig id="fig_11">
                  <label>Figure 11</label>
                  <caption>
                    <title>Accuracy results when varying the $z$ distance (Experiment 3)</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_B81Qxj8EjGK_zrUg.jpeg"/>
                </fig>
              
              
                <fig id="fig_12">
                  <label>Figure 12</label>
                  <caption>
                    <title>Precision results when varying the $z$ distance (Experiment 3)</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_iuoQYoBP8-RPgtCS.jpeg"/>
                </fig>
              
            </sec>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Communication between eye-tracker and cobot</title>
      <p>After having analyzed the optimal operation range of the eye-tracker, we will configure a setup based on the devices available in our lab: a collaborative Omron TM5-700 robot and a Tobii X2-30 eye-tracker attached to a laptop.</p>
      
        <sec>
          
            <title>3.1. Elements description</title>
          
          <p>Our system uses two different computers between which we have established an Ethernet connection that enables their communication. <xref ref-type="fig" rid="fig_13">Figure 13</xref> shows the experimental setup, which consists of the following parts:</p><p><p>OMRON Cobot-arm model TM5-700. Collaborative robots are designed to work safely with human operators thanks to technologies like force feedback, low-inertia servo motors, elastic actuators, and collision detection technology that limit their power and force capabilities to levels suitable for contact. The safety standard ISO 10218-1, ISO 10218-2, and technical specification ISO TS-15066 define the safety functions and performance of the collaborative robot. We have used this specific robot as it was the only collaborative robot available in our laboratory.</p><p>3D grip attached to the cobot to hold the marker pen that performs the drawing on the surface area. The marker pen that has been chosen is a standard one, as the robot can use the grip to hold pens that may have different thicknesses.</p><p>Drawing surface, the size of which is a standard DINA3. Here, the collaborative robot will draw by moving its arm.</p><p>Cobot controller screen. The computer executes a software that is programmed with the software OMRON TMFlow, which is a graphical Human-Machine Interface. This makes the programming of a specific algorithm easier. It should be noted that where the collaborative robot is operated by a dedicated computer hidden in a box below the cobot.</p><p><span style="font-family: Times, serif">Cobot remote control, also known as robot stick. It is vital to start or stop the instructions’ reception by the robot</p><p>Tobii X2-30 eye-tracker, which is attached to the bottom part of the laptop screen.</p><p>Eye-tracker laptop, which operates the eye-tracker software programmed in Phyton.</p><p>Ethernet connection for the communication between computers. It will be in charge of sending the instructions from the eye-tracker computer to the cobot computer.</p></p>
          
            <fig id="fig_13">
              <label>Figure 13</label>
              <caption>
                <title>Experimental setup including: (1) cobot-arm TM5-700, (2) 3D grip, (3) DIN A3 drawing surface, (4) cobot controller screen, (5) cobot remote control, (6) Tobii X2-30, (7) eye-tracker laptop, (8) Ethernet connection</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_xmvSjlnx2GLNfgyc.jpeg"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Technical block description</title>
          
          <p>The technical scheme on the communication data flow between the eye-tracker and the cobot can be seen in <xref ref-type="fig" rid="fig_14">Figure 14</xref>, which illustrates the server–client architecture. The server provides the data acquired by the eye-tracker while the client is the cobot.</p><p>Further detail on the communication between the eye-tracker and collaborative robot can be found in [<xref ref-type="bibr" rid="ref_27">27</xref>].</p>
          
            <fig id="fig_14">
              <label>Figure 14</label>
              <caption>
                <title>Technical scheme of communication data flow between eye-tracker and cobot</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_M0rCOqf5N3LgJd9F.jpeg"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. Experiments and results</title>
          
          <p>Our main goal is to examine the system. To do so, some exercises have been designed, consisting of the drawing of a simple set of figures that can be seen in <xref ref-type="fig" rid="fig_15">Figure 15</xref>.</p><p>The process begins when the subject selects which figure he or she wants to draw. Then, when the system shows it to the user and he or she will look at all of its corners (marked in red in <xref ref-type="fig" rid="fig_15">Figure 15</xref>), while the eye-tracker acquires the set of points. Feel free to visit the following YouTube link, where we have included a demonstration of the system operation: https:// youtu.be/U6KYuhel9Tk.</p><p>Once the system has been tested, its application in more realistic and daily life situations is analogous. It could be easily adapted for handicapped people or in industrial environments that deal with perilous or hazardous substances, as it has been explained in previous sections of this paper. Nevertheless, this is beyond the goal of this paper, as we do not have access to these scenarios.</p><p><xref ref-type="fig" rid="fig_16">Figure 16</xref> shows the experimental results performed by the cobot on a DINA3 paper. On the left column, we can observe the result of the drawing performed by the cobot. On the right column, it is depicted the acquired points by the eye-tracker when the user looks at the corners of the desired image (from top to bottom: triangle, rectangle, pentagon, and star inside a pentagon). In our software design, the starting and ending point are marked separately by the user. For this reason, the figures look opened and not closed. In case of programming the application, to perform a line connecting the last acquired point to the first one, the figure would be closed.</p><p><xref ref-type="table" rid="table_1">Table 1</xref> highlights the accuracy and precision of the system (accuracy refers to how close a measurement is to the true or accepted value. Precision refers to how close measurements of the same item are to each other).</p><p><xref ref-type="table" rid="table_1">Table 1</xref> has been obtained averaging 10 different users (five males and five females). All the users were not eye-tracker skilled users. In fact, most of them used the eye-tracker for the first time. <xref ref-type="fig" rid="fig_17">Figure 17</xref> shows the calibration points acquired by the eye-tracker. They cover the four screen corners as well as the center.</p><p>Based on the experimental results, we can conclude that the accuracy is 5.5 mm, which should be compared with the whole dimension of the drawing. For large drawings, say about 20 cm, it represents a small relative error (2.8%). However, for small size drawings, the error can compromise the quality of the final result.</p>
          <p>Worth to mention that the user has performed the tasks without a lot of intensive training, and it has been tested with several users. All of them agree that the system is easy-to-use.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Accuracy and precision of measurements (SD=standard deviation, RMS=root mean square)</title>
              </caption>
              <table><tbody><tr><td colspan="3"><p>Validation accuracy</p></td><td colspan="3"><p>Validation precision (SD)</p></td><td colspan="3"><p>Validation precision (RMS)</p></td></tr><tr><td><p>degrees</p></td><td><p>pixels</p></td><td><p>mm</p></td><td><p>degrees</p></td><td><p>pixels</p></td><td><p>mm</p></td><td><p>degrees</p></td><td><p>pixels</p></td><td><p>mm</p></td></tr><tr><td><p>0.70º</p></td><td><p>22</p></td><td><p>5.5</p></td><td><p>0.22º</p></td><td><p>7</p></td><td><p>1.7</p></td><td><p>0.33º</p></td><td><p>10</p></td><td><p>2.6</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <fig id="fig_15">
              <label>Figure 15</label>
              <caption>
                <title>Top, from left to right: triangle, rectangle, pentagon, and star figures presented on the screen of the eye-tracker. Bottom: user interface with several acquired dots</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_2eCxjWwN0_kgblS5.jpeg"/>
            </fig>
          
          
            <fig id="fig_16">
              <label>Figure 16</label>
              <caption>
                <title>Triangle, rectangle, pentagon, and star inside a pentagon. Drawing performed by the robot (left column) and points acquired by the eye-tracker (right column)</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_ed_yzdWAEmd5_FeA.jpeg"/>
            </fig>
          
          
            <fig id="fig_17">
              <label>Figure 17</label>
              <caption>
                <title>Calibration points acquired by the eye-tracker</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/9/img_fBByOoURczpwDgnd.jpeg"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>4. Conclusions</title>
      <p>Regarding the optimal operation range of the Tobii X2-30 screen-based eye-tracker, the different experiments that have been carried out conclude that accuracy and precision are at its best when the user is located at a <italic>y </italic>distance in the range of 55-65 cm. The less he/she moves, the better, because when the subject is centered towards the eye-tracker the measurements are more accurate than if he/she is not. Nevertheless, if he/she moves 5 cm to the right or left in the <italic>x </italic>axis, the results are not compromised. At last, when it comes to the <italic>z </italic>distance, the optimal is 52 cm.</p><p>Furthermore, an experimental setup connecting this eye-tracker to a collaborative robot OMRON TM5-700 has also been presented. In this process, the most challenging part has been the communication between both elements. Once this has been solved, it should not be difficult to adapt this system in a large number of quotidian applications where a robotic arm can be controlled by the user’s gaze in a wide variety of fields including safety in industrial environments or health, among others. Although using a different robot and eye-tracker would require a new setup, it is our belief that this paper ought to be valuable concerning the new system design.</p><p>Experimental calibration data will be freely available contacting the authors as well as in arxiv.org data repository.</p><p>This work has been funded by Spanish grant Ministerio de ciencia e innovación PID2020-113242RB-I00.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      <p>This work has been funded by Spanish grant Ministerio de ciencia e innovación PID2020-113242RB-I00.</p>
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>Experimental calibration data will be freely available contacting the authors as well as in <a target="_blank" rel="noopener noreferrer nofollow" href="http://arxiv.org">arxiv.org</a> data repository.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation/>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation/>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation/>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation/>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation/>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation/>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation/>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation/>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation/>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation/>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation/>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation/>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation/>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation/>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation/>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation/>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation/>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation/>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation/>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation/>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation/>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation/>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation/>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation/>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation/>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation/>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation/>
      </ref>
    </ref-list>
  </back>
</article>