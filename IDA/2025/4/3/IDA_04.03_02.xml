<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IDA</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Information Dynamics and Applications</journal-title>
        <abbrev-journal-title abbrev-type="issn">Inf. Dyn. Appl.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IDA</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-1494</issn>
      <issn publication-format="print">2958-1486</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-8uvSD9wPYmupE5ZaSXi1JowZWJJvZcyj</article-id>
      <article-id pub-id-type="doi">10.56578/ida040302</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Application of Deep Learning Techniques in the Diagnosis and Grading of Knee Osteoarthritis (OA)</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-7391-5200</contrib-id>
          <name>
            <surname>Yeddula</surname>
            <given-names>Varshita</given-names>
          </name>
          <email>varshitayeddula@gmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3315-8739</contrib-id>
          <name>
            <surname>Aluru</surname>
            <given-names>Ranganadha Reddy</given-names>
          </name>
          <email>hodbioinformatics@vignan.ac.in</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-4452-7598</contrib-id>
          <name>
            <surname>Budda</surname>
            <given-names>Parvathi Devi</given-names>
          </name>
          <email>buddaparvathi1988@gmail.com</email>
        </contrib>
        <aff id="aff_1">Department of Bioinformatics, Vignan’s Foundation for Science, Technology and Research, 522213 Guntur, India</aff>
        <aff id="aff_2">Department of Electrical and Electronics Engineering, Chalapathi Institute of Engineering and Technology, 522034 Guntur, India</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>25</day>
        <month>08</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>3</issue>
      <fpage>139</fpage>
      <lpage>147</lpage>
      <page-range>139-147</page-range>
      <history>
        <date date-type="received">
          <day>19</day>
          <month>06</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>19</day>
          <month>08</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Osteoarthritis (OA) affects approximately 240 million individuals globally. Knee osteoarthritis, a crippling ailment marked by joint stiffness, discomfort, and functional impairment, is particularly the most widespread kind of arthritis among the elderly. To assess the severity of this disease, physical symptoms, medical history, and further joint screening examinations including radiography, Magnetic Resonance Imaging (MRI), and Computed Tomography (CT) scans have frequently been considered. It is difficult to identify early development of this disease as conventional diagnostic methods could be subjective. Therefore, doctors utilize the Kellgren and Lawrence (KL) scale to evaluate the severity of knee OA with visual images obtained from X-ray or MRI. The detection and prediction of the severity of knee OA indeed requires a novel model that uses deep learning models, including Inception and Xception. Utilizing the KL grading scale, the model, including Xception, ResNet-50, and Inception-ResNet-v2 could determine the degree of knee OA suffered by patients. The experimental results revealed that the Xception network achieved the highest classification accuracy of 67%, surpassing ResNet-50 and Inception-ResNet-v2, demonstrating its superior ability to automatically grade OA severity from radiographic images.</p></abstract>
      <kwd-group>
        <kwd>Knee osteoarthritis</kwd>
        <kwd>Kellgren and Lawrence grade</kwd>
        <kwd>Deep learning</kwd>
        <kwd>Xception</kwd>
        <kwd>X-ray images</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="3"/>
        <fig-count count="6"/>
        <table-count count="3"/>
        <ref-count count="33"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>The pathophysiology of osteoarthritis (OA), which has a serious negative impact on the patients’ quality of life, ability to work, and finances, has long required investigation. OA goes beyond simple morphological and physiological alterations when micro- and macro-injuries start the degradation of the extracellular cartilage matrix, i.e., joint degeneration marked by abnormalities in the synovial membrane, gradual loss of cartilage between joints, bone enlargement, and loss of joint function. There is an interconnected relationship between OA and aging. Moreover, there are other risk factors such as gender, obesity, inactivity, hereditary predisposition, bone density, and trauma.</p><p>In contrast to other types of inflammatory arthritis when activity and exercise relieve symptoms, stress and excessive activity worsen the pain and stiffness in the joints. Other possible effects include instability, deformation of the joint, and loss of joint function [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>]. The area between the knee joints flattens down when cartilage is removed, hence accelerating the development of knee OA [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>].</p><p>Major alterations denoted by the term LOSS below show how knee OA develops:</p><p><p>L-“loss of joint space”, brought by degradation of cartilage;</p><p>O-“osteophytes formations”, which are protrusions that form along the joint’s margins;</p><p>S-“subarticular sclerosis”, which refers to a rise in bone mass along the joint line; and</p><p>S-“subchondral cysts”, which are brought forth by the bone’s fluid-filled holes that are formed near the joints.</p></p><p>Diagnostic procedures such as X-rays, Computed Tomography scans (CT scans), and Magnetic Resonance Imaging (MRI), are commonly employed to determine the biological condition of knee OA and to detect the structural abnormalities in the joint. Nowadays, conventional treatment of knee OA will not suffice to fully resolve the issue.</p><p>Detecting joint deformation at an early stage is vital to prevent permanent damage. The Kellgren and Lawrence (KL) grading system, recognized by the World Health Organization (WHO), is typically used to evaluate the severity of knee osteoarthritis [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>]. Grade 0 for low severity up to 4 for high severity were used on the 5-point semiquantitative progressive ordinal KL grading system. <xref ref-type="fig" rid="fig_1">Figure 1</xref> depicts the progression of knee OA and the associated KL Grade for each stage.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>X-ray images showing the progression of knee OA according to KL grades (I–V)</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_nWM7VGm_mhYMjNKq.png"/>
        </fig>
      
      <p>Computer systems which are widely used today and their concomitant benefits are the results of technological advancement. Additionally, the desire for computer use is rising daily. Despite the advancement of digital computers, it remains challenging to conduct studies on machine simulation of human functions. In order to assess OA progression, grading, and detection, new techniques and tools must be developed due to the prevalence of the disease [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>]. Advances in algorithms and use of machine learning and deep learning enable healthcare experts to better assess the state and course of osteoarthritis through medical image analysis using automatic or semi-automatic methods.</p><p>To improve diagnosis, a machine learning-based computer-assisted technique was required to meet the diagnostic challenge with automatic X-ray [<xref ref-type="bibr" rid="ref_9">9</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>]. Two stages for automatic OA diagnosis were introduced in recent machine learning-based research: (1) ROI (Region of Interest) segmentation, which reduces noise by eliminating background and unimportant information, and (2) categorization of OA severity based on machine learning, which standardized and streamlined difficult diagnostic criteria. These two stages are shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>. However, the selection of features in earlier knee recognition techniques requires a lot of manual labor [<xref ref-type="bibr" rid="ref_11">11</xref>], [<xref ref-type="bibr" rid="ref_12">12</xref>]. Furthermore, symptoms of osteoarthritis may be witnessed in several bone areas. The accuracy of evaluation is impacted by machine learning algorithms for diagnosing OA, as they seldom examine the connections between these locations. Using a sizable dataset, the workflow depicted in <xref ref-type="fig" rid="fig_2">Figure 2</xref> was replicated and an automated deep-learning method was adopted to improve OA diagnosis in this work [<xref ref-type="bibr" rid="ref_13">13</xref>], [<xref ref-type="bibr" rid="ref_14">14</xref>], [<xref ref-type="bibr" rid="ref_15">15</xref>].</p><p>The following are some of the main contributions of this study:</p><p>1. To reduce the need for human feature engineering, an object detection Convolutional Neural Network (CNN) is being improved in order to separate the knee areas from X-ray pictures;</p><p>2. To enhance classification performance and visual transformers’ self-attention mechanism is utilized;</p><p>3. To categorize the severity of OA in a sizable dataset with the Kellgren and Lawrence (KL) grading system; and</p><p>4. To evaluate the suggested approach and the findings show higher accuracy in osteoarthritis severity classification and better efficiency of knee segmentation.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Workflow illustrating the segmentation and classification stages of computer-assisted diagnosis of osteoarthritis</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_a05k034zLEaJwCuX.jpeg"/>
        </fig>
      
      <p>Dataset Used</p><p>The dataset employed in this study was obtained from the Osteoarthritis Initiative (OAI), a large-scale longitudinal project funded by the National Institutes of Health (NIH). It comprises data from 4,476 participants, providing a diverse and balanced cohort for knee osteoarthritis (OA) research. In this study, 4,446 radiographs were utilized, each annotated with Kellgren–Lawrence (KL) grades for both knee joints, resulting in a total of 8,260 knee joint images.The dataset distribution was as follows: Grade 0 – 3,253, Grade 1 – 1,495, Grade 2 – 2,175, Grade 3 – 1,086, and Grade 4 – 251. This distribution closely refelects the overall dataset characteristics, ensuring balanced representation across all severity levels for effective model training and evaluation.</p><p>This collection of data included both Kellgren and Lawrence (KL) grading and knee joint detection. Furthermore, there are versions of picture data that have 224 and 299 times more pixels, respectively. In this work, machine learning technology was used to predict the Kellgren and Lawrence grades of knees collected in the dataset, based on the following criteria:</p><p>Grade 0: No radiographic signs of osteoarthritis.</p><p>Grade 1: Possible osteophytic lipping with uncertain joint space narrowing (JSN).</p><p>Grade 2: Definite osteophytes with potential JSN.</p><p>Grade 3: Multiple osteophytes, evident JSN, sclerosis, and possible bone deformation.</p><p>Grade 4: Severe sclerosis, prominent JSN, large osteophytes, and significant bone deformity [<xref ref-type="bibr" rid="ref_16">16</xref>], [<xref ref-type="bibr" rid="ref_17">17</xref>].</p><p>The dataset was divided into training, testing, and validation subsets in a 70:20:10 ratio to ensure balanced evaluation across the severity level. <xref ref-type="table" rid="table_1">Table 1</xref> shows the distribution of samples across these subsets.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Workflow illustrating the segmentation and classification stages of computer-assisted diagnosis of osteoarthritis</title>
          </caption>
          <table><tbody><tr><th colspan="1" rowspan="1"><p>Dataset</p></th><th colspan="1" rowspan="1"><p>Grade 0</p></th><th colspan="1" rowspan="1"><p>Grade 1</p></th><th colspan="1" rowspan="1"><p>Grade 2</p></th><th colspan="1" rowspan="1"><p>Grade 3</p></th><th colspan="1" rowspan="1"><p>Grade 4</p></th><th colspan="1" rowspan="1"><p>Total</p></th></tr><tr><td colspan="1" rowspan="1"><p>Training</p></td><td colspan="1" rowspan="1"><p>2,286</p></td><td colspan="1" rowspan="1"><p>1,046</p></td><td colspan="1" rowspan="1"><p>1,516</p></td><td colspan="1" rowspan="1"><p>757</p></td><td colspan="1" rowspan="1"><p>173</p></td><td colspan="1" rowspan="1"><p>5,778</p></td></tr><tr><td colspan="1" rowspan="1"><p>Testing</p></td><td colspan="1" rowspan="1"><p>639</p></td><td colspan="1" rowspan="1"><p>296</p></td><td colspan="1" rowspan="1"><p>447</p></td><td colspan="1" rowspan="1"><p>223</p></td><td colspan="1" rowspan="1"><p>51</p></td><td colspan="1" rowspan="1"><p>1,656</p></td></tr><tr><td colspan="1" rowspan="1"><p>Validation</p></td><td colspan="1" rowspan="1"><p>328</p></td><td colspan="1" rowspan="1"><p>153</p></td><td colspan="1" rowspan="1"><p>212</p></td><td colspan="1" rowspan="1"><p>106+</p></td><td colspan="1" rowspan="1"><p>27</p></td><td colspan="1" rowspan="1"><p>826</p></td></tr><tr><td colspan="1" rowspan="1"><p>Total</p></td><td colspan="1" rowspan="1"><p>3,253</p></td><td colspan="1" rowspan="1"><p>1,495</p></td><td colspan="1" rowspan="1"><p>2,175</p></td><td colspan="1" rowspan="1"><p>1,086</p></td><td colspan="1" rowspan="1"><p>251</p></td><td colspan="1" rowspan="1"><p>8,260</p></td></tr></tbody></table>
        </table-wrap>
      
    </sec>
    <sec sec-type="">
      <title>2. Methodology</title>
      <p>Xception: Depth-Wise Split Convolutions for Deep Learning</p><p>The architecture called Xception, performs somewhat better than Inception V3 on the ImageNet dataset, where Inception V3 was created for, and much better than it on a bigger image classification dataset with 17,000 classes and 350 million pictures [<xref ref-type="bibr" rid="ref_18">18</xref>], [<xref ref-type="bibr" rid="ref_19">19</xref>].</p><p>A depthwise convolution and a pointwise convolution make up the original depthwise separable convolution:</p><p>A channel-wise n × n spatial convolution is carried out using depthwise convolution. For example, Figure \ref{fig3} applies five distinct n × n spatial convolutions if there are five channels.</p><p>The dimensionality is adjusted using pointwise convolution, which is just a 1 × 1 convolution [<xref ref-type="bibr" rid="ref_20">20</xref>], [<xref ref-type="bibr" rid="ref_21">21</xref>].</p><p>Convolution over all channels is not necessary for depthwise separable convolution, in contrast to classical convolution. Since there are fewer connections, the model is lighter and more effective.</p>
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>(a). Xception model architecture; (b). Flow of Xception network layers</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_j2ZBRjxOne8rFqVF.png"/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_AVDiWu993EhtpfCO.png"/>
        </fig>
      
      <p>ResNet</p><p>On the ImageNet dataset, residual networks up to 152 layers deep are evaluated, as it is eight times deeper than VGG nets while being less complex. The error of an ensemble of these residual networks on the ImageNet test set is 3.57%. This result won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2015. A 100 and 1,000 level Canadian Institute for Advanced Research (CIFAR)-10 analyses [<xref ref-type="bibr" rid="ref_22">22</xref>] were conducted.</p><p>In many visual recognition tasks, depth representation is important. Owing to the extraordinary deep representations, a 28% relative gain on the Common Objects in Context (COCO) object identification dataset was obtained [<xref ref-type="bibr" rid="ref_23">23</xref>]. In addition to winning first place in the ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation tasks, entries to the ILSVRC &amp;amp; COCO 2015 contests were constructed using deep residual nets [<xref ref-type="bibr" rid="ref_24">24</xref>].</p><p>Many tasks of computer vision rely on a generalized neural network called ResNet as shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>, short for Residual Networks [<xref ref-type="bibr" rid="ref_25">25</xref>]. A 50-level convolutional neural network is called ResNet-50. The capacity of ResNet to educate the ways for creating incredibly complicated neural networks with over 150 layers is its primary innovation. This novel neural network was initially introduced by Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun in their 2015 computer vision research paper, “Deep residual learning for image recognition”. One of the major shortcomings of convolutional neural networks is the “Vanishing Gradient Problem” [<xref ref-type="bibr" rid="ref_26">26</xref>], [<xref ref-type="bibr" rid="ref_27">27</xref>]. During backpropagation, the gradient value drastically drops, therefore weights barely change. ResNet is used to circumvent this. It makes use of SKIP CONNECTION.</p>
      
        <fig id="fig_4">
          <label>Figure 4</label>
          <caption>
            <title>ResNet-50 architecture with residual block connections</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_tJtkOWx6NI6hzsRD.png"/>
        </fig>
      
      <p>Inception-v4</p><p>Recent years have seen the greatest advance in picture identification, mostly because of extremely deep convolutional networks. This is the case with the Inception design, which has shown to provide outstanding performance at a comparatively low computational cost. In the 2015 ILSVRC competition, the newly added residual connections along with a more conventional architecture resulted in state-of-the-art performance that was comparable to the latest Inception-v3 network as shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>. This is dubious if combining the Inception design with the remaining connections might be advantageous. Strong empirical evidence was found in the training of Inception networks with residual links to speed up the process [<xref ref-type="bibr" rid="ref_28">28</xref>], [<xref ref-type="bibr" rid="ref_29">29</xref>].</p><p>Some research suggested that Inception networks with residual connections outperformed comparably priced Inception networks without residual connections by a small margin. Furthermore, a range of new and compact Inception network designs was provided for residual and non-residual networks. The single-frame recognition performance in the ILSVRC 2012 classification assignment was greatly enhanced by these changes. Appropriate activation scaling facilitate robust training of very large residual Inception networks. Using a collection of three residuals and one Inception-v4 model could derive a top-5 error score of 3.08 percent on the ImageNet classification (CLS) test set [<xref ref-type="bibr" rid="ref_30">30</xref>].</p>
      
        <fig id="fig_5">
          <label>Figure 5</label>
          <caption>
            <title>Inception-ResNet-v2 architecture combining inception modules with residual connections</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_WZHSk1K-CxD8GMtZ.jpeg"/>
        </fig>
      
      <p>All models were implemented using TensorFlow and Keras frameworks. The dataset was divided into 70% training, 20% testing, and 10% validation subsets. To ensure reproducibility and fair comparison, identical hyperparameters were used across all models. The training configuration is summarized in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>Training parameters and configurations</title>
          </caption>
          <table><tr><th >Parameter</th><th >Configuration</th></tr><tr><td >Loss of Function</td><td >Categorical Cross-Entropy</td></tr><tr><td >Optimizer</td><td >Adam (adaptive learning rate optimization)</td></tr><tr><td >Initial Learning Rate</td><td >1e-4, with scheduled decay</td></tr><tr><td >Batch Size</td><td >32</td></tr><tr><td >Epochs</td><td >50 (with early stopping on validation loss)</td></tr><tr><td >Metrics Monitored</td><td >Accuracy, Balanced Accuracy, Validation Loss</td></tr></table>
        </table-wrap>
      
    </sec>
    <sec sec-type="">
      <title>3. Results and disscussion</title>
      <p>To predict the beginning and development of knee OA, three distinct deep learning models have been developed: Xception, ResNet 50, and Inception-ResNet-v2 [<xref ref-type="bibr" rid="ref_31">31</xref>], [<xref ref-type="bibr" rid="ref_32">32</xref>]. Their performance is compared in <xref ref-type="table" rid="table_3">Table 3</xref>. The Multilayer Perceptron (MLP) model outperforms the logistic regression models in predicting the start and course of the disease [<xref ref-type="bibr" rid="ref_33">33</xref>]. The complex nonlinearity of the data structure may account for such an occurrence. As a result, it is more difficult for the linear classification method to manage the complexity of the data point distribution in this dataset than its non-linear version. As a result, it was determined that Xception model provided greater accuracy than the other two models.</p>
      
        <table-wrap id="table_3">
          <label>Table 3</label>
          <caption>
            <title>Training parameters and configurations</title>
          </caption>
          <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p>Accuracy</p></th><th colspan="1" rowspan="1"><p>Execution Time</p></th></tr><tr><td colspan="1" rowspan="1"><p>Xception</p></td><td colspan="1" rowspan="1"><p>67%</p></td><td colspan="1" rowspan="1"><p>68 mins</p></td></tr><tr><td colspan="1" rowspan="1"><p>ResNet-50</p></td><td colspan="1" rowspan="1"><p>65%</p></td><td colspan="1" rowspan="1"><p>80 mins</p></td></tr><tr><td colspan="1" rowspan="1"><p>Inception-ResNet-v2</p></td><td colspan="1" rowspan="1"><p>64%</p></td><td colspan="1" rowspan="1"><p>56 mins</p></td></tr></tbody></table>
        </table-wrap>
      
      
        <fig id="fig_6">
          <label>Figure 6</label>
          <caption>
            <title>Predictions of Xception model based on best model using Grad-CAM</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/10/img_XEQS6UhLxOtIT32c.jpeg"/>
        </fig>
      
      <p>The comparative analysis showed that the Xception model achieved the highest accuracy (67%), outperforming ResNet-50 (65%) and Inception-ResNet-v2 (64%). Its superior performance can be attributed to the use of depthwise separable convolutions, which efficiently capture localized spatial features while reducing parameter redundancy and overfitting. Conversely, the deeper architecture of ResNet-50 may have led to increased computational demand and a need for larger training data to reach full generalization. Overall these findings suggested that Xception provided an optimal balance between model complexity, training efficiency, and predictive accuracy, rendering it more suitable for real-time diagnostic applications in clinical environments.</p><p>A critical limitation of deep learning models in the medical domain is their perceived “black box” nature. To mitigate this, the Gradient-weighted Class Activation Mapping (Grad-CAM) technique was used to provide interpretability to the predictions of Xception model. Grad-CAM heatmaps as shown in <xref ref-type="fig" rid="fig_6">Figure 6</xref> were generated for representative images across each KL grade. The goal was to determine whether the model was attending to clinically relevant regions, particularly the joint space, bone surface, and cartilage structure.</p><p>Findings from Grad-CAM visualization include:</p><p><p>For Healthy, Doubtful, and Minimal grades, the activation regions concentrated around the joint space, in alignment with early signs of cartilage thinning and joint narrowing. </p><p> For Moderate and Severe grades, the model’s attention expanded towards the edges of the joint and bone surfaces, in correlation with the presence of osteophytes and subchondral cysts.</p></p>
    </sec>
    <sec sec-type="">
      <title>4. Conclusion</title>
      <p>This study proposed a deep learning–based framework for the automated diagnosis and grading of knee osteoarthritis (OA) from X-ray images. Three convolutional neural network architectures, i.e., Xception, ResNet-50, and Inception-ResNet-v2 were trained and evaluated using the OAI dataset. Among these, the Xception network achieved the best performance with 67% accuracy, effectively capturing key radiographic patterns linked to OA severity. These results highlight the potential of deep learning methods in enhancing radiological assessment by offering faster, more objective, and reproducible OA grading. Future research could integrate multimodal clinical data and adopt interpretable AI approaches to improve transparency and facilitate clinical implementation..</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      <p>R.R.A. conceptualized the study, designed the methodology, and supervised the experimental work. V.Y. handled data preprocessing, model training, and performance evaluation. P.D.B. contributed to result interpretation, visualization, and manuscript preparation.</p>
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>4-6</page-range>
          <issue>1</issue>
          <year>2012</year>
          <person-group person-group-type="author">
            <name>
              <surname>Poole</surname>
              <given-names>A. R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11420-011-9248-6</pub-id>
          <article-title>Osteoarthritis as a whole joint disease</article-title>
          <source>HSS J.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>4</volume>
          <page-range>29-35</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>AlShammasi</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Alkhaldi</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Alharbi</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Alyami</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.51847/jAv4cm0TTn</pub-id>
          <article-title>Shifts in parental perception of pediatric patients needing dental rehabilitation under general anesthesia post-pandemic</article-title>
          <source>Turk. J. Public Health Dent.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>29-30</volume>
          <page-range>100587</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cui</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Zhong</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.eclinm.2020.100587</pub-id>
          <article-title>Global, regional prevalence, incidence and risk factors of knee osteoarthritis in population-based studies</article-title>
          <source>EClinicalMed.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>28-31</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Macrì</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>D’Albis</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>D’Albis</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Antonacci</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Abbinante</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Stefanelli</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Pegreffi</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Festa</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.51847/OjyFxHjTxv</pub-id>
          <article-title>Assessing the psychosocial and functional impact of periodontal disease</article-title>
          <source>Ann. Orthod. Periodont. Spec.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>376-388</page-range>
          <issue>3</issue>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Reddy</surname>
              <given-names>A. R.</given-names>
            </name>
            <name>
              <surname>Venkateswarulu</surname>
              <given-names>T. C.</given-names>
            </name>
            <name>
              <surname>Babu</surname>
              <given-names>D. J.</given-names>
            </name>
            <name>
              <surname>Devi</surname>
              <given-names>N. S.</given-names>
            </name>
          </person-group>
          <article-title>Homology modeling, simulation and docking studies of tau-protein kinase</article-title>
          <source>Res. J. Pharm. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>4</volume>
          <page-range>6-12</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ludovichetti</surname>
              <given-names>F. S.</given-names>
            </name>
            <name>
              <surname>Stellini</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Zuccon</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Lucchi</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Dessupoiu</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Mazzoleni</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Parcianello</surname>
              <given-names>R. G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.51847/Pw5eiJPGY2</pub-id>
          <article-title>Comparative impact of chlorhexidine and fluoride varnish on white spot lesion prevention in orthodontic patients</article-title>
          <source>Turk. J. Public Health Dent.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>24</volume>
          <page-range>229-240</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>El-Ghany</surname>
              <given-names>S. A.</given-names>
            </name>
            <name>
              <surname>Elmogy</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>El-Aziz</surname>
              <given-names>A. A. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.eij.2023.03.005</pub-id>
          <article-title>A fully automatic fine-tuned deep learning model for knee osteoarthritis detection and progression analysis</article-title>
          <source>Egypt. Inform. J.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>4</volume>
          <page-range>6-10</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Savva</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Papastavrou</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Charalambous</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Vryonides</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Merkouris</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.51847/DkBR8F3IGx</pub-id>
          <article-title>Studying the nurses' and nursing students' attitudes towards the phenomenon of elderly</article-title>
          <source>J. Integr. Nurs. Palliat. Care</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>41</volume>
          <page-range>419-444</page-range>
          <issue>2</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Saini</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Chand</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Chouhan</surname>
              <given-names>D. K.</given-names>
            </name>
            <name>
              <surname>Prakash</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.bbe.2021.03.002</pub-id>
          <article-title>A comparative analysis of automatic classification and grading methods for knee osteoarthritis focussing on X-ray images</article-title>
          <source>Biocybern. Biomed. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>17-23</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shahzan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Paulraj</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Maiti</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.51847/RmzKyVOff6</pub-id>
          <article-title>Impact of rubber dam use on anxiety levels in children during dental procedures: A randomized controlled study</article-title>
          <source>Int. J. Dent. Res. Allied Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>6333</page-range>
          <issue>14</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Touahema</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Zaimi</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Zrira</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Ngote</surname>
              <given-names>M. N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app14146333</pub-id>
          <article-title>How can artificial intelligence identify knee osteoarthritis from radiographic images with satisfactory accuracy?: A literature review for 2018–2024</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>1-8</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Malcangi</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Patano</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Trilli</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Piras</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Ciocia</surname>
              <given-names>A. M.</given-names>
            </name>
            <name>
              <surname>Inchingolo</surname>
              <given-names>A. D.</given-names>
            </name>
            <name>
              <surname>Mancini</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Hazballa</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Venere</surname>
              <given-names>D. D.</given-names>
            </name>
            <name>
              <surname>Inchingolo</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>others</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.51847/DWXltUS9Lp</pub-id>
          <article-title>A systematic review of the role of soft tissue lasers in enhancing esthetic dental procedures</article-title>
          <source>Int. J. Dent. Res. Allied Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>58</volume>
          <page-range>1458-1473</page-range>
          <issue>10</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kinger</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s43465-024-01259-4</pub-id>
          <article-title>Deep learning for automatic knee osteoarthritis severity grading and classification</article-title>
          <source>Indian J. Orthop.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>8-13</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Leadbeatter</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Tjaya</surname>
              <given-names>K. C.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>H. S.</given-names>
            </name>
            <name>
              <surname>Phan</surname>
              <given-names>A. J.</given-names>
            </name>
            <name>
              <surname>Peck</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.51847/Ji7rjm1omB</pub-id>
          <article-title>A study of professional commitment in specialized dental assistants</article-title>
          <source>Ann. J. Dent. Med. Assist.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>91-96</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Aluru</surname>
              <given-names>R. R.</given-names>
            </name>
            <name>
              <surname>Koyi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Nalluru</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Chanda</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/eesrj.080205</pub-id>
          <article-title>Production of biopolymer from bacteria–A review</article-title>
          <source>Environ. Earth Sci. Res. J.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-5</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Arumugam</surname>
              <given-names>S. R.</given-names>
            </name>
            <name>
              <surname>Balakrishna</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Rajeshram</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Gowr</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Karuppasamy</surname>
              <given-names>S. G.</given-names>
            </name>
            <name>
              <surname>Premnath</surname>
              <given-names>S. P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/NKCon56289.2022.10126934</pub-id>
          <article-title>Prediction of severity of knee osteoarthritis on X-ray images using deep learning</article-title>
          <source>2022 IEEE North Karnataka Subsection Flagship International Conference (NKCon), Vijaypur, India</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>17-21</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chanda</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Aluru</surname>
              <given-names>R. R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.51847/GRccY6BTJ6</pub-id>
          <article-title>Anticuagulants: An overview of natural and synthetic therapeutic anticoagulants</article-title>
          <source>J. Biochem. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <page-range>e0249278</page-range>
          <issue>8</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Muhammad</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Aramvith</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Onoye</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0249278</pub-id>
          <article-title>Multi-scale Xception based depthwise separable convolution for single image super-resolution</article-title>
          <source>PloS One</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>91-96</page-range>
          <issue>2</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Aluru</surname>
              <given-names>R. R.</given-names>
            </name>
            <name>
              <surname>Koyi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Nalluru</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Chanda</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/eesrj.080205</pub-id>
          <article-title>Production of biopolymer from bacteria–A review</article-title>
          <source>Environ. Earth Sci. Res. J.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>1530</page-range>
          <issue>5</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Balmez</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Brateanu</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Orhei</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Ancuti</surname>
              <given-names>C. O.</given-names>
            </name>
            <name>
              <surname>Ancuti</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s25051530</pub-id>
          <article-title>DepthLux: Employing depthwise separable convolutions for low-light image enhancement</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>1209-1216</page-range>
          <issue>9</issue>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Reddy</surname>
              <given-names>A. R.</given-names>
            </name>
            <name>
              <surname>Venkateswarulu</surname>
              <given-names>T. C.</given-names>
            </name>
            <name>
              <surname>Indira</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Narayana</surname>
              <given-names>A. V.</given-names>
            </name>
            <name>
              <surname>Lohita</surname>
              <given-names>T. N.</given-names>
            </name>
            <name>
              <surname>Sriharsha</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.5958/0974-360X.2015.00221.8</pub-id>
          <article-title>Identification of membrane drug targets by subtractive genomic approach in mycoplasma pneumonia</article-title>
          <source>Res. J. Pharm. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>46</volume>
          <page-range>101462</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Baqar</surname>
              <given-names>Zulqarnain</given-names>
            </name>
            <name>
              <surname>Islam</surname>
              <given-names>Sk Injamamul</given-names>
            </name>
            <name>
              <surname>Das</surname>
              <given-names>Gunjan</given-names>
            </name>
            <name>
              <surname>Mahfuj</surname>
              <given-names>Sarower</given-names>
            </name>
            <name>
              <surname>Ahammad</surname>
              <given-names>Foysal</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.imu.2024.101462</pub-id>
          <article-title>Development and design of CRISPR-based diagnostic for Acinetobacter baumannii by employing off-target gene editing of sgRNA</article-title>
          <source>Inform. Med. Unlocked</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="conf-paper">
          <page-range>9415-9422</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Duta</surname>
              <given-names>I. C.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Shao</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICPR48806.2021.9412193</pub-id>
          <article-title>Improved residual networks for image and video recognition</article-title>
          <source>2020 25th International Conference on Pattern Recognition (ICPR), Milan, Italy</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="conf-paper">
          <page-range>22614-22627</page-range>
          <person-group person-group-type="author">
            <name>
              <surname>Bello</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Fedus</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Du</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Cubuk</surname>
              <given-names>E. D.</given-names>
            </name>
            <name>
              <surname>Srinivas</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Shlens</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zoph</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Revisiting ResNets: Improved training and scaling strategies</article-title>
          <source>Advances in Neural Information Processing Systems, Red Hook, NY, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>arXiv:1603.08029</volume>
          <person-group person-group-type="author">
            <name>
              <surname>Targ</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Almeida</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Lyman</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.1603.08029</pub-id>
          <article-title>Resnet in Resnet: Generalizing residual architectures</article-title>
          <source>arXiv Preprint</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="conf-paper">
          <page-range>770-778</page-range>
          <person-group person-group-type="author">
            <name>
              <surname>He</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Ren</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/cvpr.2016.90</pub-id>
          <article-title>Deep residual learning for image recognition</article-title>
          <source>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>12-17</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ranganadhareddy</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.51847/Hxh14VrhOr</pub-id>
          <article-title>A review on biotechnological approaches for the production of polyhydroxyalkanoates</article-title>
          <source>J. Biochem. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <article-title>Residual feature-reutilization Inception network for image classification</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>He</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Zhan</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Jiao</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2412.19433</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>1-6</page-range>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ranganadhareddy</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Chandrsekhar</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.51847/3qf2Wvuzl2</pub-id>
          <article-title>Polyhydroxyalkanoates, the biopolymers of microbial origin- A review</article-title>
          <source>J. Biochem. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="conf-paper">
          <volume>31</volume>
          <issue>1</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Szegedy</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Ioffe</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Vanhoucke</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Alemi</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1609/aaai.v31i1.11231</pub-id>
          <article-title>Inception-v4, Inception-ResNet and the impact of residual connections on learning</article-title>
          <source>Proceedings of the AAAI Conference on Artificial Intelligence, San Francisco, California, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="conf-paper">
          <page-range>352-357</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>McNeely-White</surname>
              <given-names>D. G.</given-names>
            </name>
            <name>
              <surname>Beveridge</surname>
              <given-names>J. R.</given-names>
            </name>
            <name>
              <surname>Draper</surname>
              <given-names>B. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-030-25719-4_45</pub-id>
          <article-title>Inception and ResNet: Same training, same features</article-title>
          <source>Advances in Intelligent Systems and Computing, Seattle, WA, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>1-6</page-range>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ranganadhareddy</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.51847/NeYIasA2Ix</pub-id>
          <article-title>Production of polyhydroxyalkanoates from microalgae- A review</article-title>
          <source>J. Biochem. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>100135</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chan</surname>
              <given-names>LC</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>HHT</given-names>
            </name>
            <name>
              <surname>Chan</surname>
              <given-names>PK</given-names>
            </name>
            <name>
              <surname>Wen</surname>
              <given-names>C</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ocarto.2020.100135</pub-id>
          <article-title>A machine learning-based approach to decipher multi-etiology of knee osteoarthritis onset and deterioration</article-title>
          <source>Osteoarthritis Cartil. Open</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>