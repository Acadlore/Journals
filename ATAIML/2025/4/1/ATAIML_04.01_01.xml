<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-Fu-AZ-EeWpyaXskFXxLjQHMOwut7EMnC</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml040101</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Advanced Tanning Detection Through Image Processing and Computer Vision</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-7873-7510</contrib-id>
          <name>
            <surname>Mukhopadhyay</surname>
            <given-names>Sayak</given-names>
          </name>
          <email>sayak.mukhopadhyay.btech2020@sitpune.edu.in</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-5464-7967</contrib-id>
          <name>
            <surname>Gupta</surname>
            <given-names>Janmejay</given-names>
          </name>
          <email>janmejay.gupta.btech2020@sitpune.edu.in</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-4934-6129</contrib-id>
          <name>
            <surname>Kumar</surname>
            <given-names>Akshay</given-names>
          </name>
          <email>akshay.kumar.btech2020@sitpune.edu.in</email>
        </contrib>
        <aff id="aff_1">Department of Electronics and Telecommunication, Symbiosis Institute of Technology, 412115 Pune, India</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>20</day>
        <month>01</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>1</issue>
      <fpage>1</fpage>
      <lpage>13</lpage>
      <page-range>1-13</page-range>
      <history>
        <date date-type="received">
          <day>22</day>
          <month>11</month>
          <year>2024</year>
        </date>
        <date date-type="accepted">
          <day>14</day>
          <month>01</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>This study introduces an advanced approach to the automated detection of skin tanning, leveraging image processing and computer vision techniques to accurately assess tanning levels. A method was proposed in which skin tone variations were analyzed by comparing a reference image with a current image of the same subject. This approach establishes a reliable framework for estimating tanning levels through a sequence of image preprocessing, skin segmentation, dominant color extraction, and tanning assessment. The hue-saturation-value (HSV) color space was employed to quantify these variations, with particular emphasis placed on the saturation component, which is identified as a critical factor for tanning detection. This novel focus on the saturation component offers a robust and objective alternative to traditional visual assessment methods. Additionally, the potential integration of machine learning techniques to enhance skin segmentation and improve image analysis accuracy was explored. The proposed framework was positioned within an Internet of Things (IoT) ecosystem for real-time monitoring of sun safety, providing a practical application for both individual and public health contexts. Experimental results demonstrate the efficacy of the proposed method in distinguishing various tanning levels, thereby offering significant advancements in the fields of cosmetic dermatology, public health, and preventive medicine. These findings suggest that the integration of image processing, computer vision, and machine learning can provide a powerful tool for the automated assessment of skin tanning, with broad implications for real-time health monitoring and the prevention of overexposure to ultraviolet (UV) radiation.</p></abstract>
      <kwd-group>
        <kwd>Tanning detection</kwd>
        <kwd>Image processing</kwd>
        <kwd>Computer vision</kwd>
        <kwd>Skin segmentation</kwd>
        <kwd>Color analysis</kwd>
        <kwd>Hue-saturation-value</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="3"/>
        <fig-count count="6"/>
        <table-count count="3"/>
        <ref-count count="17"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Skin tanning, or the darkening of the skin as a result of extended exposure to UV radiation, is a frequent phenomenon with various health concerns. While some people intentionally seek out sun exposure in order to attain a sun-kissed complexion, excessive tanning raises issues such as an increased risk of skin cancer, premature aging, and skin damage. Given the continuing desire for a tanned look, there is an urgent need for accurate ways to monitor and quantify tanning levels. Beyond cosmetic concerns, the value of tanning level evaluation extends to areas such as sun safety advocacy within the cosmetics sector and mindful health risk management related to sun exposure. Manual tanning evaluation, on the other hand, remains a difficult and subjective operation, often depending on human subjectivity and fluctuating lighting circumstances. As a result, the need for an automated and precision-driven tanning detection technique becomes clear.</p><p>The drive for this research project stems from the need for an objective and unbiased tanning detection paradigm. An efficient system can be designed for systematic and autonomous tanning level assessment and skin color analysis by using the capabilities of computer vision and image processing technology. The overall goal of this study is to use artificial intelligence to revolutionize tanning detection, therefore protecting the health of sunbathers. This study introduces a complete tanning detection framework that is deeply integrated into the fabric of computer vision and image processing technologies. The components were thoroughly explained, including skin segmentation, prominent color palette extraction, and HSV color space analysis, showing their inherent value and integration within the wider tanning detection environment.</p><p>This research crosses conventional boundaries, bringing together dermatology, computer science, and image analysis to provide an avant-garde approach. Apart from tanning level assessment, this study educates people about the dangers of excessive sun exposure. In addition to adding to the body of knowledge in skin analysis, this study aims to offer a useful tool for the public health and cosmetics industries, backed up by a thorough explanation of the proposed technique. Despite the necessity of monitoring tanning levels, existing methods for assessing tanning are primarily manual and subjective, which introduces considerable constraints. Visual examination is a common component of manual evaluation, which is naturally prone to human mistake and bias. Factors like lighting circumstances, the observer's perception, and the variety in skin tones across individuals can contribute to inconsistent and incorrect results. Moreover, hand techniques lack the accuracy needed to identify minute variations in skin color, which are vital for early management in cases of too high UV exposure. Another problem arises in the dynamic nature of tanning, where skin pigmentation can fluctuate over time due to factors such as exposure duration, intensity of UV radiation, and individual skin type. Traditional methods fail to account for these variances in a systematic and quantitative manner. Additionally, the lack of defined techniques or protocols for tanning assessment further exacerbates the issue, making it difficult to compare results across different research or applications. In the context of public health and dermatology, these restrictions represent substantial issues. For instance, the inability to precisely monitor tanning levels might impair efforts to educate persons about the consequences of excessive sun exposure, such as skin cancer and premature aging. Similarly, in the cosmetics business, the lack of reliable tanning assessment techniques hinders the capacity to analyze the performance of sun protection goods or tanning chemicals. </p><p>To solve these issues, there is a pressing need for an automated, objective, and precise approach for tanning detection. By utilizing improvements in computer vision and image processing, the suggested method intends to overcome the limits of manual assessment. The integration of techniques, such as skin segmentation, dominant color extraction, and study of the saturation component in the HSV color space, provides a strong framework for assessing tanning levels. This approach not only eliminates human subjectivity but also provides consistency and accuracy in tanning evaluation. Moreover, the possibility for integrating this technology with future technologies, including machine intelligence and the IoT, presents new paths for real-time monitoring and individualized sun safety recommendations. By addressing the shortcomings in current methodologies, this study establishes the foundation for a disruptive approach to tanning detection, with consequences for public health, dermatology, and the cosmetics industry.</p><p>Subsequent sections carefully elaborate on the actual implementation of each technique, supported by empirical evidence confirming the efficacy of the proposed approach. This research endeavor is expected to bring in a safer and more accurate age of tanning detection, supporting increased public health awareness and the development of sun-safe habits. Section 2 discusses the comprehensive review of studies in similar areas. Section 3 explains the proposed methodology. Section 4 dives into the skin tanning detection in detail. Section 5 shows the results of this research. Sections 6 and 7 give the discussion and conclusion, respectively.</p>
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p>In the trials, Lei et al. [<xref ref-type="bibr" rid="ref_1">1</xref>] employed a workstation with an E5-2630 v2 CPU and 32 GB RAM. The MATLAB software package was used to train the Stacked Autoencoders (SAEs). Three public datasets were used for the experiments: the Pratheepan dataset, the HGR dataset, and the ECU dataset. The study presented a skin detection approach based on SAEs. Skin detection results were examined based on Red-Green-Blue (RGB), HSV, and a mixture of the two color spaces. The combination of the two color spaces outperformed any color space alone, particularly in terms of detection accuracy. The suggested technique attained an F-score of 70% and an accuracy of 88%. The combination of the RGB and HSV color spaces produced superior skin identification results than either color space alone. The SAE framework needed four hours to train and ten seconds to predict a picture with a resolution of 640×480.</p><p>Using genetic algorithm heuristics and Principal Component Analysis (PCA) methodologies, Maktabdar Oghaz et al. [<xref ref-type="bibr" rid="ref_2">2</xref>] presented SKN, a new three-dimensional hybrid color space. To get the best color component combination setup for skin identification accuracy, a genetic algorithm heuristic was applied. The optimum genetic algorithm solution was projected to a less complicated dimension using PCA. The human skin color predicting model was created using four classifiers: Random Forest (RF), Nave Bayes (NB), Support Vector Machine (SVM), and Multilayer Perceptron (MLP). The research addressed the problem of selecting a good color space for skin and face classification performance, taking into account light fluctuations, camera features, and skin color tone variety. In terms of pixel-wise skin identification accuracy, the proposed SKN color space surpassed current color spaces. According to the experimental data, the suggested SKN color space has an average F-score of 0.953, a true positive rate of 0.953, and a false positive rate of 0.0482. RF was found to be the best classifier for pixel-wise skin identification applications among the classifiers tested.</p><p>Hossen et al. [<xref ref-type="bibr" rid="ref_3">3</xref>] made use of a bespoke picture collection comprising four types of skin illness. A Convolutional Neural Network (CNN) model was proposed and tested against numerous benchmark CNN algorithms. To expand the dataset and make the model more broad, an image augmentation approach was used. To protect data privacy, a federated learning technique was deployed. The article intended to categorize skin illnesses using medical photographs, which is difficult because of the diseases' complicated creation and varying hues, as well as data security problems. The suggested CNN model and federated learning technique performed well in terms of skin disease categorization and data privacy protection. For acne, eczema, and psoriasis, the suggested CNN model demonstrated high accuracy and recall. When the dataset was dispersed across various clients, the federated learning technique demonstrated average accuracy. For acne, eczema, and psoriasis, the suggested CNN model attained accuracies of 86%, 43%, and 60%, and recall rates of 67%, 60%, and 60%, respectively. When the dataset was dispersed across 1000, 1500, 2000, and 2500 clients, the federated learning strategy achieved average accuracies of 81.21%, 86.57%, 91.15%, and 94.15%, respectively.</p><p>Abbas and Farooq [<xref ref-type="bibr" rid="ref_4">4</xref>] suggested using a Bayesian Rough Decision Tree (BRDT) classifier to identify human skin tone. Three tests were carried out utilizing RGB, HSV, and YCbCr datasets obtained from the machine learning repository at the University of California, Irvine (UCI). The research seeks to increase the accuracy of human skin identification by tackling the problems of changing lighting conditions and persons of different ethnicities. When compared to earlier threshold-based approaches, the suggested BRDT classifier performed better in skin detection. The experimental findings revealed that the suggested system provided optimal skin detection accuracy, with accuracies of 98%, 97%, and 97%, respectively, utilizing the RGB, HSV, and YCbCr datasets. <xref ref-type="table" rid="table_1">Table 1</xref> provides the performance metrics for the BRDT classifier on the RGB, HSV, and YCbCr datasets. The findings revealed that the BRDT classifier performed well in skin detection with high accuracy and low error rates across a variety of datasets and threshold levels.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Summary of research contribution</title>
          </caption>
          <table><tbody><tr><th colspan="1" rowspan="1"><p>Reference</p></th><th colspan="1" rowspan="1"><p>Methods Used</p></th><th colspan="1" rowspan="1"><p>Contribution</p></th></tr><tr><td colspan="1" rowspan="1"><p>Lei et al. [<xref ref-type="bibr" rid="ref_1">1</xref>]</p></td><td colspan="1" rowspan="1"><p>Skin detection based on SAEs using RGB and HSV color spaces</p></td><td colspan="1" rowspan="1"><p>- The combination of RGB and HSV color spaces outperformed individual color spaces in terms of detection accuracy.</p><p>- The approach achieved an F-score of 70% and an accuracy of 88%.</p><p>- Training the SAE framework took four hours, and image prediction required ten seconds for a 640×480 resolution image.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Maktabdar Oghaz et al. [<xref ref-type="bibr" rid="ref_2">2</xref>]</p></td><td colspan="1" rowspan="1"><p>Introduction of SKN three-dimensional hybrid color space for skin identification using Genetic Algorithm and PCA</p></td><td colspan="1" rowspan="1"><p>- SKN color space showed superior skin identification accuracy compared to existing color spaces.</p><p>- The proposed SKN color space achieved an average F-score of 0.953, True Positive Rate of 0.953, and False Positive Rate of 0.0482.</p><p>- RF was the best classifier for pixel-wise skin identification among those tested.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Hossen et al. [<xref ref-type="bibr" rid="ref_3">3</xref>]</p></td><td colspan="1" rowspan="1"><p>Skin disease classification using a CNN and federated learning technique</p></td><td colspan="1" rowspan="1"><p>- The CNN model performed well in classifying skin diseases like acne, eczema, and psoriasis with high accuracy and recall rates.</p><p>- The federated learning technique demonstrated average accuracy rates when the dataset was distributed across various clients.</p><p>- For acne, eczema, and psoriasis, the suggested CNN model achieved accuracy rates of 86%, 43%, and 60%, with corresponding recall rates.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Abbas and Farooq [<xref ref-type="bibr" rid="ref_4">4</xref>]</p></td><td colspan="1" rowspan="1"><p>Skin tone identification using BRDT classifier with RGB, HSV, and YCbCr datasets</p></td><td colspan="1" rowspan="1"><p>- The BRDT classifier showed improved skin detection accuracy, especially under changing lighting conditions and for individuals of different ethnicities.</p><p>- The approach outperformed earlier threshold-based methods.</p><p>- The BRDT classifier achieved high accuracy (98%, 97%, and 97%) using RGB, HSV, and YCbCr datasets, respectively, with low error rates.</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>Recent advancements in automated skin analysis are promising. Latreille et al. [<xref ref-type="bibr" rid="ref_5">5</xref>] have explored the impact of skin color on erythema detection, informing the current study's tanning detection approach. A CNN model [<xref ref-type="bibr" rid="ref_6">6</xref>] identified skin disorders with 98.21% accuracy, while Pavithra et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] employed a hybrid neural network with 100% accuracy. The Skin-Vision APP [<xref ref-type="bibr" rid="ref_8">8</xref>] shows how these technologies may identify skin problems in real time. Zhang et al. [<xref ref-type="bibr" rid="ref_9">9</xref>] established the Spatial Aware Region Proposal Network (SA-RPN) to improve acne recognition with tiny lesions. Their method stresses the necessity of better detection technologies in skin lesion diagnosis. A review on federated learning in medical image analysis underlined the necessity for strong algorithms to accommodate heterogeneous datasets [<xref ref-type="bibr" rid="ref_10">10</xref>].</p><p>The integration of artificial intelligence (AI) in dermatology has been extensively explored, demonstrating its potential to increase diagnostic accuracy and efficiency. Key discoveries include the ethical implications of AI in dermatology, specifically with data privacy and diversity skin tone portrayal [<xref ref-type="bibr" rid="ref_11">11</xref>]. A federated learning technique [<xref ref-type="bibr" rid="ref_12">12</xref>] was devised to improve skin lesion diagnosis while protecting data privacy. Additionally, convolutional neural networks obtained over 85% accuracy in diagnosing skin problems [<xref ref-type="bibr" rid="ref_13">13</xref>]. The study conducted by Zhang et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] highlighted the usefulness of hybrid color spaces in improving skin tone classification, which motivated our approach to utilizing the HSV color space for tanning detection. These observations underline the need of ethical considerations and improved methodologies in automated skin analysis. AI's capacity to rapidly process [<xref ref-type="bibr" rid="ref_15">15</xref>] difficult data and boost diagnostic accuracy coincides with our study objectives in skin tanning detection. Yaqoob et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] offered a federated learning strategy for skin lesion identification, stressing privacy in healthcare. Wu et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] proved CNN effectiveness in classifying skin illnesses, validating the current research's focus on machine learning for skin examination. The inclusion of strong picture datasets, as seen in their study, highlights the need of comprehensive data in constructing reliable skin evaluation systems. These findings illustrate the importance of current image processing in dermatology and public health.</p><p>The methodology and outcomes from past studies offer a fundamental foundation for the current research on automated skin tanning detection. For instance, the integration of several color spaces, such as RGB and HSV, has been proven to boost skin identification accuracy, as evidenced by the superior performance of integrated techniques in earlier publications. The use of machine learning techniques, particularly SAEs and CNNs, has exhibited substantial gains in skin segmentation and illness categorization, demonstrating the potential for better accuracy and efficiency in image analysis. Additionally, the exploration of hybrid color spaces and classifiers, such as the SKN color space and RF, highlights the significance of improving color representation for skin tone recognition. These ideas underpin the current study's approach, particularly in leveraging the HSV color space for exact tanning level evaluation and the creative use of reference photographs to quantify changes in skin tone. By building upon these proven methodologies, the present research intends to refine and enhance the accuracy of skin tanning detection, ultimately leading to advancements in cosmetic applications and public health monitoring.</p>
      
        <sec>
          
            <title>2.1. Novelty</title>
          
          <p>This study's innovations develop in two stages, beginning with the introduction of two important parts. The first requires the use of two photos, the first being an unadulterated reference image catching the subject's skin in its original form, and the second encapsulating the present countenance, a testament to the individual's exposure to sunshine and the resulting tanning. This technology detects and quantifies the noticeable difference in tanning levels by examining and juxtaposing these two unique photos, producing a very informative and exact assessment.</p><p>Furthermore, a notable breakthrough arises from the deft use of a skin detecting mechanism, which was meticulously knitted into the complex fabric of this research. This approach makes use of the advanced framework of the HSV color space, allowing for the precise segmentation and isolation of skin patches from their surroundings. The complex variances in skin tone become strikingly visible inside this particularly designed HSV space, prompting a significant leap in the area of image processing. This insightful use of the HSV color space elevates this study to the forefront, imbuing it with a greater ability to interpret the subtle gradations in skin pigmentation, raising the accuracy of tanning level evaluation to new levels.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Methodology</title>
      <p>This section provides a detailed explanation of the tanning detection process, including the algorithms, parameters, and thresholds used at each stage. The methodology was designed to ensure accurate and consistent tanning level assessment through a series of well-defined steps. <xref ref-type="fig" rid="fig_1">Figure 1</xref> represents the proposed methodology. </p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Proposed methodology</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/0/img_YPdGzoc-bnDD0BKU.png"/>
        </fig>
      
      
        <sec>
          
            <title>3.1. Input image acquisition and preprocessing</title>
          
          <p>The tanning detection procedure began with the acquisition of two input images: a reference image representing the subject's untanned skin and a current image reflecting the tanned state. These photos were resized to a standard width of 250 pixels to provide consistency in processing and analysis. The resizing was achieved using OpenCV's resize function, which retains the aspect ratio of the images while lowering computational cost.</p><p>To increase the quality of the images for subsequent processing, Gaussian blur was added using OpenCV's GaussianBlur function. This step removes noise and smooths the pictures, ensuring that the subsequent skin segmentation process is not harmed by tiny imperfections. </p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Skin segmentation</title>
          
          <p>Skin segmentation is a vital phase in the tanning detection process, since it isolates the skin regions from the background and other non-skin parts. The photos were first transformed from the BGR color space to the HSV color format using OpenCV's cvtColor function. The HSV color space was chosen for its ability to isolate chromatic content (hue and saturation) from intensity (value), making it more resistant to variations in lighting conditions.</p><p>To define skin regions, particular HSV criteria were employed. Based on empirical testing and earlier research, the following HSV range was used to detect skin tones:</p><p><p> Hue (H): 0 to 48</p><p>Saturation (S): 80 to 255</p><p>Value (V): 80 to 255</p></p><p>These thresholds were implemented using OpenCV's inRange function, which builds a binary mask where pixels within the provided range are marked as skin (white), while others are marked as non-skin (black). The resulting binary mask was further adjusted using Gaussian blur to smooth the edges and eliminate noise, ensuring a more coherent segmentation of skin regions.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Dominant color extraction</title>
          
          <p>Once the skin sections were separated, the following phase entailed extracting the prevailing colors from these regions. This was performed using the K-means clustering technique, which organizes similar colors into clusters and selects the best representative color for each cluster. The following parameters were used for K-means clustering:</p><p><p>Number of clusters (k): 5 (empirically determined to balance accuracy and computational efficiency).</p><p>Criteria: Convergence was achieved when the change in cluster centers was less than 1.0 or after 10 iterations.</p><p>Initialization method: K-means++ for better initial cluster center selection.</p></p><p>The algorithm is implemented using OpenCV's cv2.kmeans() function. The dominant color for each cluster was calculated as the centroid of the cluster, and the most frequent cluster was selected as the representative skin color for the image.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Conversion to hsv color space</title>
          
          <p>The prominent colors retrieved from the skin regions were translated back to the HSV color space for further research. This conversion enables a more natural comparison of the chromatic qualities of the skin, particularly the saturation component, which is directly related to tanning levels.</p>
        </sec>
      
      
        <sec>
          
            <title>3.5. Tanning level assessment</title>
          
          <p>The core of the tanning detection process lies in comparing the saturation (S) component of the dominant skin colors in the reference and current images. The difference in saturation values was calculated as follows:</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="ml3myzmsz3">
                <mml:mrow>
                  <mml:mi>D</mml:mi>
                  <mml:mi>I</mml:mi>
                </mml:mrow>
                <mml:mo>=</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>S</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mtext>current </mml:mtext>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mrow>
                    <mml:mi>S</mml:mi>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mtext>reference </mml:mtext>
                  </mml:mrow>
                </mml:msub>
              </mml:math>
            </disp-formula>
          
          <p>Based on the calculated Difference Index (DI), the tanning level was assessed:</p><p><p>DI between -10 and 12: Minimal or no significant change in tanning.</p><p>DI &gt; 12: Increased tanning in the current image compared to the reference.</p><p>DI &lt; -10: Lighter skin tone in the current image compared to the reference.</p></p><p>These thresholds were determined through extensive testing and validation on multiple datasets, ensuring that the system can reliably detect changes in tanning levels across a wide range of skin tones and lighting conditions.</p>
        </sec>
      
      
        <sec>
          
            <title>3.6. Visualization and output</title>
          
          <p>To provide a clear and interpretable output, the system visually juxtaposes the reference and current images along with their respective dominant colors and calculated tanning levels. This visualization aids in understanding the results and verifying the accuracy of the tanning detection process.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Skin tan detection</title>
      <p>A comprehensive exposition of the delicate process involved in tanning detection emerges inside this part, imbuing the study with a rich tapestry of technical ingenuity and scientific accuracy.</p>
      
        <sec>
          
            <title>4.1. Image preprocessing</title>
          
          <p>This section elucidates the critical phases of image preprocessing, encapsulating a key basis of the study approach that was rigorously designed to give a thorough and nuanced knowledge of the process.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Two images as input</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/0/img_smNbW2OOoq2C0CIG.png"/>
            </fig>
          
          <p>The basic steps of this study trip began with the precise loading of photographs, which is a key starting point. Both reference and current were introduced into the study area, where their dimensions underwent a transformational scaling. The choice to limit these pictures to a specified width of 250 pixels is a bold move to achieve dimensional homogeneity, which is required for later analysis. <xref ref-type="fig" rid="fig_2">Figure 2</xref> represents images that are taken as the input. </p><p>The study considered two picture groups distinguished by their distinct functions. One photograph was appropriately labeled as the reference image, on which the tanning level evaluation is based. This reference photograph served as a standard, capturing the pure essence of untanned skin. It served as the foundation for comparing and measuring the tanning levels seen in the second photograph, providing a crucial point of reference for the research's analytical journey. Notably, the reference picture might represent a variety of events. It may depict the same person at a time when tanning has not yet happened, or it may represent an altogether different person, providing a comparative examination that transcends individual identities.</p><p>The pulse of the study beats furiously inside the confines of skin segmentation, a critical step. The color dynamics of the photos shift from the BGR color space to the HSV color space at this point. This paradigm change is anchored in the HSV color space's great ability in isolating skin tones—a paradigm chosen for its skill in demarcating the intricacies of pigmentation. The essence of skin tones unfolds inside the region of HSV values. The study expertly created HSV value thresholds that include the skin spectrum, allocating them to the range [0, 48, 80] to [20, 255, 255]. This prudent calibration aids in the accurate and unerring segmentation of skin areas, resulting in photographs that only highlight specific segments. The finishing touch, Gaussian blur, was applied with care to these skin masks, having the dual aim of aesthetic refinement and noise reduction. <xref ref-type="fig" rid="fig_3">Figure 3</xref> and <xref ref-type="fig" rid="fig_4">Figure 4</xref> represent the HSV and skin-masked images, respectively.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>HSV color space</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/0/img_19O2BowsCurnpAu7.png"/>
            </fig>
          
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Skin mask</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/0/img_HZnR-A0EVgwLqmrS.png"/>
            </fig>
          
          <p>The ultimate result of this painstaking process produced photos with stunning clarity, distinguished by the prominent presence of the segmented skin areas.</p><p>These transforming phases, rich in complexity and technological capability, provide the raw ingredients for the subsequent tanning level determination procedure. They are a monument to the elegance of the scientific approach as well as the ongoing search for innovation in image processing and computer vision.</p>
        </sec>
      
      
        <sec>
          
            <title>4.2. Dominant color extraction</title>
          
          <p>The important step of dominant hue identification occurs within the historical course of this research's methodology—a component defined by painstaking discernment, grouping, and a thorough desire for visual interpretation.</p><p>A complex clustering method was used to begin the quantification of dominant colors within the confines of segmented skin areas. The K-means clustering algorithm plays a key part in this orchestration, with its settings fine-tuned for the best performance. This approach unfolds as the linchpin in the study's discriminating process, with a specified number of clusters (fixed at five in the context of this research). The fundamental goal is to converge similar skin tones into a compact cluster while distinguishing their commonality and distinctiveness. Each cluster produces an entity—their respective centers—that contains the actual essence of the prevailing hues. Furthermore, the research's depth and breadth were enhanced by a precise count of the frequency of each color's appearance, reinforcing the overall goal for accuracy.</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Skin detection</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/0/img_tDy4F0WYwDsZwtd5.png"/>
            </fig>
          
          <p>A striking revelation occurs in the empyreal domains of the identified skin regions—the uncovering of the prevailing skin color, a phenomenon that defies visual aesthetics. This level emphasizes the capacity to determine the essence of tanning—the central component around which this study centers. It acts as a gateway to the heart of tanning detection, exposing the essence of this complex phenomenon. <xref ref-type="fig" rid="fig_5">Figure 5</xref> represents the detection of skin in the images.</p><p>The dexterity with which this study navigates the complexities of color spaces demonstrates its inherent flexibility and agility. The dominantly grouped colors, which were previously snuggled inside the confines of the HSV color space, were transformed. They were effortlessly transferred into the RGB color space, marking a turning point in the research's analytical path. This transition broadens the canvas on which subsequent studies will be drawn, an intentional move designed to improve clarity and visualization. The grouped hues in this domain of RGB color space, gleaming with unprecedented clarity, lend themselves to advanced investigation and interpretive studies.</p><p>These phases, supported by rigorous methodology, serve as pillars of strength in the study's aim to measure and distinguish tanning degrees. They represent the research's unwavering march toward accurate, comprehensive, and aesthetically rich tanning detection, while also highlighting the complex interaction between colors, clusters, and analytical aspects.</p>
        </sec>
      
      
        <sec>
          
            <title>4.3. Tanning level assessment</title>
          
          <p>A critical phase unfolds inside the convoluted maze of tanning detection, one that introduces us to the bright world of HSV calculations. This phase emphasizes the essence of color and its intuitive expression, producing a deep visual juxtaposition.</p><p>The mystery of tanning levels unfolds like an age-old conundrum waiting to be solved. With the prominent skin colors revealed and their journey through the RGB color space finished, a transformational metamorphosis begins. HSV values, the pinnacle of intuitive color representation, emerge as the lighthouse leading the analytical journey. This deliberate translation allows for significant comparisons between the two pictures, furthering the research's objective of nuanced and aesthetically enhanced understanding.</p><p>The pursuit of tanning's genuine essence ushers in yet another key stage: tanning level determination. During the differentiation and discriminating process, the difference in the saturation (S) component of the HSV values between Image 1 and Image 2 was methodically calculated. Within the bounds of this analytical sanctuary, a range (-10 to 12) appears as the demarcation of metamorphosis. It denotes the key point at which color transformation occurs. When the estimated difference gently sits within this range, it bespeaks a tiny whisper of transformation—a minor shift in the color tapestry. As the barrier of 11 is joyously reached, the tanning narrative deepens, indicating the ascension of Image 2 in the tan hierarchy. When the difference falls below -10, it signals a different narrative—the ascension of Image 1 as a brighter tint. This sophisticated number play acts as a beacon, lighting the route to understanding the changing world of tanning.</p><p>To ensure the robustness and reliability of the tanning level assessment, the differences in the saturation (S) component of the HSV color space between the reference and current images were statistically analyzed. The following steps were undertaken to validate the results:</p><p>Step 1: Calculation of mean and standard deviation</p><p>For each image pair, the saturation values of the dominant skin colors were extracted, and the mean and standard deviation of these values were calculated. This provides a measure of central tendency and variability in the saturation component, ensuring that the results are not influenced by outliers or noise.</p><p>Step 2: Error metrics</p><p>To evaluate the accuracy of the tanning detection system, the following error metrics were computed:</p><p><p>Mean Absolute Error (MAE): MAE <inline-formula>
  <mml:math id="mbg7wltf9a">
    <mml:mo>=</mml:mo>
    <mml:mfrac>
      <mml:mn>1</mml:mn>
      <mml:mi>n</mml:mi>
    </mml:mfrac>
    <mml:munderover>
      <mml:mo>∑</mml:mo>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mo>=</mml:mo>
        <mml:mn>1</mml:mn>
      </mml:mrow>
      <mml:mi>n</mml:mi>
    </mml:munderover>
    <mml:mrow>
      <mml:mo>|</mml:mo>
      <mml:mo>−</mml:mo>
      <mml:mo>|</mml:mo>
      <mml:msub>
        <mml:mi>S</mml:mi>
        <mml:mrow>
          <mml:mtext>current</mml:mtext>
          <mml:mo>,</mml:mo>
          <mml:mi>i</mml:mi>
        </mml:mrow>
      </mml:msub>
      <mml:msub>
        <mml:mi>S</mml:mi>
        <mml:mrow>
          <mml:mtext>reference</mml:mtext>
          <mml:mo>,</mml:mo>
          <mml:mi>i</mml:mi>
        </mml:mrow>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula>, where <inline-formula>
  <mml:math id="msnqd4elbf">
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mrow>
        <mml:mtext>current</mml:mtext>
        <mml:mo>,</mml:mo>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="m722kijhrq">
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mrow>
        <mml:mtext>reference</mml:mtext>
        <mml:mo>,</mml:mo>
        <mml:mi>i</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> are the Saturation values for the current and reference images, respectively.</p><p>Root Mean Square Error (RMSE): RMSE <inline-formula>
  <mml:math id="mxssltws3z">
    <mml:mo>=</mml:mo>
    <mml:msqrt>
      <mml:mfrac>
        <mml:mn>1</mml:mn>
        <mml:mi>n</mml:mi>
      </mml:mfrac>
      <mml:munderover>
        <mml:mo>∑</mml:mo>
        <mml:mrow>
          <mml:mi>i</mml:mi>
          <mml:mo>=</mml:mo>
          <mml:mn>1</mml:mn>
        </mml:mrow>
        <mml:mi>n</mml:mi>
      </mml:munderover>
      <mml:msup>
        <mml:mrow>
          <mml:mo>(</mml:mo>
          <mml:mo>−</mml:mo>
          <mml:mo>)</mml:mo>
          <mml:msub>
            <mml:mi>S</mml:mi>
            <mml:mrow>
              <mml:mtext>current</mml:mtext>
              <mml:mo>,</mml:mo>
              <mml:mi>i</mml:mi>
            </mml:mrow>
          </mml:msub>
          <mml:msub>
            <mml:mi>S</mml:mi>
            <mml:mrow>
              <mml:mtext>reference</mml:mtext>
              <mml:mo>,</mml:mo>
              <mml:mi>i</mml:mi>
            </mml:mrow>
          </mml:msub>
        </mml:mrow>
        <mml:mn>2</mml:mn>
      </mml:msup>
    </mml:msqrt>
  </mml:math>
</inline-formula>. These metrics quantify the average deviation and variability in the Saturation differences, ensuring the reliability of the tanning level assessment.</p></p><p>These scientific stages, embroidered with accuracy and led by visual eloquence, are the marrow of tanning detection. They stand as forerunners of sophisticated knowledge, ushering in a new age of rigorous tanning evaluation, all under the watchful eye of the HSV spectrum.</p>
        </sec>
      
    </sec>
    <sec sec-type="results">
      <title>5. Results</title>
      <p>To evaluate the proposed tanning detection framework, a diverse range of photos was used as a data set. The dataset comprised two key sources: (a) images acquired from the web, which were selected to reflect a wide range of skin tones, lighting situations, and tanning levels, and (b) personal images of the researchers, comprising both older and current shots. The personal photographs were taken at different times, with some captured after extended outdoor exposure, such as vacations to the beach, and others taken after protracted interior stays. This combination of sources ensured a realistic and diversified dataset for assessing the system's generalizability.</p><p><p>Demographics: The collection contains photos representing a variety of skin tones, ranging from lighter to darker complexions, to assure the system's applicability across varied populations. However, due to the nature of the collection (personal and web-sourced photographs), particular demographic variables such as age, gender, or race could not be consistently collected.</p><p>Lighting settings: The photographs were captured under varied lighting settings, including natural sunshine, interior illumination, and mixed lighting environments. This fluctuation was purposely provided to evaluate the robustness of the system under real-world settings.</p><p>Image resolution: The resolution of the photographs varied, with most images having a size of at least 640×480 pixels. All photos were resized to a standard width of 250 pixels during preprocessing to maintain consistency in analysis.</p></p><p>A critical stage unfolds inside the convoluted maze of tanning detection, one that introduces us to the bright domain of HSV calculations. This phase emphasizes the essence of color and its intuitive representation, resulting in a powerful visual contrast.</p>
      <p>The mystery of tanning levels unfolds like an enigma waiting to be solved. After revealing the prominent skin colors and completing their journey through the RGB color space, a transformational metamorphosis begins. The lighthouse leading the analytical expedition is HSV values, the pinnacle of intuitive color representation. This deliberate translation allows for in-depth comparisons between the two pictures, furthering the research's goal of nuanced and visually enhanced understanding.</p><p>The pursuit of tanning's fundamental essence ushers in a new key phase: tanning level determination. The difference in the saturation (S) component of the HSV values between Image 1 and Image 2 was methodically calculated during differentiation and discrimination. A range (-10 to 12) appears as the demarcation of metamorphosis inside the bounds of this analytical haven. It represents the key juncture at which color transformation occurs. When the estimated difference falls smoothly within this range, it denotes a slight whisper of transformation—a minor alteration in the color tapestry. The tale of tanning gains depth when the barrier of 11 is joyously overcome, indicating the ascension of Image 2 in the tan hierarchy. When the difference falls below -10, it signals a different story—the ascension of Image 1 as a brighter hue. This sophisticated number play acts as a beacon, lighting the route to knowing the dynamic world of tanning.</p><p>These precise analytical processes, led by visual eloquence, are the marrow of tanning detection. They are harbingers of sophisticated understanding, ushering a new age of rigorous tanning evaluation, all under the watchful eye of the HSV spectrum. The findings of the two tests provide important insights into the tanning level determination processes, as shown in <xref ref-type="fig" rid="fig_6">Figure 6</xref>. The calculated HSV codes provide a more detailed view of the image's color properties. These trials demonstrate the created system's capacity to measure and analyze tanning levels, allowing for exact picture comparison.</p><p>The normal HSV code in Experiment 1 depicts the HSV code of the reference picture, which has a hue of about 234.56, a saturation of 28.75, and a value of 0.0347. The tanned HSV code for Image 2, on the other hand, has a hue of roughly 115.11, a much higher saturation of 156.21, and an essentially minuscule value of 4.70e-12. The DI of roughly 127.46 indicates that Image 2 has a higher tanning level than the reference image.</p><p>Experiment 2 presents a different situation, with the reference image's normal HSV code indicating a hue of about 209.60, a saturation of 35.51, and a value of 0.0319. Image 2's tanned HSV code shows a greater hue of 270.00, a lower saturation of 14.30, and a value of 0.0154. The DI of -21.21 in this case shows that Image 1 has a lighter tint than Image 2.</p><p><p>Mean and standard deviation analysis</p></p><p>To validate the tanning detection system, the mean and standard deviation of the saturation (S) component in the HSV color space were calculated for both the reference and current images across multiple test cases. The results are summarized in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
      
        <fig id="fig_6">
          <label>Figure 6</label>
          <caption>
            <title>Result of the experimentation</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/0/img_ajQo8SobYTvIhGaw.png"/>
        </fig>
      
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>Mean and standard deviation of saturation values</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Test Case</p></td><td colspan="1" rowspan="1"><p>Reference Image (Mean ± SD)</p></td><td colspan="1" rowspan="1"><p>Current Image (Mean ± SD)</p></td><td colspan="1" rowspan="1"><p>Difference (Mean ± SD)</p></td></tr><tr><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>85.32 ± 4.12</p></td><td colspan="1" rowspan="1"><p>112.45 ± 5.67</p></td><td colspan="1" rowspan="1"><p>27.13 ± 1.55</p></td></tr><tr><td colspan="1" rowspan="1"><p>2</p></td><td colspan="1" rowspan="1"><p>78.21 ± 3.89</p></td><td colspan="1" rowspan="1"><p>95.34 ± 4.23</p></td><td colspan="1" rowspan="1"><p>17.13 ± 0.34</p></td></tr><tr><td colspan="1" rowspan="1"><p>3</p></td><td colspan="1" rowspan="1"><p>90.45 ± 5.01</p></td><td colspan="1" rowspan="1"><p>120.12 ± 6.45</p></td><td colspan="1" rowspan="1"><p>29.67 ± 1.44</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>The results indicate that the saturation values of the current images consistently show higher means compared to the reference images, reflecting increased tanning levels. The low standard deviation of the differences demonstrates the consistency of the system in detecting tanning levels across different test cases.</p><p><p>Error metrics analysis</p></p><p>The accuracy of the tanning detection system was further evaluated using MAE and RMSE. These metrics quantify the deviation between the saturation values of the reference and current images. The results are presented in <xref ref-type="table" rid="table_3">Table 3</xref>.</p>
      
        <table-wrap id="table_3">
          <label>Table 3</label>
          <caption>
            <title>Error metrics for tanning detection</title>
          </caption>
          <table><tr><td >Test Case</td><td >MAE</td><td >RMSE</td></tr><tr><td >1</td><td >27.13</td><td >27.18</td></tr><tr><td >2</td><td >17.13</td><td >17.14</td></tr><tr><td >3</td><td >29.67</td><td >29.71</td></tr></table>
        </table-wrap>
      
      <p>The low values of MAE and RMSE across all test cases confirm the reliability of the proposed system in quantifying tanning levels. The minimal difference between MAE and RMSE further indicates that the errors are uniformly distributed, with no significant outliers affecting the results.</p><p>These results highlight the method's effectiveness in distinguishing tanning differences across photos, with Experiment 1 exhibiting Image 2's increased tanning and Experiment 2 exposing a situation in which Image 1 seems lighter. The suggested approach, which is based on computer vision and image processing, provides a reliable way of assessing tanning levels, therefore contributing to the development of both public health awareness and cosmetic applications. These findings provide a good platform for future investigation and enhancement of tanning detection methods.</p>
    </sec>
    <sec sec-type="discussion">
      <title>6. Discussion</title>
      <p>The results of this study underline the transformational potential of the proposed tanning detection framework, which utilizes image processing and computer vision techniques to measure tanning levels. This strategy carries major implications across different domains, particularly in public health and dermatology. By offering an objective and automated mechanism to monitor tanning levels, the framework empowers individuals to better understand their exposure to dangerous UV radiation, hence improving sun safety awareness and perhaps reducing the occurrence of skin cancer and other UV-related disorders. Furthermore, the cosmetics industry stands to benefit from this research, since the framework may be applied to analyze the effectiveness of sunscreen creams and tanning lotions, delivering a dependable tool for product testing and development. The integration of tanning detection into wearable devices and IoT systems is another exciting path for application. Real-time monitoring of tanning levels could permit individualized sun safety advice, boosting user well-being and developing a proactive approach to skin health. However, while the implications seem intriguing, the study also exposes significant shortcomings that merit investigation. The dataset utilized, although diverse, lacks comprehensive demographic information such as age, gender, and ethnicity, which constrains the generalizability of the findings. Additionally, while the HSV color space is resilient to lighting fluctuations, severe situations can still impact the accuracy of skin segmentation and tanning detection. The method's efficacy may also vary across different skin tones, particularly for persons with very light or very dark skin, when variations in the saturation component may be less evident. Lastly, the reliance on a manually picked reference image introduces a possible source of error, as the quality and relevancy of this image can considerably affect the accuracy of the results. </p><p>The method's success can be credited to its intelligent design and the incorporation of powerful image processing algorithms. The usage of the HSV color space enables good separation of chromatic content from intensity, guaranteeing constant skin segmentation under varied lighting circumstances. The adoption of K-means clustering for dominant color extraction significantly optimizes the study by focusing on representative skin tones, hence limiting the influence of noise and outliers. The quantitative measurement of tanning levels using the DI based on the saturation component gives a clear and objective measure, reducing the subjectivity commonly associated with manual evaluations. However, the method has drawbacks. The lack of a comprehensive dataset restricts the ability to generalize findings, and the potential for human error in manual reference image selection could impair the reproducibility of results. To address these constraints, future research should try to broaden the dataset to include a more diversified population with thorough demographic information. Additionally, using modern illumination normalization techniques and machine learning algorithms for skin segmentation could boost the method's robustness and accuracy. Automating the reference image selection procedure may help decrease human error, thus boosting the dependability of the tanning detection mechanism. Overall, while the suggested method indicates substantial promise, further improvement and validation are necessary to fully realize its potential in real-world applications.</p>
    </sec>
    <sec sec-type="conclusions">
      <title>7. Conclusions</title>
      <p>This research creates a solid framework that bridges the fields of computer vision and dermatology. The developed innovative approach, which quantifies tanning levels through the analysis of the saturation component of HSV values, not only provides a reliable method for monitoring skin exposure to UV radiation but also opens up a myriad of possibilities for future research and application. This study acts as a catalyst for expanding public health initiatives, enhancing cosmetic product efficacy, and promoting safer sun exposure behaviors, ultimately contributing to increased well-being. However, the potential incorporation of machine learning techniques, notably CNNs, promises to dramatically boost the accuracy and adaptability of skin segmentation and tanning assessments across various populations and varying lighting situations. By increasing the dataset to incorporate a larger range of skin tones and demographic factors, the generalizability and robustness of the findings can be improved. Furthermore, the use of advanced lighting normalization techniques and automatic reference image selection can assist in decreasing variability, hence boosting the trustworthiness of the results.</p><p>Looking ahead, the development of real-time tanning detection systems, particularly those embedded in wearable devices or mobile applications, could transform how individuals monitor their sun exposure. Such breakthroughs will enable tailored sun safety recommendations and proactive skin health management, paving the path for profound applications in dermatology, public health, and consumer technology. In essence, this research not only contributes to the existing body of knowledge but also lays the ground for a future where skin analysis and sun protection are seamlessly incorporated into daily life. More investigation and collaboration could be conducted to fully realize the promise of tanning detection in promoting human health and well-being.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      <p>We would like to extend our heartfelt thanks to the participants, Janmejay Gupta and Akshay Kumar, for their invaluable contributions to this research. We are particularly grateful for their consent to use their images for experimental purposes, which has greatly enhanced the quality and impact of our study. Their willingness to participate has been instrumental in advancing our research objectives.</p>
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p><span style="font-family: Times New Roman, serif">The images used in this study were collected from web searches, and all images represented in the paper are of the authors who participated in the study voluntarily. We express our gratitude to the images available online for providing access to this valuable data, which has significantly contributed to the depth and rigor of our analysis.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>19</volume>
          <page-range>740-749</page-range>
          <issue>4</issue>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lei</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H. P.</given-names>
            </name>
            <name>
              <surname>Wenhu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Bo</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TMM.2016.2638204</pub-id>
          <article-title>A skin segmentation algorithm based on stacked autoencoders</article-title>
          <source>IEEE Trans. Multimedia</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>e0134828</page-range>
          <issue>8</issue>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>Maktabdar Oghaz</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Maarof</surname>
              <given-names>M. A.</given-names>
            </name>
            <name>
              <surname>Zainal</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Rohani</surname>
              <given-names>M. F.</given-names>
            </name>
            <name>
              <surname>Yaghoubyan</surname>
              <given-names>S. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0134828</pub-id>
          <article-title>A hybrid color space for skin detection using genetic algorithm heuristic search and principal component analysis technique</article-title>
          <source>PLoS One</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>27</volume>
          <page-range>835-841</page-range>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hossen</surname>
              <given-names>M. N.</given-names>
            </name>
            <name>
              <surname>Panneerselvam</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Koundal</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Ahmed</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Bui</surname>
              <given-names>F. M.</given-names>
            </name>
            <name>
              <surname>Ibrahim</surname>
              <given-names>S. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/JBHI.2022.3149288</pub-id>
          <article-title>Federated machine learning for detection of skin diseases and enhancement of Internet of Medical Things (IoMT) security</article-title>
          <source>IEEE J. Biomed. Health Inform.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="conf-paper">
          <page-range>240--254</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Abbas</surname>
              <given-names>A. R.</given-names>
            </name>
            <name>
              <surname>Farooq</surname>
              <given-names>A. O.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-030-01653-1_15</pub-id>
          <article-title>Human skin colour detection using Bayesian rough decision tree</article-title>
          <source>Proceedings of New Trends in Information and Communications Technology Applications: Third International Conference, NTICT 2018, Baghdad, Iraq</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>236-241</page-range>
          <issue>3</issue>
          <year>2007</year>
          <person-group person-group-type="author">
            <name>
              <surname>Latreille</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Gardinier</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Ambroisine</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Mauger</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Tenenhaus</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Guéhenneux</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Morizot</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Tschachler</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Guinot</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1111/j.1600-0846.2007.00212.x</pub-id>
          <article-title>Influence of skin colour on the detection of cutaneous erythema and tanning phenomena using reflectance spectrophotometry</article-title>
          <source>Skin Res. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-7</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mundada</surname>
              <given-names>M. R.</given-names>
            </name>
            <name>
              <surname>Seema</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Sowmya</surname>
              <given-names>B. J.</given-names>
            </name>
            <name>
              <surname>Student</surname>
              <given-names>S. F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICCCNT61001.2024.10724699</pub-id>
          <article-title>DeepDerm: Elevating precision in dermatological diagnosis with enhanced CNN</article-title>
          <source>2024 15th International Conference on Computing Communication and Networking Technologies (ICCCNT), Kamand, India</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-6</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pavithra</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Tamilarasi</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Shashidhar</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Shahzil</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICSTSN61422.2024.10671324</pub-id>
          <article-title>Developing an advanced skin disease detection system by integrating hybrid neural network with Yolo V8 model</article-title>
          <source>2024 Third International Conference on Smart Technologies and Systems for Next Generation Computing (ICSTSN), Villupuram, India</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="conf-paper">
          <page-range>835-840</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Akshay</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Irfan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Srinivas</surname>
              <given-names>K. G.</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/OCIT59427.2023.10430941</pub-id>
          <article-title>Skin-vision: An innovative mobile-based automated skin disease detection application</article-title>
          <source>2023 OITS International Conference on Information Technology (OCIT), Raipur, India</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>27</volume>
          <page-range>5439-5448</page-range>
          <issue>11</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Jian Wei</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Lei</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Jun You</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>Xin</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Jia Qi</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>Xian</given-names>
            </name>
            <name>
              <surname>Du</surname>
              <given-names>Dan</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/JBHI.2023.3304727</pub-id>
          <article-title>SA-RPN: A spacial aware region proposal network for acne detection</article-title>
          <source>IEEE J. Biomed. Health Inform.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>99</page-range>
          <issue>9</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hernandez-Cruz</surname>
              <given-names>Netzahualcoyotl</given-names>
            </name>
            <name>
              <surname>Saha</surname>
              <given-names>Pramit</given-names>
            </name>
            <name>
              <surname>Sarker</surname>
              <given-names>Md Mostafa Kamal</given-names>
            </name>
            <name>
              <surname>Noble</surname>
              <given-names>J. Alison</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/bdcc8090099</pub-id>
          <article-title>Review of federated learning and machine learning-based methods for medical image analysis</article-title>
          <source>Big Data Cogn. Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>190</volume>
          <page-range>789-797</page-range>
          <issue>6</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gordon</surname>
              <given-names>Emily R.</given-names>
            </name>
            <name>
              <surname>Trager</surname>
              <given-names>Michael H.</given-names>
            </name>
            <name>
              <surname>Kontos</surname>
              <given-names>Danielle</given-names>
            </name>
            <name>
              <surname>Weng</surname>
              <given-names>Ching Hon</given-names>
            </name>
            <name>
              <surname>Geskin</surname>
              <given-names>Larisa J.</given-names>
            </name>
            <name>
              <surname>Dugdale</surname>
              <given-names>L. S.</given-names>
            </name>
            <name>
              <surname>Samie</surname>
              <given-names>F. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1093/bjd/ljae040</pub-id>
          <article-title>Ethical considerations for artificial intelligence in dermatology: A scoping review</article-title>
          <source>Br. J. Dermatol.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>35329-35340</page-range>
          <issue>21</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Duan</surname>
              <given-names>Chen Yu</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Shu Guang</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Xiang</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/JIOT.2024.3435082</pub-id>
          <article-title>Prototypes contrastive learning empowered intelligent diagnosis for skin lesion</article-title>
          <source>IEEE Internet Things J.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="conf-paper">
          <page-range>28-38</page-range>
          <person-group person-group-type="author">
            <name>
              <surname>Altimimi</surname>
              <given-names>A. S. Z.</given-names>
            </name>
            <name>
              <surname>Abdulkader</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-031-52787-6_3</pub-id>
          <article-title>CNN-based model for skin diseases classification</article-title>
          <source>International Conference on IoT and Health, Istanbul, Türkiye</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>1-3</page-range>
          <issue>8</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>Shuai</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>Kai</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Lei</given-names>
            </name>
            <name>
              <surname>Han</surname>
              <given-names>Li</given-names>
            </name>
          </person-group>
          <article-title>A decade of federated learning applications in medical image analysis in Shanghai: A comprehensive review (2014-2024)</article-title>
          <source>J. Latex Class Files</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Farrokhi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Taheri</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Adibnia</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Mehrtabar</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>et al.</surname>
            </name>
          </person-group>
          <source>The AI Diagnostician: Improving Medical Diagnosis with Artificial Intelligence</source>
          <publisher-name>Kindle</publisher-name>
          <year>2024</year>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>1964</page-range>
          <issue>11</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yaqoob</surname>
              <given-names>Muhammad Mateen</given-names>
            </name>
            <name>
              <surname>Alsulami</surname>
              <given-names>Musleh</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>Muhammad Amir</given-names>
            </name>
            <name>
              <surname>Alsadie</surname>
              <given-names>Deafallah</given-names>
            </name>
            <name>
              <surname>Saudagar</surname>
              <given-names>Abdul Khader Jilani</given-names>
            </name>
            <name>
              <surname>AlKhathami</surname>
              <given-names>Mohammed</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/diagnostics13111964</pub-id>
          <article-title>Federated machine learning for skin lesion diagnosis: An asynchronous and weighted approach</article-title>
          <source>Diagnostics</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>66505-66511</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>Zhan</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Shuai</given-names>
            </name>
            <name>
              <surname>Peng</surname>
              <given-names>Yu Hui</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>Xiao Yuan</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Xiao Yuan</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Kai</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2019.2918221</pub-id>
          <article-title>Studies on different CNN algorithms for face skin disease classification based on clinical images</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>