<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-FexkaTCdG1ZDVwVLTjG4iB9WwY8Jx4bJ</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml020101</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>An End-to-End CNN Approach for Enhancing Underwater Images Using Spatial and Frequency Domain Techniques</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-0416-8601</contrib-id>
          <name>
            <surname>Rejal</surname>
            <given-names>Ayah Abo El</given-names>
          </name>
          <email>Ayah.hesham@bue.edu.eg</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-8356-3359</contrib-id>
          <name>
            <surname>Nagaty</surname>
            <given-names>Khaled</given-names>
          </name>
          <email>Khaled.Nagaty@bue.edu.eg</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <contrib contrib-type="author">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7278-7349</contrib-id>
          <name>
            <surname>Pester</surname>
            <given-names>Andreas</given-names>
          </name>
          <email>Andreas.Pester@bue.edu.eg</email>
          <xref ref-type="aff" rid="aff_1">1</xref>
        </contrib>
        <aff id="aff_1">Faculty of Informatics and Computer Science, The British University in Egypt, 11837 Cairo, Egypt</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>27</day>
        <month>03</month>
        <year>2023</year>
      </pub-date>
      <volume>2</volume>
      <issue>1</issue>
      <fpage>1</fpage>
      <lpage>12</lpage>
      <page-range>1-12</page-range>
      <history>
        <date date-type="received">
          <day>09</day>
          <month>01</month>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <day>09</day>
          <month>03</month>
          <year>2023</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2023 by the author(s)</copyright-statement>
        <copyright-year>2023</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Underwater image processing area has been a central point of interest to many people in many fields such as control of underwater vehicles, archaeology, marine biology research, etc. Underwater exploration is becoming a big part of our life such as underwater marine and creatures research, pipeline and communication logistics, military use, touristic and entertainment use. Underwater images are subject to poor visibility, distortion, poor quality, etc., due to several reasons such as light propagation. The real problem occurs when these images have to be taken at a depth which is more than 500 feet where artificial light needs to be introduced. This work tackles the underwater environment challenges such as as colour casts, lack of image sharpness, low contrast, low visibility, and blurry appearance in deep ocean images by proposing an end-to-end deep underwater image enhancement network (WGH-net) based on convolutional neural network (CNN) algorithm. Quantitative and qualitative metrics results proved that our method achieved competitive results with the previous work methods as it was experimentally tested on different images from several datasets.</p></abstract>
      <kwd-group>
        <kwd>Underwater</kwd>
        <kwd>Poor visibility</kwd>
        <kwd>Distortion</kwd>
        <kwd>Convolutional</kwd>
        <kwd>Qualitative</kwd>
        <kwd>Quantitative</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="3"/>
        <fig-count count="8"/>
        <table-count count="2"/>
        <ref-count count="38"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>In the last decade, various techniques and algorithms were proposed to solve the problem of distorted underwater images which is caused due to several reasons such as light attenuation, water type, depth of water level and different wavelengths of light rays. Light attenuation limits the visibility distance, at about five meters or less in turbid water. It is not only caused by absorption that removes light energy, but also by scattering which changes the direction of light path. Backscattering is one of the main challenges that face underwater images. It happens due to flash lighting up the small dust-like particles present between the lens of the camera and the object that has been photographed. It affects the images negatively and they appear as if they are taken in a dust storm.</p><p>Several approaches were used to enhance the images taken underwater; however, none of these approaches were suitable. Some approaches did not yield accurate results and others were expensive to implement depending on the use of hardware such as professional cameras. Under water images are taken by different types of compact digital underwater cameras; however, to achieve a high-quality image very expensive cameras such as Sony RX100 V1, Olympus Tough TG-6 and many others should be used which are not a practical solution. Less expensive cameras can be used to take the images that have the enhancing algorithm integrated inside it. This provides an affordable solution; however, a very effective model should be designed to get high satisfying results. Improving the software side will decrease the dependencies on the hardware as a result expenses will be decreased.</p><p>Images taken underwater participate in many fields such as oceanic engineering, discovering new creatures, etc. Marine biologists use underwater images for many uses such as fish classification, monitoring the health level of the marine system and identifying new species without the need to remove them. It also has a great economic impact in projects such as inspection for underwater pipelines and cables for gas and oil industry. Moreover, Marine snow is the continuous flow of particles that starts from the top levels of the ocean and accumulates at the seafloor. As a result, it introduces some sort of noise to the images taken under water and increases the negative effects of scattering. Underwater vehicle navigation is used in archaeological and geological needs. Moreover, it is also useful for international telecommunications traffic.</p><p>As illustrated in <xref ref-type="fig" rid="fig_1">Figure 1</xref>, blue and green light have short wavelengths and as a result they have higher energy and can penetrate much more deeply than red light. The long wavelengths such as red can only penetrate to a very shallow depth that is not more than approximately 50 meters. This is the reason why underwater images appear bluish green unlike normal images taken above water [<xref ref-type="bibr" rid="ref_1">1</xref>]. It is known that different salinity levels of water, affects the amount of different wavelengths absorption.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Underwater optical imaging in shallow water and deep sea [<xref ref-type="bibr" rid="ref_1">1</xref>]</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/2/img_TtHXSMod-mmYz2JF.png"/>
        </fig>
      
      <p>Several models with different techniques were introduced in the past decade to improve the appearance and quality of underwater images, trying to restore colors and details so that they are more useful. These methods can be categorized into three main categories, model-free, model-based, and data-driven network models.</p>
    </sec>
    <sec sec-type="">
      <title>2. State of the art techniques</title>
      <p>Various methods were proposed by other researchers and these methods can be categorized into 3 categories: Model -free methods, Model-based methods (also known as Model-agnostic Methods) and Data-Driven methods.</p><p>2.1 Model-Free Methods</p><p>At the beginning, researchers used model-free methods in enhancing underwater images. Model free methods are the simple enhancement techniques used in image processing. They can be further categorized into two subcategories that use either the spatial domain or the frequency domain. Spatial domain methods use simple algorithms to modify pixel values of an image without modelling the process of image formation. Some examples of this type of methods are histogram equalization, along with its different versions such as contrast limited adaptive histogram equalization (CLAHE) [<xref ref-type="bibr" rid="ref_2">2</xref>], white balancing and its versions for example automatic white balancing [<xref ref-type="bibr" rid="ref_3">3</xref>]. Moreover, in 2007, new color constancy method was proposed namely, Grey-Edge hypothesis based on using grey edge algorithms that assume that the average edge difference is achromatic. Color constancy is mainly the ability to measure object’s colors independent of the value of light source present in the image [<xref ref-type="bibr" rid="ref_4">4</xref>]. These methods improved the visual quality to some extent, but accentuated noise, introduced artifacts and caused some color distortions.</p><p>Furthermore, transform (frequency) domain methods map image pixels into a specific domain where the physical properties are exploited to perform adjustments. Most used transformers include Fourier and wavelets [<xref ref-type="bibr" rid="ref_5">5</xref>]. It improved the quality of the images by amplifying the high frequency component and suppressing the low frequency ones. Underwater images suffer from the problem of having a small difference between the high frequency component of the edge and the low frequency component of the background. As a result, approaches such as holomorphic filter [<xref ref-type="bibr" rid="ref_6">6</xref>] were used.</p><p>Later on, in 2016 and 2017, Khan et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] and Vasamsetti et al. [<xref ref-type="bibr" rid="ref_8">8</xref>] proposed two methods that were wavelet based that can be used as a pre-processing step to increase the accuracy of high-level underwater computer vision tasks. Given the fact that these methods performed well in smearing noise, they introduced artefacts and made noise more visible. Moreover, these methods suffered from low contrast, color deviations and loss of details [<xref ref-type="bibr" rid="ref_9">9</xref>]. Underwater environment conditions made these methods insufficient for enhancing underwater images as they cannot recover high quality underwater images.</p><p>2.2 Model-Based Methods</p><p>To overcome the drawbacks of model-free methods, model-based methods were used and proved to yield better outcomes. Model-based methods combines several model-free methods into a single image aiming to modify the image pixel values and as a result, improve the quality of the image. They can be divided into two subcategories which are physical and non-physical methods. One example of a non-physical model-based is two-step approach that was proposed in the research [<xref ref-type="bibr" rid="ref_10">10</xref>]. It contained both contrast enhancement and colour correction algorithms which generated promising outcomes and can be used in real-time applications. According to 8-bit images, the mean value is equal to 128. Based on this hypothesis, the colour correction technique used is based on piecewise linear transformation to spread the mean of the image until it reaches 128. A positive coefficient is used to ensure that the shifting range is logical to avoid overcorrection. Later, the objects and important details in the images should be highlighted and this is achieved by enhancing the contrast. This method proved to be suitable for real time applications.</p><p>In Jan. 2018, a method was proposed that did not depend on information about the underwater conditions for the image captured [<xref ref-type="bibr" rid="ref_11">11</xref>]. The original degraded image passes by a sequence of steps, starting with white balancing which has a main purpose of removing the colour casts that were introduced by underwater light scattering. Once a white-balanced version is produced, the image is passed in parallel to two other techniques, gamma correction and sharpening. Sharpening technique uses un-sharp masking principle in which a version blurred of the original image is added to the original image itself. This results in an image that is less blurred. These two output images are merged based on a weight map of multistate fusion algorithm and finally an enhanced image is produced. The main drawbacks of this method are that some haze is still present and could not be removed particularly in images that are taken in regions very far from the camera and that the colours of the images could not always be fully restored [<xref ref-type="bibr" rid="ref_11">11</xref>]. Another line of research modified existing algorithms such as Dark Channel Prior (DCP) [<xref ref-type="bibr" rid="ref_12">12</xref>] were combined with algorithms such as wavelength dependent compensation [<xref ref-type="bibr" rid="ref_13">13</xref>] to restore underwater images. Underwater Dark Channel Prior (UDCP) was proposed given the fact that information of the red channel is undependable [<xref ref-type="bibr" rid="ref_14">14</xref>].</p><p>Retinex based models were also made good use of in the research proposed by Zhang et al. [<xref ref-type="bibr" rid="ref_15">15</xref>]. The enhancement method consisted of colour correction, layer decomposition and enhancement. Fu et al.  [<xref ref-type="bibr" rid="ref_16">16</xref>] proposed an extended multi-scale retinex-based method that was used also for general reasons such as enhancing sandstorm images. Physical model-based methods usually follow same, specific procedure. It treats the enhancing problem as an inverse problem. The procedure starts with building the model and then estimating the unknown parameters and finally handling the inversed problem. The current techniques implemented using physical model-based methods suffer from unstable and visually unsatisfying outcomes because they are built on the assumption that the attenuation coefficients are uniform across as they are only properties of the water.</p><p>2.3 Data-Driven Methods</p><p>Over the last ten years, deep learning was made good use of in low-level vision problems. However, the performance and the amount of deep learning-based enhancement techniques for underwater images does not match the success of recent deep learning based low level vision problems. This is because trained proposed CNN models on synthetic underwater images does not always generalize to real world cases. In 2017, Perez et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] proposed a method based on CNN that trains an end-to-end transformation model between the distorted images and its corresponding clear images. Another method was proposed by Wang et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] named UIEnet (Underwater Image Enhancement-net) that aimed to correct colours and remove the haze. It adopted a pixel disrupting strategy to extract the inherent features of local patches available in the images. This helped to fasten the model convergence and improved its accuracy. Later, in 2018, Anwar et al. [<xref ref-type="bibr" rid="ref_19">19</xref>] proposed a model named Underwater Convolutional Network (UWCNN) and used it to reconstruct the clear underwater latent images. It was trained on images from different databases covering images from different scenes and conditions. This method was able to tackle the problem of colour casts; however, due to the limitation of training images, the output images produced suffered from low dynamic range and appeared too hazy. To solve these problems extra post processing was required. However, this method did not yield best results with all testing images. A model named Underwater Resnet (UResnet) is introduced in the research [<xref ref-type="bibr" rid="ref_20">20</xref>]. This model improved the visual appearance of the images; however, one of the main drawbacks of using residual blocks in models is the network gets deeper, it takes a lot of time to train the model that can reach to several weeks and months. In this case, a special process GPU is required to speed up the training.</p><p>Moreover, Generative Adversarial Network (GANs) are recently made use of in many fields such as image generation, video generation and voice recognition. Some researchers used GANs in their proposed solution to enhance underwater images. Fabbri et al. [<xref ref-type="bibr" rid="ref_21">21</xref>] used CycleGAN to reconstruct distorted images based on the undistorted images. These pairs were fed to train an underwater-GAN which can transform hazed underwater images to clear enhanced images. Li et al. [<xref ref-type="bibr" rid="ref_22">22</xref>] proposed a weakly supervised underwater colour correction model and weak supervision means that the model relaxes the need of paired underwater images for training. As discussed earlier, many researchers started using GAN-based models and some of them yielded good results; however, GANs are usually prone to training instability and are time consuming [<xref ref-type="bibr" rid="ref_23">23</xref>]. Moreover, the generated images tend to contain inconsistent stylizations with undesirable artifacts. As a result, it is found that using end-to-end networks yields much better results and is selected for the task.</p>
    </sec>
    <sec sec-type="">
      <title>3. Methodology</title>
      <p>Neural networks have proven to be successful in solving many problems from different fields such as classification, clustering, compression and many more. A simple architecture of a neural network consists of neurons or nodes that make up the layers of the network. The proposed solution is based on an end-to-end CNN model which is based on machine learning as illustrated in <xref ref-type="fig" rid="fig_2">Figure 2</xref>. It is assumed that our proposed methodology would outperform the existing state of the art techniques.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Proposed model architecture</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/2/img_ghjFLxz38zk5IWFv.png"/>
        </fig>
      
      <p>End-to-end means that the network takes the input image from one end and produces the output image at the other end of the model. The model uses a gated fusion network to learn three confidence maps. As we discussed earlier, there are two types of algorithms, spatial domain, and frequency domain techniques. The model takes a single input image and performs three enhancing algorithms. Two of these algorithms work on the spatial domain whereas the third algorithm on the frequency domain. The methods are Gamma Correction (GC), White Balancing (WB) and High Frequency Emphasis Filtering (HEF). Techniques from both categories are used to make the most benefit of the enhancing algorithms. Spatial domain techniques deal with the image as it is and enhances the overall contrast of the image. On the other side, frequency domain techniques give us the control over the whole image and allows us to observe various characteristics that were not visible in the spatial domain. Each algorithm tackles a specific problem from the issues mentioned earlier. These three enhanced versions along with the raw image are fed into the model. This step produces three derived images (<italic>I<sub>GC</sub>, I<sub>WB</sub>, I<sub>HEF</sub></italic>). The derived images along with the raw image are fed to the network. As it is not possible to feed multiple input images to the network in parallel, the 4 input images were concatenated into one single numpy array. The output of the layers are three confidence maps namely, (<italic>C<sub>GC</sub>, C<sub>HEF</sub>, C<sub>WB</sub></italic>). Multiplication of the refined inputs with their corresponding confidence maps produces the final enhanced output as seen in Eq. (1).</p>
      
        <disp-formula>
          <label>(1)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <msub>
              <mi>I</mi>
              <mrow data-mjx-texclass="ORD">
                <mi>e</mi>
                <mi>n</mi>
              </mrow>
            </msub>
            <msub>
              <mi>I</mi>
              <mrow data-mjx-texclass="ORD">
                <mi>G</mi>
                <mi>C</mi>
              </mrow>
            </msub>
            <msub>
              <mi>C</mi>
              <mrow data-mjx-texclass="ORD">
                <mi>G</mi>
                <mi>C</mi>
              </mrow>
            </msub>
            <msub>
              <mi>I</mi>
              <mrow data-mjx-texclass="ORD">
                <mi>W</mi>
                <mi>B</mi>
              </mrow>
            </msub>
            <msub>
              <mi>C</mi>
              <mrow data-mjx-texclass="ORD">
                <mi>W</mi>
                <mi>B</mi>
              </mrow>
            </msub>
            <msub>
              <mi>I</mi>
              <mrow data-mjx-texclass="ORD">
                <mi>H</mi>
                <mi>E</mi>
                <mi>F</mi>
              </mrow>
            </msub>
            <msub>
              <mi>C</mi>
              <mrow data-mjx-texclass="ORD">
                <mi>H</mi>
                <mi>E</mi>
                <mi>F</mi>
              </mrow>
            </msub>
            <mo>=</mo>
            <mo>∗</mo>
            <mo>+</mo>
            <mo>∗</mo>
            <mo>+</mo>
            <mo>∗</mo>
          </math>
        </disp-formula>
      
      <p>The model architecture includes three Feature Transformation Units (FTU) as shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref> which has a main purpose of reducing the colour casts and artefacts introduced by the enhancing techniques mentioned earlier. Filter sizes of the layers are chosen to be (7*7) then decreases to (5*5) and finally to (3*3) to be more specific to features in the images fed. Batch normalization layers were added to the main model and the FTUs as it makes training faster and more stable. It helps to solve Internal Covariate Shift problem that occurs due to the change of parameters in each layer which leads to change in the distribution of the inputs to subsequent layers. As a result, the learning process is faster, stabilized and the number of training epochs are reduced. Moreover, to make the model even more efficient max pooling layers are added to reduce the computational costs. This is done by reducing the number of parameters that the model has to learn. It is a standard benchmarking technique that is implemented to make benefit from the feature as it selects the brighter pixels from the image, and this is useful when the background is dark, and this is usually the case in underwater images.</p>
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>FTU model</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/2/img_smy-SvPUYIiSlmGC.png"/>
        </fig>
      
      <p>The perceptual loss function is a fucntion for comparing two images that are very similar to one another in terms of content and style discrepancies. In this work it is based on the implementation of the Manhathan distance. It is the distance between the feature represntations of both the enhanced (which is represented by<span style="font-family: Times New Roman, serif"> </span>(<italic>I<sub>en</sub></italic>)) and the reference (which is represented by (<italic>I<sub>RAW</sub></italic>)) image. The Manhathan distance is a method of calculating the absolute difference distance between two points and is normally used in the case of high dimensional data. It is better than Euclidean distance which takes the square root of the sum of square values of differences between two points because it gives more robust results.</p>
      
        <disp-formula>
          <label>(2)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>d</mi>
            <mi>p</mi>
            <mi>q</mi>
            <mo stretchy="false">(</mo>
            <mo>,</mo>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <msqrt>
              <munderover>
                <mo data-mjx-texclass="OP">∑</mo>
                <mrow data-mjx-texclass="ORD">
                  <mi>i</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>n</mi>
              </munderover>
              <msup>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo>−</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <msub>
                    <mi>q</mi>
                    <mi>i</mi>
                  </msub>
                  <msub>
                    <mi>p</mi>
                    <mi>i</mi>
                  </msub>
                </mrow>
                <mn>2</mn>
              </msup>
            </msqrt>
          </math>
        </disp-formula>
      
      <p>Manhathan distance shown in Eq. (3) has (<italic>C<sub>j</sub> H<sub>j</sub> W<sub>j</sub></italic>) which is the dimensions of the feature map; number, height and width respectively. Additionally, <italic>N</italic> is the number of each batch in the training process whereas <italic><inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi data-mjx-alternate="1">∅</mi>
  </math>
</inline-formula></italic> is the number of layers in the network and <italic>j</italic> are iterations over the layers. The equation is performed at each layer and the sum of all results is calculated. Only 8 layers were used to ensure that no overfitting occurs. In other words, if many layers are used, the model will learn the training data more than it should be and will negatively affect the performance of the model. The model will not be able to generalize when new data are introduced during the testing phase.</p>
      
        <disp-formula>
          <label>(3)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <msubsup>
              <mi>L</mi>
              <mi>j</mi>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="normal">∅</mi>
              </mrow>
            </msubsup>
            <mo>=</mo>
            <mfrac>
              <mn>1</mn>
              <mrow>
                <msub>
                  <mi>C</mi>
                  <mi>i</mi>
                </msub>
                <msub>
                  <mi>H</mi>
                  <mi>j</mi>
                </msub>
                <msub>
                  <mi>W</mi>
                  <mi>j</mi>
                </msub>
              </mrow>
            </mfrac>
            <munderover>
              <mo data-mjx-texclass="OP">∑</mo>
              <mrow data-mjx-texclass="ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>N</mi>
            </munderover>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN" symmetric="true">‖</mo>
              <mo data-mjx-texclass="CLOSE" symmetric="true">‖</mo>
              <msub>
                <mi mathvariant="normal">∅</mi>
                <mrow data-mjx-texclass="ORD">
                  <mi>j</mi>
                  <mrow data-mjx-texclass="INNER">
                    <mo data-mjx-texclass="OPEN">(</mo>
                    <mo data-mjx-texclass="CLOSE">)</mo>
                    <msubsup>
                      <mi>I</mi>
                      <mi>i</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mi>e</mi>
                        <mi>n</mi>
                      </mrow>
                    </msubsup>
                  </mrow>
                  <mo>−</mo>
                </mrow>
              </msub>
              <msub>
                <mi mathvariant="normal">∅</mi>
                <mrow data-mjx-texclass="ORD">
                  <mi>j</mi>
                  <mrow data-mjx-texclass="INNER">
                    <mo data-mjx-texclass="OPEN">(</mo>
                    <mo data-mjx-texclass="CLOSE">)</mo>
                    <msubsup>
                      <mi>I</mi>
                      <mi>i</mi>
                      <mrow data-mjx-texclass="ORD">
                        <mi>R</mi>
                        <mi>A</mi>
                        <mi>W</mi>
                      </mrow>
                    </msubsup>
                  </mrow>
                </mrow>
              </msub>
            </mrow>
          </math>
        </disp-formula>
      
      <p>3.1 Gamma Correction</p><p>The GC technique is used to lighten up the dark areas in the images and its gamma value varies according to the purpose of using the technique. For example, in this work the gamma value <italic>γ</italic> is chosen to be 0.7. When the gamma value is less than 1, the process is named encoding gamma correction. Eq. (4) is used to calculate the corrected output represented by <italic>g(x)</italic> [<xref ref-type="bibr" rid="ref_24">24</xref>], x is every single pixel value and <italic>γ</italic> represents gamma value.</p>
      
        <disp-formula>
          <label>(4)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>g</mi>
            <mi>x</mi>
            <mo stretchy="false">(</mo>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mn>255</mn>
            <msup>
              <mrow data-mjx-texclass="INNER">
                <mo data-mjx-texclass="OPEN">(</mo>
                <mo data-mjx-texclass="CLOSE">)</mo>
                <mfrac>
                  <mi>x</mi>
                  <mn>255</mn>
                </mfrac>
              </mrow>
              <mrow data-mjx-texclass="ORD">
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <mfrac>
                    <mn>1</mn>
                    <mi>γ</mi>
                  </mfrac>
                </mrow>
              </mrow>
            </msup>
          </math>
        </disp-formula>
      
      <p>3.2 White Balancing</p><p>The WB technique is used to correct the color casts which is unwanted tint of a particular color in the image by discarding the unwanted ones as a result of various illuminations. Due to very poor light propagation in underwater scenes, there seems to be a lack of contrast. However, at water levels deeper than 30 feet, white balancing technique is not very efficient as it is very difficult to restore colors that were absorbed [<xref ref-type="bibr" rid="ref_25">25</xref>].</p><p>3.3 High Frequency Emphasis Filtering (HEF)</p><p>One of the mentioned problems in underwater images was the lack of image sharpness and images seem to have blurry appearance. High frequency emphasis filtering (HEF) technique [<xref ref-type="bibr" rid="ref_26">26</xref>] is used with a modification of using contrast limited adaptive histogram equalization (CLAHE) instead of histogram equalization. It consists of a sequence of steps and are represented in <xref ref-type="fig" rid="fig_4">Figure 4</xref>.</p>
      
        <fig id="fig_4">
          <label>Figure 4</label>
          <caption>
            <title>HEF algorithm</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/2/img_t3p9XBu5m13tkJzd.png"/>
        </fig>
      
      <p>The first step in HEF is to convert the image into its frequency domain representation. Then the filter function is applied and in this case Gaussian high-pass filter is used to accentuate and emphasize the edges. Eq. (5) represents the filter function where Do is the cut off distance.</p>
      
        <disp-formula>
          <label>(5)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>G</mi>
            <mi>a</mi>
            <mi>u</mi>
            <mi>s</mi>
            <mi>s</mi>
            <mi>i</mi>
            <mi>a</mi>
            <mi>n</mi>
            <mi>F</mi>
            <mi>i</mi>
            <mi>l</mi>
            <mi>t</mi>
            <mi>e</mi>
            <mi>r</mi>
            <mstyle scriptlevel="0">
              <mspace width="0.278em"/>
            </mstyle>
            <mo>=</mo>
            <mo>−</mo>
            <mn>1</mn>
            <msup>
              <mi>e</mi>
              <mrow data-mjx-texclass="ORD">
                <mo>−</mo>
                <mfrac>
                  <mrow>
                    <msup>
                      <mi>D</mi>
                      <mn>2</mn>
                    </msup>
                    <mo stretchy="false">(</mo>
                    <mo>,</mo>
                    <mo stretchy="false">)</mo>
                    <mi>i</mi>
                    <mi>j</mi>
                  </mrow>
                  <mrow>
                    <mn>2</mn>
                    <msubsup>
                      <mi>D</mi>
                      <mn>0</mn>
                      <mn>2</mn>
                    </msubsup>
                  </mrow>
                </mfrac>
              </mrow>
            </msup>
          </math>
        </disp-formula>
      
      <p>The 2D Fourier transform of <italic>F(x, y)</italic> and inverse Fourier transform of <italic>F(i, j)</italic> are denoted by Eqns. (6) and (7) respectively where <italic>x</italic> and <italic>i</italic> takes value starting from 0, 1, 2 till <italic>M</italic>−1 and <italic>y</italic> and <italic>j</italic>=0, 1, 2 till <italic>N</italic>–1.</p>
      
        <disp-formula>
          <label>(6)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>F</mi>
            <mi>i</mi>
            <mi>j</mi>
            <mi>f</mi>
            <mi>x</mi>
            <mi>y</mi>
            <mo stretchy="false">(</mo>
            <mo>,</mo>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mo stretchy="false">(</mo>
            <mo>,</mo>
            <mo stretchy="false">)</mo>
            <munderover>
              <mo data-mjx-texclass="OP">∑</mo>
              <mrow data-mjx-texclass="ORD">
                <mi>X</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mrow data-mjx-texclass="ORD">
                <mi>M</mi>
                <mo>−</mo>
                <mn>1</mn>
              </mrow>
            </munderover>
            <munderover>
              <mo data-mjx-texclass="OP">∑</mo>
              <mrow data-mjx-texclass="ORD">
                <mi>y</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mrow data-mjx-texclass="ORD">
                <mi>N</mi>
                <mo>−</mo>
                <mn>1</mn>
              </mrow>
            </munderover>
            <msup>
              <mi>e</mi>
              <mrow data-mjx-texclass="ORD">
                <mo>−</mo>
                <mi>j</mi>
                <mi>π</mi>
                <mn>2</mn>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo>+</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <mfrac>
                    <mrow>
                      <mi>i</mi>
                      <mi>x</mi>
                    </mrow>
                    <mi>M</mi>
                  </mfrac>
                  <mfrac>
                    <mrow>
                      <mi>j</mi>
                      <mi>y</mi>
                    </mrow>
                    <mi>N</mi>
                  </mfrac>
                </mrow>
              </mrow>
            </msup>
          </math>
        </disp-formula>
      
      
        <disp-formula>
          <label>(7)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>F</mi>
            <mi>x</mi>
            <mi>y</mi>
            <mi>f</mi>
            <mi>i</mi>
            <mi>j</mi>
            <mo stretchy="false">(</mo>
            <mo>,</mo>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mo stretchy="false">(</mo>
            <mo>,</mo>
            <mo stretchy="false">)</mo>
            <mfrac>
              <mn>1</mn>
              <mi>M</mi>
            </mfrac>
            <munderover>
              <mo data-mjx-texclass="OP">∑</mo>
              <mrow data-mjx-texclass="ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mrow data-mjx-texclass="ORD">
                <mi>M</mi>
                <mo>−</mo>
                <mn>1</mn>
              </mrow>
            </munderover>
            <munderover>
              <mo data-mjx-texclass="OP">∑</mo>
              <mrow data-mjx-texclass="ORD">
                <mi>j</mi>
                <mo>=</mo>
                <mn>0</mn>
              </mrow>
              <mrow data-mjx-texclass="ORD">
                <mi>N</mi>
                <mo>−</mo>
                <mn>1</mn>
              </mrow>
            </munderover>
            <msup>
              <mi>e</mi>
              <mrow data-mjx-texclass="ORD">
                <mo>−</mo>
                <mi>j</mi>
                <mi>π</mi>
                <mn>2</mn>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo>+</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <mfrac>
                    <mrow>
                      <mi>i</mi>
                      <mi>x</mi>
                    </mrow>
                    <mi>M</mi>
                  </mfrac>
                  <mfrac>
                    <mrow>
                      <mi>j</mi>
                      <mi>y</mi>
                    </mrow>
                    <mi>N</mi>
                  </mfrac>
                </mrow>
              </mrow>
            </msup>
          </math>
        </disp-formula>
      
      <p>In the high frequency spectrum, the expressed edges have more significant changes in frequencies. As a result, a low contrast image is produced and that is the reason why (CLAHE) is used in the last step, to increase sharpness and contrast. The HE technique is used to adjust image intensities to enhance the contrast of the image. In this work a special type of histogram equalization is used named contrast limited adaptive histogram equalization (CLAHE) [<xref ref-type="bibr" rid="ref_27">27</xref>] and it is illustrated in <xref ref-type="fig" rid="fig_5">Figure 5</xref>. Subgraph (a) of <xref ref-type="fig" rid="fig_5">Figure 5</xref> shows the original histogram of the image whereas Subgraph (b) of <xref ref-type="fig" rid="fig_5">Figure 5</xref> shows the pixels distribution after the histogram has been clipped. It performs histogram equalization technique in small patches achieving high accuracy and contrast limited. A threshold is set, and any pixel value exceeds this threshold is cut by a clipper before computing the cumulative distribution function. The part clipped is then equally distributed upon all histogram bins. The selected threshold is 2.0 in CLAHE which helps limit the height of the histogram [<xref ref-type="bibr" rid="ref_28">28</xref>]. This means that the slope of the cumulative distribution function curve will be reduced. In other words, contrast enhancement is reduced to limit not only noise amplification, but also local over enhancement.</p>
      
        <fig id="fig_5">
          <label>Figure 5</label>
          <caption>
            <title>CLAHE algorithm</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/2/img_5D1Y6-khS5zOGBF6.png"/>
        </fig>
      
    </sec>
    <sec sec-type="results">
      <title>4. Results</title>
      <p>The model was implemented using keras and trained on Google Pro colab. Adaptive moment estimation (ADAM); a first order gradient based was used for optimization. It is considered as one of the most optimal optimizers as it requires minimal memory requirements and is straightforward to implement [<xref ref-type="bibr" rid="ref_29">29</xref>]. It combines the heuristics of momentum and RMSProp optimizers which makes it more optimal, and able to handle sparse gradients on noisy problems. It is used in models that has more than one hidden layer by using the squared gradients to scale the learning rate which is one of the four hyperparameters it has. Assigning the learning rate to a small number is good approach as it would allow the weights to reach a minimum during training which leads to better accuracy. To achieve good reliable results and improve the data model prediction accuracy, data augmentation was applied as a pre-processing step on the dataset. It helps expose our model to different versions of the data which increases its generalizing ability and decreases the chance of overfitting occurring.</p><p>4.1 Experiments</p><p>There are several underwater images datasets available for different research purposes such as image enhancement, object detection, etc. Enhancement of Underwater Visual Perception dataset (EUVP) has a collection of 890 underwater images that were collected at various conditions such as oceanic explorations and human-robot collaborative experiments [<xref ref-type="bibr" rid="ref_30">30</xref>]. HICRD dataset consists of 6665 unpaired images collected from several sample locations around Heron Reef that is located in the Southern Great Barrier Reef [<xref ref-type="bibr" rid="ref_31">31</xref>]. For training our model we used the dataset called UIEB (Underwater Image Enhancement Benchmark) [<xref ref-type="bibr" rid="ref_32">32</xref>]. This dataset was also used by other state-of-the-art techniques, so we decided to stick to it to have a fair comparison. Another reason for choosing the UIEB dataset is that it has corresponding ground truth images which are needed while training the model. 12 filtering methods were performed on the raw images to produce 12 reference images and only one is selected based on the most choice selected by 50 participants. The dataset contains images taken at various diverse scenes that has different objects such as corals, rocks, sculptures, etc as seen in <xref ref-type="fig" rid="fig_6">Figure 6</xref>. The images have various image quality degradation characteristics. <xref ref-type="fig" rid="fig_7">Figure 7</xref> has subjective comparisons on two images from the dataset.</p>
      
        <fig id="fig_6">
          <label>Figure 6</label>
          <caption>
            <title>UIEB dataset sample</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/2/img_lU6dnMGIrUwZL6eb.png"/>
        </fig>
      
      
        <fig id="fig_7">
          <label>Figure 7</label>
          <caption>
            <title>Results comparisons</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/2/img_AVJvooTdUOPEqhfW.png"/>
        </fig>
      
      <p>4.2 Metrics</p><p>There are several image quality metrics that can be used to measure and determine the quality of images after they have been enhanced using various enhancing techniques. These metrics could be divided into categories namely, full reference metrics and non-reference metrics that will be explained in the next sub section [<xref ref-type="bibr" rid="ref_33">33</xref>].</p><p>4.2.1 Full reference metrics</p><p>Full reference metrics techniques perform a direct comparison between the enhanced image and the raw image fed to the model. In this work, two full reference techniques were namely Structural Similarity Index (SSIM) [<xref ref-type="bibr" rid="ref_34">34</xref>] and Peak Signal to Noise Ratio (PSNR) [<xref ref-type="bibr" rid="ref_35">35</xref>]. In SSIM the two images are represented by windows <italic>x</italic> and <italic>y</italic>. Eq. (8) is the formulae of SSIM where <italic>μ<sub>x</sub></italic> is the average of values in <italic>x</italic>, <italic>μ<sub>y</sub></italic> is the average of values in <italic>y</italic>, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msubsup>
      <mi>σ</mi>
      <mi>x</mi>
      <mn>2</mn>
    </msubsup>
  </math>
</inline-formula> is the variance in window <italic>x</italic>, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msubsup>
      <mi>σ</mi>
      <mi>y</mi>
      <mn>2</mn>
    </msubsup>
  </math>
</inline-formula><italic> </italic>is the variance in window <italic>y</italic>, <italic>σ<sub>xy</sub></italic> represents the covariance of both <italic>x</italic> and <italic>y</italic>, <italic>c<sub>1</sub></italic> and <italic>c<sub>2</sub></italic> are variables used to stabilize the division in the equation using a weak denominator. The value ranges between: [-1, +1] and will be equal to if the two images are identical.</p>
      
        <disp-formula>
          <label>(8)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>SSIM</mi>
            <mi>x</mi>
            <mi>y</mi>
            <mo stretchy="false">(</mo>
            <mo>,</mo>
            <mo stretchy="false">)</mo>
            <mo>=</mo>
            <mfrac>
              <mrow>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo>+</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <mn>2</mn>
                  <msub>
                    <mi>μ</mi>
                    <mi>x</mi>
                  </msub>
                  <msub>
                    <mi>μ</mi>
                    <mi>y</mi>
                  </msub>
                  <msub>
                    <mi>c</mi>
                    <mn>1</mn>
                  </msub>
                </mrow>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo>+</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <mn>2</mn>
                  <msub>
                    <mi>σ</mi>
                    <mrow data-mjx-texclass="ORD">
                      <mi>x</mi>
                      <mi>y</mi>
                    </mrow>
                  </msub>
                  <msub>
                    <mi>c</mi>
                    <mn>2</mn>
                  </msub>
                </mrow>
              </mrow>
              <mrow>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo>+</mo>
                  <mo>+</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <msubsup>
                    <mi>μ</mi>
                    <mi>x</mi>
                    <mn>2</mn>
                  </msubsup>
                  <msubsup>
                    <mi>μ</mi>
                    <mi>y</mi>
                    <mn>2</mn>
                  </msubsup>
                  <msub>
                    <mi>c</mi>
                    <mn>1</mn>
                  </msub>
                </mrow>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo>+</mo>
                  <mo>+</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <msubsup>
                    <mi>σ</mi>
                    <mi>x</mi>
                    <mn>2</mn>
                  </msubsup>
                  <msubsup>
                    <mi>σ</mi>
                    <mi>y</mi>
                    <mn>2</mn>
                  </msubsup>
                  <msub>
                    <mi>c</mi>
                    <mn>2</mn>
                  </msub>
                </mrow>
              </mrow>
            </mfrac>
          </math>
        </disp-formula>
      
      <p>PSNR is calculated as shown in Eq. (9) and Eq. (10). MSE stands for mean squared error which simply represents the mean of the squares of errors between the enhanced image and the distorted one and The error is the diferrence in pixel values of both images. <italic>Y</italic> is the observed vector, whereas <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mrow data-mjx-texclass="ORD">
      <mover>
        <mi>Y</mi>
        <mo>^</mo>
      </mover>
    </mrow>
  </math>
</inline-formula> is the predicted vector produced as an output from the model. The rule for <italic>MAX<sub>f</sub></italic> is 2<sup>(n)</sup>-1 which is 255 because the images are 8 bits where <italic>n</italic> is the number of bits. If we assume that the value for <italic>MSE</italic> is 1 then this means that <italic>20 log<sub>10</sub> (255)</italic> is 48. In other words, the ideal highest value is 48 for a 8 bit image.</p>
      
        <disp-formula>
          <label>(9)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>P</mi>
            <mi>S</mi>
            <mi>N</mi>
            <mi>R</mi>
            <mo>=</mo>
            <mo data-mjx-texclass="NONE">⁡</mo>
            <mn>20</mn>
            <msub>
              <mi>log</mi>
              <mrow data-mjx-texclass="ORD">
                <mn>10</mn>
              </mrow>
            </msub>
            <mrow data-mjx-texclass="INNER">
              <mo data-mjx-texclass="OPEN">(</mo>
              <mo data-mjx-texclass="CLOSE">)</mo>
              <mfrac>
                <mrow>
                  <mi>M</mi>
                  <mi>A</mi>
                  <msub>
                    <mi>X</mi>
                    <mi>f</mi>
                  </msub>
                </mrow>
                <msqrt>
                  <mi>M</mi>
                  <mi>S</mi>
                  <mi>E</mi>
                </msqrt>
              </mfrac>
            </mrow>
          </math>
        </disp-formula>
      
      
        <disp-formula>
          <label>(10)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>M</mi>
            <mi>S</mi>
            <mi>E</mi>
            <mo>=</mo>
            <mfrac>
              <mn>1</mn>
              <mi>n</mi>
            </mfrac>
            <munderover>
              <mo data-mjx-texclass="OP">∑</mo>
              <mrow data-mjx-texclass="ORD">
                <mi>i</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mrow>
              <mi>n</mi>
            </munderover>
            <msup>
              <mrow data-mjx-texclass="INNER">
                <mo data-mjx-texclass="OPEN">(</mo>
                <mo>−</mo>
                <mo data-mjx-texclass="CLOSE">)</mo>
                <msub>
                  <mi>Y</mi>
                  <mi>i</mi>
                </msub>
                <msub>
                  <mrow data-mjx-texclass="ORD">
                    <mover>
                      <mi>Y</mi>
                      <mo stretchy="false">^</mo>
                    </mover>
                  </mrow>
                  <mi>i</mi>
                </msub>
              </mrow>
              <mn>2</mn>
            </msup>
          </math>
        </disp-formula>
      
      <p><xref ref-type="table" rid="table_1">Table 1</xref> shows the results of the previous state of the art techniques along with the results of our proposed model using the testing dataset UIEB mentioned earlier. Given the fact that SSIM and PSNR are quantitative metrics, they have a major drawback when it comes to the case of evaluating underwater image enhancement techniques which they do not consider any of the essential biological factors concerned with the human vision system. As a result, qualitative metrics should also be used which will be discussed in the next sub-section.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Full-reference evaluation results</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Method</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">SSIM</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">PSNR</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Fusion-based [<xref ref-type="bibr" rid="ref_11">11</xref>]</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.8162</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">18.7461</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">UDCP [<xref ref-type="bibr" rid="ref_14">14</xref>]</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.4999</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">11.0296</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Two-step based [<xref ref-type="bibr" rid="ref_10">10</xref>]</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.7199</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">18.7461</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Retinex-based [<xref ref-type="bibr" rid="ref_15">15</xref>]</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.6233</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">16.8757</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Water-Net [<xref ref-type="bibr" rid="ref_32">32</xref>]</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.7971</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">19.1130</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Ours</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.8995</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">22.9865</span></p></td></tr></tbody></table>
        </table-wrap>
      
      <p>4.2.2 Non reference metrics</p><p>Usually, it is easy to obtain ground truth images for image processing problems such as image super resolution. However, in underwater image enhancement problem it is challenging to achieve large number of paired images. Due to this reason non-reference metrics were proposed. In this work we evaluate our results using two of these techniques named, Underwater Colour Image Quality Evaluation (UCIQE) [<xref ref-type="bibr" rid="ref_36">36</xref>] and Underwater Image Quality Measure (UIQM) [<xref ref-type="bibr" rid="ref_37">37</xref>]. In UCIQE metric the blurring effect, low contrast and non-uniform color casts are linearly combined. In this technique the deviation of saturation is not used unlike other previous proposed method because it emphasizes the dark areas which are simply the result of images taken in limited lighting. In Eq. (11) the <italic>σ<sub>c</sub></italic> represents the standard deviation value of Chroma and ranges from (-∞, +∞), <italic>con<sub>1</sub></italic> represents the contrast of luminance which ranges from [0, +∞), and represents the global grey scale distribution of the given image. The <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>μ</mi>
      <mi>S</mi>
    </msub>
  </math>
</inline-formula> represents the average saturation and ranges from (-∞, +∞). Chroma is one of the properties in an image and represents the degree of color clarity and purity. The variables <italic>c<sub>1</sub></italic>, <italic>c<sub>2</sub></italic>, <italic>c<sub>3</sub></italic> represent weighted coefficients. Several coefficient values were experimented according to several experiments made in the research [<xref ref-type="bibr" rid="ref_36">36</xref>] and the following values proved to yield best results: <italic>c<sub>1</sub></italic> = 0.4680, <italic>c<sub>2</sub></italic> = 0.2745, <italic>c<sub>3</sub></italic> = 0.2576.</p><p>Minimum value for UCIQE is 0 and as the value increases, this means that the model has better performance.</p>
      
        <disp-formula>
          <label>(11)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>U</mi>
            <mi>C</mi>
            <mi>I</mi>
            <mi>Q</mi>
            <mi>E</mi>
            <mo>=</mo>
            <mo>∗</mo>
            <mo>+</mo>
            <mo>∗</mo>
            <mo>+</mo>
            <mo>∗</mo>
            <msub>
              <mi>c</mi>
              <mn>1</mn>
            </msub>
            <msub>
              <mi>σ</mi>
              <mi>c</mi>
            </msub>
            <msub>
              <mi>c</mi>
              <mn>2</mn>
            </msub>
            <msub>
              <mi>con</mi>
              <mn>1</mn>
            </msub>
            <msub>
              <mi>c</mi>
              <mn>3</mn>
            </msub>
            <msub>
              <mi>μ</mi>
              <mi>S</mi>
            </msub>
          </math>
        </disp-formula>
      
      <p>The underwater image quality measure (UIQM) metric uses three measures: which are the underwater image colorfulness measure (UICM), the underwater image contrast measure (UIConM) and the underwater image sharpness measure (UISM). Sharpness is a property of images that describes the clarity of edges and important fine details. In UISM, the Sobel edge detector algorithm is used on every channel of RGB. This yields three edge maps which are then multiplied with the original channel values to produce the grey scale edge maps. This preserves only the pixels representing the edges in an underwater image. UICM is used to remove the effect of bright regions due to heavy noise in underwater images. The three measures are linearly combined as shown in Eq. (12). The values given to the weights are dependent on the type of application that the underwater image is used into. Several coefficients values were experimented according to several experiments made in the research [<xref ref-type="bibr" rid="ref_37">37</xref>] and the following values proved to yield best results: <italic>c<sub>1</sub></italic> = 0.0282, <italic>c<sub>2</sub></italic> = 0.2953, <italic>c<sub>3</sub></italic> = 3.5753.</p>
      
        <disp-formula>
          <label>(12)</label>
          <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
            <mi>U</mi>
            <mi>I</mi>
            <mi>Q</mi>
            <mi>M</mi>
            <mi>U</mi>
            <mi>I</mi>
            <mi>C</mi>
            <mi>M</mi>
            <mi>U</mi>
            <mi>I</mi>
            <mi>S</mi>
            <mi>M</mi>
            <mi>U</mi>
            <mi>I</mi>
            <mi>C</mi>
            <mi>o</mi>
            <mi>n</mi>
            <mi>M</mi>
            <mo>=</mo>
            <mo>∗</mo>
            <mo>+</mo>
            <mo>∗</mo>
            <mo>+</mo>
            <mo>∗</mo>
            <msub>
              <mi>c</mi>
              <mn>1</mn>
            </msub>
            <msub>
              <mi>c</mi>
              <mn>2</mn>
            </msub>
            <msub>
              <mi>c</mi>
              <mn>3</mn>
            </msub>
          </math>
        </disp-formula>
      
      <p>As both metrics UCIQE and UIQM use Chroma of the image as one of their variables, this makes it essential to convert colour space from red, green and blue Red-Green-Blue (RGB) to LAB. Representation of a LAB image is <italic>l*a*b</italic> where <italic>l</italic> stands for lightness of the image and ranges from[0, 100] that is from black to white, <italic>a</italic> stands for red/green component and ranges [-120, +120] and <italic>b </italic>stands for yellow/blue component and has the same range of component <italic>a</italic> [-120, +120]. In LAB colour space, a change in numerical values usually corresponds to approximately the same amount of change in how it is visually perceived [<xref ref-type="bibr" rid="ref_38">38</xref>]. As shown in <xref ref-type="fig" rid="fig_8">Figure 8</xref>, neither <italic>a</italic> cannot be both red and green at the same time, nor <italic>b</italic> can be blue and yellow at the same time. This is made clear by the coordinate axes which represents valu es running from positive to neagtive values. For example, positive <italic>a</italic> values indicate red colour, whereas a negative value repesents green colour. The same concept implies for <italic>b</italic> where a negative value indicates blue colour whereas a positive value indicates yellow colour. <xref ref-type="table" rid="table_2">Table 2</xref> represents the results when the previous enhancing approaches namely fusion-based and two-step-based were tested with evaluation metrics; UCIQE and UIQM. Both methods are used to rank under water image enhancing algorithms performance. A higher score achieved by the UCIQE [<xref ref-type="bibr" rid="ref_36">36</xref>] metric indicates better balance among saturation, contrast, and Chroma whereas a higher result of UIQM [<xref ref-type="bibr" rid="ref_37">37</xref>] indicates that the output is much more consistent with the human visual perception.</p>
      
        <fig id="fig_8">
          <label>Figure 8</label>
          <caption>
            <title>CIELAB color space representation [<xref ref-type="bibr" rid="ref_38">38</xref>]</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/2/img_MF7yNerXk_RII10L.png"/>
        </fig>
      
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>Non-reference evaluation results</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Method</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">UCIQE (↑)</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">UIQM (↑)</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Fusion-based [<xref ref-type="bibr" rid="ref_11">11</xref>]</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.6414</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1.5310</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">UDCP [<xref ref-type="bibr" rid="ref_14">14</xref>]</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.5852</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1.6297</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Two-step based [<xref ref-type="bibr" rid="ref_10">10</xref>]</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.5776</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1.4002</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Retinex-based [<xref ref-type="bibr" rid="ref_15">15</xref>]</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.6062</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1.4338</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Water-Net [<xref ref-type="bibr" rid="ref_32">32</xref>]</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.6983</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1.7216</span></p></td></tr><tr><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">Ours</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">0.7851</span></p></td><td colspan="1" rowspan="1"><p style="text-align: center"><span style="color: black; font-family: Times New Roman, serif">1.9867</span></p></td></tr></tbody></table>
        </table-wrap>
      
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>In this work, we have discussed the importance of underwater images in different aspects of life such as marine engineering, control of underwater vehicles, etc. Moreover, main challenges that face underwater images and their corresponding effects on the images were also discussed. The main effects were low contrast, colour casts and blurry appearance. We presented a simple and efficient model that is based on the use of CNN to enhance underwater images that are taken at different environmental conditions. Results of our model were presented and compared to previous state of the art methods using both full-reference and non-reference metrics. Our model scored better results in both types of metrics that were used for evaluation. In the future work, we aim to extend investigating our model’s feasibility on real-time videos.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>Y. D.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>H. P.</given-names>
            </name>
            <name>
              <surname>Shang</surname>
              <given-names>D. H.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Quan</surname>
              <given-names>X. Q.</given-names>
            </name>
          </person-group>
          <article-title>An underwater image enhancement method for different illumination conditions based on color tone correction and fusion-based descattering</article-title>
          <source>Sensors</source>
          <year>2019</year>
          <volume>19</volume>
          <issue>4</issue>
          <pub-id pub-id-type="doi">https://doi.org/10.3390/s19245567</pub-id>
          <pub-id pub-id-type="publisher-id">5567</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Pizer</surname>
              <given-names>S. M.</given-names>
            </name>
            <name>
              <surname>Johnston</surname>
              <given-names>R. E.</given-names>
            </name>
            <name>
              <surname>Ericksen</surname>
              <given-names>J. P.</given-names>
            </name>
            <name>
              <surname>Yankaskas</surname>
              <given-names>B. C.</given-names>
            </name>
            <name>
              <surname>Muller</surname>
              <given-names>K. E.</given-names>
            </name>
          </person-group>
          <article-title>Contrast-limited adaptive histogram equalization: Speed and effectiveness</article-title>
          <source>Proceedings of the First Conference on Visualization in Biomedical Computing, Atlanta, GA</source>
          <publisher-name>USA</publisher-name>
          <year>1990</year>
          <page-range>337-345</page-range>
          <fpage>337</fpage>
          <lpage>345</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/VBC.1990.109340</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>Y. C.</given-names>
            </name>
            <name>
              <surname>Chan</surname>
              <given-names>W. H.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Y. Q.</given-names>
            </name>
          </person-group>
          <article-title>Automatic white balance for digital still camera</article-title>
          <source>IEEE Trans. Consum. Electron.</source>
          <year>1995</year>
          <volume>41</volume>
          <issue>3</issue>
          <page-range>460-466</page-range>
          <fpage>460</fpage>
          <lpage>466</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/30.468045</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>van de Weijer</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Gevers</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Gijsenij</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Edge-Based Color Constancy</article-title>
          <source>IEEE T. Image Process</source>
          <year>2007</year>
          <volume>16</volume>
          <issue>9</issue>
          <page-range>2207-2214</page-range>
          <fpage>2207</fpage>
          <lpage>2214</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/TIP.2007.901808</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>G.</given-names>
              <surname>Singh</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Jaggi</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Vasamsetti</surname>
            </name>
            <name>
              <given-names>H. K.</given-names>
              <surname>Sardana</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Kumar</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Mittal</surname>
            </name>
          </person-group>
          <article-title>Underwater image/video enhancement using wavelet based color correction (WBCC) method</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2015 IEEE Underwater Technology</conf-name>
          <conf-acronym>UT</conf-acronym>
          <conf-loc>Chennai, India</conf-loc>
          <conf-date>February 23-25, 2015</conf-date>
          <year>2015</year>
          <page-range>1-5</page-range>
          <fpage>1</fpage>
          <lpage>5</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/UT.2015.7108303.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Grigoryan</surname>
              <given-names>A. M.</given-names>
            </name>
            <name>
              <surname>Agaian</surname>
              <given-names>S. S.</given-names>
            </name>
          </person-group>
          <article-title>Color image enhancement via combine homomorphic ratio and histogram equalization approaches: Using underwater images as illustrative examples</article-title>
          <source>Int J. Future Revolution Comput. Sci. Commun. Eng.</source>
          <year>2018</year>
          <volume>4</volume>
          <issue>5</issue>
          <page-range>36-47</page-range>
          <fpage>36</fpage>
          <lpage>47</lpage>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>A.</given-names>
              <surname>Khan</surname>
            </name>
            <name>
              <given-names>S. S. A.</given-names>
              <surname>Ali</surname>
            </name>
            <name>
              <given-names>A. S.</given-names>
              <surname>Malik</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Anwer</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Meriaudeau</surname>
            </name>
          </person-group>
          <article-title>Underwater image enhancement by wavelet based fusion</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2016 IEEE International Conference on Underwater System Technology: Theory and Applications</conf-name>
          <conf-acronym>USYS 2016</conf-acronym>
          <conf-loc>Penang, Malaysia</conf-loc>
          <conf-date>December 13-14, 2016</conf-date>
          <year>2016</year>
          <page-range>83-88</page-range>
          <fpage>83</fpage>
          <lpage>88</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/USYS.2016.7893927.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Vasamsetti</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Mittal</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Neelapu</surname>
              <given-names>B. C.</given-names>
            </name>
            <name>
              <surname>Sardana</surname>
              <given-names>H. K.</given-names>
            </name>
          </person-group>
          <article-title>Wavelet based perspective on variational enhancement technique for underwater imagery</article-title>
          <source>Ocean Eng.</source>
          <year>2017</year>
          <volume>141</volume>
          <page-range>88-100</page-range>
          <fpage>88</fpage>
          <lpage>100</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1016/j.oceaneng.2017.06.012</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Fortino</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Qi</surname>
              <given-names>L. Z.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>W. Q.</given-names>
            </name>
            <name>
              <surname>Liotta</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>An experimental-based review of image enhancement and image restoration methods for underwater imaging</article-title>
          <source>IEEE Access</source>
          <year>2019</year>
          <volume>7</volume>
          <page-range>140233-140251</page-range>
          <fpage>140233</fpage>
          <lpage>140251</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ACCESS.2019.2932130</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>X. Y.</given-names>
              <surname>Fu</surname>
            </name>
            <name>
              <given-names>Z. W.</given-names>
              <surname>Fan</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Ling</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Huang</surname>
            </name>
            <name>
              <given-names>X. H.</given-names>
              <surname>Ding</surname>
            </name>
          </person-group>
          <article-title>Two-step approach for single underwater image enhancement</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2017 International Symposium on Intelligent Signal Processing and Communication Systems</conf-name>
          <conf-acronym>ISPACS</conf-acronym>
          <conf-loc>Xiamen, China</conf-loc>
          <conf-date>November 6-9, 2017</conf-date>
          <year>2017</year>
          <page-range>789-794</page-range>
          <fpage>789</fpage>
          <lpage>794</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ISPACS.2017.8266583.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ancuti</surname>
              <given-names>C. O.</given-names>
            </name>
            <name>
              <surname>Ancuti</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>De Vleeschouwer</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Bekaert</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Color balance and fusion for underwater image enhancement</article-title>
          <source>IEEE T. Image Process</source>
          <year>2018</year>
          <volume>27</volume>
          <issue>1</issue>
          <page-range>379-393</page-range>
          <fpage>379</fpage>
          <lpage>393</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/TIP.2017.2759252</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>K. M.</given-names>
              <surname>He</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Sun</surname>
            </name>
            <name>
              <given-names>X. O.</given-names>
              <surname>Tang</surname>
            </name>
          </person-group>
          <article-title>Single image haze removal using dark channel prior</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2009 IEEE Conference on Computer Vision and Pattern Recognition, Miami</conf-name>
          <conf-loc>FL</conf-loc>
          <conf-date>June 20-25, 2009</conf-date>
          <year>2009</year>
          <page-range>1956-1963</page-range>
          <fpage>1956</fpage>
          <lpage>1963</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/CVPR.2009.5206515.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chiang</surname>
              <given-names>J. Y.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Y. C.</given-names>
            </name>
          </person-group>
          <article-title>Underwater image enhancement by wavelength compensation and dehazing</article-title>
          <source>IEEE T. Image Process</source>
          <year>2012</year>
          <volume>21</volume>
          <issue>4</issue>
          <page-range>1756-1769</page-range>
          <fpage>1756</fpage>
          <lpage>1769</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/TIP.2011.2179666</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Drews</surname>
              <given-names>P. L. J.</given-names>
            </name>
            <name>
              <surname>Nascimento</surname>
              <given-names>E. R.</given-names>
            </name>
            <name>
              <surname>Botelho</surname>
              <given-names>S. S. C.</given-names>
            </name>
            <name>
              <surname>Montenegro Campos</surname>
              <given-names>M. F.</given-names>
            </name>
          </person-group>
          <article-title>Underwater depth estimation and image restoration based on single images</article-title>
          <source>IEEE Comput. Graph.</source>
          <year>2016</year>
          <volume>36</volume>
          <issue>2</issue>
          <page-range>24-35</page-range>
          <fpage>24</fpage>
          <lpage>35</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/MCG.2016.26</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>J. Y.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Underwater image enhancement via extended multi-scale Retinex</article-title>
          <source>Neurocomputing</source>
          <year>2017</year>
          <volume>245</volume>
          <page-range>1-9</page-range>
          <fpage>1</fpage>
          <lpage>9</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1016/j.neucom.2017.03.029</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Fu</surname>
              <given-names>X. Y.</given-names>
            </name>
            <name>
              <surname>Zhuang</surname>
              <given-names>P. X.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Liao</surname>
              <given-names>Y. H.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X. P.</given-names>
            </name>
            <name>
              <surname>Ding</surname>
              <given-names>X. H.</given-names>
            </name>
          </person-group>
          <article-title>A retinex-based enhancing approach for single underwater image</article-title>
          <source>2014 IEEE International Conference on Image Processing, Paris, France</source>
          <publisher-name>October 27-30</publisher-name>
          <year>2014</year>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICIP.2014.7025927</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>J.</given-names>
              <surname>Perez</surname>
            </name>
            <name>
              <given-names>A. C.</given-names>
              <surname>Attanasio</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Nechyporenko</surname>
            </name>
            <name>
              <given-names>P. J.</given-names>
              <surname>Sanz</surname>
            </name>
          </person-group>
          <article-title>A deep learning approach for underwater image enhancement</article-title>
          <publisher-name>Springer</publisher-name>
          <conf-name>Biomedical Applications Based on Natural and Artificial Computing, IWINAC 2017, Corunna</conf-name>
          <conf-loc>Spain</conf-loc>
          <conf-date>June 19-23, 2017</conf-date>
          <year>2017</year>
          <page-range>183-192</page-range>
          <fpage>183</fpage>
          <lpage>192</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/978-3-319-59773-7_19.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>Y.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Cao</surname>
            </name>
            <name>
              <given-names>Z. F.</given-names>
              <surname>Wang</surname>
            </name>
          </person-group>
          <article-title>A deep CNN method for underwater image enhancement</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2017 IEEE International Conference on Image Processing</conf-name>
          <conf-acronym>ICIP</conf-acronym>
          <conf-loc>Beijing, China</conf-loc>
          <conf-date>September 17-20, 2017</conf-date>
          <year>2017</year>
          <page-range>1382-1386</page-range>
          <fpage>1382</fpage>
          <lpage>1386</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICIP.2017.8296508.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Anwar</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>C. Y.</given-names>
            </name>
            <name>
              <surname>Porikli</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Deep underwater image enhancement</article-title>
          <source>arXiv</source>
          <year>2018</year>
          <volume>2018</volume>
          <pub-id pub-id-type="doi">https://doi.org/10.48550/arXiv.1807.03528</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>G. Y.</given-names>
            </name>
            <name>
              <surname>Qi</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>C. F.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>H. Y.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>Z. B.</given-names>
            </name>
          </person-group>
          <article-title>Underwater image enhancement with a deep residual framework</article-title>
          <source>IEEE Access</source>
          <year>2019</year>
          <volume>7</volume>
          <page-range>94614-94629</page-range>
          <fpage>94614</fpage>
          <lpage>94629</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ACCESS.2019.2928976</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Fabbri</surname>
            </name>
            <name>
              <given-names>M. J.</given-names>
              <surname>Islam</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Sattar</surname>
            </name>
          </person-group>
          <article-title>Enhancing Underwater imagery using generative adversarial networks</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2018 IEEE International Conference on Robotics and Automation</conf-name>
          <conf-acronym>ICRA</conf-acronym>
          <conf-loc>Brisbane, QLD, Australia</conf-loc>
          <conf-date>May 21-25, 2018</conf-date>
          <year>2018</year>
          <page-range>7159-7165</page-range>
          <fpage>7159</fpage>
          <lpage>7165</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICRA.2018.8460552.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>C. Y.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>J. C.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>C. L.</given-names>
            </name>
          </person-group>
          <article-title>Emerging from water: underwater image color correction based on weakly supervised color transfer</article-title>
          <source>IEEE Signal Proc Let.</source>
          <year>2018</year>
          <volume>25</volume>
          <issue>3</issue>
          <page-range>323-327</page-range>
          <fpage>323</fpage>
          <lpage>327</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/LSP.2018.2792050</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>H. H.</given-names>
              <surname>Yang</surname>
            </name>
            <name>
              <given-names>K. C.</given-names>
              <surname>Huang</surname>
            </name>
            <name>
              <given-names>W. T.</given-names>
              <surname>Chen</surname>
            </name>
          </person-group>
          <article-title>LAFFNet: A lightweight adaptive feature fusion network for underwater image enhancement</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2021 IEEE International Conference on Robotics and Automation</conf-name>
          <conf-acronym>ICRA</conf-acronym>
          <conf-loc>Xi'an, China</conf-loc>
          <conf-date>May 30-June 5, 2021</conf-date>
          <year>2021</year>
          <page-range>685-692</page-range>
          <fpage>685</fpage>
          <lpage>692</lpage>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>G.</given-names>
              <surname>Xu</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Jian</surname>
            </name>
            <name>
              <given-names>H. D.</given-names>
              <surname>Pan</surname>
            </name>
            <name>
              <given-names>Z. G.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>H. B.</given-names>
              <surname>Gong</surname>
            </name>
          </person-group>
          <article-title>An image enhancement method based on gamma correction</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>IEEE 2009 Second International Symposium on Computational Intelligence and Design, Changsha</conf-name>
          <conf-loc>China</conf-loc>
          <conf-date>December 12-14, 2009</conf-date>
          <year>2009</year>
          <page-range>60-63</page-range>
          <fpage>60</fpage>
          <lpage>63</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ISCID.2009.22.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Ancuti</surname>
            </name>
            <name>
              <given-names>C. O.</given-names>
              <surname>Ancuti</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Haber</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Bekaert</surname>
            </name>
          </person-group>
          <article-title>Enhancing underwater images and videos by fusion</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2012 IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI</conf-name>
          <conf-loc>USA</conf-loc>
          <conf-date>June 16-21, 2012</conf-date>
          <year>2012</year>
          <page-range>81-88</page-range>
          <fpage>81</fpage>
          <lpage>88</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/CVPR.2012.6247661.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Munadi</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Muchtar</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Maulina</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Pradhan</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Image enhancement for tuberculosis detection using deep learning</article-title>
          <source>IEEE Access</source>
          <year>2020</year>
          <volume>8</volume>
          <page-range>217897-217907</page-range>
          <fpage>217897</fpage>
          <lpage>217907</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ACCESS.2020.3041867</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Xu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Ji</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>Fog removal from color images using contrast limited adaptive histogram equalization</article-title>
          <source>2009 2nd International Congress on Image and Signal Processing, Tianjin, China</source>
          <publisher-name>October 17-19</publisher-name>
          <year>2009</year>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/CISP.2009.5301485</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fu</surname>
              <given-names>Q. Q.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z. B.</given-names>
            </name>
            <name>
              <surname>Celenk</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>A. P.</given-names>
            </name>
          </person-group>
          <article-title>A Poshe-based optimum clip-limit contrast enhancement method for ultrasonic logging images</article-title>
          <source>Sensors</source>
          <year>2018</year>
          <volume>18</volume>
          <issue>1</issue>
          <pub-id pub-id-type="doi">https://doi.org/10.3390/s18113954</pub-id>
          <pub-id pub-id-type="publisher-id">3954</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kingma</surname>
              <given-names>D. P.</given-names>
            </name>
            <name>
              <surname>Ba</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Adam: A method for stochastic optimization</article-title>
          <source>arXiv</source>
          <year>2017</year>
          <volume>2017</volume>
          <pub-id pub-id-type="doi">https://doi.org/10.48550/arXiv.1412.6980</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Islam</surname>
              <given-names>M. J.</given-names>
            </name>
            <name>
              <surname>Xia</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Sattar</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Fast underwater image enhancement for improved visual perception</article-title>
          <source>IEEE Robot. Autom. Let.</source>
          <year>2020</year>
          <volume>5</volume>
          <issue>2</issue>
          <page-range>3227-3234</page-range>
          <fpage>3227</fpage>
          <lpage>3234</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/LRA.2020.2974710</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Han</surname>
              <given-names>J. L.</given-names>
            </name>
            <name>
              <surname>Shoeiby</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Malthus</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Botha</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Anstee</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Anwar</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Armin</surname>
              <given-names>M. A.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>H. D.</given-names>
            </name>
            <name>
              <surname>Petersson</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Underwater image restoration via contrastive learning and a real-world dataset</article-title>
          <source>arXiv</source>
          <year>2021</year>
          <volume>2021</volume>
          <pub-id pub-id-type="doi">https://doi.org/10.48550/arXiv.2106.10718</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>C. Y.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>C. L.</given-names>
            </name>
            <name>
              <surname>Ren</surname>
              <given-names>W. Q.</given-names>
            </name>
            <name>
              <surname>Cong</surname>
              <given-names>R. M.</given-names>
            </name>
            <name>
              <surname>Hou</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Kwong</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Tao</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>An underwater image enhancement benchmark dataset and beyond</article-title>
          <source>arXiv</source>
          <year>2019</year>
          <volume>2019</volume>
          <pub-id pub-id-type="doi">https://doi.org/10.48550/arXiv.1901.05495</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Raveendran</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Patil</surname>
              <given-names>M. D.</given-names>
            </name>
            <name>
              <surname>Birajdar</surname>
              <given-names>G. K.</given-names>
            </name>
          </person-group>
          <article-title>Underwater image enhancement: A comprehensive review, recent trends, challenges and applications</article-title>
          <source>Artif Intell Rev.</source>
          <year>2021</year>
          <volume>54</volume>
          <page-range>5413-5467</page-range>
          <fpage>5413</fpage>
          <lpage>5467</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/s10462-021-10025-z</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Brooks</surname>
              <given-names>A. C.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Pappas</surname>
              <given-names>T. N.</given-names>
            </name>
          </person-group>
          <article-title>Structural similarity quality metrics in a coding context: Exploring the space of realistic distortions</article-title>
          <source>IEEE T. Image Process</source>
          <year>2008</year>
          <volume>17</volume>
          <issue>8</issue>
          <page-range>1261-1273</page-range>
          <fpage>1261</fpage>
          <lpage>1273</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/TIP.2008.926161</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_35">
        <label>35.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Salomon</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <source>Data Compression, UK, London: SpringerLink</source>
          <publisher-name>Springer</publisher-name>
          <year>2007</year>
          <page-range>281-282</page-range>
          <fpage>281</fpage>
          <lpage>282</lpage>
        </element-citation>
      </ref>
      <ref id="ref_36">
        <label>36.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Sowmya</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>New image quality evaluation metric for underwater video</article-title>
          <source>IEEE Signal Proc Let.</source>
          <year>2014</year>
          <volume>21</volume>
          <issue>0</issue>
          <page-range>1215-1219</page-range>
          <fpage>1215</fpage>
          <lpage>1219</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/LSP.2014.2330848</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_37">
        <label>37.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Panetta</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Agaian</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Human-visual-system-inspired underwater image quality measures</article-title>
          <source>IEEE J. Magazine</source>
          <year>2016</year>
          <volume>41</volume>
          <issue>3</issue>
          <page-range>541-551</page-range>
          <fpage>541</fpage>
          <lpage>551</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/JOE.2015.2469915</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_38">
        <label>38.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bora</surname>
              <given-names>D. J.</given-names>
            </name>
            <name>
              <surname>Gupta</surname>
              <given-names>A. K.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>F. A.</given-names>
            </name>
          </person-group>
          <article-title>Comparing the performance of LAB and HSV color spaces with respect to color image segmentation</article-title>
          <source>arXiv</source>
          <year>2015</year>
          <volume>2015</volume>
          <pub-id pub-id-type="doi">https://doi.org/10.48550/arXiv.1506.01472</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>