<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IDA</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Information Dynamics and Applications</journal-title>
        <abbrev-journal-title abbrev-type="issn">Inf. Dyn. Appl.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IDA</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-1494</issn>
      <issn publication-format="print">2958-1486</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-Xf5NWi9g6yVYW2MNFZRetxU0VUJgtIxB</article-id>
      <article-id pub-id-type="doi">10.56578/ida030102</article-id>
      <title-group>
        <article-title>Enhancing Image Captioning and Auto-Tagging Through a FCLN with Faster R-CNN Integration</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Deore</surname>
            <given-names>Shalaka Prasad</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5322-779X</contrib-id>
          <email>anudeore9@gmail.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Bagwan</surname>
            <given-names>Taibah Sohail</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true"> </contrib-id>
          <email>taibahbagwan10@gmail.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Bhukan</surname>
            <given-names>Prachiti Sunil</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true"> </contrib-id>
          <email>prachitibhukan@gmail.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Rajpal</surname>
            <given-names>Harsheen Tejindersingh</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true"> </contrib-id>
          <email>harsheenk.rajpal@gmail.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="2">2</xref>
          <name>
            <surname>Gade</surname>
            <given-names>Shantanu Bharat</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true"> </contrib-id>
          <email>u2321872@uel.ac.uk</email>
        </contrib>
        <aff id="1">Computer Engineering, MES College of Engineering, S. P. Pune University, 411001 Pune, India</aff>
        <aff id="2">School of Architecture, Computing and Engineering, University of East London, E16 2RD London, UK</aff>
      </contrib-group>
      <year>2024</year>
      <volume>3</volume>
      <issue>1</issue>
      <fpage>12</fpage>
      <lpage>20</lpage>
      <page-range>12-20</page-range>
      <history>
        <date date-type="received">
          <month>10</month>
          <day>26</day>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <month>01</month>
          <day>24</day>
          <year>2024</year>
        </date>
        <date date-type="pub">
          <month>02</month>
          <day>02</day>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the authors</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license>. Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the <a href='https://creativecommons.org/licenses/by/4.0/' target='_blank' class='text-yellow-700 hover:underline'>CC BY 4.0 license</a>.</license>
      </permissions>
      <abstract><p>In the realm of automated image captioning, which entails generating descriptive text for images, the fusion of Natural Language Processing (NLP) and computer vision techniques is paramount. This study introduces the Fully Convolutional Localization Network (FCLN), a novel approach that concurrently addresses localization and description tasks within a singular forward pass. It maintains spatial information and avoids detail loss, streamlining the training process with consistent optimization. The foundation of FCLN is laid by a Convolutional Neural Network (CNN), adept at extracting salient image features. Central to this architecture is a Localization Layer, pivotal in precise object detection and caption generation. The FCLN architecture amalgamates a region detection network, reminiscent of Faster Region-CNN (R-CNN), with a captioning network. This synergy enables the production of contextually meaningful image captions. The incorporation of the Faster R-CNN framework facilitates region-based object detection, offering precise contextual understanding and inter-object relationships. Concurrently, a Long Short-Term Memory (LSTM) network is employed for generating captions. This integration yields superior performance in caption accuracy, particularly in complex scenes. Evaluations conducted on the Microsoft Common Objects in Context (MS COCO) test server affirm the model's superiority over existing benchmarks, underscoring its efficacy in generating precise and context-rich image captions.</p></abstract>
      <kwd-group>
        <kwd>Faster Region Convolutional Neural Network (R-CNN)</kwd>
        <kwd>Long Short-Term Memory (LSTM)</kwd>
        <kwd>Image captioning</kwd>
        <kwd>Object detection</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors">5</count>
        <fig-count>3</fig-count>
        <table-count>2</table-count>
        <ref-count>20</ref-count>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec disp-level="level1" sec-type="intro">
      <title>1. Introduction</title>
      <p>The domains of object detection and image captioning stand as pivotal elements in the expansive field of computer vision. These tasks, central to numerous applications including robotics, medical imaging, content moderation, and assisting visually impaired individuals, involve the identification and descriptive articulation of objects within images. Such capabilities are essential for enabling machines to comprehend visual data and interact effectively with their surroundings. Faster R-CNN has emerged as a preeminent method in object detection. It is distinguished by its dual-stage framework, which combines deep CNNs with region proposal networks (RPNs). This approach has garnered significant attention due to its high levels of accuracy and efficiency in localizing and classifying objects within images.</p><p>Concurrently, the task of image captioning requires the generation of descriptive textual captions that accurately convey the visual content of images. This necessitates an intricate understanding of both the visual and semantic dimensions of the images, bridging the gap between visual perception and textual representation. The fusion of computer vision and NLP enables these models to effectively translate visual data into coherent textual descriptions. In this study, the integration of Faster R-CNN within the realms of object detection and image captioning is critically examined. The objective is to harness the advanced detection capabilities of Faster R-CNN to augment the accuracy and efficacy of image captioning systems. By leveraging the rich spatial information and detailed object features provided by Faster R-CNN, the precision of generated captions can be significantly enhanced, yielding more accurate depictions of objects within images.</p><p>In this research, a comprehensive examination of the Faster R-CNN framework is conducted, with a focus on delineating its crucial components and underlying mechanisms. The challenges inherent in integrating Faster R-CNN into image captioning frameworks are explored, along with the pertinent techniques devised to surmount these obstacles. The study also encompasses a discussion on the dataset requisites, training methodologies, and evaluation metrics deployed in the experimental validation of the proposed approach. This investigation contributes significantly in two distinct aspects. Firstly, it elucidates the proficiency of Faster R-CNN in object detection, evidenced by its exemplary performance on established benchmark datasets. Secondly, the research elucidates how the incorporation of Faster R-CNN into image captioning systems augments their capability, resulting in textual descriptions of images that are both more accurate and informative.</p><p>The primary objective of this paper is to shed light on the potential of employing Faster R-CNN as a dual-purpose framework, serving both object detection and image captioning tasks. By capitalizing on the robustness of this model, a considerable advancement in computer vision systems is anticipated, enhancing their proficiency in understanding and interpreting visual data, which holds immense potential for real-world application enhancement. The intersection of computer vision and NLP has made notable strides in generating image captions, thereby bridging the visual-textual comprehension divide. This advancement is pivotal, as it equips machines with the capability to interpret and articulate the content of images, marking a significant stride towards realizing artificial intelligence. Nonetheless, current image captioning methodologies often falter in accurately capturing the intricate context of images, consequently restricting their effectiveness in generating insightful descriptions. To address this gap, the present study proposes an innovative image captioning system that capitalizes on contextual information, thereby markedly improving the precision of image descriptions.</p><p>The core aim of this study is the development and demonstration of a visual feature extraction component, leveraging the state-of-the-art Faster R-CNN for the extraction of pertinent visual features from input images. Subsequently, captions are generated using advanced neural language models, notably CNN and Artificial Neural Networks (ANN). Building upon previous research, a transition from Recurrent Neural Networks (RNN) to CNN is undertaken in the neural network architecture, aiming to enhance performance. Furthermore, this research diverges from the traditional focus on individual words, opting instead to explore the use of phrases as fundamental units in order to augment both the semantic and syntactic quality of the generated captions.</p><p>This paper presents a thorough analysis of the proposed novel approach to image captioning, highlighting its architectural enhancements and the integration of contextual information. This is pursued with the objective of surmounting the limitations inherent in existing methodologies and providing captions that more accurately reflect the relationships among entities within images. An exploration of the data set requirements, training protocols, and evaluation metrics employed in assessing the system's performance is also included. A comparative analysis of the results obtained from the modified neural network, juxtaposed against existing models, is conducted to underscore its superiority in generating more precise and meaningful image captions. The implications of this research are far-reaching, potentially advancing machine-based image comprehension and supporting a spectrum of applications spanning from image retrieval to autonomous systems and human-machine interaction. Subsequent sections encompass an overview of related works in image captioning, an exposition of the proposed model's architecture, a delineation of the data set and training methodologies, a presentation of experimental results, and a conclusion that offers perspectives on future enhancements in image captioning systems.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>2. Related work</title>
      <p>The domain of automated image captioning, particularly when employing the Faster R-CNN model, has received considerable focus in recent scholarly research. This literature review section aims to encapsulate key contributions that have significantly influenced the evolution of automated image captioning methodologies utilizing Faster R-CNN. Anderson et al. [<xref ref-type="bibr" rid="ref_1">1</xref>] introduced a novel approach incorporating both bottom-up and top-down attention mechanisms in the context of image captioning and Visual Question Answering. This research underscored the pivotal role of attention mechanisms in enhancing image comprehension and the accuracy of caption generation. The integration of Faster R-CNN for object detection, combined with a top-down attention mechanism, resulted in marked improvements in caption generation, as evidenced by their findings. Lu et al. [<xref ref-type="bibr" rid="ref_2">2</xref>] explored an adaptive attention mechanism employing a visual sentinel for image captioning. This model was distinguished by its dynamic learning capability, which enabled it to focus on varying regions of an image during the process of caption generation. The enhanced relevance and contextual understanding of the captions generated by this model illustrated the efficacy of adaptive attention mechanisms in identifying salient features within images. The adoption of Faster R-CNN for object detection, coupled with the integration of semantic knowledge, significantly elevated the performance of their image captioning approach.</p><p>A seminal contribution in the field was made by Xu et al. [<xref ref-type="bibr" rid="ref_3">3</xref>] through their work on neural image caption generation with visual attention. They introduced an attention-based model that was adept at focusing on pertinent regions of an image while formulating captions. The integration of a CNN for feature extraction, along with a LSTM network-based decoder, significantly bolstered the quality and pertinence of the image captions generated by their model. The introduction of the Faster R-CNN model by Ren et al. [<xref ref-type="bibr" rid="ref_4">4</xref>] marked a significant advancement in the field of object detection. This framework, which integrates RPNs with a CNN for feature extraction, has facilitated real-time object detection. The implementation of Faster R-CNN in automated image captioning models has proven to be instrumental in enhancing object localization and recognition accuracy, a development corroborated by numerous scholarly studies. Xiao et al. [<xref ref-type="bibr" rid="ref_5">5</xref>] presented novel LSTM attention based model (ALSTM) for tagging images. In existing models, ALSTM learns to refine input vector via sequential context information and network hidden states, instead of typical LSTM. As a result, ALSTM is able to react to more pertinent characteristics, including spatial attention, visual relations, and a greater focus on the most pertinent context terms. Additionally, Chen and Hu [<xref ref-type="bibr" rid="ref_6">6</xref>] presented a novel text-based visual attention model for image captioning that employed self-attention mechanisms, eschewing recurrent structures. This simplification of the captioning process, while maintaining competitive performance, illustrated an alternative methodology in image captioning that relied solely on attention mechanisms. It gives previously generated text, automatically eliminates unnecessary information to focus on a single salient object. Ren et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] delved into deep reinforcement learning-based image captioning, employing an embedding reward model. This approach incorporated Faster R-CNN for object detection and harnessed LSTM-based caption generation. By applying reinforcement learning techniques, their model demonstrated marked improvements in the quality of generated image captions. Zhan et al. [<xref ref-type="bibr" rid="ref_8">8</xref>], presented model which will improve both picture representation and caption generation by better utilizing the semantics found in captions. Initially, this model uses supervised multi-instance learning weak model to build graph which explains caption-guided visual relationship. Next, contextual and nearby nodes with their textual and visual characteristics are added to improve the representation. Using this author achieved promissing results. Many researchers used faster R-CNN model for image captioning and achieved promising results [<xref ref-type="bibr" rid="ref_9">9</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>].</p><p>Zhou et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] introduced an innovative approach VQA by embedding external knowledge and attribute-based reasoning into their model. Capsul Network is implemented which uses dynamic routing to achieve the attention output. This method computes coupling coefficients between the underlying and output capsules in order to update the attention weights. Omri et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] and Zhu and Yan [<xref ref-type="bibr" rid="ref_14">14</xref>] proposed deep learning method to improve results of automated image captioning. In the study of Thangave et al. [<xref ref-type="bibr" rid="ref_15">15</xref>], provides a heterogeneous data fusion-based deep learning model for image captioning. The descriptive text is created, the long short-term memory is used for decoding, and mask recurrent neural networks i.e faster R-CNN are used in the coding layer. In the study of Rehab and Hahn [<xref ref-type="bibr" rid="ref_16">16</xref>] also porved that deep leaning approache is very effective for improving results of image captioning. In the study of Abdelrahman et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] suggested and investigated two approaches to the picture captioning problem utilizing two distinct self-supervised learning models. The comprehensive study done in several studies [<xref ref-type="bibr" rid="ref_18">18</xref>], [<xref ref-type="bibr" rid="ref_19">19</xref>], [<xref ref-type="bibr" rid="ref_20">20</xref>] suggested to use deep learning models to get promissing results in image captioning especially R-CNN proved the best model.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>3. Proposed approach</title>
      <p>The methodology adopted for automated image captioning in this study integrates the capabilities of Faster R-CNN and LSTM, utilizing a combination of bottom-up and top-down attention mechanisms, as depicted in <xref ref-type="fig" rid="fig_1">Figure 1</xref>. The integration of these mechanisms leverages the object detection capabilities of Faster R-CNN and the FCLN to pinpoint salient visual features within an image. This approach encompasses the use of a pre-trained Faster R-CNN model for the extraction of object features, a LSTM network for caption generation, and a bottom-up/top-down attention mechanism to focus on significant object regions.</p><p>The method employs Faster R-CNN to detect objects by generating region proposals, which serve as inputs to the FCLN. The FCLN performs fine-grained localization in the images, mapping them to their respective class labels. Subsequently, the features extracted from the images are processed by the LSTM model, which is responsible for generating a caption that describes the image.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>Architecture of the proposed system</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/1/img_mq5Puk54Ywod3ABd.png"/>
        </fig>
      
      <p>The proposed methodology contains the following steps.</p>
      
        <sec disp-level="level2">
          
            <title>3.1. Dataset preparation</title>
          
          <p>For image captioning, a dataset comprising images paired with corresponding captions is imperative. The MS COCO dataset, known for its diversity in image content and complexity, was utilized in this study. It features annotations for 80 different object categories. The dataset is segmented into various splits, including training, validation, and test sets. For the purposes of this study, a selection of 5000 images was made for testing and another 5000 for validation. Each image in the MS COCO dataset is accompanied by five captions and labels for object classes, providing a comprehensive set for training and evaluating the model.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.2. Object detection using faster r-cnn</title>
          
          <p>The pre-trained Faster R-CNN model, based on ResNet-101, was employed to extract features and perform object detection on the MS COCO dataset. The model underwent fine-tuning and was trained on the training data. It generated region proposals using the RPN, identifying potential object bounding boxes to accurately detect objects within the dataset.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.3. Bottom-up attention</title>
          
          <p>Bottom-up features were extracted from the Faster R-CNN model, representing individual object regions and their associated feature vectors. These object regions were then ranked based on their importance scores or confidence values. The top-k object regions, determined by their importance scores (where k is a predefined parameter), were processed by the FCLN.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.4. Lstm for caption generation</title>
          
          <p>The LSTM network is initialized for the generation of captions. Inputs to the LSTM include the visual features derived from the bottom-up attention mechanism and the words generated previously. Words are represented as dense vectors via an embedding layer. The LSTM network undergoes training with caption pairs from the dataset, predicting subsequent words based on earlier words and visual features. During training, ground truth words are inputted into the LSTM network at each timestep. A softmax layer calculates the probability distribution over the vocabulary for each word.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.5. Top-down attention</title>
          
          <p>The hidden state of the LSTM serves as a query to focus on the visual features obtained from the bottom-up attention mechanism. Techniques such as dot product are employed to compute attention weights between the LSTM hidden state and visual features. These attention weights are crucial for computing the weighted sum of visual features, resulting in the formation of a context vector.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.6. Caption generation</title>
          
          <p>This context vector, obtained from the top-down attention mechanism, is amalgamated with the current timestep’s LSTM hidden state. The amalgamated vector is then inputted into the LSTM for the generation of the subsequent word. This procedure is iteratively executed until an end-of-sentence token is produced or the maximum caption length is reached.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>3.7. Training and evaluation</title>
          
          <p>The model undergoes end-to-end training through backpropagation and optimization techniques. The discrepancy between generated captions and provided captions is measured using cross-entropy loss during training. For evaluating the model's performance, the Bilingual Evaluation Understudy (BLEU) score metrics are utilized. The BLEU score, a prominent metric in various NLP tasks, assesses the similarity between generated text and the captions in the dataset.</p>
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>4. Implementation</title>
      
        <sec disp-level="level2">
          
            <title>4.1. Faster r-cnn model</title>
          
          <p>In the employed approach, the Faster R-CNN model, pre-trained on the ResNet-101 backbone network, was utilized. ResNet-101, characterized by its 101-layer architecture, is responsible for comprehending the input image through high-level visual feature extraction. These features are represented via a feature map, which is subsequently inputted into the RPN. The RPN, integrated with the backbone network, generates candidate object proposals, essentially potential bounding boxes encapsulating objects of interest. The generation of these proposals is influenced by anchor boxes of predefined shapes, scales, and aspect ratios. The bounding boxes proposed by the RPN are forwarded to the Region of Interest (RoI) pooling layer, which extracts fixed-size feature maps for each region from the output feature maps of the backbone network. Subsequently, a fully connected layer, attached to the RoI pooling layer, undertakes object classification within the proposed regions and bounding box regression to refine their coordinates. The process concludes with the application of non-maximum suppression, filtering out redundant bounding box proposals based on confidence scores and overlapping regions. The highest-scoring proposals are retained, while others are discarded.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>4.2. Bottom-up attention mechanism</title>
          
          <p>The Bottom-up Attention Mechanism synergizes the functionalities of Faster R-CNN and FCLN in the realm of image caption generation. This mechanism commences with Faster R-CNN extracting visual features and identifying regions of interest within the image. These regions are then assigned importance scores, facilitating the attention mechanism during caption generation. Integrated with FCLN, the attention mechanism at each timestep dynamically focuses on the most relevant regions, considering the current context and the evolving captions. FCLN aids in further refining the localization of these selected regions. Crucially, attention weights are computed, factoring in both the importance scores and the detailed localization provided by FCLN. This ensures that the captions generated are not only descriptive but also centered on the most significant visual aspects of the image, thereby elevating the quality of the captions.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>4.3. Lstm model</title>
          
          <p>In this study, the LSTM network is employed for the sequential generation of image captions. Image features, once extracted, are inputted into the LSTM network along with word embeddings. The LSTM model is subjected to training utilizing a dataset that pairs images with their respective captions. At each timestep, the model processes the current word embedding and the preceding hidden state to forecast the subsequent word in the caption. The optimization of the model during training involves updating the LSTM weights to enhance performance. Key hyperparameters are established, with the maximum caption length set to 25 words and the word embedding dimension fixed at 250. The LSTM model, comprising two layers, is designed to maintain hidden states and memory cells, vital for contextual comprehension and the generation of captions. The hidden states from the first layer of LSTM are channeled as input to the second layer, thereby enabling the model to discern and encapsulate more complex relationships inherent in the caption generation process. Throughout the process, the LSTM layers, at each timestep, are tasked with predicting the next word in the caption, based on the current input and historical hidden states. This iterative process persists until the predetermined maximum caption length is reached.</p>
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>5. Dataset</title>
      <p>For the evaluation of the proposed captioning model, the MS COCO 2014 captions dataset, a benchmark in the field, was employed. The Karpathy splits, previously utilized in various studies for result comparison, were adopted for model hyperparameter validation and offline testing. The MS COCO 2014 dataset encompasses 123,287 training images, each paired with five captions. In addition, separate sets, each comprising 5,000 images, were designated for validation and testing. These sets were instrumental in conjunction with the submissions to the MS COCO test server. The training of the model was conducted on a merged dataset of training and validation images, totaling 123,000 images.</p><p>Text pre-processing on the captions was executed in line with standard practices, entailing conversion of all sentences to lowercase, tokenization based on white spaces, and exclusion of words appearing fewer than five times. Consequently, the model's vocabulary comprised 10,010 words. For the assessment of caption accuracy, widely recognized automatic evaluation metrics, specifically Metric for Evaluation of Translation with Explicit ORdering (METEOR) and BLEU, were utilized. These metrics provide objective criteria for evaluating the model’s performance against the ground truth captions. The selection of the MS COCO dataset and these standard evaluation metrics facilitates meaningful comparisons between the proposed model and existing methods in automated image captioning.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>6. Result</title>
      <p>In this study, a two-step training approach was adopted for the image caption generator employing Faster R-CNN and LSTM. Initially, the model underwent pretraining for 30 epochs, utilizing the Adaptive Moment Estimation (ADAM) optimizer alongside softmax cross-entropy loss. Performance monitoring occurred on the validation set, which comprised 5000 images. An early-stopping mechanism was implemented to optimize results. The training, focused on cross-entropy loss, was executed on a single Graphic Processing Unit (GPU) and spanned approximately one day.</p><p>The pretraining phase was critical in initializing the model and capturing essential visual features for the generation of accurate and meaningful captions. The optimization of model parameters via cross-entropy loss was aimed at enhancing the performance and generalization capabilities of the caption generator.</p><p>Evaluations based on multiple metrics revealed significant results. As indicated in <xref ref-type="table" rid="table_1">Table 1</xref>, the performance was assessed using BLEU, Consensus-based Image Description Evaluation (CIDEr), METEOR, and Semantic Propositional Image Caption Evaluation (SPICE) scores, which are standard metrics for evaluating the quality and accuracy of generated captions.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>Evaluation metrics</caption>
          <abstract/>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>BLEU -1</p></td><td colspan="1" rowspan="1"><p>70.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>BLEU -2</p></td><td colspan="1" rowspan="1"><p>52.6</p></td></tr><tr><td colspan="1" rowspan="1"><p>BLEU -3</p></td><td colspan="1" rowspan="1"><p>38.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>BLEU -4</p></td><td colspan="1" rowspan="1"><p>28</p></td></tr><tr><td colspan="1" rowspan="1"><p>SPICE</p></td><td colspan="1" rowspan="1"><p>23.7</p></td></tr><tr><td colspan="1" rowspan="1"><p>METEOR</p></td><td colspan="1" rowspan="1"><p>25.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>CIDEr</p></td><td colspan="1" rowspan="1"><p>116.8</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>BLEU scores achieved were 70.3 for BLEU-1 and 28 for BLEU-4, as detailed in <xref ref-type="table" rid="table_1">Table 1</xref>. BLEU metric, predominantly used in machine translation and image captioning, evaluates the quality and accuracy of machine-generated text. It measures the similarity between the generated text and a set of reference texts. The BLEU-1 score reflects the precision of unigram matches between the text produced by the model and the reference texts, while BLEU-4 score assesses the precision of matching n-grams (sequences of four words). Higher BLEU scores, as shown in <xref ref-type="table" rid="table_2">Table 2</xref>, denote better alignment between the model-generated text and reference texts, underlining the efficacy and accuracy of the image captioning model.</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>BLEU score comparison</caption>
          <abstract/>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>MODEL</p></td><td colspan="1" rowspan="1"><p>BLEU-1</p></td><td colspan="1" rowspan="1"><p>BLEU-2</p></td><td colspan="1" rowspan="1"><p>BLEU-3</p></td><td colspan="1" rowspan="1"><p>BLEU-4</p></td></tr><tr><td colspan="1" rowspan="1"><p>NIC</p></td><td colspan="1" rowspan="1"><p>65.5</p></td><td colspan="1" rowspan="1"><p>45.2</p></td><td colspan="1" rowspan="1"><p>30.8</p></td><td colspan="1" rowspan="1"><p>23.5</p></td></tr><tr><td colspan="1" rowspan="1"><p>LRCN</p></td><td colspan="1" rowspan="1"><p>63.68</p></td><td colspan="1" rowspan="1"><p>42.18</p></td><td colspan="1" rowspan="1"><p>29.31</p></td><td colspan="1" rowspan="1"><p>20</p></td></tr><tr><td colspan="1" rowspan="1"><p>Proposed Faster R-CNN model</p></td><td colspan="1" rowspan="1"><p>70.3</p></td><td colspan="1" rowspan="1"><p>52.6</p></td><td colspan="1" rowspan="1"><p>38.1</p></td><td colspan="1" rowspan="1"><p>28</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>A METEOR score of 25.5 was attained by the model, reflecting the quality of the generated captions in terms of fluency, grammar, and overall linguistic accuracy. The METEOR score, assessing both precision and recall, indicates that higher scores correlate with more accurate and linguistically sound captions. Furthermore, a SPICE score of 31.2 was obtained, evaluating the semantic content and relevance of the captions. This metric emphasizes not only the correctness of individual words but also the semantic relationships and coherence across the entire caption. An impressive CIDEr score of 116.8 was recorded, indicating a high degree of similarity between the generated captions and the reference captions in the dataset. This score is particularly indicative of the effectiveness of the proposed model in generating captions that closely resemble the ground truth provided in the dataset.</p><p>The enhanced performance of the proposed model underscores its capability to generate more accurate and contextually relevant captions for images. This achievement highlights the integration of Faster R-CNN's object detection capabilities into the image captioning process, enabling a deeper understanding of the visual content and the generation of more informative captions. In addition to metric evaluations, a qualitative analysis was conducted. An example, depicted in <xref ref-type="fig" rid="fig_2">Figure 2</xref>, shows an image of a woman eating a piece of cake with a candle on top. The model successfully generated the caption, "A woman is eating a piece of cake with a candle," accurately capturing the main subject and activity in the image. This example illustrates the model's proficiency in identifying relevant objects, activities, and contextual details within the visual content.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>Generated caption from input image</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/1/img_DqUqhEoeTvgpNWcS.png"/>
        </fig>
      
      <p><xref ref-type="fig" rid="fig_3">Figure 3</xref>, in particular, displays an image featuring two puppies positioned adjacent to one another on a grassy field. The caption generated for this image, stating "Two puppies sitting on a grassy field next to each other," aligns precisely with the visual content. The model was effective in identifying the key elements: the subjects (puppies), their location (grassy field), and their relative positioning (sitting next to each other). This example illustrates the model's capability to provide comprehensive and descriptive captions that accurately reflect the visual content.</p>
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>Generated caption from input image</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/1/img_TN1SKDjbSZVETJky.png"/>
        </fig>
      
      <p>These instances serve as evidence of the model's ability to produce captions that are both accurate and contextually relevant. The model demonstrates its capacity to recognize vital visual cues, correctly identify objects, and succinctly describe their actions or attributes. The congruence between the generated captions and the corresponding images underscores the efficacy of the proposed approach in generating meaningful and precise captions across a diverse range of visual content.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>7. Conclusion</title>
      <p>This study has culminated in the development and training of a deep learning model for automated image captioning. The process entailed preprocessing of images and textual data, preparing them for the training phase, and enabling the model to generate captions for newly inputted images. The integration of visual attention mechanisms at both the bottom-up and top-down levels in this work facilitates a more targeted calculation of attention, focusing primarily on objects and other notable regions within the images. A potential extension of this model could involve its application in real-time image captioning, particularly in assistive technologies for the visually impaired, thereby enhancing their perception of the environment through live image descriptions.</p><p>Moreover, the research aims to bridge the gap between visual and linguistic comprehension by incorporating cutting-edge advancements in object detection. This approach presents several potential avenues for future exploration. However, even with the current methodology, substantial improvements are observed by replacing pre-trained CNN features with pre-trained bottom-up attention features. The proposed approach demonstrates competitive results across various images, signifying an improvement in the performance of the image captioning system.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p></p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="book">
          <volume/>
          <page-range>6077–6086</page-range>
          <issue/>
          <year>2018</year>
          <publisher-name>Salt Lake City, UT, USA</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>P.</given-names>
              <surname>Anderson</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>He</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Buehler</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Teney</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Johnson</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Gould</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Zhang</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/cvpr.2018.00636</pub-id>
          <article-title>Bottom-up and top-down attention for image captioning and visual question answering</article-title>
          <source>Conference on Computer Vision and Pattern Recognition</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="book">
          <volume/>
          <page-range>3242–3250</page-range>
          <issue/>
          <year>2017</year>
          <publisher-name>Honolulu, HI, USA</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>J.</given-names>
              <surname>Lu</surname>
            </name>
            <name>
              <given-names>C.</given-names>
              <surname>Xiong</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Parikh</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Socher</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/cvpr.2017.345</pub-id>
          <article-title>Knowing when to look: Adaptive attention via a visual sentinel for image captioning</article-title>
          <source>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="book">
          <volume>37</volume>
          <page-range>2048–2057</page-range>
          <issue/>
          <year>2015</year>
          <publisher-name>Lille, France</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>K.</given-names>
              <surname>Xu</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Ba</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Kiros</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>Cho</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Courville</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Salakhudinov</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Zemel</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Bengio</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi"/>
          <article-title>Show, attend and tell: Neural image caption generation with visual attention</article-title>
          <source>32nd International Conference on Machine Learning</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>39</volume>
          <page-range>1137-1149</page-range>
          <issue>6</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Ren</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>He</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Girshick</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Sun</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/tpami.2016.2577031</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Faster R-CNN: Towards real-time object detection with region proposal networks</article-title>
          <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>54</volume>
          <page-range>3157-3171</page-range>
          <issue>4</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>F.</given-names>
              <surname>Xiao</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Xue</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Shen</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Gao</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11063-022-10759-z</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A new attention-based LSTM for image captioning</article-title>
          <source>Neural Process. Lett.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>49</volume>
          <page-range>177-185</page-range>
          <issue/>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>H.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Hu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11063-018-9807-7</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Image captioning with text-based visual attention</article-title>
          <source>Neural Process. Lett.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="book">
          <volume/>
          <page-range>290–298</page-range>
          <issue/>
          <year>2017</year>
          <publisher-name>Honolulu, HI, USA</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>Z.</given-names>
              <surname>Ren</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Lv</surname>
            </name>
            <name>
              <given-names>L. J.</given-names>
              <surname>Li</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/cvpr.2017.128</pub-id>
          <article-title>Deep reinforcement learning-based image captioning with embedding reward</article-title>
          <source>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>2006</volume>
          <page-range>11807</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Zhan</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Zhou</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Qiu</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Zhu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2006.11807</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Improving image captioning with better use of captions</article-title>
          <source>arXiv preprint arXiv</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>8</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A.</given-names>
              <surname>Yadav</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>Image captioning using R-CNN &amp; LSTM deep learning mode</article-title>
          <source>Image</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="book">
          <volume>2017</volume>
          <page-range>109–118</page-range>
          <issue/>
          <year>2017</year>
          <publisher-name>Shanghai, China</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>Z.</given-names>
              <surname>Yang</surname>
            </name>
            <name>
              <given-names>Y. J.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>u. Rehman</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Huang</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1007/978-3-319-71589-6</pub-id>
          <article-title>Image captioning with object detection and localization</article-title>
          <source>9th International Conference on Image and Graphics, ICIG 2017</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>1712</volume>
          <page-range>012015</page-range>
          <issue>1</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>G.</given-names>
              <surname>Geetha</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Kirthigadevi</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Ponsam</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Karthik</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Safa</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1088/1742-6596/1712/1/012015</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Image captioning using deep convolutional neural networks (CNNs)</article-title>
          <source>J. Phys.: Conf. Ser.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conference-proceedings">
          <volume>33</volume>
          <page-range>9324-9331</page-range>
          <issue>01</issue>
          <year>2019</year>
          <publisher-name>Washington DC, USA</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>Y.</given-names>
              <surname>Zhou</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Ji</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Su</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Sun</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Chen</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1609/aaai.v33i01.33019324</pub-id>
          <article-title>Dynamic capsule attention for visual question answering</article-title>
          <source>, https://doi.org/10.1609/aaai.v33i01.33019324</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>288</page-range>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Omri</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Abdel-Khalek</surname>
            </name>
            <name>
              <given-names>M. E.</given-names>
              <surname>Khalil</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Bouslimi</surname>
            </name>
            <name>
              <given-names>G. P.</given-names>
              <surname>Joshi</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/math10030288</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Modeling of hyperparameter tuned deep learning model for automated image captioning</article-title>
          <source>Mathematics</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="book">
          <volume/>
          <page-range/>
          <issue/>
          <year>2022</year>
          <publisher-name>Xiamen, Fujian, China</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>Y.</given-names>
              <surname>Zhu</surname>
            </name>
            <name>
              <given-names>W. Q.</given-names>
              <surname>Yan</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1145/3561613.3561641</pub-id>
          <article-title>Image-Based Storytelling Using Deep Learning</article-title>
          <source>5th International Conference on Control and Computer Vision, ICCCV 2022</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>27</volume>
          <page-range>14205-14218</page-range>
          <issue/>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>K.</given-names>
              <surname>Thangavel</surname>
            </name>
            <name>
              <given-names>N.</given-names>
              <surname>Palanisamy</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Muthusamy</surname>
            </name>
            <name>
              <given-names>O. P.</given-names>
              <surname>Mishra</surname>
            </name>
            <name>
              <given-names>S. C. M.</given-names>
              <surname>Sundararajan</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Panchal</surname>
            </name>
            <name>
              <given-names>A. K.</given-names>
              <surname>Loganathan</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Ramamoorthi</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00500-023-08448-7</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A novel method for image captioning using multimodal feature fusion employing mask RNN and LSTM models</article-title>
          <source>Soft Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="book">
          <volume/>
          <page-range>1025–1034</page-range>
          <issue/>
          <year>2022</year>
          <publisher-name>Waikoloa, HI, USA</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>A.</given-names>
              <surname>Rehab</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Hahn</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/wacv51458.2022.00251</pub-id>
          <article-title>Improve image captioning by estimating the gazing patterns from the caption</article-title>
          <source>IEEE/CVF Winter Conference on Applications of Computer Vision</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range/>
          <issue/>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M.</given-names>
              <surname>Abdelrahman</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Dmitry</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Sebaitre</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.13140/RG.2.2.24293.88802</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Image captioning through self-supervised learning</article-title>
          <source>Tech. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>55</volume>
          <page-range>3833-3862</page-range>
          <issue>5</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Z.</given-names>
              <surname>Zanyar</surname>
            </name>
            <name>
              <given-names>J. K.</given-names>
              <surname>Kalita</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10462-021-10092-2</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Neural attention for image captioning: Review of outstanding methods</article-title>
          <source>Artif. Intell. Rev.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>2021</volume>
          <page-range>1-19</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A.</given-names>
              <surname>Oluwasammi</surname>
            </name>
            <name>
              <given-names>M. U.</given-names>
              <surname>Aftab</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Qin</surname>
            </name>
            <name>
              <given-names>S. T.</given-names>
              <surname>Ngo</surname>
            </name>
            <name>
              <given-names>T. V.</given-names>
              <surname>Doan</surname>
            </name>
            <name>
              <given-names>S. B.</given-names>
              <surname>Nguyen</surname>
            </name>
            <name>
              <given-names>S. H.</given-names>
              <surname>Nguyen</surname>
            </name>
            <name>
              <given-names>G. H.</given-names>
              <surname>Nguyen</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2021/5538927</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Features to text: A comprehensive survey of deep learning on semantic segmentation and image captioning</article-title>
          <source>Complexity</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="book">
          <volume/>
          <page-range>3104–3112</page-range>
          <issue/>
          <year>2014</year>
          <publisher-name>Montreal, Quebec, Canada</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>I.</given-names>
              <surname>Sutskever</surname>
            </name>
            <name>
              <given-names>O.</given-names>
              <surname>Vinyals</surname>
            </name>
            <name>
              <given-names>Q. V.</given-names>
              <surname>Le</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi"/>
          <article-title>Sequence to sequence learning with neural networks</article-title>
          <source>Advances in Neural Information Processing Systems</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>