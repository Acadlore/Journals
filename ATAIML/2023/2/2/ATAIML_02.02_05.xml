<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="review-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-1CgYEXY46iybRjhphOaYPa0KNTx2_t7C</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml020205</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Artificial Intelligence in Cervical Cancer Research and Applications</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9457-5563</contrib-id>
          <name>
            <surname>Liu</surname>
            <given-names>Chunhui</given-names>
          </name>
          <email>liuchs@hbu.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-2009-5158</contrib-id>
          <name>
            <surname>Yang</surname>
            <given-names>Jiahui</given-names>
          </name>
          <email>y3116531637@163.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-8735-3612</contrib-id>
          <name>
            <surname>Liu</surname>
            <given-names>Ying</given-names>
          </name>
          <email>yliu@bournemouth.ac.uk</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-8945-7595</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Ying</given-names>
          </name>
          <email>18236913293@163.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9552-1364</contrib-id>
          <name>
            <surname>Liu</surname>
            <given-names>Shuang</given-names>
          </name>
          <email>whlius@hbu.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_4">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-2008-4270</contrib-id>
          <name>
            <surname>Chaikovska</surname>
            <given-names>Tetiana</given-names>
          </name>
          <email>TetianaChaikovs@hotmail.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_5">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-4882-7100</contrib-id>
          <name>
            <surname>Liu</surname>
            <given-names>Chan</given-names>
          </name>
          <email>15930242026@126.com</email>
        </contrib>
        <aff id="aff_1">Department of Gynecology, Affiliated Hospital of Hebei University, 071002 Baoding, China</aff>
        <aff id="aff_2">College of Quality and Technical Supervision, Hebei University, 071002 Baoding, China</aff>
        <aff id="aff_3">Department of Lecturer in Strategy, Bournemouth University, BH12 5BB Bournemouth, United Kingdom</aff>
        <aff id="aff_4">Department of Medical Imaging, Clinical Infectious Diseases Hospital N1, 02154 Kryvyi Rih, Ukraine</aff>
        <aff id="aff_5">Radiotherapy Department, Affiliated Hospital of Hebei University, 071002 Baoding, China</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>13</day>
        <month>06</month>
        <year>2023</year>
      </pub-date>
      <volume>2</volume>
      <issue>2</issue>
      <fpage>99</fpage>
      <lpage>115</lpage>
      <page-range>99-115</page-range>
      <history>
        <date date-type="received">
          <day>24</day>
          <month>03</month>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <day>27</day>
          <month>05</month>
          <year>2023</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2023 by the author(s)</copyright-statement>
        <copyright-year>2023</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Cervical cancer remains a leading cause of death among females, posing a severe threat to women's health. Due to the uneven distribution of resources in different regions, there are challenges regarding physicians' experience, quantity, and medical conditions. Early screening, diagnosis, and treatment of cervical cancer still face significant obstacles. In recent years, artificial intelligence (AI) has been increasingly applied to various diseases' screening, diagnosis, and treatment. Currently, AI has many research applications in cervical cancer screening, diagnosis, treatment, and prognosis, assisting doctors and clinical experts in decision-making, improving efficiency and accuracy. This study discusses the application of AI in cervical cancer screening, including HPV typing and detection, cervical cytology screening, and colposcopy screening, as well as AI in cervical cancer diagnosis and treatment, including magnetic resonance imaging (MRI) and computed tomography (CT). Finally, the study briefly describes the current challenges faced by AI applications in cervical cancer and proposes future research directions.</p></abstract>
      <kwd-group>
        <kwd>Cervical cancer</kwd>
        <kwd>Cervical intraepithelial neoplasia (CIN)</kwd>
        <kwd>Artificial intelligence</kwd>
        <kwd>Deep learning</kwd>
        <kwd>Cervical cancer early screening</kwd>
        <kwd>Cervical cancer diagnosis</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="7"/>
        <fig-count count="2"/>
        <table-count count="7"/>
        <ref-count count="101"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Cervical cancer is one of the most common malignant tumors in women. On the basis of GLOBOCAN estimates for 185 countries in 2020, there were 604,000 new cases of cervical cancer and 342,000 deaths [<xref ref-type="bibr" rid="ref_1">1</xref>]. It is the only can through the efficient price of 9 human papilloma virus (HPV) vaccine, early detection and timely treatment to primary prevention strategies to eliminate cancer [<xref ref-type="bibr" rid="ref_2">2</xref>].</p><p>Nearly all cases of cervical cancer consist of 15 kinds of carcinogenic HPV genotypes caused by persistent infection. There are four main stages in the development of cervical cancer: metaplastic epithelial infection in the cervical transitional zone, persistent HPV infection, progression of persistently infected epithelium to precancerous lesions of the cervix, and infiltration of the epithelial basement membrane [<xref ref-type="bibr" rid="ref_3">3</xref>]. The HPV vaccine can protect age-appropriate females from HPV infection. However, even in some developed countries, the coverage rate of the HPV vaccine remains very low [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>]. The slow progress of cervical lesion detection and treatment provides a number of very precious opportunities, such as, about 30% of all cervical intraepithelial neoplasia (CIN) level 3 lesions in 30 years progression to invasive cancer [<xref ref-type="bibr" rid="ref_5">5</xref>]. As screening techniques have improved, cancer detection rates have increased and death rates have declined. However, most deaths occur in low - and middle-income countries [<xref ref-type="bibr" rid="ref_6">6</xref>]. Despite advances in effective screening, diagnosis, and treatment programs, the accuracy and generalizability of screening, diagnosis, and treatment are relatively low due to the lack of physicians' experience, quantity, and medical conditions, posing significant challenges for early cervical cancer screening, diagnosis, and subsequent individualized treatment and prognosis. Therefore, it is crucial to develop a more accurate and cost-effective method for cervical cancer screening, diagnosis, and treatment.</p><p>In recent years, artificial intelligence (AI) is increasingly used in the diagnosis of various diseases, such as skin cancer classification [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>], retinal disease diagnosis and classification [<xref ref-type="bibr" rid="ref_9">9</xref>], and tumor imaging diagnosis [<xref ref-type="bibr" rid="ref_10">10</xref>], demonstrating good application value. In cervical cancer screening, diagnosis, and treatment, AI is also used to address the limited human resources and improve diagnostic accuracy. As described below, currently, AI has made many research achievements and progress in cervical cancer screening, including HPV typing and detection, cervical cytology screening, colposcopy screening, diagnosis, and treatment, including MRI and CT. This greatly improves the accuracy and specificity of cervical cancer screening, diagnosis, and treatment, assisting doctors and clinical experts in diagnosis and decision-making, contributing to overcoming the problems of inadequate accuracy and generalizability caused by the lack of physicians' experience, quantity, and medical conditions.</p><p>This study aims to introduce the latest artificial intelligence in cervical cancer research and applications, such as the integration of AI with HPV typing and detection, cervical cytology screening, colposcopy screening, cervical cancer lesion segmentation, and local staging in MRI, diagnosis of lymph node metastasis in cervical cancer, and diagnosis and treatment of cervical cancer in CT. This demonstrates the practicality, potential, and future challenges of AI in the early screening, diagnosis, and treatment of cervical cancer.</p>
    </sec>
    <sec sec-type="">
      <title>2. Artificial intelligence in cervical cancer screening, diagnosis, and treatment</title>
      <p>Alan Turing first described the concept of simulating intelligent behavior and critical thinking in computers in 1950 [<xref ref-type="bibr" rid="ref_11">11</xref>]. In his book “Computing Machinery and Intelligence," he described a simple test to determine if a computer possesses human intelligence, which later became known as the “Turing Test" [<xref ref-type="bibr" rid="ref_12">12</xref>]. Six years later, John McCarthy defined Artificial Intelligence (AI) as the “science and engineering of making intelligent machines" [<xref ref-type="bibr" rid="ref_13">13</xref>], [<xref ref-type="bibr" rid="ref_14">14</xref>]. Over the following decades, the performance of artificial intelligence evolved into more complex algorithms resembling human-like capabilities. AI encompasses several subfields, such as Machine Learning (ML), Deep Learning (DL), and Computer Vision.</p><p>Machine learning refers to the analytical techniques which involved in technologies that learn patterns and derive criteria from data to predict and classify unknown objects based on these criteria [<xref ref-type="bibr" rid="ref_15">15</xref>]. ML is mainly divided into three types: Supervised learning, unsupervised learning, and reinforcement learning. Supervised learning is a kind of machine learning based on training data to provide the results or the answer, it is mainly used for regression and classification [<xref ref-type="bibr" rid="ref_16">16</xref>]. In supervised learning, training data is used as the known information for learning, building regression or classification models that can respond to unknown information. Representative techniques include decision tree-based methods, such as random forest methods and regression analysis. Unsupervised learning does not require training data to determine correct answers. It is used for grouping and summarizing data [<xref ref-type="bibr" rid="ref_17">17</xref>].</p><p>Deep Learning is a form of machine learning that drives the current AI boom, with over 90% being supervised learning [<xref ref-type="bibr" rid="ref_16">16</xref>], [<xref ref-type="bibr" rid="ref_17">17</xref>]. The neural network in deep learning consists of three types of layer categories: input layer, intermediate layer and output layer. It uses mathematical models to simulate neurons in human neural networks [<xref ref-type="bibr" rid="ref_15">15</xref>], [<xref ref-type="bibr" rid="ref_16">16</xref>]. The neural network's output is compared to the training data, adjusting the weights of the information to increase the accuracy of the output. The development and use of deep learning make anomaly detection, image processing, natural language processing and speech recognition possible [<xref ref-type="bibr" rid="ref_16">16</xref>], [<xref ref-type="bibr" rid="ref_17">17</xref>].</p><p>The relationship between artificial intelligence, machine learning, and deep learning is shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Relationship between artificial intelligence, machine learning, and deep learning</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/5/img_IKm_RK6rnihsoUjN.png"/>
        </fig>
      
      <p>Artificial intelligence, particularly in the deep learning domain, has many applications in medicine, such as Automatic Visual Evaluation (AVE), automated dual-staining cytology, diagnostic radiology, and automated diabetic retinopathy screening [<xref ref-type="bibr" rid="ref_18">18</xref>], [<xref ref-type="bibr" rid="ref_19">19</xref>], [<xref ref-type="bibr" rid="ref_20">20</xref>]. Deep learning-based cervical image Automatic Visual Evaluation (AVE) is emerging as an alternative, low-cost solution for screening, diagnosis, and treatment [<xref ref-type="bibr" rid="ref_21">21</xref>]. Currently, AI is extensively applied in cervical cancer diagnosis and treatment, where machine learning SVM models and deep learning neural networks are rifely used in cervical cancer screening, including HPV typing and detection, cervical cytology screening, and colposcopy. AI is also used in cervical cancer diagnosis and treatment, including Magnetic Resonance Imaging (MRI) and computed tomography scans, as shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>. However, there are still challenges, such as the scarcity of high-quality clinical data and the protection of patient data privacy. At the same time, there is a broader research prospect.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Application of artificial intelligence in screening, diagnosis and treatment of cervical cancer</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/5/img_Q2oxQcLQgOn2-64P.png"/>
        </fig>
      
    </sec>
    <sec sec-type="">
      <title>3. Artificial intelligence in early screening of cervical cancer</title>
      <p>Nearly all cervical cancers are caused by persistent infection with one of the high-risk HPV genotypes, of which there are 15 types in the cervical epithelium. The slow progress of cervical lesion detection and treatment provides a number of very precious opportunities. Therefore, early screening of cervical cancer is very important for the prevention, diagnosis, and early cure of cervical cancer. Related detection and diagnosis include HPV genotyping and testing, cervical cytology screening and colposcopy screening.</p><p>Based on deep learning, automatic visual assessment (AVE) of cervical images is becoming an alternative new low-cost screening and diagnostic solution. By leveraging big data and advanced computing resources, it provides accurate screening of cervical cancer and precancerous lesions. The following will specifically introduce three methods of early cervical cancer screening: HPV genotyping and testing, cervical cytology screening and colposcopy screening, as well as related research and progress of artificial intelligence technology in these methods. And discuss the potential benefits, limitations, and challenges of using artificial intelligence in cervical cancer screening, and future prospects and research directions.</p>
      
        <sec>
          
            <title>3.1. Hpv genotyping and testing</title>
          
          <p>Preliminary screening for cervical cancer in recent years, more and more dependent on the type of high-risk human papilloma virus (hrHPV), has been proved that the detection usually higher than cytologic examination has higher sensitivity and negative predictive value [<xref ref-type="bibr" rid="ref_22">22</xref>], [<xref ref-type="bibr" rid="ref_23">23</xref>].</p><p>ATHENA, a large trial has shown that patients infected with HPV16 or HPV18 which causes most cervical cancers are more likely to develop CIN3 or higher grade lesions [<xref ref-type="bibr" rid="ref_24">24</xref>], [<xref ref-type="bibr" rid="ref_25">25</xref>]. Therefore, if HPV testing is used as the primary screening tool for young women, those with HPV16 or HPV18 positivity should be promptly referred for the colposcopy, while for the patients with non-16 or 18 HPV, positivity cytological classification was further conducted [<xref ref-type="bibr" rid="ref_26">26</xref>]. Therefore, HPV genotyping is more conducive to cervical cancer screening and management. Many scholars are devoted to research related to AI and HPV testing. Deep learning methods such as CNN models and ANN models have achieved high accuracy on internal datasets of research units, reaching over 90%, showing a good application prospect of deep learning models in HPV detection, as shown in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Relevant research on AI and HPV testing</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Author</p></td><td colspan="1" rowspan="1"><p>Publication year</p></td><td colspan="1" rowspan="1"><p>Method</p></td><td colspan="1" rowspan="1"><p>Case load</p></td><td colspan="1" rowspan="1"><p>Classification category</p></td><td colspan="1" rowspan="1"><p>Bear fruit</p></td><td colspan="1" rowspan="1"><p>Superiority</p></td></tr><tr><td colspan="1" rowspan="1"><p>Castro et al. [<xref ref-type="bibr" rid="ref_27">27</xref>]</p></td><td colspan="1" rowspan="1"><p>2019</p></td><td colspan="1" rowspan="1"><p><mml:math id="mclg8n05xc">
  <mml:mrow>
    <mml:mi data-mjx-auto-op="false">CNN</mml:mi>
  </mml:mrow>
</mml:math></p></td><td colspan="1" rowspan="1"><p>96 People</p></td><td colspan="1" rowspan="1"><p>Binary classification</p></td><td colspan="1" rowspan="1"><p>Precision: $100 \%<mml:math id="mlinek003t">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>.</mml:mo>
  <mml:mo>[</mml:mo>
  <mml:mo>]</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>D</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>v</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>f</mml:mi>
  <mml:mi>f</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>b</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>D</mml:mi>
  <mml:mi>N</mml:mi>
  <mml:mi>A</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>m</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>h</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>M</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>28</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>2019</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>\mathrm{CNN}<mml:math id="myji8pl0jh">
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>&amp;gt;&amp;lt;</mml:mo>
  <mml:mo>=</mml:mo>
  <mml:mo>−</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mo>&amp;gt;</mml:mo>
  <mml:mo>:</mml:mo>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>/</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mrow>
    <mml:mo>"</mml:mo>
  </mml:mrow>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>P</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>B</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>f</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>d</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>o</mml:mi>
  <mml:mi>w</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>x</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>l</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>g</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>A</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>u</mml:mi>
  <mml:mi>r</mml:mi>
  <mml:mi>a</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>253</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
  <mml:mn>1</mml:mn>
</mml:math>94.1 \%<mml:math id="mwcdy4jy9y">
  <mml:mi>s</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>n</mml:mi>
  <mml:mi>s</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>v</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mo>:</mml:mo>
</mml:math>95.6 \%<mml:math id="m2euvyqcvi">
  <mml:mi>s</mml:mi>
  <mml:mi>p</mml:mi>
  <mml:mi>e</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>f</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>c</mml:mi>
  <mml:mi>i</mml:mi>
  <mml:mi>t</mml:mi>
  <mml:mi>y</mml:mi>
  <mml:mo>:</mml:mo>
</mml:math>83.3 \%$</p></td><td colspan="1" rowspan="1"><p>Exploring the feasibility of classifying cervical squamous epithelial disease using deep learning from colposcopy images in combination with HPV types</p></td></tr><tr><td colspan="1" rowspan="1"><p>Bogani et al. [<xref ref-type="bibr" rid="ref_29">29</xref>]</p></td><td colspan="1" rowspan="1"><p>2019</p></td><td colspan="1" rowspan="1"><p>ANN</p></td><td colspan="1" rowspan="1"><p>5104 People</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>The accuracy of the AI classifier and gynecological oncologists were 0.941 and 0.843 respectively.</p></td><td colspan="1" rowspan="1"><p>Bogani et al. [<xref ref-type="bibr" rid="ref_29">29</xref>] studied whether the pretreatment human papillomavirus (HPV)genotype predicted the risk of persistence or recurrence of cervical dysplasia. Artificial neural network (ANN) analysis was used to assess the importance of different HPV genotypes in predicting the persistence and recurrence of cervical dysplasia.</p></td></tr><tr><td colspan="1" rowspan="1"><p>He et al. [<xref ref-type="bibr" rid="ref_30">30</xref>]</p></td><td colspan="1" rowspan="1"><p>2022</p></td><td colspan="1" rowspan="1"><p>25971 People</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>To analyze the feasibility and application value of AI-assisted cervical cytology screening combined with human papillomavirus (HPV) shunt in free cervical cancer screening in rural women. The results showed that AI-assisted cytology and high-risk HPV shunt reduced the colposcopy referral rate and improved the diagnostic agreement rate between cytology and biopsy pathology.</p></td><td colspan="1" rowspan="1"><p>To analyze the feasibility and</p><p>application value of AI-assisted</p><p>cervical cytology screening</p><p>combined with human</p><p>papillomavirus (HPV) shunt in free</p><p>cervical cancer screening in rural</p><p>women. The results showed that</p><p>AI-assisted cytology and high-risk</p><p>HPV shunt reduced the colposcopy</p><p>referral rate and improved the</p><p>diagnostic agreement rate between</p><p>cytology and biopsy pathology.</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Cervical cytology screening</title>
          
          <p>There are two methods of cervical cytology. The first is the traditional Pap smear, and the second is liquid-based cytology (LBC) [<xref ref-type="bibr" rid="ref_31">31</xref>], [<xref ref-type="bibr" rid="ref_32">32</xref>], [<xref ref-type="bibr" rid="ref_33">33</xref>], [<xref ref-type="bibr" rid="ref_34">34</xref>]. Over the past 60 years, the pap smear screening has had a significant effect on reducing cervical cancer mortality [<xref ref-type="bibr" rid="ref_35">35</xref>]. Due to cost issues, the most common method for diagnosing malignant cervical tumors is currently cervical cytology [<xref ref-type="bibr" rid="ref_36">36</xref>], [<xref ref-type="bibr" rid="ref_37">37</xref>]. These tests are carried out by specialist cytologists who analyse a sample of cervical cells taken from a patient's cervix under a microscope to detect the effects of HPV. However, because each slide contains about 3 million cells with different orientations and overlapping [<xref ref-type="bibr" rid="ref_38">38</xref>]. So manual screening is expensive, difficult, time consuming, expensive, and error-prone, because each slide contains about 3 million different oriented and overlapping cells [<xref ref-type="bibr" rid="ref_39">39</xref>], [<xref ref-type="bibr" rid="ref_40">40</xref>].</p><p>In medical image analysis, DL programs are now a repeating and successful type of machine learning algorithm. Cervical cytology image analysis is no exception. The popular deep architecture is convolutional neural network (CNN) is widely used in this field. This method has good results in cell detection, cell segmentation, cell classification, and cell region of interest (ROI) extraction [<xref ref-type="bibr" rid="ref_41">41</xref>].</p>
          
            <sec>
              
                <title>3.2.1 Segmentation of cervical cells</title>
              
              <p>The main goal of segmentation is to segment medical images into multiple regions for rapid analysis of cells [<xref ref-type="bibr" rid="ref_42">42</xref>]. Accurate segmentation of the cell nucleus and cytoplasm is critical because the cell nucleus carries reliable information for cancer detection. In the medical field, automatic segmentation saves patients' lives by providing fast, reliable and accurate disease diagnosis. Due to these advantages, many researchers are using deep learning (such as convolutional neural networks and feature attention networks) to perform segmentation tasks on cervical cells, with an accuracy and recall rate of over 90% on internal datasets. This demonstrates the good application prospects of deep learning models in cervical cell segmentation, which can greatly improve the screening efficiency of expert detection. Relevant research is shown in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
              
                <table-wrap id="table_2">
                  <label>Table 2</label>
                  <caption>
                    <title>Relevant research on cervical cell segmentation using deep learning</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="1"><p>Author</p></td><td colspan="1" rowspan="1"><p>Publication year</p></td><td colspan="1" rowspan="1"><p>Method</p></td><td colspan="1" rowspan="1"><p>Case load</p></td><td colspan="1" rowspan="1"><p>Classification category</p></td><td colspan="1" rowspan="1"><p>Bear fruit</p></td></tr><tr><td colspan="1" rowspan="1"><p>Song et al. [<xref ref-type="bibr" rid="ref_43">43</xref>]</p></td><td colspan="1" rowspan="1"><p>2019</p></td><td colspan="1" rowspan="1"><p>Adaptive shape priors extracted from cytoplasmic profile fragmentation and shape statistics to segment the overlapping cytoplasm of cells in the cervical smear image.</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>-</p></td><td colspan="1" rowspan="1"><p>The experimental results show that the proposed method is general enough to be applied to other similar microscopy image segmentation tasks in the presence of a large number of overlapping objects.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Wan et al. [<xref ref-type="bibr" rid="ref_44">44</xref>]</p></td><td colspan="1" rowspan="1"><p>2019</p></td><td colspan="1" rowspan="1"><p>TernausDetDee pLab V2</p></td><td colspan="1" rowspan="1"><p>ISBI2014 (945) </p><p>ISBI2015 (210)</p><p> Internal dataset (580)</p></td><td colspan="1" rowspan="1"><p>ISBI2014: DCS 93%</p><p>ISBI2015: DSC 92%</p><p>Internal dataset: 92%</p></td><td colspan="1" rowspan="1"><p>A new framework based on deep convolutional neural network (DCNN) is proposed for automatic segmentation of overlapping cells.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Wang et al. [<xref ref-type="bibr" rid="ref_45">45</xref>]</p></td><td colspan="1" rowspan="1"><p>2021</p></td><td colspan="1" rowspan="1"><p>VGG16+SGD</p></td><td colspan="1" rowspan="1"><p>Internal dataset (143)</p></td><td colspan="1" rowspan="1"><p>Precision rate: 93%</p><p>Recall: 90%</p></td><td colspan="1" rowspan="1"><p>The proposed method handles the whole Pap smear for only 210 seconds, which is 20 times faster than U-Net and 19 times faster than SegNet.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Zhao et al. [<xref ref-type="bibr" rid="ref_46">46</xref>]</p></td><td colspan="1" rowspan="1"><p>2022</p></td><td colspan="1" rowspan="1"><p>LEANE</p></td><td colspan="1" rowspan="1"><p>Herlex Dataset (917) </p><p>WBC Dataset (400) </p><p>Warwick-QU dataset (165)</p></td><td colspan="1" rowspan="1"><p>Precision: 93.01%</p><p>Recall: 96.1%</p></td><td colspan="1" rowspan="1"><p>A lightweight feature attention network (LEANet) is proposed to accurately segment the nucleus and cytoplasm regions in cervical images.</p></td></tr></tbody></table>
                </table-wrap>
              
            </sec>
          
          
            <sec>
              
                <title>3.2.2 Classification of cervical cells</title>
              
              <p>Image classification is a major area of research in medical image analysis. It is a process in computer vision. In this field, deep learning-based methods have made enormous contributions by providing the most advanced accuracy [<xref ref-type="bibr" rid="ref_47">47</xref>]. Computer vision is also an indispensable part of cervical cell classification. A large number of studies have shown that cervical cells can achieve an accuracy of 95% or more in binary classification and multi-classification tasks on homemade datasets and public datasets, providing an effective tool for cervical cancer classification in clinical settings. Relevant research is shown in <xref ref-type="table" rid="table_3">Table 3</xref>.</p>
            </sec>
          
        </sec>
      
      
        <table-wrap id="table_3">
          <label>Table 3</label>
          <caption>
            <title>Relevant research on colposcopy image classification using deep learning</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1" colwidth="158"><p>Author</p></td><td colspan="4" rowspan="1" colwidth="25"><p>Publication Method year</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Data set</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Classification category</p></td><td colspan="1" rowspan="1"><p>Bear fruit</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Superiority</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="158"><p>Wang et al. [<xref ref-type="bibr" rid="ref_48">48</xref>]</p></td><td colspan="2" rowspan="1" colwidth="25"><p>2020</p></td><td colspan="2" rowspan="1"><p>BsiNet-TAP</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Self-made data set (389)</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Three classification</p></td><td colspan="1" rowspan="1"><p>Precision: 98.49%</p></td><td colspan="1" rowspan="1" colwidth="173"><p>An adaptive pruning deep transfer leaming model is proposed for Pap smear image classification.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="158"><p>Huang et al. [<xref ref-type="bibr" rid="ref_49">49</xref>]</p></td><td colspan="2" rowspan="1" colwidth="25"><p>2020</p></td><td colspan="2" rowspan="1"><p>LASSO+</p><p>EL-SVM</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Self-made data set (468)</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Seven classification</p></td><td colspan="1" rowspan="1"><p>Nomal accuracy: 99.64%,</p><p>HSIL accuracy: 87.4%,</p><p>LSIL accuracy: 91.88%,</p><p>Cancer accuracy: 81.4%</p></td><td colspan="1" rowspan="1" colwidth="173"><p>We propose a cervical biopsy tissue image classification method based on minimum absolute contraction and selection operator and integrated Learning-support vector machine.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="158"><p>Dong et al. [<xref ref-type="bibr" rid="ref_50">50</xref>]</p></td><td colspan="2" rowspan="1" colwidth="25"><p>2020</p></td><td colspan="2" rowspan="1"><p>Inception v3</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Herlex data set (917)</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Binary classification</p></td><td colspan="1" rowspan="1"><p>Accuracy: 98.23%,</p><p>Sensitivity: 99.44%,</p><p>Specific: 96.73%</p></td><td colspan="1" rowspan="1" colwidth="173"><p>To propose a cell classification algorithm combining Inception v3 and artificial features.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="158"><p>Dong et al. [<xref ref-type="bibr" rid="ref_51">51</xref>]</p></td><td colspan="2" rowspan="1" colwidth="25"><p>2021</p></td><td colspan="2" rowspan="1"><p>PSO-SVM</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Herlex data set (917)</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Seven classification</p></td><td colspan="1" rowspan="1"><p>Accuracy: 99.81%</p><p>Sensitivity: 99.89%</p><p>Recall: 99.26%</p></td><td colspan="1" rowspan="1" colwidth="173"><p>We propose a machine learning method for cervical cell classification based on a feature selection algorithm.</p></td></tr><tr><td colspan="1" rowspan="1" colwidth="158"><p>Liu et al. [<xref ref-type="bibr" rid="ref_52">52</xref>]</p></td><td colspan="2" rowspan="1" colwidth="25"><p>2021</p></td><td colspan="2" rowspan="1"><p>LSPS-net</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Self-made data (2119)</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Two categories and three categories</p></td><td colspan="1" rowspan="1"><p>Accuracy of three</p><p>classification: 90.90%</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Developed an LSPS net integrated 2 D light-scattering static flow cytometer for single-cervical cell analysis.</p></td></tr><tr><td colspan="2" rowspan="1" colwidth="158,25"><p>Rahaman et al. [<xref ref-type="bibr" rid="ref_53">53</xref>]</p></td><td colspan="1" rowspan="1"><p>2021</p></td><td colspan="1" rowspan="1"><p>DeepCervix</p></td><td colspan="2" rowspan="1" colwidth="0,173"><p>SIPakded Se</p><p>Data Set</p><p>(4049) Herlex</p><p>Data set (917)</p></td><td colspan="1" rowspan="1" colwidth="173"><p>STAaldyed</p><p>Data set: two classifichtion; three classification;</p><p>five</p><p>classification</p><p>Herlex data set:</p><p>two classification;</p><p>seven classification</p></td><td colspan="1" rowspan="1"><p>SIPalbyeD accuracy on the data set:</p><p>2: 99.85%</p><p>3.99.38%</p><p>7.99.14%</p><p>Herlev accuracy on</p><p>the data set: 2 classification: 98.32% 7: 90.32%</p></td><td colspan="1" rowspan="1" colwidth="173"><p>A DeepCerrix algorithm based on DL hybrid depth feature fusion (HDFF) technology is proposed.</p></td></tr><tr><td colspan="2" rowspan="1" colwidth="158,25"><p>Chen et al. [<xref ref-type="bibr" rid="ref_54">54</xref>]</p></td><td colspan="1" rowspan="1"><p>2022</p></td><td colspan="1" rowspan="1"><p>HI+Ghostnet</p></td><td colspan="2" rowspan="1" colwidth="0,173"><p>SIPakkMeD data set</p><p>(4049)</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Five classification</p></td><td colspan="1" rowspan="1"><p>Accuracy: 96.39%,</p><p>Sensitivity: 96.42 %,</p><p>Specific: 99.09 %,</p><p>Recall: 96.39%</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Proposed a hybrid loss function HL with label smoothing</p></td></tr><tr><td colspan="2" rowspan="1" colwidth="158,25"><p>Cho et al. [<xref ref-type="bibr" rid="ref_55">55</xref>]</p></td><td colspan="1" rowspan="1"><p>2022</p></td><td colspan="1" rowspan="1"><p>DenseNet-</p><p>161Effic ientNet-B7</p></td><td colspan="2" rowspan="1" colwidth="0,173"><p>Self-made data set</p><p>(1106)</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Three classification</p></td><td colspan="1" rowspan="1"><p>DenseNet-161</p><p>Accuracy: 91.4%</p><p>EfficientNet-B7</p><p>Accuracy: 92.6%</p></td><td colspan="1" rowspan="1" colwidth="173"><p>The performance of two pre-trained convolutional neural network (CNN) models using the DenseNet-16 and EfficientNet-B7 architectures was evaluated.</p></td></tr><tr><td colspan="2" rowspan="1" colwidth="158,25"><p>Kanavati et al. [<xref ref-type="bibr" rid="ref_56">56</xref>]</p></td><td colspan="1" rowspan="1"><p>2022</p></td><td colspan="1" rowspan="1"><p>CNN+RNN</p></td><td colspan="2" rowspan="1" colwidth="0,173"><p>Self-made data set</p><p>(1468)</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Binary classification</p></td><td colspan="1" rowspan="1"><p>Accuracy: 90.7%,</p><p>Sensitivity: 85%,</p><p>Specific: 91.1%</p></td><td colspan="1" rowspan="1" colwidth="173"><p>A dataset of 1,605 cervical WSI was used. We evaluated the model on three test sets, with ROC AUC in the range 0.89-0.96.</p></td></tr><tr><td colspan="2" rowspan="1" colwidth="158,25"><p>Yaman and Tuncer [<xref ref-type="bibr" rid="ref_57">57</xref>]</p></td><td colspan="1" rowspan="1"><p>2022</p></td><td colspan="1" rowspan="1"><p>DarkHet</p></td><td colspan="2" rowspan="1" colwidth="0,173"><p>STPAWWeD</p><p>Data Set</p><p>Mendeley</p><p>Data Set</p><p>(4049)</p></td><td colspan="1" rowspan="1" colwidth="173"><p>Five classification</p></td><td colspan="1" rowspan="1"><p>SIPaKMGD,</p><p>Accuracy of data set: 95.43%</p><p>Accuracy of</p><p>Mendeley data set: 99.23%</p></td><td colspan="1" rowspan="1" colwidth="173"><p>A cervical cancer detection method based on the typical pyramid deep feature extraction has been proposed.</p></td></tr></tbody></table>
        </table-wrap>
      
      
        <sec>
          
            <title>3.3. Colposcopy screening</title>
          
          <p>Colposcopy involves magnifying a fully exposed cervix 5 to 40 times using the specific instruments used for this examination to visually assess the cervix in real time, especially the transformation zone, to detect cervical intraepithelial neoplasia (CIN) or squamous intraepithelial lesion (SIL) and invasive cancer [<xref ref-type="bibr" rid="ref_58">58</xref>]. Comprehensive colposcopy should include visibility of the cervix, visibility of the squamous columnar junction, presence or absence of acetic acid whitening, presence or absence of lesions, visibility of lesions, size and location of lesions, changes in blood vessels, other characteristics of lesions, and colposcopic impression [<xref ref-type="bibr" rid="ref_59">59</xref>].</p>
          
            <sec>
              
                <title>3.3.1 Segmentation of colposcopy images</title>
              
              <p>Automatic segmentation of acetic acid lesions in colposcopy images is critical for assisting gynecologists in grading cervical intraepithelial neoplasia and cervical cancer [<xref ref-type="bibr" rid="ref_60">60</xref>]. Relevant research on colposcopy image segmentation using deep learning is currently relatively weak compared to research on colposcopy image classification, but existing relevant research has shown considerable results [<xref ref-type="bibr" rid="ref_61">61</xref>], [<xref ref-type="bibr" rid="ref_62">62</xref>]. Existing research shows high accuracy and specificity on homemade and public datasets, which can assist experts in improving detection and screening efficiency and accuracy [<xref ref-type="bibr" rid="ref_63">63</xref>], [<xref ref-type="bibr" rid="ref_64">64</xref>]. Relevant research results are shown in <xref ref-type="table" rid="table_4">Table 4</xref>.</p>
              
                <table-wrap id="table_4">
                  <label>Table 4</label>
                  <caption>
                    <title>Relevant research on colposcopy image segmentation using deep learning</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="1"><p>Author</p></td><td colspan="1" rowspan="1"><p>Publication year</p></td><td colspan="1" rowspan="1"><p>Method</p></td><td colspan="1" rowspan="1"><p>Data set</p></td><td colspan="1" rowspan="1"><p>Bear fruit</p></td><td colspan="1" rowspan="1"><p>Superiority</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yuan et al. [<xref ref-type="bibr" rid="ref_65">65</xref>]</p></td><td colspan="1" rowspan="1"><p>2020</p></td><td colspan="1" rowspan="1"><p>U-Net</p></td><td colspan="1" rowspan="1"><p>Self-made data set (22330)</p></td><td colspan="1" rowspan="1"><p>Average accuracy of acetic acid image: 95.59%, accuracy of iodine image: 95.70%</p></td><td colspan="1" rowspan="1"><p>An independent dataset of HD images was collected and, in addition, a comparison of diagnostic accuracy between the colposcopist and the model.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Guo et al. [<xref ref-type="bibr" rid="ref_66">66</xref>]</p></td><td colspan="1" rowspan="1"><p>2020</p></td><td colspan="1" rowspan="1"><p>Mask R-CNNMaskX R-CNN</p></td><td colspan="1" rowspan="1"><p>CVT (Costa Rica Vaccine Trial) dataset (3398); ALTS dataset (939); MobileQDT. dataset (1960)</p></td><td colspan="1" rowspan="1"><p>Dice:0.9471oU: 0.901</p></td><td colspan="1" rowspan="1"><p>Two state-of-the-art deep learning-based object localization and segmentation methods, the Mask R Convolutional Neural Network (CNN) and MaskX R-CNN, were evaluated for automated cervical segmentation using three datasets.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yue et al. [<xref ref-type="bibr" rid="ref_67">67</xref>]</p></td><td colspan="1" rowspan="1"><p>2021</p></td><td colspan="1" rowspan="1"><p>AWL-CNN</p></td><td colspan="1" rowspan="1"><p>Self-made data set (3045)</p></td><td colspan="1" rowspan="1"><p>Dice: 0.823 <mml:math id="mz5gkkjvs3">
  <mml:mo>±</mml:mo>
</mml:math> 0.129; Precision: 0.928 <mml:math id="m6leysio9r">
  <mml:mo>±</mml:mo>
</mml:math> 0.139</p></td><td colspan="1" rowspan="1"><p>A novel AW, lesion-sensing convolutional Neural Network (AWLCNN), for the segmentation of AW lesions in cervical maps, is presented.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Liu et al. [<xref ref-type="bibr" rid="ref_68">68</xref>]</p></td><td colspan="1" rowspan="1"><p>2022</p></td><td colspan="1" rowspan="1"><p>DeepLab V3+</p></td><td colspan="1" rowspan="1"><p>Self-made data set (280)</p></td><td colspan="1" rowspan="1"><p>Average specificity was 94.9%; average accuracy 91.2%; and average sensitivity 78.2%</p></td><td colspan="1" rowspan="1"><p>The cervical region was first extracted from the original colposcopy images by the k-means clustering algorithm. The AW region was again segmented from the neck region with DeepLabV3 +.</p></td></tr></tbody></table>
                </table-wrap>
              
            </sec>
          
          
            <sec>
              
                <title>3.3.2 Classification of colposcopy images</title>
              
              <p>Colposcopy is easy to misdiagnose and miss diagnosis because of its poor consistency with pathology. In addition, colposcopy performed by an inexperienced clinician can lead to potential harm (including vaginal discharge, pain or even bleeding, infection, etc.), so the doctor needs adequate training to achieve a certain level of proficiency to be competent. However, the long training time of the relevant doctors and the lack of qualified or skilled personnel pose a great challenge to the application of colposcopy diagnosis. In the past, deep learning has been widely and effectively applied in medical imaging. Therefore, deep learning technology can be applied in colposcopy classification tasks, which helps to solve the bottleneck and problems of traditional colposcopy, thus significantly improving its diagnostic performance. At present, there are also many research results showing that deep learning has achieved good results in classifying colposcopy images on homemade and public datasets, which can greatly improve the classification and diagnosis efficiency of doctors and experts. Relevant research is shown in <xref ref-type="table" rid="table_5">Table 5</xref>.</p>
              
                <table-wrap id="table_5">
                  <label>Table 5</label>
                  <caption>
                    <title>Relevant research on colposcopy image classification using deep learning</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="1"><p>Author</p></td><td colspan="1" rowspan="1"><p>Publication year</p></td><td colspan="1" rowspan="1"><p>Method</p></td><td colspan="1" rowspan="1"><p>Data set</p></td><td colspan="1" rowspan="1"><p>Classification category</p></td><td colspan="1" rowspan="1"><p>Bear fruit</p></td><td colspan="1" rowspan="1"><p>Superiority</p></td></tr><tr><td colspan="1" rowspan="1"><p>Kudva et al. [<xref ref-type="bibr" rid="ref_69">69</xref>]</p></td><td colspan="1" rowspan="1"><p>2019</p></td><td colspan="1" rowspan="1"><p>AlexNet;VGG-16</p></td><td colspan="1" rowspan="1"><p>IEEE Dataport cervigram (3339)</p></td><td colspan="1" rowspan="1"><p>Two categories and four categories</p></td><td colspan="1" rowspan="1"><p>Accuracy of two classification: 91.66%, Accuracy of four classification: 83.33%</p></td><td colspan="1" rowspan="1"><p>Proposed as a novel hybrid transfer learning technique</p></td></tr><tr><td colspan="1" rowspan="1"><p>Buiu et al. [<xref ref-type="bibr" rid="ref_70">70</xref>]</p></td><td colspan="1" rowspan="1"><p>2020</p></td><td colspan="1" rowspan="1"><p>MobileNetV2</p></td><td colspan="1" rowspan="1"><p>253 People</p></td><td colspan="1" rowspan="1"><p>Binary classification</p></td><td colspan="1" rowspan="1"><p>Accuracy: 94.1%, sensitivity: 95.6%. specificity: 83.3%</p></td><td colspan="1" rowspan="1"><p>In this paper, we propose an automated colposcopy image analysis framework based on an ensemble of MobileNetV2 networks.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Saini et al. [<xref ref-type="bibr" rid="ref_71">71</xref>]</p></td><td colspan="1" rowspan="1"><p>2020</p></td><td colspan="1" rowspan="1"><p>ColpoNet</p></td><td colspan="1" rowspan="1"><p>Self-made data set (400)</p></td><td colspan="1" rowspan="1"><p>Three classification</p></td><td colspan="1" rowspan="1"><p>Precision:81.353%</p></td><td colspan="1" rowspan="1"><p>Proposed a deep learning based ColpoNet network using colposcopy images for cervical cancer classification</p></td></tr><tr><td colspan="1" rowspan="1"><p>Luo et al. [<xref ref-type="bibr" rid="ref_72">72</xref>]</p></td><td colspan="1" rowspan="1"><p>2020</p></td><td colspan="1" rowspan="1"><p>DenseNet121 ResNet50</p></td><td colspan="1" rowspan="1"><p>homemade data set (3920)</p></td><td colspan="1" rowspan="1"><p>Four classification</p></td><td colspan="1" rowspan="1"><p>Accuracy: 79% Sensitive: 70.4% Specificity: 82.2%</p></td><td colspan="1" rowspan="1"><p>A multiple CNN (DenseNet121 ResNet50) decision feature integrated system MDFI for diagnosis of cervical precancerous is proposed.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yu et al. [<xref ref-type="bibr" rid="ref_73">73</xref>]</p></td><td colspan="1" rowspan="1"><p>2021</p></td><td colspan="1" rowspan="1"><p>C-GCNN</p></td><td colspan="1" rowspan="1"><p>Homemade dataset MSCI (679)</p></td><td colspan="1" rowspan="1"><p>Four classification</p></td><td colspan="1" rowspan="1"><p>Accuracy: 96.87% Sensitivity: 95.68% Specificity: 98.72%.</p></td><td colspan="1" rowspan="1"><p>A multistate Colposcopy Image dataset (MSCI) is presented. Establish a CIN hierarchical model C-GCNN based on the MSCI dataset.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Adweb et al. [<xref ref-type="bibr" rid="ref_74">74</xref>]</p></td><td colspan="1" rowspan="1"><p>2021</p></td><td colspan="1" rowspan="1"><p>PreLU-ResNet</p></td><td colspan="1" rowspan="1"><p>Datasets in Intel and MobileODT cervical cancer screening</p></td><td colspan="1" rowspan="1"><p>Three classification</p></td><td colspan="1" rowspan="1"><p>Accuracy: 100% Sensitive: 97.8% Specificity: 98.1%</p></td><td colspan="1" rowspan="1"><p>Three residual networks of the same structure were constructed using different activation functions.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Park et al. [<xref ref-type="bibr" rid="ref_75">75</xref>]</p></td><td colspan="1" rowspan="1"><p>2021</p></td><td colspan="1" rowspan="1"><p>ResNet-50; XGB; SVM; RF</p></td><td colspan="1" rowspan="1"><p>Self-made data set (4119)</p></td><td colspan="1" rowspan="1"><p>Binary classification</p></td><td colspan="1" rowspan="1"><p>Accuracy: ResNet-50:91% XGB: 74% SVM: 76%RF: 71%</p></td><td colspan="1" rowspan="1"><p>The performance of two different models, machine learning and deep learning, are compared.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Liu et al. [<xref ref-type="bibr" rid="ref_76">76</xref>]</p></td><td colspan="1" rowspan="1"><p>2021</p></td><td colspan="1" rowspan="1"><p>ResNet</p></td><td colspan="1" rowspan="1"><p>Self-made data set (15276)</p></td><td colspan="1" rowspan="1"><p>Binary classification</p></td><td colspan="1" rowspan="1"><p>NC and LSIL + Classification: Accuracy: 88.6% Sensitivity: 93.2% Specificity: 84.6% HSIL-and HSIL + Classification: Accuracy: 80.7% Sensitivity: 82.3% Specificity: 80%</p></td><td colspan="1" rowspan="1"><p>The residual neural network (ResNet) was calculated for each patient. And the results were compared with the diagnosis of a senior colposcoscopist and a junior colposcopist.</p></td></tr></tbody></table>
                </table-wrap>
              
              <p>Artificial intelligence in early cervical cancer screening, including HPV genotyping and testing, cervical cytology screening and colposcopy screening, has achieved good research results and has good application prospects. It can greatly improve the screening efficiency and accuracy of doctors and experts. It helps to solve problems such as missed diagnosis and misdiagnosis caused by insufficient experience, quantity and medical conditions of physicians. However, the clinical data currently available is generally of poor quality. At the same time, privacy issues such as the protection of patient data also need to be considered. In the future, improving the quality of clinical image data can further improve the accuracy and applicability of artificial intelligence technology in early cervical cancer screening. It has good research prospects and is expected to further improve the popularity and diagnostic accuracy of early cervical cancer screening, especially in underdeveloped regions.</p>
            </sec>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Artificial intelligence in the diagnosis and treatment of cervical cancer</title>
      <p>Cross-sectional imaging modalities generally include computed tomography (CT), magnetic resonance imaging (MRI) and positron emission tomography (PET-CT). It is an important tool for studying prognostic factors of cervical cancer, such as lymph node status, parametrial invasion, cervical canal extension, tumor size, pelvic sidewall, tumor size and so on. Imaging indications also include cervical cancer follow-up, assessment of tumor response to treatment, and selection of appropriate candidates for less radical surgery, such as radical cervical excision to preserve fertility. MRI is the preferred imaging method for local cervical cancer assessment, while CT is also effective for assessing extrauterine spread of the disease [<xref ref-type="bibr" rid="ref_77">77</xref>].</p><p>However, at present, due to the lack of experience, quantity and medical conditions of physicians, the accuracy and generalization of existing diagnosis and treatment are relatively low. Therefore, early screening, diagnosis and subsequent individualized treatment and prognosis judgment of cervical cancer still face enormous challenges. This paper will mainly introduce the application of artificial intelligence in the diagnosis and treatment of cervical cancer on magnetic resonance imaging (MRI) images, including cervical cancer lesion segmentation and local staging, and diagnosis of cervical cancer lymph node metastasis (LNM) on MRI, as well as applications on computed tomography (CT) images in the diagnosis and treatment of cervical cancer.</p>
      
        <sec>
          
            <title>4.1. Magnetic resonance imaging (mri)</title>
          
          <p>MRI has been shown to be highly accurate in preoperative staging of cervical cancer [<xref ref-type="bibr" rid="ref_78">78</xref>], [<xref ref-type="bibr" rid="ref_79">79</xref>]. Therefore, MRI is the preferred method for local staging, treatment response evaluation, tumor recurrence detection and follow-up of cervical cancer patients [<xref ref-type="bibr" rid="ref_80">80</xref>]. The main purpose of MRI is to determine the presence of tumor surrounding infiltration and lymph node metastasis (LNM) [<xref ref-type="bibr" rid="ref_81">81</xref>].</p>
          
            <sec>
              
                <title>4.1.1 Segmentation of cervical cancer lesions and local staging on mri</title>
              
              <p>MRI has higher soft tissue resolution than CT. It can determine tumor size and adjacent pelvic structures, and assess invasion around the uterus and involvement of uterus and vagina [<xref ref-type="bibr" rid="ref_82">82</xref>]. SVM model, U-Net model, 3D-CNN model, CapsNet model, etc. The accuracy on self-built datasets can reach more than 90%. Related research is shown in <xref ref-type="table" rid="table_6">Table 6</xref>.</p>
              
                <table-wrap id="table_6">
                  <label>Table 6</label>
                  <caption>
                    <title>Related research on segmentation of cervical cancer lesions and local staging, as well as diagnosis of cervical cancer LNM on MRI</title>
                  </caption>
                  <table><tbody><tr><td colspan="1" rowspan="1"><p>Author</p></td><td colspan="1" rowspan="1"><p>Publication year</p></td><td colspan="1" rowspan="1"><p>Learning goals</p></td><td colspan="1" rowspan="1"><p>Data set</p></td><td colspan="1" rowspan="1"><p>Method</p></td><td colspan="1" rowspan="1"><p>Bear fruit</p></td></tr><tr><td colspan="1" rowspan="1"><p>Wang et al. [<xref ref-type="bibr" rid="ref_83">83</xref>]</p></td><td colspan="1" rowspan="1"><p>2020</p></td><td colspan="1" rowspan="1"><p>Partition: the prediction of parauterine invasion</p></td><td colspan="1" rowspan="1"><p>There were 137 patients.</p></td><td colspan="1" rowspan="1"><p>SVM model</p></td><td colspan="1" rowspan="1"><p>Training Set AUC T2WI: 0.797 T 2 WI and DWI 0.780 (95% CI) Validation Set T2WI 0.946 (95% CI) T 2 WI and DWI 0.921 (95% CI)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Lin et al. [<xref ref-type="bibr" rid="ref_84">84</xref>]</p></td><td colspan="1" rowspan="1"><p>2020</p></td><td colspan="1" rowspan="1"><p>To evaluate the performance of U-Net in fully automated localization and segmentation of cervical tumors in magnetic resonance (MR) images</p></td><td colspan="1" rowspan="1"><p>There were 169 patients.</p></td><td colspan="1" rowspan="1"><p>U-Net model</p></td><td colspan="1" rowspan="1"><p>Dice coefficient: 82% Sensitivity: 89% Positive prediction: 92%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Wang et al. [<xref ref-type="bibr" rid="ref_85">85</xref>]</p></td><td colspan="1" rowspan="1"><p>2021</p></td><td colspan="1" rowspan="1"><p>Identification and segmentation of cervical cancer lesions</p></td><td colspan="1" rowspan="1"><p>TThere were 80 patients.</p></td><td colspan="1" rowspan="1"><p>3D-CNN model</p></td><td colspan="1" rowspan="1"><p>Precision:93.11%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Cibi et al. [<xref ref-type="bibr" rid="ref_86">86</xref>]</p></td><td colspan="1" rowspan="1"><p>2022</p></td><td colspan="1" rowspan="1"><p>Local staging of the cervical cancer</p></td><td colspan="1" rowspan="1"><p>The 12,771 pieces of Fig</p></td><td colspan="1" rowspan="1"><p>CapsNet model</p></td><td colspan="1" rowspan="1"><p>Precision:90.28%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Yan et al. [<xref ref-type="bibr" rid="ref_87">87</xref>]</p></td><td colspan="1" rowspan="1"><p>2019</p></td><td colspan="1" rowspan="1"><p>Assisted in the diagnosis of lymph node metastasis</p></td><td colspan="1" rowspan="1"><p>There were 153 patents.</p></td><td colspan="1" rowspan="1"><p>Radiomics model</p></td><td colspan="1" rowspan="1"><p>Accuracy: 78.4% Sensitive: 86.7% Specificity: 75%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Wang et al. [<xref ref-type="bibr" rid="ref_88">88</xref>]</p></td><td colspan="1" rowspan="1"><p>2019</p></td><td colspan="1" rowspan="1"><p>Assisted in the diagnosis of lymph node metastasis</p></td><td colspan="1" rowspan="1"><p>There were 96 patients.</p></td><td colspan="1" rowspan="1"><p>SVM model</p></td><td colspan="1" rowspan="1"><p>C-index: 0.922 (P=3.412*10-2</p></td></tr><tr><td colspan="1" rowspan="1"><p>Wu et al. [<xref ref-type="bibr" rid="ref_89">89</xref>]</p></td><td colspan="1" rowspan="1"><p>2020</p></td><td colspan="1" rowspan="1"><p>Assisted in the diagnosis of lymph node metastasis</p></td><td colspan="1" rowspan="1"><p>There were 479 patients.</p></td><td colspan="1" rowspan="1"><p>DL model</p></td><td colspan="1" rowspan="1"><p>AUC 0.933 (95% CI)</p></td></tr></tbody></table>
                </table-wrap>
              
            </sec>
          
          
            <sec>
              
                <title>4.1.2 Diagnosis of cervical cancer lnm</title>
              
              <p>It helps early diagnosis of cervical cancer LNM. Although CT and MRI have an accuracy of only 83% to 85% in assessing lymph node involvement, but they are particularly high specificity, can even reach to 93% from 66% [<xref ref-type="bibr" rid="ref_90">90</xref>]. In 2018, the cervical cancer staging system was revised, and lymph node status was included as a staging criterion for the first time. Imaging or pathology showing lymph node involvement in cervical cancer is classified as stage IIIC [<xref ref-type="bibr" rid="ref_91">91</xref>]. Radiology has now advanced to the point where it can bridge the gap between fusion imaging and precision medicine. Radiology extracts the wealth of information hidden in medical images by combining the use of statistical analysis with sophisticated image analysis tools [<xref ref-type="bibr" rid="ref_92">92</xref>]. Many scholars have achieved good accuracy using radiogenomics models, SVM models, DL models, etc. on self-built datasets. Related research is shown in <xref ref-type="table" rid="table_6">Table 6</xref>.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>4.2. Computed tomography (ct)</title>
          
          <p>Accurate detection of cervical cancer plays a crucial role in disease treatment and prognosis prediction, so the accuracy and timeliness of detection are very important [<xref ref-type="bibr" rid="ref_93">93</xref>]. Computed tomography (PET/CT) and fluorodeoxyglucose positron emission tomography (FDG-PET/CT) play an important role in cervical cancer detection because of their superior sensitivity and specificity [<xref ref-type="bibr" rid="ref_94">94</xref>]. However, traditional FDG-PET/CT data analysis takes an especially long time and is not efficient because it requires interpretation of hundreds of images for each patient. However, with the development of computer hardware and algorithm and progress, especially in machine learning, especially on behalf of the development of deep learning [<xref ref-type="bibr" rid="ref_95">95</xref>], image processing techniques [<xref ref-type="bibr" rid="ref_96">96</xref>] play an indispensable role in many fields of clinical medicine [<xref ref-type="bibr" rid="ref_97">97</xref>]. The application of this technology in the diagnosis of cervical cancer can help clinicians make judgements, reduce workload and improve diagnostic accuracy [<xref ref-type="bibr" rid="ref_98">98</xref>]. Many scholars have achieved high accuracy using CNN, DpnUNet, YoloV5 and other deep learning models on self-built datasets. Related research is shown in <xref ref-type="table" rid="table_7">Table 7</xref>.</p>
          
            <table-wrap id="table_7">
              <label>Table 7</label>
              <caption>
                <title>Related research on the application of computed tomography in the diagnosis of cervical cancer</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Author</p></td><td colspan="1" rowspan="1"><p>Publication year</p></td><td colspan="1" rowspan="1"><p>Learning goals</p></td><td colspan="1" rowspan="1"><p>Case load</p></td><td colspan="1" rowspan="1"><p>Method</p></td><td colspan="1" rowspan="1"><p>Bear fruit</p></td></tr><tr><td colspan="1" rowspan="1"><p>Shen et al. [<xref ref-type="bibr" rid="ref_99">99</xref>]</p></td><td colspan="1" rowspan="1"><p>2019</p></td><td colspan="1" rowspan="1"><p>To achieve early prediction of local and distant failure in patients with locally advanced cervical cancer.</p></td><td colspan="1" rowspan="1"><p>142</p></td><td colspan="1" rowspan="1"><p>CNN</p></td><td colspan="1" rowspan="1"><p>Tumor prediction: sensitivity 71%, and specificity 93%; distant metastasis: sensitivity 77% and specificity 90%</p></td></tr><tr><td colspan="1" rowspan="1"><p>Liu et al. [<xref ref-type="bibr" rid="ref_100">100</xref>]</p></td><td colspan="1" rowspan="1"><p>2021</p></td><td colspan="1" rowspan="1"><p>To realize automatic segmentation of clinical target volume contour of cervical cancer.</p></td><td colspan="1" rowspan="1"><p>CT:237</p></td><td colspan="1" rowspan="1"><p>DpnUNet</p></td><td colspan="1" rowspan="1"><p>Dice similarity coefficient (DSC): 0.88; 95th percentile Hausdorff distance (95HD)3.46mm</p></td></tr><tr><td colspan="1" rowspan="1"><p>Ming et al. [<xref ref-type="bibr" rid="ref_101">101</xref>]</p></td><td colspan="1" rowspan="1"><p>2022</p></td><td colspan="1" rowspan="1"><p>Image registration, multimodal image fusion, and detection of lesion objects.</p></td><td colspan="1" rowspan="1"><p>CT/PET:220</p></td><td colspan="1" rowspan="1"><p>YoloV5</p></td><td colspan="1" rowspan="1"><p>Meverage accuracy above the joint threshold (AP50): 84.3</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>Artificial intelligence has been extensively studied and applied in the diagnosis and treatment of cervical cancer. It has achieved good results in cervical cancer lesion segmentation and local staging on MRI, early diagnosis of cervical cancer LNM, and computed tomography, greatly improving the accuracy and specificity of early prediction and diagnosis, improving the efficiency of cervical cancer diagnosis and treatment, helping clinicians make decisions, reducing the workload of physicians and reducing misdiagnosis rates. However, there are still problems such as lack of high-quality clinical data, reliability and stability of models need to be improved. At the same time, issues such as patient data protection and security also need to be considered. Currently, there is less research on the treatment and prognosis prediction of cervical cancer using artificial intelligence, with greater challenges and research prospects.</p>
        </sec>
      
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>In summary, artificial intelligence has performed well in computer vision and imaging, especially in the medical field, helping clinicians make decisions, reducing the workload of physicians and reducing misdiagnosis rates. Artificial intelligence has achieved good results in early screening, diagnosis and treatment of cervical cancer, and prognosis prediction, improving the specificity and accuracy of screening and diagnosis, and has good applicability. Overall, while improving the specificity and accuracy of screening and diagnosis, it has overcome a series of problems such as time constraints, limited specialists and subjective bias caused by physicians, which will enable cervical cancer screening to be implemented in resource-poor areas, thereby significantly reducing the incidence of cervical cancer.</p><p>However, the application of artificial intelligence currently still faces a series of problems such as lack of high-quality clinical data, obstacles in the management of medical data, lack of technical maintenance, reliability and stability of models need to be improved, and models have not yet been promoted in clinical applications. At the same time, the use of artificial intelligence in cervical cancer screening and diagnosis will also involve ethical and privacy issues, such as the protection and security of patient data.</p><p>Artificial intelligence has a promising application prospect in cervical cancer screening, especially in cervical cytology screening, where the application of convolutional neural networks (CNN) has been relatively mature. CNN has achieved great success in cell detection, segmentation, classification and region of interest (ROI) extraction [<xref ref-type="bibr" rid="ref_34">34</xref>]. The assistance of related models can greatly improve the detection efficiency of cervical cytology experts. However, segmentation techniques still face many challenges, which may be the direction of future development. Effective segmentation techniques can further improve the accuracy and reliability of artificial intelligence in cervical cancer screening and diagnosis. In addition to early screening and diagnosis, artificial intelligence can also be applied to treatment, prognosis prediction and prevention of cervical cancer, which will also be an important research direction in the future with good application prospects. It is believed that in the future, artificial intelligence will greatly improve the predictive ability of cervical cancer, maximize the improvement of cervical cancer screening and diagnosis, optimize the staging system, improve patient prognosis, and be fully applied to the early diagnosis, treatment and prognosis prediction of cervical cancer.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p>This paper was funded by Baoding Science and Technology Planning Project (Grant No.: 2141ZF306, 2141ZF135); Youth Foundation of Affiliated Hospital of Hebei University (Grant No.: 2022QC54).</p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>Not applicable.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>71</volume>
          <page-range>209-249</page-range>
          <issue>3</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sung</surname>
              <given-names>Hyuna</given-names>
            </name>
            <name>
              <surname>Ferlay</surname>
              <given-names>Jacques</given-names>
            </name>
            <name>
              <surname>Siegel</surname>
              <given-names>Rebecca L.</given-names>
            </name>
            <name>
              <surname>Laversanne</surname>
              <given-names>Mathieu</given-names>
            </name>
            <name>
              <surname>Soerjomataram</surname>
              <given-names>Isabelle</given-names>
            </name>
            <name>
              <surname>Jemal</surname>
              <given-names>Ahmedin</given-names>
            </name>
            <name>
              <surname>Bray</surname>
              <given-names>Freddie</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3322/caac.21660</pub-id>
          <article-title>Global cancer statistics 2020: Globocan estimates of incidence and mortality worldwide for 36 cancers in 185 countries</article-title>
          <source>CA Cancer J. Clin.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>395</volume>
          <page-range>575-590</page-range>
          <issue>10224</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Brisson</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>J. J.</given-names>
            </name>
            <name>
              <surname>Canfell</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Sung</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Brisson</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Jane  Kim</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Canfell</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Sung</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Brisson</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Jane  Kim</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Canfell</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Sung</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Brisson</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Jane  Kim</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Canfell</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Sung</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Brisson</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Jane  Kim</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Canfell</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Sung</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/s0140-6736(20)30068-4</pub-id>
          <article-title>Impact of HPV vaccination and cervical screening on cervical cancer elimination: A comparative modelling analysis in 78 low-income and lower-middle-income countries</article-title>
          <source>Lancet</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>370</volume>
          <page-range>890-907</page-range>
          <issue>9590</issue>
          <year>2007</year>
          <person-group person-group-type="author">
            <name>
              <surname>Schiffman</surname>
              <given-names>Mark</given-names>
            </name>
            <name>
              <surname>Castle</surname>
              <given-names>Philip E.</given-names>
            </name>
            <name>
              <surname>Jeronimo</surname>
              <given-names>Jose</given-names>
            </name>
            <name>
              <surname>Rodriguez</surname>
              <given-names>Ana Cecilia</given-names>
            </name>
            <name>
              <surname>Wacholder</surname>
              <given-names>Sholom</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/s0140-6736(07)61416-0</pub-id>
          <article-title>Human papillomavirus and cervical cancer</article-title>
          <source>Lancet</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>24</volume>
          <issue>3</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Moradi</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1136/ebnurs-2020-103281</pub-id>
          <article-title>Modelling suggests that high human papilloma virus (HPV) vaccination coverage in combination with high uptake screening can lead to cervical cancer elimination in most low-income and lower-middle-income countries (LMICs) by the end of the century</article-title>
          <source>Evid.-Based Nurs.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>256</volume>
          <page-range>57-62</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Redman</surname>
              <given-names>C. W. E.</given-names>
            </name>
            <name>
              <surname>Kesic</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Cruickshank</surname>
              <given-names>M. E.</given-names>
            </name>
            <name>
              <surname>Charles  Redman</surname>
              <given-names>W. E.</given-names>
            </name>
            <name>
              <surname>Kesic</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Cruickshank</surname>
              <given-names>M. E.</given-names>
            </name>
            <name>
              <surname>Charles  Redman</surname>
              <given-names>W. E.</given-names>
            </name>
            <name>
              <surname>Kesic</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Cruickshank</surname>
              <given-names>M. E.</given-names>
            </name>
            <name>
              <surname>Charles  Redman</surname>
              <given-names>W. E.</given-names>
            </name>
            <name>
              <surname>Kesic</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Cruickshank</surname>
              <given-names>M. E.</given-names>
            </name>
            <name>
              <surname>Charles  Redman</surname>
              <given-names>W. E.</given-names>
            </name>
            <name>
              <surname>Kesic</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Cruickshank</surname>
              <given-names>M. E.</given-names>
            </name>
            <name>
              <surname>Charles  Redman</surname>
              <given-names>W. E.</given-names>
            </name>
            <name>
              <surname>Kesic</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Cruickshank</surname>
              <given-names>M. E.</given-names>
            </name>
            <name>
              <surname>Charles  Redman</surname>
              <given-names>W. E.</given-names>
            </name>
            <name>
              <surname>Kesic</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Cruickshank</surname>
              <given-names>M. E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ejogrb.2020.06.029</pub-id>
          <article-title>European consensus statement on essential colposcopy</article-title>
          <source>Eur. J. Obstet. Gynecol. Reprod. Biol.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>68</volume>
          <page-range>394-424</page-range>
          <issue>6</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bray</surname>
              <given-names>Freddie</given-names>
            </name>
            <name>
              <surname>Ferlay</surname>
              <given-names>Jacques</given-names>
            </name>
            <name>
              <surname>Soerjomataram</surname>
              <given-names>Isabelle</given-names>
            </name>
            <name>
              <surname>Siegel</surname>
              <given-names>Rebecca L.</given-names>
            </name>
            <name>
              <surname>Torre</surname>
              <given-names>Lindsey A.</given-names>
            </name>
            <name>
              <surname>Jemal</surname>
              <given-names>Ahmedin</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3322/caac.21492</pub-id>
          <article-title>Global cancer statistics 2018: Globocan estimates of incidence and mortality worldwide for 36 cancers in 185 countries</article-title>
          <source>CA Cancer J. Clin.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>542</volume>
          <page-range>115-118</page-range>
          <issue>7639</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Esteva</surname>
              <given-names>Andre</given-names>
            </name>
            <name>
              <surname>Kuprel</surname>
              <given-names>Brett</given-names>
            </name>
            <name>
              <surname>Novoa</surname>
              <given-names>Ramón A.</given-names>
            </name>
            <name>
              <surname>Ko</surname>
              <given-names>Justin</given-names>
            </name>
            <name>
              <surname>Swetter</surname>
              <given-names>Susan M.</given-names>
            </name>
            <name>
              <surname>Blau</surname>
              <given-names>Helen M.</given-names>
            </name>
            <name>
              <surname>Thrun</surname>
              <given-names>Sebastian</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/nature21056</pub-id>
          <article-title>Dermatologist-level classification of skin cancer with deep neural networks</article-title>
          <source>Nature</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>7</volume>
          <page-range>29</page-range>
          <issue>1</issue>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Janowczyk</surname>
              <given-names>Andrew</given-names>
            </name>
            <name>
              <surname>Madabhushi</surname>
              <given-names>Anant</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.4103/2153-3539.186902</pub-id>
          <article-title>Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases</article-title>
          <source>J. Pathol. Inform.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>68</volume>
          <page-range>127-157</page-range>
          <issue>2</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hosny</surname>
              <given-names>Ahmed</given-names>
            </name>
            <name>
              <surname>Parmar</surname>
              <given-names>Chintan</given-names>
            </name>
            <name>
              <surname>Quackenbush</surname>
              <given-names>John</given-names>
            </name>
            <name>
              <surname>Schwartz</surname>
              <given-names>Lawrence H.</given-names>
            </name>
            <name>
              <surname>Aerts</surname>
              <given-names>Hugo J.W.L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3322/caac.21452</pub-id>
          <article-title>Artificial intelligence in cancer imaging: Clinical challenges and applications</article-title>
          <source>CA Cancer J. Clin.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>657-665</page-range>
          <issue>4</issue>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yuan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Failmezger</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Rueda</surname>
              <given-names>M. O.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Failmezger</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Rueda</surname>
              <given-names>M. O.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Failmezger</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Rueda</surname>
              <given-names>M. O.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Failmezger</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Rueda</surname>
              <given-names>M. O.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Failmezger</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Rueda</surname>
              <given-names>M. O.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Failmezger</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Rueda</surname>
              <given-names>M. O.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Failmezger</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Rueda</surname>
              <given-names>M. O.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1136/amiajnl-2013-001849</pub-id>
          <article-title>Artificial intelligence for cancer-associated fibroblasts</article-title>
          <source>J. Am. Med. Inform. Assoc.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>86</volume>
          <page-range>334-338</page-range>
          <year>2004</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ramesh</surname>
              <given-names>A. N.</given-names>
            </name>
            <name>
              <surname>Kambhampati</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Monson</surname>
              <given-names>J. R. T.</given-names>
            </name>
            <name>
              <surname>Drew</surname>
              <given-names>P. J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1308/147363504290</pub-id>
          <article-title>Artificial intelligence in medicine</article-title>
          <source>Ann R Coll Surg Engl.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>85-89</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Greenhill</surname>
              <given-names>Alexandra T.</given-names>
            </name>
            <name>
              <surname>Edmunds</surname>
              <given-names>Bethany R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.tgie.2019.150642</pub-id>
          <article-title>A primer of artificial intelligence in medicine</article-title>
          <source>Techn Gastrointest Endosc</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>2328-2331</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Amisha</surname>
            </name>
            <name>
              <surname>Malik</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Pathania</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Rathaur</surname>
              <given-names>V. K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.4103/jfmpc.jfmpc_440_19</pub-id>
          <article-title>Overview of artificial intelligence in medicine</article-title>
          <source>J. Family Med Prim Care</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>69S</volume>
          <page-range>S36-S40</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hamet</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Tremblay</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.metabol.2017.01.011</pub-id>
          <article-title>Artificial intelligence in medicine</article-title>
          <source>Metabolism</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="conf-paper">
          <volume>2018</volume>
          <page-range>1–6</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shinde</surname>
              <given-names>P. P.</given-names>
            </name>
            <name>
              <surname>Shah</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICCUBEA.2018.8697857</pub-id>
          <article-title>A review of machine learning and deep learning applications</article-title>
          <source>2018 Fourth International Conference on Computing Communication Control and Automation (ICCUBEA), Pune, India</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>1-9</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Emmert-Streib</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Feng</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Tripathi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Dehmer</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/frai.2020.00004</pub-id>
          <article-title>An introductory review of deep learning for prediction models with big data</article-title>
          <source>Front. Artif. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>1-21</page-range>
          <issue>12</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/cancers12123532</pub-id>
          <article-title>Application of artificial intelligence technology in oncology: towards the establishment of precision medicine</article-title>
          <source>Cancers (Basel)</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>719-731</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yu</surname>
              <given-names>K. H.</given-names>
            </name>
            <name>
              <surname>Beam</surname>
              <given-names>A. L.</given-names>
            </name>
            <name>
              <surname>Kohane</surname>
              <given-names>I. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41551-018-0305-z</pub-id>
          <article-title>Artificial intelligence in healthcare</article-title>
          <source>Nat. Biomed. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>110</volume>
          <page-range>1222-1228</page-range>
          <issue>11</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yu</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Hyun</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Fetterman</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Lorey</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Raine-Bennett</surname>
              <given-names>T. R.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Stamps</surname>
              <given-names>R. E.</given-names>
            </name>
            <name>
              <surname>Poitras</surname>
              <given-names>N. E.</given-names>
            </name>
            <name>
              <surname>Wheeler</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Befano</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Gage</surname>
              <given-names>J. C.</given-names>
            </name>
            <name>
              <surname>Castle</surname>
              <given-names>P. E.</given-names>
            </name>
            <name>
              <surname>Wentzensen</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Schiffman</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1093/jnci/djy044</pub-id>
          <article-title>Automated cervical screening and triage, based on HPV testing and computer-interpreted cytology</article-title>
          <source>JNCI</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>113</volume>
          <page-range>72-79</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wentzensen</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Lahrmann</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Clarke</surname>
              <given-names>M. A.</given-names>
            </name>
            <name>
              <surname>Kinney</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Tokugawa</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Poitras</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Fetterman</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Lorey</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Poitras</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Castle</surname>
              <given-names>P. E.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1093/jnci/djaa066</pub-id>
          <article-title>Accuracy and efficiency of deep-learning-based automation of dual stain cytology in cervical cancer screening</article-title>
          <source>JNCI</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="webpage">
          <article-title>Introduction to neural networks and deep learning</article-title>
          <source>, undefined</source>
          <year>2021</year>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>320</volume>
          <page-range>687-705</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Melnikow</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Henderson</surname>
              <given-names>J. T.</given-names>
            </name>
            <name>
              <surname>Burda</surname>
              <given-names>B. U.</given-names>
            </name>
            <name>
              <surname>Senger</surname>
              <given-names>C. A.</given-names>
            </name>
            <name>
              <surname>Durbin</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Weyrich</surname>
              <given-names>M. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1001/jama.2018.10400</pub-id>
          <article-title>Screening for cervical cancer with high-risk human papillomavirus testing: updated evidence report and systematic review for the us preventive services task force</article-title>
          <source>JAMA</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>320</volume>
          <page-range>43-52</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ogilvie</surname>
              <given-names>G. S.</given-names>
            </name>
            <name>
              <surname>Niekerk</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Krajden</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>L. W.</given-names>
            </name>
            <name>
              <surname>Cook</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Gondara</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Ceballos</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Quinlan</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Martin</surname>
              <given-names>R. E.</given-names>
            </name>
            <name>
              <surname>Gentile</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Peacock</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Stuart</surname>
              <given-names>G. C. E.</given-names>
            </name>
            <name>
              <surname>Franco</surname>
              <given-names>E. L.</given-names>
            </name>
            <name>
              <surname>Coldman</surname>
              <given-names>A. J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1001/jama.2018.7464</pub-id>
          <article-title>Effect of screening with primary cervical HPV testing vs cytology testing on high-grade cervical intraepithelial neoplasia at 48 months: the HPV focal randomized clinical trial</article-title>
          <source>JAMA</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <year>2012</year>
          <article-title>Human papillomavirus-associated cancers-united states, 2004-2008</article-title>
          <source>MMWR Morb Mortal Wkly Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>880-890</page-range>
          <issue>9</issue>
          <year>2011</year>
          <person-group person-group-type="author">
            <name>
              <surname>Castle</surname>
              <given-names>P. E.</given-names>
            </name>
            <name>
              <surname>Stoler</surname>
              <given-names>M. H.</given-names>
            </name>
            <name>
              <surname>Wright</surname>
              <given-names>T. C. J.</given-names>
            </name>
            <name>
              <surname>Sharma</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Wright</surname>
              <given-names>T. L.</given-names>
            </name>
            <name>
              <surname>Behrens</surname>
              <given-names>C. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/s1470-2045(11)70188-7</pub-id>
          <article-title>Performance of carcinogenic human papillomavirus (HPV) testing and HPV16 or HPV18 genotyping for cervical cancer screening of women aged 25 years and older: a subanalysis of the Athena study</article-title>
          <source>Lancet Oncol.</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <year>2016</year>
          <article-title>Guidelines for Cervical Cancer Prevention and Screening</article-title>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>154</volume>
          <page-range>38</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Castro</surname>
              <given-names>C. M.</given-names>
            </name>
            <name>
              <surname>Im</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Avila-Wallace</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Weissleder</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Randall</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ygyno.2019.04.090</pub-id>
          <article-title>Harnessing artificial intelligence and digital diffraction to advance pointof-care HPV 16 and 18 detection</article-title>
          <source>Gynecol. Oncol.</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <volume>19</volume>
          <page-range>1602-1610</page-range>
          <issue>2</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Miyagi</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Takehara</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Nagayasu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Miyake</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3892/ol.2019.11214</pub-id>
          <article-title>Application of deep learning to the classification of uterine cervical squamous epithelial lesion from colposcopy images combined with HPV types</article-title>
          <source>Oncol. Lett.</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <volume>28</volume>
          <page-range>81-86</page-range>
          <issue>2</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bogani</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Ditto</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Martinelli</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Mauroa</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Valentina</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Umberto</surname>
              <given-names>L. R. M.</given-names>
            </name>
            <name>
              <surname>Francesca</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Claudia</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Chiarae</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Cono</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Domenica</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Francesco</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1097/cej.0000000000000432</pub-id>
          <article-title>Artificial intelligence estimates the impact of human papillomavirus types in influencing the risk of cervical dysplasia recurrence: progress toward a more personalized approach</article-title>
          <source>Eur. J. Cancer Prev.</source>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <volume>42</volume>
          <page-range>273-278</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>He</surname>
              <given-names>X. M.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Du</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>M. Y.</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3969/j.issn.2096-3882.2022.04.007</pub-id>
          <article-title>Analysis of artificial intelligence-assisted cervical cytology screening combined with HPV detection in cervical cancer screening</article-title>
          <source>Acta Acad. Med. Xuzhou</source>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <issue>12</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hashmi</surname>
              <given-names>A. A.</given-names>
            </name>
            <name>
              <surname>Naz</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Ahmed</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Yaqeen</surname>
              <given-names>S. R.</given-names>
            </name>
            <name>
              <surname>Irfan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Asif</surname>
              <given-names>M. G.</given-names>
            </name>
            <name>
              <surname>Kamal</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Faridi</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.7759/cureus.12293</pub-id>
          <article-title>Comparison of liquid-based cytology and conventional papanicolaou smear for cervical cancer screening: An experience from Pakistan</article-title>
          <source>Cureus</source>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <volume>57</volume>
          <page-range>86-89</page-range>
          <issue>1</issue>
          <year>2012</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cox</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1111/j.1542-2011.2011.00116.x</pub-id>
          <article-title>Guidelines for papanicolaou test screening and follow-up</article-title>
          <source>J. Midwifery Womens Health</source>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <volume>19</volume>
          <page-range>1331-1336</page-range>
          <issue>5</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Phaliwong</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Pariyawateekul</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Khuakoonratt</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Sirichai</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Bhamarapravatana</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Suwannarurk</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.22034/apjcp.2018.19.5.1331</pub-id>
          <article-title>Cervical cancer detection between conventional and liquid based cervical cytology: A 6-year experience in northern Bangkok Thailand</article-title>
          <source>Asian Pac. J. Cancer Prev.</source>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="journal">
          <volume>41</volume>
          <page-range>257-278</page-range>
          <issue>3</issue>
          <year>2013</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hoda</surname>
              <given-names>R. S.</given-names>
            </name>
            <name>
              <surname>Loukeris</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Abdul-Karim</surname>
              <given-names>F. W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1002/dc.22842</pub-id>
          <article-title>Gynecologic cytology on conventional and liquid-based preparations: A comprehensive review of similarities and differences</article-title>
          <source>Diagn. Cytopathol.</source>
        </element-citation>
      </ref>
      <ref id="ref_35">
        <label>35.</label>
        <element-citation publication-type="journal">
          <year>1994</year>
          <person-group person-group-type="author">
            <name>
              <surname>Marchevsky</surname>
              <given-names>A. M.</given-names>
            </name>
            <name>
              <surname>Bartels</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Image analysis: A primer for pathologists</article-title>
        </element-citation>
      </ref>
      <ref id="ref_36">
        <label>36.</label>
        <element-citation publication-type="journal">
          <volume>62</volume>
          <page-range>147-172</page-range>
          <year>2012</year>
          <person-group person-group-type="author">
            <name>
              <surname>Saslow</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Solomon</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Lawson</surname>
              <given-names>H. W.</given-names>
            </name>
            <name>
              <surname>Killackey</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kulasingam</surname>
              <given-names>S. L.</given-names>
            </name>
            <name>
              <surname>Cain</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Garcia</surname>
              <given-names>F. A.</given-names>
            </name>
            <name>
              <surname>Moriarty</surname>
              <given-names>A. T.</given-names>
            </name>
            <name>
              <surname>Waxman</surname>
              <given-names>A. G.</given-names>
            </name>
            <name>
              <surname>Wilbur</surname>
              <given-names>D. C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1097/lgt.0b013e31824ca9d5</pub-id>
          <article-title>American cancer society, American society for colposcopy and cervical pathology, and American society for clinical pathology screening guidelines for the prevention and early detection of cervical cancer</article-title>
          <source>CA Cancer J. Clin.</source>
        </element-citation>
      </ref>
      <ref id="ref_37">
        <label>37.</label>
        <element-citation publication-type="journal">
          <volume>367</volume>
          <page-range>122-132</page-range>
          <issue>9505</issue>
          <year>2006</year>
          <person-group person-group-type="author">
            <name>
              <surname>Davey</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Barratt</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Irwig</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Chan</surname>
              <given-names>S. F.</given-names>
            </name>
            <name>
              <surname>Macaskill</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Mannes</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Saville</surname>
              <given-names>A. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/s0140-6736(06)67961-0</pub-id>
          <article-title>Effect of study design and quality on unsatisfactory rates, cytology classifications, and accuracy in liquid-based versus conventional cervical cytology: A systematic review</article-title>
          <source>Lancet</source>
        </element-citation>
      </ref>
      <ref id="ref_38">
        <label>38.</label>
        <element-citation publication-type="journal">
          <volume>45</volume>
          <page-range>4151-4168</page-range>
          <issue>12</issue>
          <year>2012</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gençtav</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Aksoy</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Önder</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.patcog.2012.05.006</pub-id>
          <article-title>Unsupervised segmentation and classification of cervical cell images</article-title>
          <source>Pattern Recogn.</source>
        </element-citation>
      </ref>
      <ref id="ref_39">
        <label>39.</label>
        <element-citation publication-type="journal">
          <volume>41</volume>
          <page-range>174-178</page-range>
          <issue>2</issue>
          <year>2013</year>
          <person-group person-group-type="author">
            <name>
              <surname>Elsheikh</surname>
              <given-names>T. M.</given-names>
            </name>
            <name>
              <surname>Austin</surname>
              <given-names>R. M.</given-names>
            </name>
            <name>
              <surname>Chhieng</surname>
              <given-names>D. F.</given-names>
            </name>
            <name>
              <surname>Miller</surname>
              <given-names>F. S.</given-names>
            </name>
            <name>
              <surname>Moriarty</surname>
              <given-names>A. T.</given-names>
            </name>
            <name>
              <surname>Renshaw</surname>
              <given-names>A. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1002/dc.22817</pub-id>
          <article-title>American society of cytopathology workload recommendations for automated pap test screening: Developed by the productivity and quality assurance in the era of automated screening task force</article-title>
          <source>Diagn. Cytopathol.</source>
        </element-citation>
      </ref>
      <ref id="ref_40">
        <label>40.</label>
        <element-citation publication-type="journal">
          <volume>104</volume>
          <page-range>134-138</page-range>
          <issue>1</issue>
          <year>2007</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lozano</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ygyno.2006.07.025</pub-id>
          <article-title>Comparison of computer-assisted and manual screening of cervical cytology</article-title>
          <source>Gynecol. Oncol.</source>
        </element-citation>
      </ref>
      <ref id="ref_41">
        <label>41.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>74-79</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Aswathy</surname>
              <given-names>M. A.</given-names>
            </name>
            <name>
              <surname>Jagannath</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.imu.2016.11.001</pub-id>
          <article-title>Detection of breast cancer on digital histopathology images: Present status and future possibilities</article-title>
          <source>Inf. Med. Unlocked</source>
        </element-citation>
      </ref>
      <ref id="ref_42">
        <label>42.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Tan</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <source>GPU-Based Parallel Implementation of Swarm Intelligence Algorithms</source>
          <publisher-name>San Mateo, CA, USA: Morgan Kaufmann</publisher-name>
          <year>2016</year>
        </element-citation>
      </ref>
      <ref id="ref_43">
        <label>43.</label>
        <element-citation publication-type="journal">
          <volume>38</volume>
          <page-range>2849-2862</page-range>
          <issue>12</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Song</surname>
              <given-names>Y. Y.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Qin</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lei</surname>
              <given-names>B. Y.</given-names>
            </name>
            <name>
              <surname>Sheng</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Choi</surname>
              <given-names>K. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/tmi.2019.2915633</pub-id>
          <article-title>Segmentation of overlapping cytoplasm in cervical smear images via adaptive shape priors extracted from contour fragments</article-title>
          <source>IEEE Trans. Med. Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_44">
        <label>44.</label>
        <element-citation publication-type="journal">
          <volume>365</volume>
          <page-range>157-170</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wan</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Sang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neucom.2019.06.086</pub-id>
          <article-title>Accurate segmentation of overlapping cells in cervical cytology with deep convolutional neural networks</article-title>
          <source>Neurocomputing</source>
        </element-citation>
      </ref>
      <ref id="ref_45">
        <label>45.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>C. W.</given-names>
            </name>
            <name>
              <surname>Liou</surname>
              <given-names>Y. A.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Y. J.</given-names>
            </name>
            <name>
              <surname>Chang</surname>
              <given-names>C. C.</given-names>
            </name>
            <name>
              <surname>Chu</surname>
              <given-names>P. H.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>Y. C.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>C. H.</given-names>
            </name>
            <name>
              <surname>Chao</surname>
              <given-names>T. K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-021-95545-y</pub-id>
          <article-title>Artificial intelligence-assisted fast screening cervical high grade squamous intraepithelial lesion and squamous cell carcinoma diagnosis and treatment planning</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_46">
        <label>46.</label>
        <element-citation publication-type="journal">
          <volume>145</volume>
          <page-range>105500</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Cao</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>H. F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compbiomed.2022.105500</pub-id>
          <article-title>LFANet: Lightweight feature attention network for abnormal cell segmentation in cervical cytology images</article-title>
          <source>Comput. Biol. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_47">
        <label>47.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>946-957</page-range>
          <year>2019</year>
          <publisher-name>Springer</publisher-name>
          <person-group person-group-type="author">
            <name>
              <surname>Zawadzka-Gosk</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Wolk</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Czarnowski</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-030-16181-1_89</pub-id>
          <article-title>Deep learning in state-of-the-art image classification exceeding 99 percent accuracy</article-title>
          <source>, https://doi.org/10.1007/978-3-030-16181-1_89</source>
        </element-citation>
      </ref>
      <ref id="ref_48">
        <label>48.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>50674-50683</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J. X.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y. M.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>L. Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>H. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2020.2979926</pub-id>
          <article-title>Adaptive pruning of transfer learned deep convolutional neural network for classification of cervical pap smear images</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_49">
        <label>49.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>24219-24228</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Huang</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>S. L.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>C. L.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>B. W.</given-names>
            </name>
            <name>
              <surname>Lv</surname>
              <given-names>X. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2020.2970121</pub-id>
          <article-title>Classification of cervical biopsy images basedon lasso and EL-SVM</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_50">
        <label>50.</label>
        <element-citation publication-type="journal">
          <volume>93</volume>
          <page-range>106311</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Dong</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>C. H.</given-names>
            </name>
            <name>
              <surname>Chang</surname>
              <given-names>J. F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.asoc.2020.106311</pub-id>
          <article-title>Inception v3 based cervical cell classification combined with artificially extracted features</article-title>
          <source>Appl. Soft Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_51">
        <label>51.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>1837-1849</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Dong</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Zhai</surname>
              <given-names>M. D.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>C. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s12652-020-02256-9</pub-id>
          <article-title>Cervical cell classification based on the cart feature selection algorithm</article-title>
          <source>J. Ambient Intell. Humaniz. Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_52">
        <label>52.</label>
        <element-citation publication-type="journal">
          <volume>99</volume>
          <page-range>610-621</page-range>
          <issue>6</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Qiao</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Kong</surname>
              <given-names>B. H.</given-names>
            </name>
            <name>
              <surname>Su</surname>
              <given-names>X. T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1002/cyto.a.24349</pub-id>
          <article-title>Light scattering pattern specific convolutional network static cytometry for label-free classification of cervical cells</article-title>
          <source>Cytom. Part A</source>
        </element-citation>
      </ref>
      <ref id="ref_53">
        <label>53.</label>
        <element-citation publication-type="journal">
          <volume>136</volume>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Rahaman</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Kulwa</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>X.C.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>X. Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Q.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compbiomed.2021.104649</pub-id>
          <article-title>Deepcervix: A deep learning-based framework for the classification of cervical cells using hybrid deep feature fusion techniques</article-title>
          <source>Comput. Biol. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_54">
        <label>54.</label>
        <element-citation publication-type="journal">
          <volume>22</volume>
          <page-range>3272</page-range>
          <issue>9</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>W. M.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>X. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s22093272</pub-id>
          <article-title>Hybrid loss-constrained lightweight convolutional neural networks for cervical cell classification</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_55">
        <label>55.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>548</page-range>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cho</surname>
              <given-names>B. J.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>J. W.</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Kwon</surname>
              <given-names>G. Y.</given-names>
            </name>
            <name>
              <surname>Hong</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Jang</surname>
              <given-names>S. H.</given-names>
            </name>
            <name>
              <surname>Bang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>S. T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/diagnostics12020548</pub-id>
          <article-title>Automated diagnosis of cervical intraepithelial neoplasia in histology images via deep learning</article-title>
          <source>Diagnostics</source>
        </element-citation>
      </ref>
      <ref id="ref_56">
        <label>56.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>1159</page-range>
          <issue>5</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kanavati</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Hirose</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Ishii</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Fukuda</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ichihara</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Tsuneki</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/cancers14051159</pub-id>
          <article-title>A deep learning model for cervical cancer screening on liquid-based cytology specimens in whole slide images</article-title>
          <source>Cancers</source>
        </element-citation>
      </ref>
      <ref id="ref_57">
        <label>57.</label>
        <element-citation publication-type="journal">
          <volume>73</volume>
          <page-range>103428</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yaman</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Tuncer</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.bspc.2021.103428</pub-id>
          <article-title>Exemplar pyramid deep feature extraction based cervical cancer image classification model using pap-smear images</article-title>
          <source>Biomed. Signal Process. Control</source>
        </element-citation>
      </ref>
      <ref id="ref_58">
        <label>58.</label>
        <element-citation publication-type="journal">
          <volume>21</volume>
          <page-range>223-229</page-range>
          <issue>4</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Khan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Werner</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Darragh</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1097/lgt.0000000000000338</pub-id>
          <article-title>ASCCP colposcopy standards: Role of colposcopy, benefits, potential harms, and terminology for colposcopic practice</article-title>
          <source>J. Lower Genit. Tract Dis.</source>
        </element-citation>
      </ref>
      <ref id="ref_59">
        <label>59.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <issue>6</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Miyagi</surname>
              <given-names>Yasunari</given-names>
            </name>
            <name>
              <surname>Takehara</surname>
              <given-names>Kazuhiro</given-names>
            </name>
            <name>
              <surname>Miyake</surname>
              <given-names>Takahito</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3892/mco.2019.1932</pub-id>
          <article-title>Application of deep learning to the classification of uterine cervical squamous epithelial lesion from colposcopy images</article-title>
          <source>Mol. Clin. Oncol.</source>
        </element-citation>
      </ref>
      <ref id="ref_60">
        <label>60.</label>
        <element-citation publication-type="journal">
          <volume>138</volume>
          <page-range>15-19</page-range>
          <issue>Suppl 1</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ogilvie</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Nakisige</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Huh</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Mehrotra</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Franco</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Jeronimo</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1002/ijgo.12187</pub-id>
          <article-title>Optimizing secondary prevention of cervical cancer: recent advances and future challenges</article-title>
          <source>Int. J. Gynaecol. Obstet.</source>
        </element-citation>
      </ref>
      <ref id="ref_61">
        <label>61.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>16086</page-range>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Schiffman</surname>
              <given-names>Mark</given-names>
            </name>
            <name>
              <surname>Doorbar</surname>
              <given-names>John</given-names>
            </name>
            <name>
              <surname>Wentzensen</surname>
              <given-names>Nicolas</given-names>
            </name>
            <name>
              <surname>Sanjose</surname>
              <given-names>Silvia</given-names>
            </name>
            <name>
              <surname>Fakhry</surname>
              <given-names>Carole</given-names>
            </name>
            <name>
              <surname>Monk</surname>
              <given-names>Bradley J.</given-names>
            </name>
            <name>
              <surname>Stanley</surname>
              <given-names>Margaret A.</given-names>
            </name>
            <name>
              <surname>Franceschi</surname>
              <given-names>Silvia</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/nrdp.2016.86</pub-id>
          <article-title>Carcinogenic human papillomavirus infection</article-title>
          <source>Nat. Rev. Dis. Primers</source>
        </element-citation>
      </ref>
      <ref id="ref_62">
        <label>62.</label>
        <element-citation publication-type="journal">
          <volume>393</volume>
          <page-range>969-970</page-range>
          <issue>10175</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>Fanghui</given-names>
            </name>
            <name>
              <surname>Qiao</surname>
              <given-names>Youlin</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/s0140-6736(18)32849-6</pub-id>
          <article-title>Cervical cancer prevention in China: A key to cancer control</article-title>
          <source>Lancet</source>
        </element-citation>
      </ref>
      <ref id="ref_63">
        <label>63.</label>
        <element-citation publication-type="journal">
          <volume>69</volume>
          <page-range>127-157</page-range>
          <issue>2</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bi</surname>
              <given-names>W. L.</given-names>
            </name>
            <name>
              <surname>Hosny</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Schabath</surname>
              <given-names>M. B.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3322/caac.21552</pub-id>
          <article-title>Artificial intelligence in cancer imaging: clinical challenges and applications</article-title>
          <source>CA Cancer J. Clin.</source>
        </element-citation>
      </ref>
      <ref id="ref_64">
        <label>64.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>195</page-range>
          <issue>1</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kelly</surname>
              <given-names>Christopher J.</given-names>
            </name>
            <name>
              <surname>Karthikesalingam</surname>
              <given-names>Alan</given-names>
            </name>
            <name>
              <surname>Suleyman</surname>
              <given-names>Mustafa</given-names>
            </name>
            <name>
              <surname>Corrado</surname>
              <given-names>Greg</given-names>
            </name>
            <name>
              <surname>King</surname>
              <given-names>Dominic</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s12916-019-1426-2</pub-id>
          <article-title>Key challenges for delivering clinical impact with artificial intelligence</article-title>
          <source>BMC Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_65">
        <label>65.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <issue>1</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yuan</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Cheng</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-020-68252-3</pub-id>
          <article-title>The application of deep learning based diagnostic system to cervical squamous intraepithelial lesions recognition in colposcopy images</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_66">
        <label>66.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>44</page-range>
          <issue>1</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Guo</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Xue</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Long</surname>
              <given-names>L. R.</given-names>
            </name>
            <name>
              <surname>Antani</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/diagnostics10010044</pub-id>
          <article-title>Cross-dataset evaluation of deep learning networks for uterine cervix segmentation</article-title>
          <source>Diagnostics</source>
        </element-citation>
      </ref>
      <ref id="ref_67">
        <label>67.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>3529-3540</page-range>
          <issue>9</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yue</surname>
              <given-names>Zijian</given-names>
            </name>
            <name>
              <surname>Ding</surname>
              <given-names>Shaoxiong</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Xiaojun</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Shuang</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Yutao</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/jbhi.2021.3064366</pub-id>
          <article-title>Automatic acetowhite lesion segmentation via specular reflection removal and deep attention network</article-title>
          <source>IEEE J. Biomed. Health Inform.</source>
        </element-citation>
      </ref>
      <ref id="ref_68">
        <label>68.</label>
        <element-citation publication-type="journal">
          <volume>30</volume>
          <page-range>469-482</page-range>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>Jia</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>Tao</given-names>
            </name>
            <name>
              <surname>Peng</surname>
              <given-names>Yousong</given-names>
            </name>
            <name>
              <surname>Peng</surname>
              <given-names>Guan</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>Ling</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Lan</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>Hui</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3233/thc-212890</pub-id>
          <article-title>Segmentation of acetowhite region in uterine cervical image based on deep learning</article-title>
          <source>Technol. Health Care</source>
        </element-citation>
      </ref>
      <ref id="ref_69">
        <label>69.</label>
        <element-citation publication-type="journal">
          <volume>33</volume>
          <page-range>619-631</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kudva</surname>
              <given-names>Vidya</given-names>
            </name>
            <name>
              <surname>Prasad</surname>
              <given-names>Keerthana</given-names>
            </name>
            <name>
              <surname>Guruvare</surname>
              <given-names>Shyamala</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10278-019-00269-1</pub-id>
          <article-title>Hybrid transfer learning for classification of uterine cervix images for cervical cancer screening</article-title>
          <source>J. Digit. Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_70">
        <label>70.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>595</page-range>
          <issue>5</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Buiu</surname>
              <given-names>Catalin</given-names>
            </name>
            <name>
              <surname>Dănăilă</surname>
              <given-names>Vasile Răzvan</given-names>
            </name>
            <name>
              <surname>Răduţă</surname>
              <given-names>Cristina Nicoleta</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/pr8050595</pub-id>
          <article-title>Mobilenetv2 ensemble for cervical precancerous lesions classification</article-title>
          <source>Processes</source>
        </element-citation>
      </ref>
      <ref id="ref_71">
        <label>71.</label>
        <element-citation publication-type="journal">
          <volume>31</volume>
          <page-range>15</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Saini</surname>
              <given-names>S. K.</given-names>
            </name>
            <name>
              <surname>Bansal</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Kaur</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Juneja</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00138-020-01063-8</pub-id>
          <article-title>Colponet for automated cervical cancer screening using colposcopy images</article-title>
          <source>Mach. Vis. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_72">
        <label>72.</label>
        <element-citation publication-type="journal">
          <page-range>29616-29626</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Luo</surname>
              <given-names>Y. M.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>P. M.</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>B. H.</given-names>
            </name>
            <name>
              <surname>Ruan</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2020.2972610</pub-id>
          <article-title>Mdfi: Multi-CNN decision feature integration for diagnosis of cervical precancerous lesions</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_73">
        <label>73.</label>
        <element-citation publication-type="journal">
          <volume>146</volume>
          <page-range>104352</page-range>
          <issue>1</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>W. D.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Z. M.</given-names>
            </name>
            <name>
              <surname>Ding</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ijmedinf.2020.104352</pub-id>
          <article-title>MSCI: A multistate dataset for colposcopy image classification of cervical cancer screening</article-title>
          <source>Int. J. Med. Inform.</source>
        </element-citation>
      </ref>
      <ref id="ref_74">
        <label>74.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>46612-46625</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Adweb</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Cavus</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Sekeroglu</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2021.3067195</pub-id>
          <article-title>Cervical cancer diagnosis using very deep networks over different activation functions</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_75">
        <label>75.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Park</surname>
              <given-names>Y. R.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>Y. Jae</given-names>
            </name>
            <name>
              <surname>Ju</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Nam</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>K. G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-021-95748-3</pub-id>
          <article-title>Comparison of machine and deep learning for the classification of cervical cancer based on cervicography images</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_76">
        <label>76.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <issue>13</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>X. L.</given-names>
            </name>
            <name>
              <surname>Han</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Jia</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Meng</surname>
              <given-names>L. H.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Z. Y.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y. Z.</given-names>
            </name>
            <name>
              <surname>Qiao</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.21037/atm-21-885</pub-id>
          <article-title>Computer-aided diagnostic system based on deep learning for classifying colposcopy images</article-title>
          <source>Ann. Transl. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_77">
        <label>77.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <issue>4</issue>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bourgioti</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Chatoupis</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Moulopoulos</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.4329/wjr.v8.i4.342</pub-id>
          <article-title>Current imaging strategies for the evaluation of uterine cervical cancer</article-title>
          <source>World J. Radiol.</source>
        </element-citation>
      </ref>
      <ref id="ref_78">
        <label>78.</label>
        <element-citation publication-type="journal">
          <volume>28</volume>
          <page-range>620-627</page-range>
          <issue>5</issue>
          <year>2004</year>
          <person-group person-group-type="author">
            <name>
              <surname>Choi</surname>
              <given-names>S. H.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>S. H.</given-names>
            </name>
            <name>
              <surname>Choi</surname>
              <given-names>H. J.</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>B. K.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>H. J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1097/01.rct.0000138007.77725.0a</pub-id>
          <article-title>Preoperative magnetic resonance imaging staging of uterine cervical carcinoma: Results of prospective study</article-title>
          <source>J. Comput. Assist. Tomogr.</source>
        </element-citation>
      </ref>
      <ref id="ref_79">
        <label>79.</label>
        <element-citation publication-type="journal">
          <volume>23</volume>
          <page-range>9329-9337</page-range>
          <issue>36</issue>
          <year>2005</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hricak</surname>
              <given-names>Hedvig</given-names>
            </name>
            <name>
              <surname>Gatsonis</surname>
              <given-names>Constantine</given-names>
            </name>
            <name>
              <surname>Chi</surname>
              <given-names>Dennis S.</given-names>
            </name>
            <name>
              <surname>Amendola</surname>
              <given-names>Marco A.</given-names>
            </name>
            <name>
              <surname>Brandt</surname>
              <given-names>Kathy</given-names>
            </name>
            <name>
              <surname>Schwartz</surname>
              <given-names>Lawrence H.</given-names>
            </name>
            <name>
              <surname>Koelliker</surname>
              <given-names>Susan</given-names>
            </name>
            <name>
              <surname>Siegelman</surname>
              <given-names>Evan S.</given-names>
            </name>
            <name>
              <surname>Brown</surname>
              <given-names>Jeffrey J.</given-names>
            </name>
            <name>
              <surname>McGhee Jr</surname>
              <given-names>Robert B.</given-names>
            </name>
            <name>
              <surname>Iyer</surname>
              <given-names>Revathy</given-names>
            </name>
            <name>
              <surname>Vitellas</surname>
              <given-names>Kenneth M.</given-names>
            </name>
            <name>
              <surname>Snyder</surname>
              <given-names>Bradley</given-names>
            </name>
            <name>
              <surname>Long III</surname>
              <given-names>Harry J.</given-names>
            </name>
            <name>
              <surname>Fiorica</surname>
              <given-names>James V.</given-names>
            </name>
            <name>
              <surname>Mitchell</surname>
              <given-names>Donald G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1200/jco.2005.02.0354</pub-id>
          <article-title>Role of imaging in pretreatment evaluation of early invasive cervical cancer: Results of the intergroup study American college ofradiology imaging network 6651-gynecologic oncology group 183</article-title>
          <source>J. Clin. Oncol.</source>
        </element-citation>
      </ref>
      <ref id="ref_80">
        <label>80.</label>
        <element-citation publication-type="journal">
          <volume>192</volume>
          <page-range>937-944</page-range>
          <issue>10</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Merz</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Bossart</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Bamberg</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Eisenblaetter</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1055/a-1198-5729</pub-id>
          <article-title>Revised figo staging for cervical cancer - a new role for MRI</article-title>
          <source>Rofo</source>
        </element-citation>
      </ref>
      <ref id="ref_81">
        <label>81.</label>
        <element-citation publication-type="journal">
          <volume>393</volume>
          <page-range>169-182</page-range>
          <issue>10167</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cohen</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Jhingran</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Oaknin</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Denny</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/S0140-6736(18)32470-X</pub-id>
          <article-title>Cervical cancer</article-title>
          <source>Lancet</source>
        </element-citation>
      </ref>
      <ref id="ref_82">
        <label>82.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Kim</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>A data driven approach to cervigram image analysis and classification</article-title>
          <source>Lecture Notes in Computational Vision and Biomechanics</source>
          <publisher-name>Netherlands: Springer</publisher-name>
          <year>2013</year>
          <volume>6</volume>
          <page-range>1–13</page-range>
          <pub-id pub-id-type="doi">10.1007/978-94-007-5389-1</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_83">
        <label>83.</label>
        <element-citation publication-type="journal">
          <volume>30</volume>
          <issue>3</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>X. B.</given-names>
            </name>
            <name>
              <surname>Tian</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>L. Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00330-019-06655-1</pub-id>
          <article-title>Preoperative prediction of parametrial invasion in early-stage cervical cancer with MRI-based radiomics nomogram</article-title>
          <source>Eur. Radiol.</source>
        </element-citation>
      </ref>
      <ref id="ref_84">
        <label>84.</label>
        <element-citation publication-type="journal">
          <volume>30</volume>
          <page-range>1297-1305</page-range>
          <issue>3</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lin</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>H. Y.</given-names>
            </name>
            <name>
              <surname>Chiang</surname>
              <given-names>H. J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>H. K.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Y. T.</given-names>
            </name>
            <name>
              <surname>Ng</surname>
              <given-names>S. H.</given-names>
            </name>
            <name>
              <surname>Hong</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Yen</surname>
              <given-names>T. C.</given-names>
            </name>
            <name>
              <surname>Lai</surname>
              <given-names>C. H.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>G. G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00330-019-06467-3</pub-id>
          <article-title>Deep learning for fully automated tumor segmentation and extraction of magnetic resonance radiomics features in cervical cancer</article-title>
          <source>Eur. Radiol.</source>
        </element-citation>
      </ref>
      <ref id="ref_85">
        <label>85.</label>
        <element-citation publication-type="journal">
          <volume>2021</volume>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y. Y.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>C. Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2021/1673490</pub-id>
          <article-title>Multimodal MRI analysis of cervical cancer on the basis of artificial intelligence algorithm</article-title>
          <source>Contrast Media Mol. Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_86">
        <label>86.</label>
        <element-citation publication-type="journal">
          <volume>2022</volume>
          <page-range>1-9</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cibi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Rose</surname>
              <given-names>R. J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11571-021-09777-9</pub-id>
          <article-title>Classification of stages in cervical cancer MRI by customized CNN and transfer learning</article-title>
          <source>Cogn. Neurodyn.</source>
        </element-citation>
      </ref>
      <ref id="ref_87">
        <label>87.</label>
        <element-citation publication-type="journal">
          <volume>2019</volume>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>R. T.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>Q. Y.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>Y. H.</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1259/bjr.20180986</pub-id>
          <article-title>Feasibility of an ADC-based radiomics model for predicting pelvic lymph node metastases in patients with stage IB–IIA cervical squamous cell carcinoma</article-title>
          <source>Br. J. Radiol.</source>
        </element-citation>
      </ref>
      <ref id="ref_88">
        <label>88.</label>
        <element-citation publication-type="journal">
          <volume>114</volume>
          <page-range>128-135</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>T. T.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>J. B.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>X. J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y. B.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>X. B.</given-names>
            </name>
            <name>
              <surname>Tian</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>L. Y.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ejrad.2019.01.003</pub-id>
          <article-title>Preoperative prediction of pelvic lymph nodes metastasis in early-stage cervical cancer using radiomics nomogram developed based on T2-weighted MRI and diffusion-weighted imaging</article-title>
          <source>Eur. J. Radiol.</source>
        </element-citation>
      </ref>
      <ref id="ref_89">
        <label>89.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>e2011625</page-range>
          <issue>7</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>Q. X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>S. X.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1001/jamanetworkopen.2020.11625</pub-id>
          <article-title>Development of a deep learning model to identify lymph node metastasis on magnetic resonance imaging in patients with cervical cancer</article-title>
          <source>JAMA Netw. Open</source>
        </element-citation>
      </ref>
      <ref id="ref_90">
        <label>90.</label>
        <element-citation publication-type="journal">
          <volume>18</volume>
          <page-range>406</page-range>
          <issue>1</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Xue</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1186/s12916-020-01860-y</pub-id>
          <article-title>Development and validation of an artificial intelligence system for grading colposcopic impressions and guiding biopsies</article-title>
          <source>BMC Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_91">
        <label>91.</label>
        <element-citation publication-type="journal">
          <volume>145</volume>
          <page-range>129-135</page-range>
          <issue>1</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bhatla</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Berek</surname>
              <given-names>J. S.</given-names>
            </name>
            <name>
              <surname>Fredes</surname>
              <given-names>M. C.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1002/ijgo.12749</pub-id>
          <article-title>Revised Figo staging for carcinoma of the cervix uteri</article-title>
          <source>Int. J. Gynaecol. Obstet.</source>
        </element-citation>
      </ref>
      <ref id="ref_92">
        <label>92.</label>
        <element-citation publication-type="journal">
          <volume>42</volume>
          <page-range>426-440</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Guiot</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Vaidyanathan</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Deprez</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Zerka</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1002/med.21846</pub-id>
          <article-title>A review in radiomics: Making personalized medicine a reality via routine imaging</article-title>
          <source>Med. Res. Rev.</source>
        </element-citation>
      </ref>
      <ref id="ref_93">
        <label>93.</label>
        <element-citation publication-type="journal">
          <volume>28</volume>
          <page-range>iv72-iv83</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Marth</surname>
              <given-names>Christian</given-names>
            </name>
            <name>
              <surname>Landoni</surname>
              <given-names>Fabio</given-names>
            </name>
            <name>
              <surname>Mahner</surname>
              <given-names>Sven</given-names>
            </name>
            <name>
              <surname>McCormack</surname>
              <given-names>Mary</given-names>
            </name>
            <name>
              <surname>Gonzalez-Martin</surname>
              <given-names>Antonio</given-names>
            </name>
            <name>
              <surname>Colombo</surname>
              <given-names>Nicoletta</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1093/annonc/mdx220</pub-id>
          <article-title>Cervical cancer: Esmo clinical practice guidelines for diagnosis, treatment and follow-up</article-title>
          <source>Ann. Oncol.</source>
        </element-citation>
      </ref>
      <ref id="ref_94">
        <label>94.</label>
        <element-citation publication-type="journal">
          <volume>6</volume>
          <page-range>37-45</page-range>
          <issue>1</issue>
          <year>2008</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gold</surname>
              <given-names>Michael A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.6004/jnccn.2008.0004</pub-id>
          <article-title>Pet in cervical cancer-implications for staging, treatment planning, assessment of prognosis, and prediction of response</article-title>
          <source>J. Natl. Compr. Canc. Netw.</source>
        </element-citation>
      </ref>
      <ref id="ref_95">
        <label>95.</label>
        <element-citation publication-type="journal">
          <volume>521</volume>
          <page-range>436-444</page-range>
          <issue>7553</issue>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <surname>LeCun</surname>
              <given-names>Yann</given-names>
            </name>
            <name>
              <surname>Bengio</surname>
              <given-names>Yoshua</given-names>
            </name>
            <name>
              <surname>Hinton</surname>
              <given-names>Geoffrey</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
          <article-title>Deep learning</article-title>
          <source>Nature</source>
        </element-citation>
      </ref>
      <ref id="ref_96">
        <label>96.</label>
        <element-citation publication-type="journal">
          <volume>470</volume>
          <page-range>204-216</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ma</surname>
              <given-names>Bin</given-names>
            </name>
            <name>
              <surname>Yin</surname>
              <given-names>Xiancheng</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Dali</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>Huanfeng</given-names>
            </name>
            <name>
              <surname>Ban</surname>
              <given-names>Xuegang</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Yongjun</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neucom.2021.10.115</pub-id>
          <article-title>End-to-end learning for simultaneously generating decision map and multi-focus image fusion result</article-title>
          <source>Neurocomputing</source>
        </element-citation>
      </ref>
      <ref id="ref_97">
        <label>97.</label>
        <element-citation publication-type="journal">
          <volume>42</volume>
          <page-range>1-13</page-range>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Anwar</surname>
              <given-names>Sajid</given-names>
            </name>
            <name>
              <surname>Majid</surname>
              <given-names>Muhammad</given-names>
            </name>
            <name>
              <surname>Qayyum</surname>
              <given-names>Anum</given-names>
            </name>
            <name>
              <surname>Awais</surname>
              <given-names>Muhammad</given-names>
            </name>
            <name>
              <surname>Alnowami</surname>
              <given-names>Mahmoud</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>Shoab</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10916-018-1088-1</pub-id>
          <article-title>Medical image analysis using convolutional neural networks: A review</article-title>
          <source>J. Med. Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_98">
        <label>98.</label>
        <element-citation publication-type="journal">
          <volume>172</volume>
          <page-range>1122-1131</page-range>
          <issue>5</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kermany</surname>
              <given-names>D.S.</given-names>
            </name>
            <name>
              <surname>Goldbaum</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Cai</surname>
              <given-names>W. J.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.cell.2018.02.010</pub-id>
          <article-title>Identifying medical diagnoses and treatable diseases by image-based deep learning</article-title>
          <source>Cell</source>
        </element-citation>
      </ref>
      <ref id="ref_99">
        <label>99.</label>
        <element-citation publication-type="journal">
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shen</surname>
              <given-names>Wei Chih</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Shang Wen</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Kuo Chen</given-names>
            </name>
            <name>
              <surname>Hsieh</surname>
              <given-names>Te Chun</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>Ji An</given-names>
            </name>
            <name>
              <surname>Hung</surname>
              <given-names>Yao Ching</given-names>
            </name>
            <name>
              <surname>Yeh</surname>
              <given-names>Lian Shung</given-names>
            </name>
            <name>
              <surname>Chang</surname>
              <given-names>Wei Chun</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Wu Chou</given-names>
            </name>
            <name>
              <surname>Yen</surname>
              <given-names>Kuo Yang</given-names>
            </name>
            <name>
              <surname>Kao</surname>
              <given-names>Chia Hung</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.2139/ssrn.3292576</pub-id>
          <article-title>Prediction of local relapse and distant metastasis in patients with definitive chemoradiotherapy-treated cervical cancer by deep learning from [18f]-fluorodeoxyglucose positron emission tomography/computed tomography</article-title>
          <source>SSRN</source>
        </element-citation>
      </ref>
      <ref id="ref_100">
        <label>100.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>702270</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liu</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Guan</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hamamoto</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Suvarna</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fonc.2021.702270</pub-id>
          <article-title>An adversarial deep-learning-based model for cervical cancer CTV segmentation with multicenter blinded randomized controlled validation</article-title>
          <source>Front. Oncol.</source>
        </element-citation>
      </ref>
      <ref id="ref_101">
        <label>101.</label>
        <element-citation publication-type="journal">
          <volume>205</volume>
          <page-range>46-52</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ming</surname>
              <given-names>Yan</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>Xiaoyi</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Jinhui</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Zefeng</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Hui</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Ning</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ymeth.2022.05.004</pub-id>
          <article-title>Deep learning-based multimodal image analysis for cervical cancer detection</article-title>
          <source>Methods</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>