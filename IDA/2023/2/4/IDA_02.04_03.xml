<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IDA</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Information Dynamics and Applications</journal-title>
        <abbrev-journal-title abbrev-type="issn">Inf. Dyn. Appl.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IDA</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-1494</issn>
      <issn publication-format="print">2958-1486</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-cn79CESoc9CyN0gZo1LZINOcoJgQgsCI</article-id>
      <article-id pub-id-type="doi">10.56578/ida020403</article-id>
      <title-group>
        <article-title>Enhanced Detection of COVID-19 in Chest X-ray Images: A Comparative Analysis of CNNs and the DL+ Ensemble Technique</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Jereni</surname>
            <given-names>Bwanali Haji Ntaibu</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7082-1240</contrib-id>
          <email>202006488@ub.ac.bw</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="2">2</xref>
          <name>
            <surname>Sundire</surname>
            <given-names>Iota</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3216-3288</contrib-id>
          <email>sundirei@staff.msu.ac.zw</email>
        </contrib>
        <aff id="1">Adjunct Lecturer, Department of Medicine, University of Botswana, 0061 Gaborone, Botswana</aff>
        <aff id="2">Fuels and Energy Department, Midlands State University, ZW170407 Gweru, Zimbabwe</aff>
      </contrib-group>
      <year>2023</year>
      <volume>2</volume>
      <issue>4</issue>
      <fpage>186</fpage>
      <lpage>198</lpage>
      <page-range>186-198</page-range>
      <history>
        <date date-type="received">
          <month>10</month>
          <day>09</day>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <month>11</month>
          <day>22</day>
          <year>2023</year>
        </date>
        <date date-type="pub">
          <month>11</month>
          <day>30</day>
          <year>2023</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2023 by the authors</copyright-statement>
        <copyright-year>2023</copyright-year>
        <license>. Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the <a href='https://creativecommons.org/licenses/by/4.0/' target='_blank' class='text-yellow-700 hover:underline'>CC BY 4.0 license</a>.</license>
      </permissions>
      <abstract><p>The swift global spread of Corona Virus Disease 2019 (COVID-19), identified merely four months prior, necessitates rapid and precise diagnostic methods. Currently, the diagnosis largely depends on computed tomography (CT) image interpretation by medical professionals, a process susceptible to human error. This research delves into the utility of Convolutional Neural Networks (CNNs) in automating the classification of COVID-19 from medical images. An exhaustive evaluation and comparison of prominent CNN architectures, namely Visual Geometry Group (VGG), Residual Network (ResNet), MobileNet, Inception, and Xception, are conducted. Furthermore, the study investigates ensemble approaches to harness the combined strengths of these models. Findings demonstrate the distinct advantage of ensemble models, with the novel deep learning (DL)+ ensemble technique notably surpassing the accuracy, precision, recall, and F-score of individual CNNs, achieving an exceptional rate of 99.5%. This remarkable performance accentuates the transformative potential of CNNs in COVID-19 diagnostics. The significance of this advancement lies not only in its reliability and automated nature, surpassing traditional, subjective human interpretation but also in its contribution to accelerating the diagnostic process. This acceleration is pivotal for the effective implementation of containment and mitigation strategies against the pandemic. The abstract delineates the methodological choices, highlights the unparalleled efficacy of the DL+ ensemble technique, and underscores the far-reaching implications of employing CNNs for COVID-19 detection.</p></abstract>
      <kwd-group>
        <kwd>Computed tomography images</kwd>
        <kwd>Diagnostic methods</kwd>
        <kwd>Ensemble techniques</kwd>
        <kwd>Visual Geometry Group (VGG)</kwd>
        <kwd>Residual Network (ResNet)</kwd>
        <kwd>MobileNet</kwd>
        <kwd>Inception</kwd>
        <kwd>Xception</kwd>
        <kwd>Deep learning+ ensemble technique</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors">2</count>
        <fig-count>10</fig-count>
        <table-count>4</table-count>
        <ref-count>19</ref-count>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec disp-level="level1" sec-type="intro">
      <title>1. Introduction</title>
      <p>Coronaviruses, a diverse family of viruses, are recognized for their capacity to instigate respiratory infections in humans and animals. In December 2019, a novel coronavirus, designated SARS-CoV-2, emerged in Wuhan, China. This virus rapidly disseminated globally, precipitating a widespread pandemic of COVID-19 [<xref ref-type="bibr" rid="ref_1">1</xref>]. Characterized as RNA viruses, coronaviruses possess the most extensive viral RNA genome known, predominantly hosted by bats, yet capable of zoonotic transmission to humans. As of August 24, 2020, over 23 million cases of coronavirus have been documented worldwide, resulting in approximately 800,000 fatalities. Notably, about five million individuals have recuperated, with the United States, Brazil, India, and Russia reporting the highest incidence rates [<xref ref-type="bibr" rid="ref_2">2</xref>].</p><p>Diagnosis of COVID-19 necessitates the identification of the SARS-CoV-2 virus. Current diagnostic approaches bifurcate into laboratory-based and point-of-care methodologies. Laboratory-based diagnostics, though more precise, demand specialized apparatus and skilled personnel. These encompass nucleic acid testing (NAT), antigen testing, and serology testing [<xref ref-type="bibr" rid="ref_3">3</xref>]. NAT is renowned for its high accuracy in detecting SARS-CoV-2 from nasal or throat samples but suffers from protracted result turnaround times. In contrast, antigen tests, while faster, offer diminished accuracy. Serology tests, detecting antibodies against SARS-CoV-2, can indicate past infections but are not viable for diagnosing active cases. Point-of-care diagnostics, though expedient and more user-friendly, compromise on accuracy. These include rapid antigen tests and lateral flow immunoassays [<xref ref-type="bibr" rid="ref_3">3</xref>]. Predominant limitations of current COVID-19 diagnostics encompass cost, speed, accuracy, and accessibility, particularly in resource-limited settings.In the domain of image processing, CNNs, a subset of artificial intelligence (AI), have gained prominence. Their application in medical imaging for tasks such as image segmentation and target recognition is well-documented [<xref ref-type="bibr" rid="ref_4">4</xref>]. CNNs have demonstrated efficacy in classifying CT images, including those of COVID-19 patients. The proposed methodology in this study aims to mitigate the constraints of existing COVID-19 diagnostics by developing a CNN-based rapid, accurate, and cost-effective test, suitable for point-of-care settings such as clinics and pharmacies [<xref ref-type="bibr" rid="ref_5">5</xref>]. Advantages of CNN-based diagnostics include affordability, rapidity, and potentially equal or greater accuracy compared to laboratory-based tests, thereby enhancing accessibility.</p><p>The advent of CNN-based diagnostic methods could substantially influence the battle against the pandemic. By offering quicker, more precise, and affordable testing solutions, these methods promise to enhance testing efficiency and accessibility across diverse communities. Consequently, this could contribute to a reduction in COVID-19 transmission and associated mortalities [<xref ref-type="bibr" rid="ref_6">6</xref>].</p>
      
        <sec disp-level="level2">
          
            <title>1.1. Deep cnn</title>
          
          <p>In recent advancements, CNNs have been recognized as the most extensively researched machine learning methodologies for the diagnosis of medical conditions through imaging. The efficacy of CNNs is attributed to their capability to retain complex features while scanning input data. This characteristic is particularly crucial in radiology, where spatial relationships, such as the interfaces between bones and muscles or the transition from healthy to diseased lung tissue, are pivotal. As depicted in <xref ref-type="fig" rid="fig_1">Figure 1</xref>, the architecture of the system under study is detailed.</p><p>Each chest image, presented as a tensor of dimensions 244×244, is processed through a CNN structure comprising five convolutional layers. In the initial convolutional layer, 553 kernel filters are utilized with a stride of one, generating a maximum of 64 filters. This is followed by a max-pooling layer, which receives the output of the first layer and reduces the input dimensions by half to 112×112, employing a stride of two. The outcome of this pooling layer undergoes the Rectified Linear Unit (ReLU) activation function across all levels [<xref ref-type="bibr" rid="ref_7">7</xref>]. The processed nonlinear data then enters the subsequent convolutional layer, equipped with 55×64×128 filters and maintaining the same stride value. This output is again subject to max-pooling with identical strides of 2×2, further reducing dimensions to 56×56.</p><p>After ReLU activation, the output proceeds to the third convolutional layer, which houses 256 filters with a 5×5×128 kernel size and a stride of 1×1. The resulting output is channelled to a max-pooling layer, yielding a tensor of 28×28 dimensions. Post-ReLU activation, the signal enters the fourth convolutional layer, consisting of 512 filters with a 5×5×256 kernel size and a 1×1 stride. The fourth convolutional operation's output undergoes max-pooling, diminishing its dimensions to 14×14. Following ReLU activation, this data feeds into the fifth convolutional layer, which features 512 filters with a kernel size designed to accommodate the output from the preceding layers. This layer's output is then subjected to max-pooling with a 2×2 stride, maintaining an output size of 14×14. The resultant tensor assumes a form of 7×7×512. A compression of this tensor results in 25,088 neurons. The weighted values produced by these neurons are indicative of their correlation with COVID-19 symptoms. To prevent system overfitting, a dropout layer is employed, selectively omitting information.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>Deep CNN [<xref ref-type="bibr" rid="ref_7">7</xref>]</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_otbJjQvdMf5A9Kwy.png"/>
            </fig>
          
          <p>• Convolutional layer: This layer incorporates numerous filters, also known as kernels, each of which is applied to the input image to extract features and construct a new layer. These layers encapsulate key characteristics of the input image. The convolution operation, denoted by the symbol *, is a mathematical process where an input<italic> ln (t)</italic> is convolved with a kernel<italic> f(a)</italic>, resulting in a feature map<italic> F(t)</italic> represented as:</p>
          
            <disp-formula>
              <label>(1)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">F</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">t</mi>
                </mrow>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <msub>
                    <mrow data-mjx-texclass="ORD">
                      <mi mathvariant="normal">I</mi>
                    </mrow>
                    <mrow data-mjx-texclass="ORD">
                      <mrow data-mjx-texclass="ORD">
                        <mi mathvariant="normal">n</mi>
                      </mrow>
                    </mrow>
                  </msub>
                  <msup>
                    <mrow data-mjx-texclass="ORD"/>
                    <mo>∗</mo>
                  </msup>
                  <mrow data-mjx-texclass="ORD">
                    <mi mathvariant="normal">f</mi>
                  </mrow>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">t</mi>
                </mrow>
                <mo stretchy="false">(</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <mo stretchy="false">(</mo>
                <mo stretchy="false">)</mo>
              </math>
            </disp-formula>
          
          <p>For discrete convolution, where t is restricted to integer values, the process is defined as:</p>
          
            <disp-formula>
              <label>(2)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">F</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">t</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">a</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">f</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">t</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">a</mi>
                </mrow>
                <mo stretchy="false">(</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <mo stretchy="false">(</mo>
                <mo stretchy="false">)</mo>
                <mo>⋅</mo>
                <mo stretchy="false">(</mo>
                <mo>−</mo>
                <mo stretchy="false">)</mo>
                <munder>
                  <mo data-mjx-texclass="OP">∑</mo>
                  <mi>n</mi>
                </munder>
                <mstyle scriptlevel="0">
                  <mspace width="1em"/>
                </mstyle>
                <msub>
                  <mi>I</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mrow data-mjx-texclass="ORD">
                      <mi mathvariant="normal">n</mi>
                    </mrow>
                  </mrow>
                </msub>
              </math>
            </disp-formula>
          
          <p>In the context of CNNs as applied in this study, a key aspect of the methodology involves the implementation of a two-dimensional convolution procedure. This process entails the application of an input matrix, denoted as <italic>in(m, n)</italic>, and a convolution kernel, represented as <italic>f(a, b)</italic>. The convolution operation is defined mathematically as:</p>
          
            <disp-formula>
              <label>(3)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">F</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">t</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">a</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">b</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">f</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">m</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">a</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">n</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">b</mi>
                </mrow>
                <mo stretchy="false">(</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <mo stretchy="false">(</mo>
                <mo>,</mo>
                <mo stretchy="false">)</mo>
                <mo>⋅</mo>
                <mo stretchy="false">(</mo>
                <mo>−</mo>
                <mo>,</mo>
                <mo>−</mo>
                <mo stretchy="false">)</mo>
                <munder>
                  <mo data-mjx-texclass="OP">∑</mo>
                  <mrow data-mjx-texclass="ORD">
                    <mrow data-mjx-texclass="ORD">
                      <mi mathvariant="normal">a</mi>
                    </mrow>
                  </mrow>
                </munder>
                <munder>
                  <mo data-mjx-texclass="OP">∑</mo>
                  <mi>b</mi>
                </munder>
                <msub>
                  <mi>I</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mrow data-mjx-texclass="ORD">
                      <mi mathvariant="normal">n</mi>
                    </mrow>
                  </mrow>
                </msub>
              </math>
            </disp-formula>
          
          <p>It is pertinent to note that the convolution operation adheres to the commutative law. Consequently, this law allows for the reversal of the kernel, rendering an equivalent expression as:</p>
          
            <disp-formula>
              <label>(4)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">F</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">t</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">m</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">a</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">n</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">b</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">f</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">a</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">b</mi>
                </mrow>
                <mo stretchy="false">(</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <mo stretchy="false">(</mo>
                <mo>−</mo>
                <mo>,</mo>
                <mo>−</mo>
                <mo stretchy="false">)</mo>
                <mo>⋅</mo>
                <mo stretchy="false">(</mo>
                <mo>,</mo>
                <mo stretchy="false">)</mo>
                <munder>
                  <mo data-mjx-texclass="OP">∑</mo>
                  <mrow data-mjx-texclass="ORD">
                    <mrow data-mjx-texclass="ORD">
                      <mi mathvariant="normal">a</mi>
                    </mrow>
                  </mrow>
                </munder>
                <munder>
                  <mo data-mjx-texclass="OP">∑</mo>
                  <mrow data-mjx-texclass="ORD">
                    <mrow data-mjx-texclass="ORD">
                      <mi mathvariant="normal">b</mi>
                    </mrow>
                  </mrow>
                </munder>
                <msub>
                  <mi>I</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mrow data-mjx-texclass="ORD">
                      <mi mathvariant="normal">n</mi>
                    </mrow>
                  </mrow>
                </msub>
              </math>
            </disp-formula>
          
          <p>While convolution typically involves flipping the kernel, neural networks often use the cross-correlation formula, which is similar to convolution but does not involve this flipping. In this case, the operation is defined as:</p>
          
            <disp-formula>
              <label>(5)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">F</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">t</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">m</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">a</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">n</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">b</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">f</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">a</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="normal">b</mi>
                </mrow>
                <mo stretchy="false">(</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <mo stretchy="false">(</mo>
                <mo>+</mo>
                <mo>,</mo>
                <mo>+</mo>
                <mo stretchy="false">)</mo>
                <mo>⋅</mo>
                <mo stretchy="false">(</mo>
                <mo>,</mo>
                <mo stretchy="false">)</mo>
                <munder>
                  <mo data-mjx-texclass="OP">∑</mo>
                  <mi>a</mi>
                </munder>
                <munder>
                  <mo data-mjx-texclass="OP">∑</mo>
                  <mi>b</mi>
                </munder>
                <msub>
                  <mi>I</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mrow data-mjx-texclass="ORD">
                      <mi mathvariant="normal">n</mi>
                    </mrow>
                  </mrow>
                </msub>
              </math>
            </disp-formula>
          
          <p>• ReLU layer: The ReLU layer serves as an activation function, turning negative input values to zero. This operation, mathematically expressed as the following Eq. (6), optimizes learning by accelerating convergence and mitigating the issue of gradient vanishing.</p>
          
            <disp-formula>
              <label>(6)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>R</mi>
                <mi>x</mi>
                <mi>x</mi>
                <mo stretchy="false">(</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <mo data-mjx-texclass="OP" movablelimits="true">max</mo>
                <mo stretchy="false">(</mo>
                <mo>,</mo>
                <mo stretchy="false">)</mo>
                <mn>0</mn>
              </math>
            </disp-formula>
          
          <p>• Max pooling layer: Employing a sample-based discretization process, this layer aims to reduce the spatial dimensions of the input (such as an image or output from previous layers), thereby lowering the number of parameters and computation in the network. A typical implementation in this study used a kernel size of 33%, reducing the dimensions of the output from the last convolutional block.</p><p>• Batch normalization: This technique normalizes the output from the previous layer by subtracting the batch mean and dividing by the batch standard deviation. It enhances network stability and allows each layer to learn more independently from others.</p><p>• Fully connected layer: This layer uses the output from the previous layer to create a probability function for classification into different categories.</p><p>• Loss function: At this stage, the input data sample is subjected to a softmax function. This level is critical for making the final prediction. The loss function, essential for model training, is formulated accordingly.</p>
          
            <disp-formula>
              <label>(7)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <msub>
                  <mrow data-mjx-texclass="ORD">
                    <mi mathvariant="normal">L</mi>
                  </mrow>
                  <mrow data-mjx-texclass="ORD">
                    <mrow data-mjx-texclass="ORD">
                      <mi mathvariant="normal">i</mi>
                    </mrow>
                  </mrow>
                </msub>
                <mo>=</mo>
                <mo>−</mo>
                <mo data-mjx-texclass="NONE">⁡</mo>
                <mi>log</mi>
                <mrow data-mjx-texclass="INNER">
                  <mo data-mjx-texclass="OPEN">(</mo>
                  <mo data-mjx-texclass="CLOSE">)</mo>
                  <mfrac>
                    <mrow>
                      <mi>e</mi>
                      <mi>β</mi>
                      <mi>γ</mi>
                    </mrow>
                    <mrow>
                      <munderover>
                        <mo data-mjx-texclass="OP">∑</mo>
                        <mi>j</mi>
                        <mi>c</mi>
                      </munderover>
                      <mi>e</mi>
                      <mi>β</mi>
                      <mi>j</mi>
                    </mrow>
                  </mfrac>
                </mrow>
              </math>
            </disp-formula>
          
          <p>• Regularization: To prevent overfitting, a dropout technique, as proposed by Srivastava et al. [<xref ref-type="bibr" rid="ref_8">8</xref>], is employed. During training, this involves randomly setting a proportion of neurons to zero, effectively thinning the network.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>1.2. Transfer learning</title>
          
          <p>Transfer learning has emerged as a pivotal technique in specialized fields where acquiring extensive, high-quality data poses a challenge. In such contexts, the transfer of knowledge from a source to a target task often becomes a vital strategy [<xref ref-type="bibr" rid="ref_9">9</xref>]. This method leverages pre-trained models, optimizing for categorical sensitivities. It is observed that the initial layers of a CNN are trained to discern common features such as edges, textures, and shapes, while the deeper layers are adept at identifying more complex and unique characteristics of the image, such as pathological lesions.</p><p>In the field of Computer-Aided Diagnosis (CAD), a prevalent approach involves training only the top layer of the model with the target dataset, while utilizing the pre-initialized values of the base layers. This methodology reduces the likelihood of overfitting, a significant concern in neural network training cycles, particularly when limited by the number of model parameters. In this study, S. Pal and K. Simonyan utilized the ImageNet database, incorporating the VGG16, InceptionV3, and Xception models equipped with pre-trained weights. The ImageNet database, structured based on the WorldNet hierarchy, comprises over 3.2 million meticulously annotated images across 5,247 categories [<xref ref-type="bibr" rid="ref_10">10</xref>].</p>
          
            <sec disp-level="level3">
              
                <title>1.2.1 Vgg16</title>
              
              <p>The VGG16 architecture, also known as VGGNet, is a pre-trained deep CNN proficient in extracting visual features for class differentiation, thereby enhancing outcome accuracy. This architecture includes 16 convolutional layers with a significant receptive field of 3×3 and five max-pooling layers, each of size 2×2, for spatial pooling [<xref ref-type="bibr" rid="ref_11">11</xref>]. The model's proficiency in distinguishing visual differences between images is attributed to its depth. It also incorporates three fully connected layers, with the terminal layer being a softmax layer. ReLU activation functions are applied across all hidden units, and dropout regularization is integrated into the fully connected layers [<xref ref-type="bibr" rid="ref_10">10</xref>]. When the densely interconnected classifier is detached from the pre-trained VGG16 model, it can serve as an effective feature vector generator. The VGG16 architecture was employed as a pre-trained classifier, with SoftMax utilized for classification.</p>
            </sec>
          
          
            <sec disp-level="level3">
              
                <title>1.2.2 Inceptionv3</title>
              
              <p>The 'Inception' micro-architecture, introduced by Szegedy et al. [<xref ref-type="bibr" rid="ref_12">12</xref>], represents a deep CNN that utilizes varied filters for critical feature identification within images. Functioning as a multi-level feature extractor, the Inception model calculates 1×1, 3×3, and 5×5 convolutional layers within a unified system. The input undergoes processing through these filters, with the outcomes being concatenated along the channel dimension before proceeding to the next layer. The InceptionV3 architecture, as delineated in "Rethinking the Inception Architecture for Computer Vision" by Szegedy et al. [<xref ref-type="bibr" rid="ref_12">12</xref>], was employed. This version evaluated the Inception-v3 classifier using images from 1000 labeled classes in the ImageNet benchmark datasets, comprising both a CNN for feature extraction and fully connected &amp;amp;amp; softmax layers for classification.</p>
            </sec>
          
          
            <sec disp-level="level3">
              
                <title>1.2.3 Xception</title>
              
              <p>The Xception architecture, introduced by Chollet [<xref ref-type="bibr" rid="ref_13">13</xref>], represents a significant advancement in deep learning. It is predicated on depth-wise separable convolution, distinguishing it from conventional CNNs. Comprising 36 layers, this model excels in feature extraction due to its unique architecture. The publicly available version of Xception, developed using Keras and TensorFlow under the MIT license, includes these 36 convolutional layers, with exceptions only at the beginning and end of the network. In performance comparisons on the ImageNet database, Xception has demonstrated superior results over models like InceptionV3, various ResNet architectures, and VGG. These models have shown exceptional efficacy in classifying medical images, indicating their potential in aiding COVID-19 detection.</p><p>In the realm of medical imaging, CNNs have emerged as invaluable tools, particularly in the rapid and efficient analysis of large volumes of images. The utility of CNNs lies in their capability to discern critical features within medical images, a function crucial for accurate diagnosis. However, it is imperative to select the appropriate type of CNN for specific diagnostic tasks, as different architectures possess unique strengths and limitations. This study undertook a comprehensive evaluation of various CNN models to determine their efficacy in detecting COVID-19 from medical images. The ImageNet dataset was employed to provide these models with an initial learning base, leveraging its extensive collection of images. This approach ensures the models are robust and effective in their diagnostic capabilities. The performance of these models was rigorously assessed using a range of metrics to confirm their accuracy and reliability. The structure of the study is organized as follows: The subsequent section elucidates the related work undertaken for COVID-19 identification, along with the objectives and approach of this study. Section 3 delineates the CNN topologies utilized, while Section 4 presents the results of simulations comparing the proposed models. Finally, Section 5 encapsulates the findings of this study.</p><p>Recent studies have extensively explored COVID-19 detection using machine learning methodologies. A notable challenge faced by researchers, attributable to the scarcity of specialized databases, has been the application of CNN algorithms to CT images [<xref ref-type="bibr" rid="ref_14">14</xref>]. Although various studies have presented outcomes in tabular formats, direct comparisons often prove challenging due to the heterogeneity and varying complexity of the utilized databases. This section further details the endeavours related to COVID-19 detection on CT images, emphasizing the employed methodologies.</p><p>Anu et al. (2021) implemented a deep ensemble-based technique for detecting COVID-19 related fake news, incorporating Support Vector Machine (SVM), dense neural networks, and CNN within the ensemble classifier. The study conducted exhaustive testing using character and word n-gram term frequency–inverse document frequency (TF-IDF) features, comparing the ensemble architecture against eight traditional machine learning classifiers. The findings indicated that character n-gram features were superior to word n-gram features, with the ensemble classifier achieving a weighted F1-score of 0.97.</p>
              
                <table-wrap id="table_1">
                  <label>Table 1</label>
                  <caption>Comparative analysis of several surveyed studies and their respective methodologies</caption>
                  <abstract/>
                  <table><tbody><tr><td colspan="1" rowspan="1"><p>Author Name</p></td><td colspan="1" rowspan="1"><p>Year</p></td><td colspan="1" rowspan="1"><p>Method</p></td><td colspan="1" rowspan="1"><p>Parameter Used</p></td><td colspan="1" rowspan="1"><p>Dataset Used</p></td><td colspan="1" rowspan="1"><p>Weaknesses</p></td><td colspan="1" rowspan="1"><p>Strengths/Conclusions</p></td></tr><tr><td colspan="1" rowspan="1"><p>Muhammad et al.</p></td><td colspan="1" rowspan="1"><p>2020</p></td><td colspan="1" rowspan="1"><p>DT, SVM, NB, LR, RF, K-NN</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>Epidemiological database of COVID-19 patients in South Korea</p><p></p></td><td colspan="1" rowspan="1"><p>The absence of data points reduces predictive accuracy and can lead to biased conclusions.</p></td><td colspan="1" rowspan="1"><p>The DT model exhibited the highest accuracy at 99.85%, followed by RF (99.60%), SVM (98.85%), K-NN (98.06%), NB (97.52%), and LR (97.49%). These models are instrumental in advancing healthcare strategies against COVID-19.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Anu et al.</p></td><td colspan="1" rowspan="1"><p>2021</p></td><td colspan="1" rowspan="1"><p>SVM, DNN, and CNN.</p></td><td colspan="1" rowspan="1"><p>Precision, Recall, ROC-AUC, F1 score</p></td><td colspan="1" rowspan="1"><p>COVID-19-related fake news</p></td><td colspan="1" rowspan="1"><p>The omission of character-level features in detecting COVID-19 fake news warrants further investigation.</p></td><td colspan="1" rowspan="1"><p>The study demonstrates superior performance using character-level features over word-level features with the classifiers.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Ali et al.</p></td><td colspan="1" rowspan="1"><p>2020</p></td><td colspan="1" rowspan="1"><p>DeepLabV3+</p></td><td colspan="1" rowspan="1"><p>F1 Score</p></td><td colspan="1" rowspan="1"><p>Publicly available dataset</p><p>provided by Shenzhen Hospital, that contains 566 CRs with</p><p>manually segmented lungs (ground truth)</p></td><td colspan="1" rowspan="1"><p>The lack of automated lung segmentation hinders efficiency in medical imaging.</p></td><td colspan="1" rowspan="1"><p>The model achieves an Intersection-Over-Union (IoU, Jaccard Index) score of 0.97 on the test set, illustrating its effectiveness.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Saha et al.</p></td><td colspan="1" rowspan="1"><p>2020</p></td><td colspan="1" rowspan="1"><p>Several traditional CNN architectures are tested and</p><p>finally in the ensemble operation, MobileNet, InceptionV3,</p><p>DenseNet201, DenseNet121 and Xception are used.</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>The</p><p>Kaggle dataset is composed of 1,583 normal and 4273 pneumonia images of pediatric patients from Guangzhou Women</p><p>and Children’s Medical Center, Guangzhou</p></td><td colspan="1" rowspan="1"><p>Exploration of other models and ensemble techniques with a more extensive dataset is needed.</p></td><td colspan="1" rowspan="1"><p>The model achieves 96% accuracy in 3-class (COVID-19/normal/pneumonia) diagnosis and 89.21% in 4-class (COVID-19/normal/viral pneumonia/bacterial pneumonia) diagnosis.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Tewari et al.</p></td><td colspan="1" rowspan="1"><p>2020</p></td><td colspan="1" rowspan="1"><p>VGG-16, ResNet-50 and MobileNetV2</p></td><td colspan="1" rowspan="1"><p>Accuracy, precision, recall and F1-Score</p></td><td colspan="1" rowspan="1"><p>The final augmented</p><p>dataset consists a total of 7,585 images with varying resolutions</p><p>in which 2,255 are of COVID-19, 2,614 are of viral pneumonia</p><p>and 2,716 are of normal category</p></td><td colspan="1" rowspan="1"><p>Limitations in training and evaluating the model on a larger dataset.</p></td><td colspan="1" rowspan="1"><p>The proposed model attained an overall accuracy of 96.34%. For the COVID-19 class, precision, recall, and F1-Score were recorded at 100%, 96%, and 98%, respectively.</p></td></tr><tr><td colspan="1" rowspan="1"><p>Deep Deb et al.</p></td><td colspan="1" rowspan="1"><p>2020</p></td><td colspan="1" rowspan="1"><p>DCNN</p></td><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>The</p><p>chest X-ray dataset used for our experimentation. The same was acquired on 17 April 2020</p></td><td colspan="1" rowspan="1"><p>Slight delay in processing is acceptable for achieving higher accuracy.</p></td><td colspan="1" rowspan="1"><p>An accuracy of 91.99% was achieved, marginally surpassing current state-of-the-art performances.</p></td></tr></tbody></table>
                </table-wrap>
              
              <p>Ali et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] showcased the application of a lung segmentation ensemble deep network, based on an advanced iteration of DeepLabV3, known as DeepLabV3+. This system utilized diverse topologies including ResNet18, ResNet50, Mobilenetv2, Xception, and inceptionresnetv2. Enhancements were made to the spatial pyramid pooling’s receptive field within the DeepLabV3+ encoder module. The method underwent testing on a publicly available dataset from Shenzhen Hospital, comprising 566 chest radiographs with manually segmented lungs, achieving an Intersection-Over-Union (IoU) score of 0.97 on the test dataset. In recent advancements in AI-assisted medical diagnostics, notable strides have been made in the application of CNNs for COVID-19 detection. Saha et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] developed a novel approach that integrates multiple CNN architectures in an ensemble framework. This methodology encompassed two distinct strategies: feature-level fusion and decision-level ensemble techniques. Prior to the assembly phase, several standard CNN designs, including MobileNet, InceptionV3, DenseNet201, DenseNet121, and Inception, were evaluated. The transfer learning approach was implemented, utilizing ImageNet pre-trained weights to manage the computational demands of multiple networks. The convolutional feature maps from various layers were globally averaged, followed by passage through fully connected layers, facilitating joint optimization in feature-level ensemble technique. Additionally, the decision-level ensemble method employed majority voting to combine final predictions from multiple networks. Despite the integration of various techniques, neither strategy exceeded the performance of individual models. Nonetheless, testing on the COVID-CT database demonstrated remarkable results, with a 96% accuracy in 3-class identification (COVID-19/normal/pneumonia) and 89.21% accuracy in 4-class identification [<xref ref-type="bibr" rid="ref_16">16</xref>]. Tewari et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] explored a deep learning approach incorporating fuzzy image enhancement, offline data augmentation, image segmentation, and CNN-based classification for detecting COVID-19 in chest X-ray images. The proposed model integrated features from VGG-16, ResNet-50, and MobileNetV2, achieving an overall accuracy of 96.34%. Notably, the precision, recall, and F1-score for the COVID-19 class were 100%, 96%, and 98%, respectively. Deb et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] formulated an ensemble architecture based on deep CNN for feature extraction from chest X-ray images, categorizing them into three classes: CAP, healthy, and COVID-19. The ensemble system incorporated NASNet, MobileNet, and DenseNet, combining low-level features extracted from these frameworks for classification. This method yielded a precision of 91.99%, marginally surpassing existing state-of-the-art performances. Muhammad et al. [<xref ref-type="bibr" rid="ref_19">19</xref>] utilized epidemiological databases of COVID-19 patients in South Korea to develop data mining techniques for predicting patient recovery. The implementation involved various methods, including decision trees, SVM, Naïve Bayes, logistic regression, random forests, and K-nearest neighbors, using Python programming. The decision tree-based model exhibited the highest level of effectiveness with an average accuracy of 99.85%, followed by random forests, SVM, K-nearest neighbors, Naïve Bayes, and logistic regression in descending order of accuracy. These findings hold substantial promise in enhancing healthcare strategies against COVID-19.</p><p>The COVID-19 pandemic has emerged as a formidable global health crisis, exerting unprecedented strain on societal and healthcare frameworks. This is attributed to the escalating rates of infection and mortality. Early identification of patients is a critical intervention in mitigating the pandemic's impact and alleviating the burden on healthcare systems. The delayed diagnostic process has been identified as a key factor in the rapid spread of COVID-19. Acceleration of the diagnostic procedure can be facilitated through imaging techniques such as chest X-rays. Numerous studies have been conducted with the aim of enhancing the speed and accuracy of COVID-19 detection using imaging. A prominent approach in recent research is the application of machine learning techniques for classification purposes. An emerging method involves the utilization of deep transfer learning algorithms as classifiers. Despite the effectiveness of these models, detailed analysis reveals potential areas for improvement. A notable deficiency in the current systems is the lack of feature extraction or preprocessing techniques, which are critical in analyzing X-ray images where textures and patterns constitute significant diagnostic information. Consequently, the objectives of this study are outlined as follows:</p><p>• To develop and implement a model for feature extraction based on Principal Component Analysis (PCA).</p><p>• To devise and apply a method for COVID-19 detection using chest X-rays, incorporating data preprocessing and augmentation.</p><p>• To enhance accuracy through the implementation of advanced deep learning algorithms, employing multiple transfer learning techniques, and ensemble and novel ensemble classifiers.</p><p>• To conduct a comprehensive analysis and comparison of the proposed model with existing methodologies.</p><p>Several surveyed studies and their respective methodologies are compared in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
            </sec>
          
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>2. Methodology</title>
      <p>The methodology adopted for this study encompasses distinct phases, as illustrated in <xref ref-type="fig" rid="fig_2">Figure 2</xref>.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>Different phases of the methodology</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_IaKil-RWgLjGGmOo.png"/>
        </fig>
      
      <p>Phase 1: Data collection</p><p>Data was sourced from online repositories containing key attributes such as age, gender, and symptoms (cough, sore throat, shortness of breath, fever, headache), along with additional data regarding contact with COVID-19 patients. The dataset includes:</p><p>Basic information:gender (male/female), age <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mo>≥</mo>
  </math>
</inline-formula> 60 years (yes/no);</p><p>Symptoms: cough (yes/no), fever (yes/no), sore throat (yes/no), shortness of breath (yes/no), headache (yes/no);</p><p>Additional data: confirmed interaction with a COVID-19 positive individual (true/false).</p><p>Phase 2: Data preprocessing or cleaning</p><p>The raw data obtained required preprocessing and cleaning. This phase involved a series of filtration processes, such as replacing missing values, eliminating entries without a class index, converting data types, and normalizing the dataset. In instances where attribute values were absent, a missing value filter was applied, substituting the missing entries with the attribute’s average value. Furthermore, PCA was utilized for feature extraction, aiming to distill the data into a more concise and informative representation. PCA facilitates the identification of patterns and reduces dimensionality while preserving critical information and eliminating redundant features. This procedure is expected to augment the efficiency of the subsequent machine learning models by concentrating on the most pertinent aspects of the input data.</p><p>Phase 3: Feature selection</p><p>Feature selection involves the meticulous selection of pivotal attributes from a dataset, emphasizing those crucial for the specific objective. This process eliminates superfluous or irrelevant features, focusing on detailed selection, attribute selection, or variable selection. Features with a broad range of values are typically preferred, though care is taken to avoid overfitting. In this study, Random Forest was employed for feature selection due to its proficiency in handling diverse features and resilience against noisy data. Random rorest, an ensemble machine learning method, constructs numerous decision trees during training and outputs the mode of the classes for classification purposes. Its ability to handle complex data relationships and mitigate overfitting makes it an ideal choice for feature selection, classification, and regression tasks. Random forest's versatility in handling various data types enhances the performance of the classification model by identifying and prioritizing the most influential features.</p><p>Phase 4: Implementation of hybrid classification algorithm</p><p>In this study, a hybrid classification algorithm, integrating random forest and gradient boosting, was employed. This amalgamation of deep learning predictions with ensemble classification forecasts is designed to enhance predictive accuracy, surpassing the capabilities of individual models. In sentiment analysis, where classification is crucial, the use of such a combination is advantageous, as each method compensates for the limitations of the other. A voting mechanism is incorporated to refine the model’s architecture further. Despite the increased complexity associated with using multiple algorithms, the primary objective is the development of a model with superior performance. The most effective algorithm is determined through an average probability phase, ensuring a robust classification approach.</p><p>Phase 5: Model evaluation techniques</p><p>Various model evaluation strategies were considered, with four options provided by the Weka machine learning workbench selected for this study:</p><p>a) Percentage split: The dataset is randomly divided into training and testing partitions. This approach offers a rapid performance approximation and is recommended for large datasets.</p><p>b) Cross-validation: The data is divided into k-folds, with each fold serving as a test set in turn, while the model is trained on the remaining folds. This method is a standard for performance assessment, though it requires the generation of multiple model variants.</p><p>c) Training dataset: The model is trained and tested on the same dataset. This approach can be misleading as a perfect algorithm might memorize training patterns, resulting in inflated performance metrics.</p><p>d) Provided test set: The dataset is manually split into training and testing sets. The model is trained on the entire training set and evaluated on a separate test set. This method is suitable for datasets with a large number of instances.</p><p>Phase 6: Performance metrics</p><p>The performance of the two classification algorithms was compared using metrics such as accuracy, recall, precision, F-score, and error rate. The algorithm exhibiting the highest accuracy, recall, precision, F-measure, and the lowest error rate was deemed superior. Performance parameters are calculated using the following terms:</p><p>TP (True Positive): Correctly classified positive instance.</p><p>TN (True Negative): Correctly classified negative instance.</p><p>FP (False Positive): Incorrectly classified positive instance.</p><p>FN (False Negative): Incorrectly classified negative instance.</p><p>These terms are incorporated into the confusion matrix for analytical purposes. A confusion matrix, also known as an error matrix, displays the TP, FN, TP, and TN values in a two-row, two-column table. <xref ref-type="table" rid="table_2">Table 2</xref> illustrates the confusion matrix utilized in this study for machine learning purposes.</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>Confusion matrix for machine learning</caption>
          <abstract/>
          <table><tbody><tr><td colspan="1" rowspan="2" colwidth="309"><p></p><p> </p></td><td colspan="1" rowspan="2"><p> </p></td><td colspan="2" rowspan="1"><p>Correct Labels</p></td></tr><tr><td colspan="1" rowspan="1"><p>Positive</p></td><td colspan="1" rowspan="1"><p>Negative</p></td></tr><tr><td colspan="1" rowspan="2" colwidth="309"><p> </p><p>Classified Labels</p></td><td colspan="1" rowspan="1"><p>Positive</p></td><td colspan="1" rowspan="1"><p>TP</p></td><td colspan="1" rowspan="1"><p>FP</p></td></tr><tr><td colspan="1" rowspan="1"><p>Negative</p></td><td colspan="1" rowspan="1"><p>FN</p></td><td colspan="1" rowspan="1"><p>TN</p></td></tr></tbody></table>
        </table-wrap>
      
      
        <sec disp-level="level2">
          
            <title>2.1. Precision and recall</title>
          
          <p>Precision and recall are critical metrics employed in the evaluation of performance in fields such as text mining and information extraction. These metrics are instrumental in assessing the exactitude and comprehensiveness of a dataset. Precision is defined as the ratio of true positive instances to the sum of true positive and false positive instances. It reflects the accuracy of positive classifications. Recall, on the other hand, is the ratio of true positive instances to the sum of true positive and false negative instances, indicating the completeness of the positive classifications.</p>
          
            <disp-formula>
              <label>(8)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mtext> Precision </mtext>
                <mo>=</mo>
                <mfrac>
                  <mtext> True Positive </mtext>
                  <mrow>
                    <mtext> True Positive </mtext>
                    <mtext> False Positive </mtext>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(9)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mtext> Recall </mtext>
                <mo>=</mo>
                <mfrac>
                  <mtext> True Positive </mtext>
                  <mrow>
                    <mtext> True Positive </mtext>
                    <mtext> False Negative </mtext>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>2.2. F-score</title>
          
          <p>The F-score, is the harmonic mean of precision and recall. This metric provides a balanced measure that considers both the precision and recall of a classification system. The F-score is particularly useful when seeking an equilibrium between precision and recall, ensuring neither is disproportionately emphasized.</p>
          
            <disp-formula>
              <label>(10)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mtext> F-measure </mtext>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mn>2</mn>
                    <mo>∗</mo>
                    <mo>∗</mo>
                    <mtext> recall </mtext>
                    <mtext> precision </mtext>
                  </mrow>
                  <mrow>
                    <mtext> precision </mtext>
                    <mtext> recall </mtext>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>2.3. Accuracy</title>
          
          <p>Accuracy stands as one of the most commonly utilized metrics for evaluating classification performance. It is calculated as the ratio of correctly classified instances (both true positive and true negative) to the total number of instances in the dataset. Conversely, the error rate is determined by the proportion of incorrectly classified instances (both false positive and false negative) to the total dataset.</p>
          
            <disp-formula>
              <label>(11)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mtext> Accuracy </mtext>
                <mo>=</mo>
                <mfrac>
                  <mrow>
                    <mtext> True Positive </mtext>
                    <mtext> True Negative </mtext>
                    <mo>+</mo>
                  </mrow>
                  <mrow>
                    <mtext> True Positive </mtext>
                    <mtext> False Positive </mtext>
                    <mtext> True Negative </mtext>
                    <mtext> False Negative </mtext>
                    <mo>+</mo>
                    <mo>+</mo>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
          <p>In the aforementioned equations:</p><p>True Positive denotes instances where the prediction is positive, and the actual outcome is also positive.</p><p>True Negative represents instances where the prediction is negative, and the actual outcome is negative.</p><p>False Positive signifies instances where the prediction is positive, but the actual outcome is negative.</p><p>False Negative refers to instances where the prediction is negative, but the actual outcome is positive.</p>
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>3. Result and discussion</title>
      <p>The integration of DL algorithms in advanced medical diagnostic systems, particularly for COVID-19 detection through medical image processing, is increasingly pivotal. The research methodology for COVID-19 detection in this study is characterized by three principal phases:</p><p>(a) Development of DL methods and ensemble procedure</p><p>• Three distinct DL methods, along with an ensemble procedure, were developed.</p><p>• Models 1 and 3 consist of three convolutional blocks, each succeeded by a max pooling layer, while Model 2 comprises four convolutional blocks with subsequent max pooling layers.</p><p>• Batch normalization follows the first convolutional block to accelerate the learning process of the CNN model.</p><p>• Dropout layers are incorporated to mitigate overfitting in deep CNN architectures.</p><p>•  Each method includes fully connected layers to facilitate comprehensive data analysis.</p><p>(b) Validation using CT images</p><p>• CT images were utilized to validate the DL framework. The compilation of a balanced database was prioritized, encompassing CT scans of both COVID-19 and non-COVID pulmonary infections.</p><p>(c) Experimental evaluation</p><p>• The DL + ensemble technique underwent evaluation using the COVID-CT database, with a focus on smaller-scale databases for robust analysis.</p><p>•  The fully connected layer, subsequent to the fusion layer, comprised 256 neurons with ReLU activation and parameter regularization.</p><p>•  A dropout rate of 0.2 was applied post each fully connected layer.</p><p>•  Accuracy, precision, recall, and F-score of the proposed 5-Clf, proposed 8-Clf, and DL with ensemble approach were compared, as depicted in <xref ref-type="fig" rid="fig_3">Figure 3</xref>.</p><p>The results of this study underscore the efficacy of the proposed innovative ensemble method in detecting COVID-19 infection from chest CT images. Experimental analyses were conducted using the COVID-CT database, evaluating the performance of the suggested DL + ensemble method, particularly in the context of smaller-scale databases.</p>
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>Framework of the proposed novel ensemble classifier</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_5y853O_mK5xkhReX.png"/>
        </fig>
      
      <p>Each fully connected layer, following the fusion layer, contained 256 neurons with ReLU activation and regularization of parameters. Furthermore, a dropout rate of 0.2 was implemented following each fully connected layer. <xref ref-type="fig" rid="fig_4">Figure 4</xref> shows the training, validation of accuracy and loss in case of CNN design. <xref ref-type="fig" rid="fig_5">Figure 5</xref> and <xref ref-type="fig" rid="fig_6">Figure 6</xref> exhibit the training and validation of accuracy and loss for VGG-19, respectively.</p>
      
        <fig id="fig_4">
          <label>Figure 4</label>
          <caption>Training, validation of accuracy and loss (CNN design)</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_P2B6B49GPAvKjVU8.png"/>
        </fig>
      
      
        <fig id="fig_5">
          <label>Figure 5</label>
          <caption>Training and validation of loss (VGG-19)</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_iQvuMi8Z_4b5gdX4.png"/>
        </fig>
      
      
        <fig id="fig_6">
          <label>Figure 6</label>
          <caption>Training and validation of accuracy (VGG-19)</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_sgwRstET-DM9_J_p.png"/>
        </fig>
      
      
        <fig id="fig_7">
          <label>Figure 7</label>
          <caption>Training and validation of accuracy (ResNet 50)</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_jNsnhFNpftCJs33b.png"/>
        </fig>
      
      
        <fig id="fig_8">
          <label>Figure 8</label>
          <caption>Training and validation of loss (ResNet 50)</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_3DwTpo5Vs8BCT0xU.png"/>
        </fig>
      
      <p><xref ref-type="fig" rid="fig_4">Figure 4</xref>, <xref ref-type="fig" rid="fig_5">Figure 5</xref>, <xref ref-type="fig" rid="fig_6">Figure 6</xref>, <xref ref-type="fig" rid="fig_7">Figure 7</xref> and <xref ref-type="fig" rid="fig_8">Figure 8</xref> illustrate the training and validation of accuracy and loss for various CNN designs, including VGG-19 and ResNet-50. These figures depict the effectiveness of each model in the context of training and validation phases.</p>
      <p><xref ref-type="fig" rid="fig_9">Figure 9</xref> presents a comparative analysis of key performance metrics, namely, accuracy, precision, recall, and F-score, for the proposed 5-Clf, 8-Clf models, and the DL with ensemble approach.</p>
      
        <fig id="fig_9">
          <label>Figure 9</label>
          <caption>Comparison of performance parameter</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_rTq0gJGZACAL3rfy.png"/>
        </fig>
      
      <p>The efficacy of the proposed DL+ ensemble technique was evaluated against the top 5 designs (5-Clf) and all models (8-Clf) to ascertain whether lower-performing models adversely impact the effectiveness of stronger models in ensemble algorithms. As shown in <xref ref-type="table" rid="table_3">Table 3</xref>, the DL+ ensemble approach demonstrated the highest precision across all parameters, with accuracy at 99.50%, precision at 99.50%, recall at 99.50%, and F-score at 99.50%. Furthermore, <xref ref-type="fig" rid="fig_10">Figure 10</xref> showcases the performance of various ensemble designs juxtaposed with the proposed technique, providing insights into the relative effectiveness of each approach.</p>
      
        <table-wrap id="table_3">
          <label>Table 3</label>
          <caption>Comparative analysis of performance metrics</caption>
          <abstract/>
          <table><tbody><tr><td colspan="1" rowspan="1"><p> Parameters</p></td><td colspan="1" rowspan="1"><p> Proposed 5-Clf</p></td><td colspan="1" rowspan="1"><p> Proposed 8-Clf</p></td><td colspan="1" rowspan="1"><p>DL+ Ensemble</p></td></tr><tr><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>98.99</p></td><td colspan="1" rowspan="1"><p>98.99</p></td><td colspan="1" rowspan="1"><p>99.500000</p></td></tr><tr><td colspan="1" rowspan="1"><p>Precision</p></td><td colspan="1" rowspan="1"><p>99.02</p></td><td colspan="1" rowspan="1"><p>98.98</p></td><td colspan="1" rowspan="1"><p>99.500000</p></td></tr><tr><td colspan="1" rowspan="1"><p>Recall</p></td><td colspan="1" rowspan="1"><p>98.97</p></td><td colspan="1" rowspan="1"><p>99.00</p></td><td colspan="1" rowspan="1"><p>99.504950</p></td></tr><tr><td colspan="1" rowspan="1"><p>F-score</p></td><td colspan="1" rowspan="1"><p>98.99</p></td><td colspan="1" rowspan="1"><p>98.99</p></td><td colspan="1" rowspan="1"><p>99.497487</p></td></tr></tbody></table>
        </table-wrap>
      
      
        <fig id="fig_10">
          <label>Figure 10</label>
          <caption>Comparison of performance parameter</caption>
          <abstract/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/11/img_omR3qYLjp3F9VU2a.png"/>
        </fig>
      
      <p><xref ref-type="table" rid="table_4">Table 4</xref> presents a comparative analysis of several ensemble designs, highlighting the effectiveness of the proposed DL+ ensemble technique. In this approach, predictions from three distinct deep learning models were integrated using a weighted average, where the weight assigned to each model correlated with its accuracy on the validation set.</p>
      
        <table-wrap id="table_4">
          <label>Table 4</label>
          <caption>Comparative analysis of ensemble techniques</caption>
          <abstract/>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Parameters</p></td><td colspan="1" rowspan="1"><p> Max Voting</p></td><td colspan="1" rowspan="1"><p>Bagging (Random Forest)</p></td><td colspan="1" rowspan="1"><p>Boosting (Gradient Boosting)</p></td><td colspan="1" rowspan="1"><p>DL+ Ensemble</p></td></tr><tr><td colspan="1" rowspan="1"><p>Accuracy</p></td><td colspan="1" rowspan="1"><p>98.39</p></td><td colspan="1" rowspan="1"><p>96.18</p></td><td colspan="1" rowspan="1"><p>98.19</p></td><td colspan="1" rowspan="1"><p>99.500000</p></td></tr><tr><td colspan="1" rowspan="1"><p>Precision</p></td><td colspan="1" rowspan="1"><p>98.37</p></td><td colspan="1" rowspan="1"><p>98.26</p></td><td colspan="1" rowspan="1"><p>98.17</p></td><td colspan="1" rowspan="1"><p>99.500000</p></td></tr><tr><td colspan="1" rowspan="1"><p>Recall</p></td><td colspan="1" rowspan="1"><p>98.40</p></td><td colspan="1" rowspan="1"><p>96.32</p></td><td colspan="1" rowspan="1"><p>98.25</p></td><td colspan="1" rowspan="1"><p>99.504950</p></td></tr><tr><td colspan="1" rowspan="1"><p>F-score</p></td><td colspan="1" rowspan="1"><p>98.39</p></td><td colspan="1" rowspan="1"><p>96.18</p></td><td colspan="1" rowspan="1"><p>98.19</p></td><td colspan="1" rowspan="1"><p>99.497487</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>The superior performance of the DL+ ensemble method compared to other models is attributed to its integration of varied deep learning architectures. Each model within the ensemble possesses unique strengths and compensates for the limitations of others, contributing to a reduction in the overall error rate. The application of weighted averaging in aggregating predictions further enhances the model's accuracy, allocating greater significance to predictions from better-performing models. Ensemble methods, by amalgamating the forecasts of multiple models, effectively decrease the incidence of misclassifications. This reduction is achieved as different models in the ensemble may commit disparate errors, but their collective predictions lead to a diminished error rate. Moreover, ensemble methods exhibit enhanced generalizability compared to individual models, thereby reducing the propensity to overfit to training data. </p><p>The DL+ ensemble model outperformed the other ensemble methods due to its integration of diverse deep learning models. Each model contributes unique strengths and mitigates individual weaknesses, collectively reducing the overall error rate. The application of weighted averaging in combining predictions further accentuates the influence of better-performing models. Ensemble methods, by amalgamating multiple model predictions, reduce misclassifications more effectively than individual models. These methods offer enhanced generalizability, thus reducing the likelihood of overfitting to training data.</p><p>The implications of these findings extend to both future research and practical applications. The results demonstrate that ensemble methods are a promising avenue for COVID-19 detection from chest CT images. The diversity of models within the ensemble underscores the importance of varied approaches. Additionally, the study offers insights into optimizing ensemble models for improved diagnostic accuracy.</p><p>In practical terms, the DL+ ensemble approach holds potential for the development of a CAD system for COVID-19 detection. Such a system could significantly aid radiologists in rendering more accurate and expedient diagnoses.</p>
    </sec>
    <sec disp-level="level1" sec-type="conclusions">
      <title>4. Conclusions</title>
      <p>The ensemble approach proposed in this study for the detection of COVID-19 from chest CT scan images has been demonstrated to surpass other models in terms of accuracy, precision, recall, and F-score. This approach integrates the deep pre-trained convolutional bases from VGGNet, ResNet, MobileNet, InceptionV3, Xception, and IRV2, enabling the extraction of discriminative features from CT scan images. Several advantages have been identified in the proposed ensemble method compared to traditional diagnostic approaches. Firstly, the integration of predictions from multiple models contributes to a reduction in the overall error rate. Secondly, the utilization of deep pre-trained convolutional bases facilitates the extraction of key features from CT scan images, obviating the need for extensive model training. Thirdly, the implementation of a regularized categorization head aids in preventing overfitting to the training data.</p><p>The enhanced diagnostic accuracy afforded by this ensemble approach holds significant potential for impacting the diagnosis and treatment of COVID-19 patients, particularly in real-world scenarios. Moreover, the potential of this approach in developing a CAD system for COVID-19 detection is considerable. Such a system could assist radiologists, especially in settings where resources are limited. Future research directions include evaluating the performance of the proposed ensemble approach with a more diverse dataset. The intention is to incorporate CT scan images from patients with various types of pneumonia and other medical conditions that may present similar characteristics to COVID-19 on CT scans. Additionally, the incorporation of other medical data types, such as clinical and laboratory data, will be explored to further enhance the ensemble approach's performance.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p></p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>4-15</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>F.</given-names>
              <surname>Shi</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Shi</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Wu</surname>
            </name>
            <name>
              <given-names>Q.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Tang</surname>
            </name>
            <name>
              <given-names>K.</given-names>
              <surname>He</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Shi</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Shen</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/rbme.2020.2987975</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Review of artificial intelligence techniques in imaging data acquisition, segmentation, and diagnosis for COVID-19</article-title>
          <source>IEEE Rev. Biomed. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>296</volume>
          <page-range>E115-E117</page-range>
          <issue>2</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Y.</given-names>
              <surname>Fang</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Xie</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Lin</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Ying</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Pang</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Ji</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1148/radiol.2020200432</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Sensitivity of chest CT for COVID-19: Comparison to RT-PCR</article-title>
          <source>Radiology</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>155</volume>
          <page-range>159-168</page-range>
          <issue/>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Valverde</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Cabezas</surname>
            </name>
            <name>
              <given-names>E.</given-names>
              <surname>Roura</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>González-Villà</surname>
            </name>
            <name>
              <given-names>D.</given-names>
              <surname>Pareto</surname>
            </name>
            <name>
              <given-names>J. C.</given-names>
              <surname>Vilanova</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Ramió-Torrentà</surname>
            </name>
            <name>
              <given-names>À.</given-names>
              <surname>Rovira</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Oliver</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Lladó</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.04.034</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Improving automated multiple sclerosis lesion segmentation with a cascaded 3D CNN approach</article-title>
          <source>NeuroImage</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>89</volume>
          <page-range>389-396</page-range>
          <issue/>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <given-names>U. R.</given-names>
              <surname>Acharya</surname>
            </name>
            <name>
              <given-names>S. L.</given-names>
              <surname>Oh</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Hagiwara</surname>
            </name>
            <name>
              <given-names>J. H.</given-names>
              <surname>Tan</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Adam</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Gertych</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>San Tan</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compbiomed.2017.08.022</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A deep convolutional neural network model to classify heartbeats</article-title>
          <source>Comput. Biol. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>31</volume>
          <page-range>5147-5158</page-range>
          <issue>9</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Z.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Enoh</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00521-018-04007-6</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Removing ring artifacts in CBCT images via generative adversarial networks with unidirectional relative total variation loss</article-title>
          <source>Neural Comput. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>36</volume>
          <page-range>2524-2535</page-range>
          <issue>12</issue>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <given-names>H.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>M. K.</given-names>
              <surname>Kalra</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Lin</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>P.</given-names>
              <surname>Liao</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Zhou</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Wang</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/tmi.2017.2715284</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Low-Dose CT with a residual encoder-decoder CNN</article-title>
          <source>IEEE Trans. Med. Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>42</volume>
          <page-range>60-88</page-range>
          <issue/>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <given-names>G.</given-names>
              <surname>Litjens</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Kooi</surname>
            </name>
            <name>
              <given-names>B. E.</given-names>
              <surname>Bejnordi</surname>
            </name>
            <name>
              <given-names>A. A. A.</given-names>
              <surname>Setio</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Ciompi</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Ghafoorian</surname>
            </name>
            <name>
              <given-names>J. A.W. M.</given-names>
              <surname>van der Laak</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>van Ginneken</surname>
            </name>
            <name>
              <given-names>C. I.</given-names>
              <surname>Sánchez</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.media.2017.07.005</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A survey on deep learning in medical image analysis</article-title>
          <source>Med. Image Anal.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>1929–1958</page-range>
          <issue/>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <given-names>N.</given-names>
              <surname>Srivastava</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Hinton</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Krizhevsky</surname>
            </name>
            <name>
              <given-names>I.</given-names>
              <surname>Sutskever</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Salakhutdinov</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>Dropout: A simple way to prevent neural networks from overfitting</article-title>
          <source>J. Mach. Learn. Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range/>
          <issue>1</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>M. T.</given-names>
              <surname>Letsatsi</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Agarwal</surname>
            </name>
            <name>
              <given-names>O. M.</given-names>
              <surname>Seretse</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>The battle towards skill-based competency integration to knowledge-based competency in the sustainable development of growing economy</article-title>
          <source>Int. J. Recent Technol. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="conference-proceedings">
          <volume/>
          <page-range>248–255</page-range>
          <issue/>
          <year>0</year>
          <publisher-name>Miami, USA, 2009</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>J.</given-names>
              <surname>Deng</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Dong</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Socher</surname>
            </name>
            <name>
              <given-names>L.</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names/>
              <surname>Kai Li</surname>
            </name>
            <name>
              <given-names/>
              <surname>Li Fei-Fei</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/cvpr.2009.5206848</pub-id>
          <article-title>ImageNet: A large-scale hierarchical image database</article-title>
          <source>, http://dx.doi.org/10.1109/CVPR.2009.5206848</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="webpage">
          <article-title>Transfer learning and fine tuning for cross domain image classification with keras</article-title>
          <source>, https://www.slideshare.net/sujitpal/transfer-learning-and-fine-tuning-for-cross-domain-image-classification-with-keras.</source>
          <year>2016</year>
          <uri/>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conference-proceedings">
          <volume/>
          <page-range>2818–2826</page-range>
          <issue/>
          <year>0</year>
          <publisher-name>Las Vegas, USA, 2016</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>C.</given-names>
              <surname>Szegedy</surname>
            </name>
            <name>
              <given-names>V.</given-names>
              <surname>Vanhoucke</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Ioffe</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Shlens</surname>
            </name>
            <name>
              <given-names>Z.</given-names>
              <surname>Wojna</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/cvpr.2016.308</pub-id>
          <article-title>Rethinking the inception architecture for computer vision</article-title>
          <source>, http://dx.doi.org/10.1109/CVPR.2016.308</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="webpage">
          <article-title>keras</article-title>
          <source>, https://github.com/fchollet/keras.</source>
          <year>2015</year>
          <uri/>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range/>
          <issue>6</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>W.</given-names>
              <surname>Alawad</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Alburaidi</surname>
            </name>
            <name>
              <given-names>A.</given-names>
              <surname>Alzahrani</surname>
            </name>
            <name>
              <given-names>F.</given-names>
              <surname>Alflaj</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.14569/ijacsa.2021.01206102</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>A comparative study of stand-alone and hybrid CNN models for COVID-19 detection</article-title>
          <source>Int. J. Adv. Comput. Sci. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="conference-proceedings">
          <volume/>
          <page-range>1–5</page-range>
          <issue/>
          <year>0</year>
          <publisher-name>Chennai, India, 2020</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>R.</given-names>
              <surname>Ali</surname>
            </name>
            <name>
              <given-names>R. C.</given-names>
              <surname>Hardie</surname>
            </name>
            <name>
              <given-names>H. K.</given-names>
              <surname>Ragb</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/aipr50011.2020.9425311</pub-id>
          <article-title>Ensemble lung segmentation system using deep neural networks</article-title>
          <source>, http://dx.doi.org/10.1109/AIPR50011.2020.9425311</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="conference-proceedings">
          <volume/>
          <page-range>591–595</page-range>
          <issue/>
          <year>0</year>
          <publisher-name>Washington DC, USA, 2020</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>O.</given-names>
              <surname>Saha</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Tasnim</surname>
            </name>
            <name>
              <given-names>M. T.</given-names>
              <surname>Raihan</surname>
            </name>
            <name>
              <given-names>T.</given-names>
              <surname>Mahmud</surname>
            </name>
            <name>
              <given-names>I.</given-names>
              <surname>Ahmmed</surname>
            </name>
            <name>
              <given-names>S. A.</given-names>
              <surname>Fattah</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/tencon50793.2020.9293802</pub-id>
          <article-title>A multi-model based ensembling approach to detect COVID-19 from chest X-ray images</article-title>
          <source>, http://dx.doi.org/10.1109/TENCON50793.2020.9293802</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="conference-proceedings">
          <volume/>
          <page-range>1–6</page-range>
          <issue/>
          <year>0</year>
          <publisher-name>Chennai, India, 2020</publisher-name>
          <person-group person-group-type="author">
            <name>
              <given-names>S.</given-names>
              <surname>Tewari</surname>
            </name>
            <name>
              <given-names>U.</given-names>
              <surname>Agrawal</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Verma</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Kumar</surname>
            </name>
            <name>
              <given-names>S.</given-names>
              <surname>Jeevaraj</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/cict51604.2020.9312076</pub-id>
          <article-title>Ensemble model for COVID-19 detection from chest X-ray scans using image segmentation, fuzzy color and stacking approaches</article-title>
          <source>, http://dx.doi.org/10.1109/CICT51604.2020.9312076</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>39</volume>
          <page-range>85-98</page-range>
          <issue/>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <given-names>S. D.</given-names>
              <surname>Deb</surname>
            </name>
            <name>
              <given-names>R. K.</given-names>
              <surname>Jha</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Kumar</surname>
            </name>
            <name>
              <given-names>P. S.</given-names>
              <surname>Tripathi</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Talera</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Kumar</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s42600-022-00254-8</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>CoVSeverity-Net: An efficient deep learning model for COVID-19 severity estimation from Chest X-ray images</article-title>
          <source>Res. Biomed. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>206</page-range>
          <issue>4</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>L. J.</given-names>
              <surname>Muhammad</surname>
            </name>
            <name>
              <given-names>M. M.</given-names>
              <surname>Islam</surname>
            </name>
            <name>
              <given-names>S. S.</given-names>
              <surname>Usman</surname>
            </name>
            <name>
              <given-names>S. I.</given-names>
              <surname>Ayon</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s42979-020-00216-w</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Predictive data mining models for novel coronavirus (COVID-19) infected patients’ recovery</article-title>
          <source>SN Comput. Sci.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>