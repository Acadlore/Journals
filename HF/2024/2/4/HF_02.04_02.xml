<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">HF</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Healthcraft Frontiers</journal-title>
        <abbrev-journal-title abbrev-type="issn">Healthcraft. Front.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">HF</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">3005-799X</issn>
      <issn publication-format="print">3005-7981</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-kvTnXCmMjqT2FRkf7ccsWp---0sEh9-r</article-id>
      <article-id pub-id-type="doi">10.56578/hf020402</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>A DenseNet-Based Deep Learning Framework for Automated Brain Tumor Classification</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-5751-6997</contrib-id>
          <name>
            <surname>Fakheri</surname>
            <given-names>Soheil</given-names>
          </name>
          <email>fakherisoheil@iau.ac.ir</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3581-6600</contrib-id>
          <name>
            <surname>Yamaghani</surname>
            <given-names>Mohammadreza</given-names>
          </name>
          <email>o_yamaghani@iau.ac.ir</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2464-667X</contrib-id>
          <name>
            <surname>Nourbakhsh</surname>
            <given-names>Azamossadat</given-names>
          </name>
          <email>nourbakhsh@iau.ac.ir</email>
        </contrib>
        <aff id="aff_1">Department of Computer Engineering and Information Technology, La.C., Islamic Azad University, 1616 Lahijan, Iran</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>30</day>
        <month>12</month>
        <year>2024</year>
      </pub-date>
      <volume>2</volume>
      <issue>4</issue>
      <fpage>188</fpage>
      <lpage>202</lpage>
      <page-range>188-202</page-range>
      <history>
        <date date-type="received">
          <day>29</day>
          <month>09</month>
          <year>2024</year>
        </date>
        <date date-type="accepted">
          <day>12</day>
          <month>12</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Brain tumors represent a critical medical condition where early and accurate detection is paramount for effective treatment and improved patient outcomes. Traditional diagnostic methods relying on Magnetic Resonance Imaging (MRI) are often labor-intensive, time-consuming, and susceptible to human error, underscoring the need for more reliable and efficient approaches. In this study, a novel deep learning (DL) framework based on the Densely Connected Convolutional Network (DenseNet) architecture is proposed for the automated classification of brain tumors, aiming to enhance diagnostic precision and streamline medical image analysis. The framework incorporates adaptive filtering for noise reduction, Mask Region-based Convolutional Neural Network (Mask R-CNN) for precise tumor segmentation, and Gray Level Co-occurrence Matrix (GLCM) for robust feature extraction. The DenseNet architecture is employed to classify brain tumors into four categories: gliomas, meningiomas, pituitary tumors, and non-tumor cases. The model is trained and evaluated using the Kaggle MRI dataset, achieving a state-of-the-art classification accuracy of 96.96%. Comparative analyses demonstrate that the proposed framework outperforms traditional methods, including Back Propagation (BP), U-Net, and Recurrent Convolutional Neural Network (RCNN), in terms of sensitivity, specificity, and precision. The experimental results highlight the potential of integrating advanced DL techniques with medical image processing to significantly improve diagnostic accuracy and efficiency. This study not only provides a robust and reliable solution for brain tumor detection but also underscores the transformative impact of DL in medical imaging, offering radiologists a powerful tool for faster and more accurate diagnosis.</p></abstract>
      <kwd-group>
        <kwd>Brain tumor classification</kwd>
        <kwd>Deep learning</kwd>
        <kwd>DenseNet architecture</kwd>
        <kwd>Magnetic Resonance Imaging</kwd>
        <kwd>Adaptive filtering</kwd>
        <kwd>Feature extraction</kwd>
        <kwd>Medical image processing</kwd>
        <kwd>Tumor segmentation</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="3"/>
        <fig-count count="11"/>
        <table-count count="2"/>
        <ref-count count="29"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Brain tumors are potentially fatal illnesses brought on by the brain's aberrant cells growing out of control. The location and category of these tumors have a major influence on neurological function, causing a variety of symptoms, including headaches, seizures, and cognitive impairments (<xref ref-type="bibr" rid="ref_22">Saleh et al., 2020</xref>). Early accurate diagnostics are the basis for effective therapies and increasing survival rates. Radiologists manually assess Magnetic Resonance Imaging (MRI) as part of standard diagnostic procedures. This is a laborious process that is subject to human interpretation errors. Medical imaging has been transformed by advances in deep learning (DL), especially Convolutional Neural Networks (CNNs), which allow for accurate and automated processing of brain MRI data.</p><p>DL stands for disciplines of Artificial Intelligence (AI) that learn characteristics and spot patterns in data using neural networks. Images and other grid-like data are processed using CNNs, which are specialized DL architectures (<xref ref-type="bibr" rid="ref_1">Alqudah et al., 2020</xref>; <xref ref-type="bibr" rid="ref_25">Sharma et al., 2020</xref>). Starting with basic patterns like edges and working their way up to more intricate structures like tumor borders in <xref ref-type="fig" rid="fig_1">Figure 1</xref>, they employ convolutional layers to extract characteristics in a hierarchical manner. Because CNNs can analyze huge volumes of image data fast and lessen the need for manual feature extraction, they have shown remarkable efficacy in medical imaging for tumor identification and classification (<xref ref-type="bibr" rid="ref_20">Rasool et al., 2022</xref>). CNNs can recognize patterns that distinguish between healthy and diseased tissues, including pituitary tumors, meningiomas, and gliomas, by training on labeled datasets.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Location of common tumors within the brain</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_bsDCeYaa9m8CwY8K.jpeg"/>
        </fig>
      
      <p>Recurrent Convolutional Neural Networks (RCNNs) expand upon the advantages of CNNs by adding recurrent layers, which capture contextual and spatial connections amongst features (<xref ref-type="bibr" rid="ref_12">Khan et al., 2022</xref>). RCNNs improve CNNs' proficiencies in spatial feature extractions by considering connections between various image components. Contextual information (<xref ref-type="bibr" rid="ref_15">Lakshmi et al., 2023</xref>) enables more accurate classifications of brain tumor identifications, which makes RCNNs useful for identifying different tumors or evaluating irregular borders. Convolutional and recurrent layer integration is useful in high-precision medical imaging tasks.</p><p>The use of DL in brain tumor detection has produced notable advantages. CNN/RCNN-powered automated systems analyze MRI more quickly and reliably, relieving radiologists from workloads and increasing the diagnostic precision (<xref ref-type="bibr" rid="ref_7">Gao et al., 2022</xref>; <xref ref-type="bibr" rid="ref_13">Kokila et al., 2021</xref>) as they spot even minute patterns and irregularities that human evaluations miss. However, because they assume spherical clusters, classic segmentation techniques like k-means clustering sometimes require assistance with uneven tumor borders. This restriction may lower the accuracy of other processes, such as feature extraction and classification.</p><p>A Densely Connected Convolutional Network (DenseNet) architecture for automated brain tumor identification was proposed in this research. The technique starts with segmentation using Mask Region-based Convolutional Neural Network (Mask R-CNN) and pre-processing using adaptive filters to eliminate noise from MRI. Mask R-CNN overcomes the drawbacks of conventional methods by enabling accurate, pixel-level segmentation (<xref ref-type="bibr" rid="ref_5">Choudhury et al., 2020</xref>; <xref ref-type="bibr" rid="ref_16">Latif et al., 2021</xref>). Gray Level Co-occurrence Matrix (GLCM), which records texture-related characteristics, extracts features and tumor types are classified into gliomas, meningiomas, pituitary tumors, or non-tumorous categories using the DenseNet architecture, which is known for effective feature reuse through densely linked layers. The suggested methodology combines sophisticated segmentation and classification algorithms in order to overcome the shortcomings of current systems. While DenseNet guarantees effective processing of complicated feature representations, it improves accuracy by managing uneven tumor borders. This method is a viable way to incorporate DL into clinical operations as it increases diagnostic precision and provides scalability and dependability.</p>
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p><xref ref-type="bibr" rid="ref_11">Jia &amp;amp; Chen (2020)</xref> suggested brain tumor segmentation using Fully Automatic Heterogeneous Segmentation with Support Vector Machines (FAHS-SVM) where automated methods based on anatomical, morphological, and relaxometry parameters scan the cerebral venous system in MRI scans. High degrees of consistency between structures and surrounding brain tissues are indicative of the segmentation. One or more layers of hidden nodes in Extreme Learning Machine (ELM) learning analyze data and learn using probabilistic neural networks in classifications. The work’s numerical findings precisely identified both normal and diseased brain tissues in MRI, demonstrating the efficacy of the technique. <xref ref-type="bibr" rid="ref_4">Chellakh et al. (2023)</xref> introduced deep rule-based (DRB) classifiers for brain tumor classification in MRI, where features extracted using AlexNet, Visual Geometry Group (VGG)-16, Residual Network (ResNet)-50, and ResNet-18 were compared and evaluated for their performance. For classification, a DRB classifier was employed. The first database uses two Kaggle website datasets that are available to the public: tumor and no tumor. Meningiomas, gliomas, and pituitary tumors are all included in this multiclass database. Notable results were obtained from experimental data. A comparative analysis with other state-of-the-art distance methodologies and traditional methods demonstrated the suggested strategy’s efficacy in classifying brain tumors on MRI samples.</p><p><xref ref-type="bibr" rid="ref_17">Malla et al. (2023)</xref> recommended Deep Convolutional Neural Networks (DCNNs) based on transfer learning for the categorization of brain tumors into pituitary, glioma, and meningioma. Visual Geometry Group Network (VGGNet) is a pre-trained DCNN architecture that transfers learning parameters to target datasets after extensive training on large datasets. It also improves the performance by freezing and fine-tuning the neural network's layers. To overcome problems with data overfitting and vanishing gradients, the proposed solution includes Global Average Pooling (GAP) layers at outputs. The suggested architecture was evaluated and compared to other DL-based algorithms for classifying brain tumors on Figshare datasets. <xref ref-type="bibr" rid="ref_8">Haq et al. (2022)</xref> used enhanced CNNs to categorize brain tumors from brain MRI data. The usage of data augmentation and the transfer learning approach enhanced models’ classification, resulting in high prediction accuracy when compared to the baseline models. The suggested method could be used to detect brain tumors in healthcare systems based on the Internet of Things (IoT). <xref ref-type="bibr" rid="ref_2">Bhanothu et al. (2020)</xref> provided Faster R-CNNs by employing the Region Proposal Network (RPN) to locate and localize tumors. The three most frequent forms of brain cancers identified in MRI were pituitary tumors, gliomas, and meningiomas. The VGG-16 architecture served as foundational layers for classifiers and regional suggestions in the suggested method. The classification of the algorithm demonstrated that it had an average level of precision in detecting pituitary tumors, meningiomas, and gliomas.</p><p><xref ref-type="bibr" rid="ref_27">Ullah et al. (2023)</xref> presented TumorDetNet, an integrated end-to-end DL network for classifying and detecting brain tumors. 48 convolution layers with leaky ReLU and ReLU activations were used to produce most unique deep feature maps. Dropout layers and average pooling found distinct patterns and reduced data overfitting. Brain tumors were identified and categorized using softmax and fully connected layers. Their results on six popular Kaggle brain tumor MRI datasets successfully identified meningioma, pituitary, and glioma tumors, classified benign and malignant brain tumors, and detected brain malignancies, demonstrating the accuracy of classifying brain tumors. <xref ref-type="bibr" rid="ref_23">Saxena &amp;amp; Singh (2024)</xref> suggested two CNN designs (DenseNet169 and DenseNet201) for brain tumor identification. The models of DenseNet169 and DenseNet201 were trained on large MRI datasets of brain tumors of all sizes and types. The wide range of links between these models facilitates feature reuse and data interchange, which boosts tumor localization accuracy and performance. The separate testing dataset evaluates trained models, and performance metrics like accuracy and loss were examined where results demonstrated the efficacy of both models. In terms of test accuracy, train accuracy, train loss, and test loss, DenseNet201 performed better than DenseNet169, ResNet-50, and VGG19.</p><p><xref ref-type="bibr" rid="ref_6">Fakouri et al. (2024)</xref> classified brain cancers in MRI using ResNet architectures. The MRI quality of 159 individuals from cancer image archives was enhanced by Gaussian and median filters. In addition, image edges were identified by edge detection operators. The training process involved two stages: the network was initially trained on the original images, followed by the incorporation of preprocessed images enhanced with Gaussian and median filters. This two-stage approach was demonstrated to improve the performance of the DL network, as evidenced by the experimental results. <xref ref-type="bibr" rid="ref_26">Stephe et al. (2024)</xref> suggested automated tumor detection and classification in MRI using the Osprey Optimization Algorithm (OOA) with DL (BTDC-OOADL). The Wiener filtering (WF) model was used in the BTDC-OOADL technique to remove noise. The BTDC-OOADL method uses the MobileNetV2 technique to extract features. OOA was used for the MobileNetv2 model's optimal hyperparameter selection. The Graph Convolutional Network (GCN) model can recognize and classify brain tumors. The experimental results can be evaluated using benchmark data. The simulation results suggest that the BTDC-OOADL system improves with new methods.</p><p><xref ref-type="bibr" rid="ref_28">Veeramuthu et al. (2022)</xref> introduced combined feature and image-based classifiers (CFIC) that use features and images to categorize brain tumors. Actual image feature-based classifiers (AIFC), segmented image feature-based classifiers (SIFC), exacted features and segmented image feature-based classifiers (ASIFC), actual image-based classifiers (AIC), segmented image-based classifiers (SIC), and actual and segmented image-based classifiers (ASIC) with CFIC were all part of the architecture for image classification based on various deep neural networks and DCNNs. The proposed classifiers were trained and tested using the brain tumor detection 2020 dataset from Kaggle. In terms of the acquired accuracy, specificity, and sensitivity values, CFIC performed better than the other classifiers. The proposed CFIC method performs better than the existing classification methods.</p>
    </sec>
    <sec sec-type="">
      <title>3. Proposed methodology</title>
      <p>This work proposes a unique DL-based classification technique, DenseNet, for handling issues highlighted in this study, aiming to categorize brain tumors in order to increase human longevity and lower the death rate. The suggested low-complexity technique classifies brain malignancies accurately compared with other methods. <xref ref-type="fig" rid="fig_2">Figure 2</xref> shows the four phases of the recommended technique. An adaptive filtering technique was used for pre-processing in the first phase, and the Mask R-CNN was used for segmentation in the second phase. In the third phase, feature extractions were carried out using GLCM. DenseNet was used in the fourth phase for classification.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Overall workflow of the proposed model</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_w8ILmLhX67x09K08.png"/>
        </fig>
      
      
        <sec>
          
            <title>3.1. Pre-processing</title>
          
          <p>Pre-processing is a crucial step for identifying and classifying brain tumors from MRI. The adaptive filter plays a central role in enhancing image quality by removing noise while preserving essential features like edges, which are critical for accurate tumor detection and segmentation (<xref ref-type="bibr" rid="ref_10">Irmak, 2021</xref>). Pre-processing minimizes noise and distortion in images to enhance segmentation precision. Techniques like median filtering preserve edges while smoothing, and adaptive filtering adjusts dynamically to image variations, ensuring cleaner and more accurate images ready for segmentation.</p><p>The adaptive filter adjusts dynamically to the local characteristics of the image. It calculates the denoised pixel value <inline-formula>
  <mml:math id="mijsl4ivu1">
    <mml:mrow>
      <mml:mover>
        <mml:mi>I</mml:mi>
        <mml:mo>^</mml:mo>
      </mml:mover>
    </mml:mrow>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> using the local mean (<inline-formula>
  <mml:math id="mulaphwcxh">
    <mml:msub>
      <mml:mrow>
        <mml:mover>
          <mml:mi>μ</mml:mi>
          <mml:mo>^</mml:mo>
        </mml:mover>
      </mml:mrow>
      <mml:mi>L</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>) and variance (<inline-formula>
  <mml:math id="m75iihx3fz">
    <mml:msubsup>
      <mml:mrow>
        <mml:mover>
          <mml:mi>σ</mml:mi>
          <mml:mo>^</mml:mo>
        </mml:mover>
      </mml:mrow>
      <mml:mi>y</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msubsup>
  </mml:math>
</inline-formula>) within a defined window, along with the global noise variance (<inline-formula>
  <mml:math id="mxyc0qntjt">
    <mml:msubsup>
      <mml:mi>σ</mml:mi>
      <mml:mi>y</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msubsup>
  </mml:math>
</inline-formula>) as per Eq. (1).</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="mvrmmi1an5">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mover>
                          <mml:mi>I</mml:mi>
                          <mml:mo>^</mml:mo>
                        </mml:mover>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mover>
                          <mml:mi>I</mml:mi>
                          <mml:mo>^</mml:mo>
                        </mml:mover>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mo>−</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mrow>
                          <mml:mover>
                            <mml:mi>I</mml:mi>
                            <mml:mo>^</mml:mo>
                          </mml:mover>
                        </mml:mrow>
                        <mml:mi>x</mml:mi>
                        <mml:mi>y</mml:mi>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mover>
                              <mml:mi>μ</mml:mi>
                              <mml:mo>^</mml:mo>
                            </mml:mover>
                          </mml:mrow>
                          <mml:mi>L</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>⋅</mml:mo>
                      <mml:mi>x</mml:mi>
                      <mml:mi>y</mml:mi>
                      <mml:mi>x</mml:mi>
                      <mml:mi>y</mml:mi>
                      <mml:mfrac>
                        <mml:msubsup>
                          <mml:mi>σ</mml:mi>
                          <mml:mi>y</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                        <mml:msubsup>
                          <mml:mrow>
                            <mml:mover>
                              <mml:mi>σ</mml:mi>
                              <mml:mo>^</mml:mo>
                            </mml:mover>
                          </mml:mrow>
                          <mml:mi>y</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msubsup>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>The filter performs as follows:</p><p>Noise-free regions: If <inline-formula>
  <mml:math id="mh4eri8dfo">
    <mml:msubsup>
      <mml:mi>σ</mml:mi>
      <mml:mi>y</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msubsup>
    <mml:mo>=</mml:mo>
    <mml:mn>0</mml:mn>
  </mml:math>
</inline-formula>, indicating no noise, the pixel remains unchanged, which can be expressed by Eq. (2).</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="mders8j8o6">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mover>
                          <mml:mi>I</mml:mi>
                          <mml:mo>^</mml:mo>
                        </mml:mover>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mover>
                          <mml:mi>I</mml:mi>
                          <mml:mo>^</mml:mo>
                        </mml:mover>
                      </mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mi>x</mml:mi>
                      <mml:mi>y</mml:mi>
                      <mml:mi>x</mml:mi>
                      <mml:mi>y</mml:mi>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>Edge preservation: In areas with high local variance of <inline-formula>
  <mml:math id="makme92dtd">
    <mml:msubsup>
      <mml:mrow>
        <mml:mover>
          <mml:mi>σ</mml:mi>
          <mml:mo>^</mml:mo>
        </mml:mover>
      </mml:mrow>
      <mml:mi>y</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msubsup>
    <mml:msubsup>
      <mml:mi>σ</mml:mi>
      <mml:mi>y</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msubsup>
    <mml:mo>&amp;gt;</mml:mo>
  </mml:math>
</inline-formula>, the filter retains details such as edges, crucial for distinguishing tumor boundaries.</p><p>Uniform regions: When <inline-formula>
  <mml:math id="moufjcexsb">
    <mml:msubsup>
      <mml:mrow>
        <mml:mover>
          <mml:mi>σ</mml:mi>
          <mml:mo>^</mml:mo>
        </mml:mover>
      </mml:mrow>
      <mml:mi>y</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msubsup>
    <mml:msubsup>
      <mml:mi>σ</mml:mi>
      <mml:mi>y</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msubsup>
    <mml:mo>≈</mml:mo>
  </mml:math>
</inline-formula>, the pixel value smoothens towards local means and it can be expressed by Eq. (3).</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="m6yxwkec5g">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mrow>
                        <mml:mover>
                          <mml:mi>I</mml:mi>
                          <mml:mo>^</mml:mo>
                        </mml:mover>
                      </mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mi>x</mml:mi>
                      <mml:mi>y</mml:mi>
                      <mml:msub>
                        <mml:mrow>
                          <mml:mover>
                            <mml:mi>μ</mml:mi>
                            <mml:mo>^</mml:mo>
                          </mml:mover>
                        </mml:mrow>
                        <mml:mi>L</mml:mi>
                      </mml:msub>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>Adaptive filters optimize the balance between noise reduction and feature preservation, where edges and other characteristics are preserved while smoothing uniform areas by examining local variations. This improves identification and classification of malignancies such as gliomas, meningiomas, and pituitary tumors and guarantees high-quality inputs for segmentation and classification. The dependability of DL models in medical imaging is enhanced by the usage of adaptive filters in MRI pre-processing.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Segmentation</title>
          
          <p>Mask R-CNN, a DL model designed for instance segmentation, is ideal for brain tumor segmentation in MRI (<xref ref-type="bibr" rid="ref_9">Hussain &amp;amp; Khunteta, 2020</xref>). It extends Faster R-CNN by adding a pixel-level segmentation branch to detect and segment tumors accurately. The process involves feature extraction, region proposal, bounding box refinement, and mask generation.</p><p>The input MRI is passed through a backbone network like ResNet or ResNeXt, which extracts a feature map. This feature map highlights essential patterns in the image, such as edges and textures, critical for detecting tumors. The feature map serves as the basis for region proposal and segmentation.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>Instance segmentation of the Mask R-CNN framework</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_A7SJ3qHqsUPKZ3nq.jpeg"/>
            </fig>
          
          <p>RPN identifies Regions of Interest (ROIs)—areas in the feature map likely to contain a tumor. It generates anchor boxes of various sizes and aspect ratios and assigns a score to each, indicating the probability of containing a tumor, as shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>. The RPN loss function optimizes both classification and localization represented in Eq. (4).</p>
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="m6661oksrt">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mi>R</mml:mi>
                          <mml:mi>P</mml:mi>
                          <mml:mi>N</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mi>C</mml:mi>
                          <mml:mi>l</mml:mi>
                          <mml:mi>s</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mi>r</mml:mi>
                          <mml:mi>e</mml:mi>
                          <mml:mi>g</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mo>+</mml:mo>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mtm5z02i3q">
    <mml:msub>
      <mml:mi>L</mml:mi>
      <mml:mrow>
        <mml:mi>C</mml:mi>
        <mml:mi>l</mml:mi>
        <mml:mi>s</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> is the classification loss for distinguishing tumor and non-tumor regions, and <inline-formula>
  <mml:math id="mkez17nl4u">
    <mml:msub>
      <mml:mi>L</mml:mi>
      <mml:mrow>
        <mml:mi>r</mml:mi>
        <mml:mi>e</mml:mi>
        <mml:mi>g</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> is the regression loss for refining the anchor box coordinates.</p><p>Detected ROIs are mapped back to the feature map using ROI Align, which resolves spatial misalignment issues caused by ROI pooling in previous methods (<xref ref-type="bibr" rid="ref_24">Shabu &amp;amp; Jayakumar, 2020</xref>). This alignment is achieved via bilinear interpolation, ensuring pixel-level precision can be expressed by Eq. (5).</p>
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="mgp4oj38x1">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mi>A</mml:mi>
                      <mml:mi>l</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>g</mml:mi>
                      <mml:mi>n</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>d</mml:mi>
                      <mml:mi>F</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>a</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>u</mml:mi>
                      <mml:mi>r</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>s</mml:mi>
                      <mml:mi>F</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>j</mml:mi>
                      <mml:mtext> </mml:mtext>
                      <mml:mo>=</mml:mo>
                      <mml:mo>⋅</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:munder>
                        <mml:mo>∑</mml:mo>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mi>j</mml:mi>
                          <mml:mo>,</mml:mo>
                        </mml:mrow>
                      </mml:munder>
                      <mml:msub>
                        <mml:mi>w</mml:mi>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mi>j</mml:mi>
                          <mml:mo>,</mml:mo>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mjarwuxp48">
    <mml:mi>F</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> represents feature map values, and <inline-formula>
  <mml:math id="m0zfweepc5">
    <mml:msub>
      <mml:mi>w</mml:mi>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mi>j</mml:mi>
        <mml:mo>,</mml:mo>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> are the interpolation weights.</p><p>Each aligned ROI undergoes classification to determine if it contains a tumor and what type. Simultaneously, the bounding box is refined to better localize the tumor.</p>
          
            <disp-formula>
              <label>(6)</label>
              <mml:math id="m4vps6mn6k">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mi>b</mml:mi>
                            <mml:mi>o</mml:mi>
                            <mml:mi>x</mml:mi>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mi>C</mml:mi>
                          <mml:mi>l</mml:mi>
                          <mml:mi>s</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mi>r</mml:mi>
                            <mml:mi>e</mml:mi>
                            <mml:mi>g</mml:mi>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mo>+</mml:mo>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="m5elmuualg">
    <mml:msub>
      <mml:mi>L</mml:mi>
      <mml:mrow>
        <mml:mi>C</mml:mi>
        <mml:mi>l</mml:mi>
        <mml:mi>s</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> classifies the ROI (e.g., gliomas, meningiomas, or non-tumor) and <inline-formula>
  <mml:math id="mnbi6urhza">
    <mml:msub>
      <mml:mi>L</mml:mi>
      <mml:mrow>
        <mml:mi>r</mml:mi>
        <mml:mi>e</mml:mi>
        <mml:mi>g</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> adjusts the size and location of the bounding box.</p><p>For each ROI classified as a tumor, a segmentation mask is generated using a convolutional mask branch. This branch outputs a binary mask that segments the tumor from the surrounding tissue. The mask prediction is optimized using the binary cross-entropy loss, as expressed by Eq. (7).</p>
          
            <disp-formula>
              <label>(7)</label>
              <mml:math id="mct7oxs9ex">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mi>m</mml:mi>
                          <mml:mi>a</mml:mi>
                          <mml:mi>s</mml:mi>
                          <mml:mi>k</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mfrac>
                        <mml:mn>1</mml:mn>
                        <mml:mi>N</mml:mi>
                      </mml:mfrac>
                      <mml:munderover>
                        <mml:mo>∑</mml:mo>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:munderover>
                      <mml:mrow>
                        <mml:mo>[</mml:mo>
                        <mml:mo>⁡</mml:mo>
                        <mml:mo>+</mml:mo>
                        <mml:mo>⁡</mml:mo>
                        <mml:mo>]</mml:mo>
                        <mml:msub>
                          <mml:mi>y</mml:mi>
                          <mml:mi>i</mml:mi>
                        </mml:msub>
                        <mml:mi>log</mml:mi>
                        <mml:mi>log</mml:mi>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:msub>
                            <mml:mrow>
                              <mml:mover>
                                <mml:mi>y</mml:mi>
                                <mml:mo>^</mml:mo>
                              </mml:mover>
                            </mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mn>1</mml:mn>
                          <mml:msub>
                            <mml:mi>y</mml:mi>
                            <mml:mi>i</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mn>1</mml:mn>
                          <mml:msub>
                            <mml:mrow>
                              <mml:mover>
                                <mml:mi>y</mml:mi>
                                <mml:mo>^</mml:mo>
                              </mml:mover>
                            </mml:mrow>
                            <mml:mi>i</mml:mi>
                          </mml:msub>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="me9xgcguys">
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is the ground truth mask for pixel $i<inline-formula>
  <mml:math id="m0o054rys1">
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>\hat{y}_i<inline-formula>
  <mml:math id="mncbkobhu8">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
  </mml:math>
</inline-formula>i<inline-formula>
  <mml:math id="mr47amaodm">
    <mml:mo>;</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>N$ is total number of pixels in the ROI.</p><p>MRI is input into the network and passed through the backbone to generate feature maps. RPN identifies ROIs, which are refined and aligned using ROI Align. Finally, the mask prediction branch outputs pixel-level segmentation masks for the detected tumor regions, ensuring precise and accurate delineation of tumor boundaries (<xref ref-type="bibr" rid="ref_14">Kordemir et al., 2024</xref>). After the model produces the final output, such as building boxes around detected tumors, classification labels, categorizing tumors, or marking them as non-tumor and pixel-level segmentation masks, it accurately delineates tumor boundaries, as shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>.</p><p>The total loss function combines contributions from RPN, bounding box refinement, and mask prediction.</p>
          
            <disp-formula>
              <label>(8)</label>
              <mml:math id="mw1c5z01wf">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mi>L</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:msub>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mi>R</mml:mi>
                          <mml:mi>P</mml:mi>
                          <mml:mi>N</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mi>b</mml:mi>
                            <mml:mi>o</mml:mi>
                            <mml:mi>x</mml:mi>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>L</mml:mi>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mi>m</mml:mi>
                            <mml:mi>a</mml:mi>
                            <mml:mi>s</mml:mi>
                            <mml:mi>k</mml:mi>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>This ensures that all components—region detection, classification, and segmentation—are optimized for precise tumor segmentation. By integrating these components, Mask R-CNN excels at segmenting brain tumors in MRI scans. It is capable of handling irregular tumor boundaries, producing pixel-level masks, and maintaining high accuracy, thereby making it indispensable in medical imaging and diagnostics.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Feature extraction</title>
          
          <p>Texture analysis helps machine learning (ML) algorithms and visual perception distinguish between healthy and sick tissues. Additionally, it highlights the distinction between healthy tissues and cancerous growth that could otherwise go unnoticed. Accuracy can be improved by selecting significant statistical criteria for early diagnosis (<xref ref-type="bibr" rid="ref_18">Nasrudin, 2024</xref>). GLCM may be used to obtain second-order statistical texture information. One may determine the frequency with which pixels with specified values and a certain spatial relationship appear in an image by first constructing a GLCM and then utilizing it to extract statistical metrics.</p><p>GLCM counts frequencies for statistical evaluations of image textures. It is made up of pixel pairs with identical values and relative positions. GLCM functions can extract statistical information that can be used to describe the texture of an image by determining the frequency of pixel pairings with a specific weight and spatial relationship. In GLCM, a two-dimensional histogram, each pair of <italic>p</italic> and <italic>q</italic> denotes the frequency at which they occur. It makes use of the grayscales of <italic>p</italic> and <italic>q</italic>, the distance of <italic>S</italic> = 1, the angle (with 0, 45, 90, and 135 degrees representing horizontal, positive diagonal, vertical, and negative diagonal, respectively), and the understanding that, at a specific distance <italic>S</italic> and orientation, a pixel of intensity <italic>p</italic> looks close to a pixel of intensity <italic>q</italic>. <xref ref-type="fig" rid="fig_4">Figure 4</xref> shows the GLCM feature extraction for generation.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>GLCM feature extraction for generation</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_MkisVn56MQIIXkNa.png"/>
            </fig>
          
          <p>In this study, five distinct statistical features are extracted from GLCM after its calculation. These features are essential for capturing various texture properties of the image, as described below:</p><p>Contrast: Identifies GLCM's local deviations.</p>
          
            <disp-formula>
              <label>(9)</label>
              <mml:math id="m6yjygg3ob">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mi>C</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>n</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>r</mml:mi>
                      <mml:mi>a</mml:mi>
                      <mml:mi>s</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>P</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>j</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mo>∑</mml:mo>
                      <mml:mo>∗</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>,</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>−</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>i</mml:mi>
                        <mml:msup>
                          <mml:mi>j</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msup>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>Homogeneity: Determines proximities of GLCM element distributions to its diagonals.</p>
          
            <disp-formula>
              <label>(10)</label>
              <mml:math id="mrjkymyc5c">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mi>H</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>m</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>g</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>n</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>y</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mo>∑</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>P</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mi>j</mml:mi>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>+</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mn>1</mml:mn>
                          <mml:mrow>
                            <mml:mo>|</mml:mo>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mo>|</mml:mo>
                          </mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>Dissimilarity: Measures the intensity range of grayscale.</p>
          
            <disp-formula>
              <label>(11)</label>
              <mml:math id="mcwxixoler">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mi>D</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>s</mml:mi>
                      <mml:mi>s</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>m</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>l</mml:mi>
                      <mml:mi>a</mml:mi>
                      <mml:mi>r</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>y</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>j</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mo>∑</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>|</mml:mo>
                      <mml:msub>
                        <mml:mi>P</mml:mi>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mi>j</mml:mi>
                          <mml:mo>,</mml:mo>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mrow>
                        <mml:mo>|</mml:mo>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>Energy: Reflects pixel uniformity.</p>
          
            <disp-formula>
              <label>(12)</label>
              <mml:math id="mvgffk4oyn">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mi>E</mml:mi>
                      <mml:mi>n</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>r</mml:mi>
                      <mml:mi>g</mml:mi>
                      <mml:mi>y</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mo>∑</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mi>P</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mi>j</mml:mi>
                          <mml:mo>(</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>)</mml:mo>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>+</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mn>1</mml:mn>
                          <mml:mrow>
                            <mml:mo>|</mml:mo>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mo>|</mml:mo>
                          </mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mi>j</mml:mi>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>Correlation: Determines the average degree of connectivity between each pixel in the image and its neighbors.</p>
          
            <disp-formula>
              <label>(13)</label>
              <mml:math id="m13x11ufl3">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mi>C</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>r</mml:mi>
                      <mml:mi>r</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>l</mml:mi>
                      <mml:mi>a</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>n</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mo>∑</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mo>(</mml:mo>
                          <mml:mo>−</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mi>i</mml:mi>
                          <mml:mi>μ</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mi>j</mml:mi>
                          <mml:mi>μ</mml:mi>
                          <mml:mi>J</mml:mi>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mo>(</mml:mo>
                          <mml:mo>∗</mml:mo>
                          <mml:mo>)</mml:mo>
                          <mml:mi>σ</mml:mi>
                          <mml:mi>i</mml:mi>
                          <mml:mi>σ</mml:mi>
                          <mml:mi>j</mml:mi>
                          <mml:mtext> </mml:mtext>
                          <mml:mtext> </mml:mtext>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>where, <italic>i</italic> and <italic>j</italic> denote the co-occurrence matrix indices; <italic>P</italic>(<italic>i</italic>, <italic>j</italic>) denotes the elements of the co-occurrence matrix at position (<italic>i</italic>, <italic>j</italic>); <italic>µx</italic> and <italic>µy </italic>denote the averages of row and column weights in the matrix; and <inline-formula>
  <mml:math id="m7rzxv9du3">
    <mml:mi>σ</mml:mi>
    <mml:mi>x</mml:mi>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mnldho8u26">
    <mml:mi>σ</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> represent the standard deviations of row and column weights in the matrix.</p><p>GLCM’s ability to extract meaningful texture-based features makes it an integral part of the brain tumor classification workflow (<xref ref-type="bibr" rid="ref_19">Özkaraca et al., 2023</xref>). By combining these features with advanced classification models, the approach achieves high accuracy and reliability.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Classification</title>
          
          <p>DenseNet is a cutting-edge DL model, known for its densely connected layers, making it efficient and powerful in classifying gliomas, meningiomas, pituitary tumors, and non-tumor cases from MRI (<xref ref-type="bibr" rid="ref_21">Sabila &amp;amp; Tjahyaningtyas, 2024</xref>) and by leveraging its unique feature-propagation mechanisms.</p><p>DenseNet connects each layer directly to all subsequent layers in the network. Unlike traditional CNNs, where each layer feeds into the next sequentially, DenseNet enables all layers to share information, optimizing feature extraction and gradient flow.</p>
          
            <disp-formula>
              <label>(14)</label>
              <mml:math id="m46w62q8lv">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>x</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>H</mml:mi>
                        <mml:mi>l</mml:mi>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mrow>
                          <mml:mo>[</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>…</mml:mo>
                          <mml:mo>.</mml:mo>
                          <mml:mo>.</mml:mo>
                          <mml:mo>,</mml:mo>
                          <mml:mo>]</mml:mo>
                          <mml:msub>
                            <mml:mi>x</mml:mi>
                            <mml:mn>0</mml:mn>
                          </mml:msub>
                          <mml:msub>
                            <mml:mi>x</mml:mi>
                            <mml:mn>1</mml:mn>
                          </mml:msub>
                          <mml:msub>
                            <mml:mi>x</mml:mi>
                            <mml:mrow>
                              <mml:mi>l</mml:mi>
                              <mml:mo>−</mml:mo>
                              <mml:mn>1</mml:mn>
                            </mml:mrow>
                          </mml:msub>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="m6x0o3tstx">
    <mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> represents the output of layer $l<inline-formula>
  <mml:math id="mnd4h7c8bo">
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>H_l<inline-formula>
  <mml:math id="mn7bb1hp1h">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>B</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>B</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>U</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>\left[x_0, x_1, \ldots . ., x_{l-1}\right]$ is the concatenation of all preceding layer outputs. This connectivity reduces redundancy, ensures efficient parameter usage, and improves gradient flow, leading to better classification performance.</p><p>Dense blocks: These are groups of layers where outputs are concatenated instead of being added, allowing all layers to share features. Each dense block uses the transformation.</p>
          
            <disp-formula>
              <label>(15)</label>
              <mml:math id="m79s0vxhmt">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>H</mml:mi>
                        <mml:mi>l</mml:mi>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mi>B</mml:mi>
                      <mml:mi>N</mml:mi>
                      <mml:mi>x</mml:mi>
                      <mml:mrow>
                        <mml:mi>R</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>L</mml:mi>
                        <mml:mi>U</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>C</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>n</mml:mi>
                        <mml:mi>v</mml:mi>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>This architecture ensures that the model reuses learned features, capturing tumor-specific details like size, shape, and texture.</p><p>Growth rate ($k<inline-formula>
  <mml:math id="mk2ec62tvi">
    <mml:mo>)</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>D</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula>k<inline-formula>
  <mml:math id="mobagund1x">
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>k$ captures more complex tumor features.</p><p>Transition layers: Located between dense blocks, these layers compress the network by reducing spatial dimensions and feature maps using a 1 × 1 convolution followed by pooling.</p>
          
            <disp-formula>
              <label>(16)</label>
              <mml:math id="myhi03r2d2">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>H</mml:mi>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mi>t</mml:mi>
                            <mml:mi>r</mml:mi>
                            <mml:mi>a</mml:mi>
                            <mml:mi>n</mml:mi>
                            <mml:mi>s</mml:mi>
                            <mml:mi>i</mml:mi>
                            <mml:mi>t</mml:mi>
                            <mml:mi>i</mml:mi>
                            <mml:mi>o</mml:mi>
                            <mml:mi>n</mml:mi>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mrow>
                        <mml:mi>P</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>l</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>R</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>L</mml:mi>
                        <mml:mi>U</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>C</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>n</mml:mi>
                        <mml:mi>v</mml:mi>
                      </mml:mrow>
                      <mml:mi>x</mml:mi>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>After the dense blocks, GAP layers convert feature maps into compact vectors, which are then input to fully connected layers, with the final output class probabilities being determined through a softmax activation function.</p>
          
            <disp-formula>
              <label>(17)</label>
              <mml:math id="mji7j05efw">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>p</mml:mi>
                        <mml:mi>i</mml:mi>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:msup>
                          <mml:mi>e</mml:mi>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mi>z</mml:mi>
                              <mml:mi>i</mml:mi>
                            </mml:msub>
                          </mml:mrow>
                        </mml:msup>
                        <mml:mrow>
                          <mml:munderover>
                            <mml:mo>∑</mml:mo>
                            <mml:mrow>
                              <mml:mi>j</mml:mi>
                              <mml:mo>=</mml:mo>
                              <mml:mn>1</mml:mn>
                            </mml:mrow>
                            <mml:mi>C</mml:mi>
                          </mml:munderover>
                          <mml:msup>
                            <mml:mi>e</mml:mi>
                            <mml:mrow>
                              <mml:msub>
                                <mml:mi>z</mml:mi>
                                <mml:mi>j</mml:mi>
                              </mml:msub>
                            </mml:mrow>
                          </mml:msup>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mpk2bfx3pw">
    <mml:msub>
      <mml:mi>p</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is the probability of the input belonging to class $i<inline-formula>
  <mml:math id="mcmslgs5f4">
    <mml:mo>;</mml:mo>
  </mml:math>
</inline-formula>z_i<inline-formula>
  <mml:math id="md0nhdpb4z">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>i<inline-formula>
  <mml:math id="m12bntkjf0">
    <mml:mo>;</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>C$ indicates a number of classes (gliomas, meningiomas, pituitary tumors, and non-tumors).</p>
          
            <fig id="fig_5">
              <label>Figure 5</label>
              <caption>
                <title>Architecture of the DenseNet layers</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_tzoZRMRW1bnmd8ae.png"/>
            </fig>
          
          <p>As shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>, the dense block in a densely linked convolutional network must have the same feature map size before concatenation can be performed between blocks (<xref ref-type="bibr" rid="ref_29">Wakili et al., 2022</xref>). While keeping the feature map size constant, it should be noted that down-sampling is a crucial element in a CNN, which can be accomplished by conducting convolution and pooling outside of dense blocks. The layers responsible for convolution and pooling are known as transition layers. In DenseNet designs, transition layers encompass batch-norm layers, 1 × 1 convolutions, and 2 × 2 average pooling, where 1 × 1 convolutions down-sample input features and produce outputs.</p><p>The model is optimized using the categorical cross-entropy loss function.</p>
          
            <disp-formula>
              <label>(18)</label>
              <mml:math id="maspazvp2f">
                <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mi>L</mml:mi>
                      <mml:mi>log</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>⁡</mml:mo>
                      <mml:mfrac>
                        <mml:mn>1</mml:mn>
                        <mml:mi>N</mml:mi>
                      </mml:mfrac>
                      <mml:munderover>
                        <mml:mo>∑</mml:mo>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mi>N</mml:mi>
                      </mml:munderover>
                      <mml:munderover>
                        <mml:mo>∑</mml:mo>
                        <mml:mrow>
                          <mml:mi>j</mml:mi>
                          <mml:mo>=</mml:mo>
                          <mml:mn>1</mml:mn>
                        </mml:mrow>
                        <mml:mi>C</mml:mi>
                      </mml:munderover>
                      <mml:msub>
                        <mml:mi>y</mml:mi>
                        <mml:mrow>
                          <mml:mi>i</mml:mi>
                          <mml:mi>j</mml:mi>
                          <mml:mo>,</mml:mo>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mover>
                              <mml:mi>y</mml:mi>
                              <mml:mo>^</mml:mo>
                            </mml:mover>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>i</mml:mi>
                            <mml:mi>j</mml:mi>
                            <mml:mo>,</mml:mo>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>where, $N<inline-formula>
  <mml:math id="mewzx65jsk">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>C<inline-formula>
  <mml:math id="m50iz338uu">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>y_{i, j}<inline-formula>
  <mml:math id="mu67uohloi">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>i<inline-formula>
  <mml:math id="mecj2qosn0">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>j<inline-formula>
  <mml:math id="mz7bic31rq">
    <mml:mo>,</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\hat{y}_{i , j}<inline-formula>
  <mml:math id="muoh4p5lm3">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>i<inline-formula>
  <mml:math id="mojmvch1xl">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>j$.</p><p>The training involves stochastic gradient descent (SGD) with learning rate scheduling, ensuring effective convergence. Data augmentation (e.g., rotation, flipping, and scaling) is employed to make the model robust to variations in MRI scans.</p><p>DenseNet's efficient architecture ensures that even with limited MRI data, it extracts tumor-specific features with high precision, achieving robust and reliable classification. This is crucial for aiding medical professionals in diagnosing and planning treatments for brain tumor patients.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Results and discussion</title>
      <p>This proposed model used the Kaggle dataset to test the suggested methods. The two categories of this dataset were testing and training. Glioma tumor, meningioma tumor, pituitary tumor, and non-tumor photos were all included in each training and testing set. The testing set consisted of 394 images, while the training set comprised 2,870 images. Data preprocessing techniques like brain stripping enhanced descriptions of data. Glioma, meningioma, non-tumor, and pituitary tumors were the categories for performance evaluation. Examples of several tumor categories in various locations are depicted in <xref ref-type="fig" rid="fig_6">Figure 6</xref> (<xref ref-type="bibr" rid="ref_3">Bhuvaji, 2020</xref>).</p><p>Dataset: https://www.kaggle.com/sartajbhuvaji/brain-tumor-classification-mri</p>
      
        <fig id="fig_6">
          <label>Figure 6</label>
          <caption>
            <title>Types of tumors in different places</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_hI9VCSVFvpjx3FDl.png"/>
        </fig>
      
      <p>The suggested approach was implemented in Python, a high-level ML programming language using Keras, and TensorFlow to create neural networks. This is advantageous for both Central Processing Unit (CPU) and Graphics Processing Unit (GPU) processing. The hyper-parameters were adjusted using a network search, with the parameters selected based on the model’s performance on the validation set. Variables like energy and testing rate changed as the test was being conducted. The learning rate was first set at 0.003 and then gradually decreased to 0.3 × 10<sup>−5</sup>; the energy was first set at 0.5 and then increased to 0.9.</p><p>In terms of evaluation parameters, five performance metrics—specificity, sensitivity, accuracy, precision, and F1-score—were used to assess the effectiveness of the suggested DenseNet-based framework for classifying brain tumors. These metrics together offer a thorough evaluation of the model's capacity to distinguish between tumor and non-tumor instances while correcting classification discrepancies. False positives, false negatives, true positives, and true negatives were the primary data used to create the metrics.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Implemented stepwise MATLAB output</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Input Image</p></td><td colspan="1" rowspan="1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_hl-e_UKgnr24nGuY.png" /></td></tr><tr><td colspan="1" rowspan="1"><p>Processed Image</p></td><td colspan="1" rowspan="1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_LR_nKT_vov0sZt4p.png" /></td></tr><tr><td colspan="1" rowspan="1"><p>Segmentation</p></td><td colspan="1" rowspan="1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_P9-5gicWvd1PGDOZ.png" /></td></tr><tr><td colspan="1" rowspan="1"><p>Classification</p></td><td colspan="1" rowspan="1"><graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_y16f4yR6XQU8juot.png" /></td></tr></tbody></table>
        </table-wrap>
      
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>Performance metric evaluation</title>
          </caption>
          <table><tbody><tr><td><p>Parameter</p></td><td><p>BP</p></td><td><p>U-Net</p></td><td><p>RCNN</p></td><td><p>Proposed DenseNet</p></td></tr><tr><td><p>Sensitivity</p></td><td><p>97.87</p></td><td><p>97.51</p></td><td><p>98.42</p></td><td><p>98.84</p></td></tr><tr><td><p>Specificity</p></td><td><p>75.47</p></td><td><p>80.39</p></td><td><p>89.28</p></td><td><p>93.43</p></td></tr><tr><td><p>Accuracy</p></td><td><p>88.83</p></td><td><p>90.86</p></td><td><p>95.17</p></td><td><p>96.96</p></td></tr><tr><td><p>Precision</p></td><td><p>85.50</p></td><td><p>88.68</p></td><td><p>94.34</p></td><td><p>96.59</p></td></tr><tr><td><p>F1-score</p></td><td><p>91.22</p></td><td><p>92.80</p></td><td><p>96.34</p></td><td><p>97.70</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>The experimental results were derived from the MATLAB implementation, which processes MRI through segmentation, feature extraction, and classification. The processed results for each step are systematically illustrated in <xref ref-type="table" rid="table_1">Table 1</xref>, which highlights the incremental improvements achieved during the workflow. The overall evaluation metrics and their corresponding formulas, as detailed in <xref ref-type="table" rid="table_2">Table 2</xref>, elucidate how each metric reflects the model's performance.</p><p>Sensitivity (recall): The ratio of the number of true positives and false negatives, which can be expressed by Eq. (19).</p>
      
        <disp-formula>
          <label>(19)</label>
          <mml:math id="mme00jcwvh">
            <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
              <mml:mtr>
                <mml:mtd>
                  <mml:mi>S</mml:mi>
                  <mml:mi>e</mml:mi>
                  <mml:mi>n</mml:mi>
                  <mml:mi>s</mml:mi>
                  <mml:mi>i</mml:mi>
                  <mml:mi>t</mml:mi>
                  <mml:mi>i</mml:mi>
                  <mml:mi>v</mml:mi>
                  <mml:mi>i</mml:mi>
                  <mml:mi>t</mml:mi>
                  <mml:mi>y</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mi>T</mml:mi>
                      <mml:mi>r</mml:mi>
                      <mml:mi>u</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>P</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>s</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>v</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mtext> </mml:mtext>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>u</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>P</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>v</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>l</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>N</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>g</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>v</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                      <mml:mtext> </mml:mtext>
                      <mml:mtext> </mml:mtext>
                      <mml:mo>+</mml:mo>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
      
      <p>Specificity: The ratio of true negatives to different false positives and true negatives, which can be used to assess the specificity of brain tumor detection.</p>
      
        <disp-formula>
          <label>(20)</label>
          <mml:math id="m24j8iropk">
            <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
              <mml:mtr>
                <mml:mtd>
                  <mml:mi>S</mml:mi>
                  <mml:mi>p</mml:mi>
                  <mml:mi>e</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:mi>i</mml:mi>
                  <mml:mi>f</mml:mi>
                  <mml:mi>i</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:mi>i</mml:mi>
                  <mml:mi>t</mml:mi>
                  <mml:mi>y</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mi>T</mml:mi>
                      <mml:mi>r</mml:mi>
                      <mml:mi>u</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>N</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>g</mml:mi>
                      <mml:mi>a</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>v</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mtext> </mml:mtext>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>u</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>N</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>g</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>v</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>l</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>P</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>v</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                      <mml:mtext> </mml:mtext>
                      <mml:mtext> </mml:mtext>
                      <mml:mo>+</mml:mo>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
      
      <p>Accuracy: The ratio of the precise values found in the population.</p>
      
        <disp-formula>
          <label>(21)</label>
          <mml:math id="mkku0vees2">
            <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
              <mml:mtr>
                <mml:mtd>
                  <mml:mi>A</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:mi>u</mml:mi>
                  <mml:mi>r</mml:mi>
                  <mml:mi>a</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:mi>y</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>u</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>P</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>v</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>u</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>N</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>g</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>v</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                      <mml:mtext> </mml:mtext>
                      <mml:mtext> </mml:mtext>
                      <mml:mo>+</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>u</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>P</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>v</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>l</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>P</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>v</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>l</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>N</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>g</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>v</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>u</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>N</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>g</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>v</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                      <mml:mtext> </mml:mtext>
                      <mml:mtext> </mml:mtext>
                      <mml:mtext> </mml:mtext>
                      <mml:mtext> </mml:mtext>
                      <mml:mtext> </mml:mtext>
                      <mml:mtext> </mml:mtext>
                      <mml:mo>+</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>+</mml:mo>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
      
      <p>Precision: The ratio of true positives to the sum of true positives and false positives.</p>
      
        <disp-formula>
          <label>(22)</label>
          <mml:math id="moqw83eydo">
            <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
              <mml:mtr>
                <mml:mtd>
                  <mml:mi>P</mml:mi>
                  <mml:mi>r</mml:mi>
                  <mml:mi>e</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:mi>i</mml:mi>
                  <mml:mi>s</mml:mi>
                  <mml:mi>i</mml:mi>
                  <mml:mi>o</mml:mi>
                  <mml:mi>n</mml:mi>
                  <mml:mo>=</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mi>T</mml:mi>
                      <mml:mi>r</mml:mi>
                      <mml:mi>u</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>P</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>s</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>v</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mtext> </mml:mtext>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>u</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>P</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>v</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>F</mml:mi>
                        <mml:mi>a</mml:mi>
                        <mml:mi>l</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>P</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>v</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mtext> </mml:mtext>
                      </mml:mrow>
                      <mml:mtext> </mml:mtext>
                      <mml:mtext> </mml:mtext>
                      <mml:mo>+</mml:mo>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
      
      <p>F1-score: The F1-score value can be calculated by Eq. (23).</p>
      
        <disp-formula>
          <label>(23)</label>
          <mml:math id="mryj7oba2a">
            <mml:mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
              <mml:mtr>
                <mml:mtd>
                  <mml:mi>F</mml:mi>
                  <mml:mi>s</mml:mi>
                  <mml:mi>c</mml:mi>
                  <mml:mi>o</mml:mi>
                  <mml:mi>r</mml:mi>
                  <mml:mi>e</mml:mi>
                  <mml:mn>1</mml:mn>
                  <mml:mn>2</mml:mn>
                  <mml:mo>−</mml:mo>
                  <mml:mo>=</mml:mo>
                  <mml:mo>×</mml:mo>
                  <mml:mfrac>
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mi>P</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>c</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>n</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>n</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>v</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>y</mml:mi>
                      </mml:mrow>
                      <mml:mtext> </mml:mtext>
                      <mml:mtext> </mml:mtext>
                      <mml:mo>×</mml:mo>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mi>P</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>c</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>o</mml:mi>
                        <mml:mi>n</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>S</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>n</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>v</mml:mi>
                        <mml:mi>i</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>y</mml:mi>
                      </mml:mrow>
                      <mml:mtext> </mml:mtext>
                      <mml:mtext> </mml:mtext>
                      <mml:mo>+</mml:mo>
                    </mml:mrow>
                  </mml:mfrac>
                </mml:mtd>
              </mml:mtr>
            </mml:mtable>
          </mml:math>
        </disp-formula>
      
      <p><xref ref-type="fig" rid="fig_7">Figure 7</xref> illustrates the sensitivity performance of four classification methods: Back Propagation (BP), U-Net, RCNN, and the proposed DenseNet. Among these, the proposed DenseNet achieves the highest sensitivity, approximately 98.84%, demonstrating its superior capability in correctly identifying true positive cases. The RCNN method follows closely with a sensitivity of 98.42%, indicating robust detection accuracy. BP and U-Net methods exhibit slightly lower sensitivity levels of 97.87% and 97.51%, respectively. These results highlight the proposed DenseNet’s effectiveness in ensuring minimal false negatives, which is critical for accurate tumor detection.</p>
      
        <fig id="fig_7">
          <label>Figure 7</label>
          <caption>
            <title>Comparison of the proposed model with existing methods in sensitivity</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_dkJCTfNM9n1UuN_H.png"/>
        </fig>
      
      <p><xref ref-type="fig" rid="fig_8">Figure 8</xref> compares the specificity of the same four methods. The suggested DenseNet achieves the highest specificity of 93.43%, outperforming others and indicating superior ability to correctly identify true negative cases (non-tumor instances) while minimizing false positives. RCNN comes second with an 89.28% specificity, demonstrating the potent ability to lower false alarms. The specificities of U-Net and BP are lower, at 80.39% and 75.47%, respectively. The outcomes highlight the reliability of the suggested DenseNet in differentiating non-tumorous patients while lowering diagnostic mistakes.</p>
      
        <fig id="fig_8">
          <label>Figure 8</label>
          <caption>
            <title>Comparison of the proposed model with existing methods in specificity</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_UcX9GojqYIy2tiKI.png"/>
        </fig>
      
      <p><xref ref-type="fig" rid="fig_9">Figure 9</xref> demonstrates accurate categorization with the greatest accuracy of 96.96% achieved by the proposed DenseNet, demonstrating its overall superior performance in accurately categorizing tumor instances. RCNN comes second with an accuracy of 95.17%. BP has the lowest accuracy of 88.83%, while U-Net attains an accuracy of 90.86%. These outcomes highlight the sophisticated design and effective feature extraction techniques of the suggested DenseNet, which produce incredibly accurate classification.</p>
      
        <fig id="fig_9">
          <label>Figure 9</label>
          <caption>
            <title>Comparison of the proposed model with existing methods in accuracy</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_pRCy5hrbigDG9IwT.png"/>
        </fig>
      
      <p><xref ref-type="fig" rid="fig_10">Figure 10</xref> presents the precision performance of the four methods, highlighting their abilities to minimize false positives. The best precision of 96.59% is attained by the proposed DenseNet, demonstrating its capacity to accurately detect real positive cases with little misclassification. 94.34% accuracy is attained by RCNN, 88.68% by U-Net, and 85.50% by BP. The outcomes confirm that the suggested DenseNet performs better at correctly detecting tumor instances while lowering the possibility of false alarms.</p>
      
        <fig id="fig_10">
          <label>Figure 10</label>
          <caption>
            <title>Comparison of the proposed model with existing methods in precision</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_900wxebqp2LtCLmL.png"/>
        </fig>
      
      <p>F1-score balances precision and recall (sensitivity) values. As shown in <xref ref-type="fig" rid="fig_11">Figure 11</xref>, the proposed DenseNet's exceptional balance between sensitivity and accuracy is demonstrated by its greatest F1-score of 97.70%. With an F1-score of 96.34%, RCNN comes in second, demonstrating dependable performance. BP has the lowest F1-score at 91.22%, while U-Net has an F1-score at 92.80%. These outcomes demonstrate that the suggested DenseNet is resilient in reaching a high degree of classification accuracy while preserving sensitivity and precision.</p>
      
        <fig id="fig_11">
          <label>Figure 11</label>
          <caption>
            <title>Comparison of the proposed model with existing methods in F1-score</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_0l__nV86ufEe1QTc.png"/>
        </fig>
      
    </sec>
    <sec sec-type="conclusions">
      <title>5. Conclusions</title>
      <p>The proposed DenseNet-based framework for brain tumor classification combines adaptive filtering for noise reduction (with an impressive accuracy of 96.96%), Mask R-CNN for accurate tumor segmentation, and DenseNet for effective feature extraction and classification. This method improves the identification of tumor categories, such as gliomas, meningiomas, pituitary tumors, and non-tumors, and tackles difficulties in irregular tumor border segmentation. Even though the method performs better than existing models like BP, U-Net, and RCNN, its computational complexity and training needs still allow for improvement. To further increase accuracy and therapeutic application, future research should concentrate on maximizing the model's computing efficiency and integrating multimodal data, such as genetic and clinical information.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Alqudah</surname>
              <given-names>A. M.</given-names>
            </name>
            <name>
              <surname>Alquraan</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Qasmieh</surname>
              <given-names>I. A.</given-names>
            </name>
            <name>
              <surname>Alqudah</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Al-Sharu</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2001.08844.</pub-id>
          <article-title>Brain tumor classification using deep learning technique—A comparison between cropped, uncropped, and segmented lesion images with different sizes</article-title>
          <source>arXiv:2001.08844</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>248-252</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bhanothu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Kamalakannan</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Rajamanickam</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICACCS48705.2020.9074375.</pub-id>
          <article-title>Detection and classification of brain tumor in MRI images using deep convolutional network</article-title>
          <source>, https://doi.org/10.1109/ICACCS48705.2020.9074375.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bhuvaji</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Brain tumor classification (MRI)</article-title>
          <source>Kaggle</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>28</volume>
          <page-range>13-22</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chellakh</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Moussaoui</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Attia</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Akhtar</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/isi.280102.</pub-id>
          <article-title>MRI brain tumor identification and classification using deep learning techniques</article-title>
          <source>Ingén. Syst. Inf.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>1-4</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Choudhury</surname>
              <given-names>C. L.</given-names>
            </name>
            <name>
              <surname>Mahanty</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Kumar</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Mishra</surname>
              <given-names>B. K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICCSEA49143.2020.9132874.</pub-id>
          <article-title>Brain tumor detection and classification using convolutional neural network and deep neural network</article-title>
          <source>, https://doi.org/10.1109/ICCSEA49143.2020.9132874.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>27-35</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Fakouri</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Nikpour</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Soleymani Amiri</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.22044/jadm.2024.13148.2452.</pub-id>
          <article-title>Automatic brain tumor detection in brain MRI images using deep learning methods</article-title>
          <source>J. AI Data Mining</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>e2225608</page-range>
          <issue>8</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gao</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Shan</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Cai</surname>
              <given-names>J. et al.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1001/jamanetworkopen.2022.25608.</pub-id>
          <article-title>Development and validation of a deep learning model for brain tumor diagnosis and classification using magnetic resonance imaging</article-title>
          <source>JAMA Netw. Open</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>15331</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Haq</surname>
              <given-names>A. U.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J. P.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Alshara</surname>
              <given-names>M. A.</given-names>
            </name>
            <name>
              <surname>Alotaibi</surname>
              <given-names>R. M.</given-names>
            </name>
            <name>
              <surname>Mawuli</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-022-19465-1.</pub-id>
          <article-title>DACBT: Deep learning approach for classification of brain tumors using MRI data in IoT healthcare environment</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>38-43</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hussain</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Khunteta</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICIRCA48905.2020.9183385.</pub-id>
          <article-title>Semantic segmentation of brain tumor from MRI images and SVM classification using GLCM features</article-title>
          <source>, https://doi.org/10.1109/ICIRCA48905.2020.9183385.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>45</volume>
          <page-range>1015-1036</page-range>
          <issue>3</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Irmak</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s40998-021-00426-9.</pub-id>
          <article-title>Multi-classification of brain tumor MRI images using deep convolutional neural network with fully optimized framework</article-title>
          <source>Iran. J. Sci. Technol. Trans. Electr. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jia</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3016319.</pub-id>
          <article-title>Brain tumor identification and classification of MRI images using deep learning techniques</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>2022</volume>
          <page-range>8104054</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Khan</surname>
              <given-names>A. H.</given-names>
            </name>
            <name>
              <surname>Abbas</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>M. A.</given-names>
            </name>
            <name>
              <surname>Farooq</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>W. A.</given-names>
            </name>
            <name>
              <surname>Siddiqui</surname>
              <given-names>S. Y.</given-names>
            </name>
            <name>
              <surname>Ahmad</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2022/8104054.</pub-id>
          <article-title>Intelligent model for brain tumor identification using deep learning</article-title>
          <source>Appl. Comput. Intell. Soft Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>1916</volume>
          <page-range>012226</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kokila</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Devadharshini</surname>
              <given-names>M. S.</given-names>
            </name>
            <name>
              <surname>Anitha</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sankar</surname>
              <given-names>S. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1088/1742-6596/1916/1/012226.</pub-id>
          <article-title>Brain tumor detection and classification using deep learning techniques based on MRI images</article-title>
          <source>IOP Conf. Ser. J. Phys.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>2301391</page-range>
          <issue>7</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kordemir</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Cevik</surname>
              <given-names>K. K.</given-names>
            </name>
            <name>
              <surname>Bozkurt</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/21681163.2023.2301391.</pub-id>
          <article-title>A mask R-CNN approach for detection and classification of brain tumours from MR images</article-title>
          <source>Comput. Methods Biomech. Biomed. Eng. Imaging Vis.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>95-100</page-range>
          <issue>7</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lakshmi</surname>
              <given-names>H. B.</given-names>
            </name>
            <name>
              <surname>Sirisha</surname>
              <given-names>A. S.</given-names>
            </name>
            <name>
              <surname>Niharika</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Sravya</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Sudeepthi</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>Brain tumor detection from MRI image using digital image processing</article-title>
          <source>J. Sci. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>56-63</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Latif</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Iskandar</surname>
              <given-names>D. A.</given-names>
            </name>
            <name>
              <surname>Alghazo</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Butt</surname>
              <given-names>M. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.2174/1573405616666200311122429.</pub-id>
          <article-title>Brain MR image classification for glioma tumor detection using deep convolutional neural network features</article-title>
          <source>Curr. Med. Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>679</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Malla</surname>
              <given-names>P. P.</given-names>
            </name>
            <name>
              <surname>Sahu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Alutaibi</surname>
              <given-names>A. I.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/pr11030679.</pub-id>
          <article-title>Classification of tumor in brain MR images using deep convolutional neural network and global average pooling</article-title>
          <source>Processes</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>1-9</page-range>
          <issue>3</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Nasrudin</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18495/comengapp.v13i03.490.</pub-id>
          <article-title>MRI-based brain tumor instance segmentation using mask R-CNN</article-title>
          <source>Comput. Eng. Appl. J.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>349</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Özkaraca</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Bağrıaçık</surname>
              <given-names>O. İ.</given-names>
            </name>
            <name>
              <surname>Gürüler</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Hussain</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Laila</surname>
              <given-names>U. E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/life13020349.</pub-id>
          <article-title>Multiple brain tumor classification with dense CNN architecture using brain MRI images</article-title>
          <source>Life</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>24</volume>
          <page-range>799</page-range>
          <issue>6</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Rasool</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Ismail</surname>
              <given-names>N. A.</given-names>
            </name>
            <name>
              <surname>Boulila</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Ammar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Samma</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Yafooz</surname>
              <given-names>W. M.</given-names>
            </name>
            <name>
              <surname>Emara</surname>
              <given-names>A. H. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/e24060799.</pub-id>
          <article-title>A hybrid deep learning model for brain tumour classification</article-title>
          <source>Entropy</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>147-158</page-range>
          <issue>3</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sabila</surname>
              <given-names>S. S.</given-names>
            </name>
            <name>
              <surname>Tjahyaningtyas</surname>
              <given-names>H. P. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.21107/kursor.v12i3.379.</pub-id>
          <article-title>The multiple brain tumor with modified DenseNet121 architecture using brain MRI images: Classification multiclass brain tumor using brain MRI images with modified DenseNet121</article-title>
          <source>J. Ilm. Kursor</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>131-136</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Saleh</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sukaik</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Abu-Naser</surname>
              <given-names>S. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/iCareTech49914.2020.00032.</pub-id>
          <article-title>Brain tumor classification using deep learning</article-title>
          <source>, https://doi.org/10.1109/iCareTech49914.2020.00032.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>3072</volume>
          <page-range>020028</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Saxena</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1063/5.0198664.</pub-id>
          <article-title>Deep learning based brain tumor detection using MRI images with transfer learning technique</article-title>
          <source>AIP Conf. Proc.</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>793-799</page-range>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shabu</surname>
              <given-names>S. J.</given-names>
            </name>
            <name>
              <surname>Jayakumar</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICISS49785.2020.9315971.</pub-id>
          <article-title>Brain tumor classification with MRI brain images using 2-level GLCM features and sparse representation based segmentation</article-title>
          <source>, https://doi.org/10.1109/ICISS49785.2020.9315971.</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="conference-proceedings">
          <page-range>559-569</page-range>
          <year>2020</year>
          <publisher-name>Singapore: Springer</publisher-name>
          <person-group person-group-type="author">
            <name>
              <surname>Sharma</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Wahlang</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Sanyal</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Maji</surname>
              <given-names>A. K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-981-15-0751-9_52.</pub-id>
          <article-title>Classification of brain MRI using deep learning techniques</article-title>
          <source>, https://doi.org/10.1007/978-981-15-0751-9_52.</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>33</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Stephe</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Nivedita</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Karthikeyan</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Nithya</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Sikkandar</surname>
              <given-names>M. Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.54216/JISIoT.120103.</pub-id>
          <article-title>Enhancing brain tumor detection and classification using osprey optimization algorithm with deep learning on MRI images</article-title>
          <source>J. Intell. Syst. Internet Things</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <volume>18</volume>
          <page-range>e0291200</page-range>
          <issue>9</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ullah</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Javed</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Alhazmi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Hasnain</surname>
              <given-names>S. M.</given-names>
            </name>
            <name>
              <surname>Tahir</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ashraf</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0291200.</pub-id>
          <article-title>TumorDetNet: A unified deep learning model for brain tumor detection and classification</article-title>
          <source>PLoS One</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>848784</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Veeramuthu</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Meenakshi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Mathivanan</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Kotecha</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Saini</surname>
              <given-names>J. R.</given-names>
            </name>
            <name>
              <surname>Vijayakumar</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Subramaniyaswamy</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3389/fpsyg.2022.848784.</pub-id>
          <article-title>MRI brain tumor image classification using a combined feature and image-based classifier</article-title>
          <source>Front. Psychol.</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <volume>2022</volume>
          <page-range>8904768</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wakili</surname>
              <given-names>M. A.</given-names>
            </name>
            <name>
              <surname>Shehu</surname>
              <given-names>H. A.</given-names>
            </name>
            <name>
              <surname>Sharif</surname>
              <given-names>M. H.</given-names>
            </name>
            <name>
              <surname>Sharif</surname>
              <given-names>M. H. U.</given-names>
            </name>
            <name>
              <surname>Umar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kusetogullari</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Ince</surname>
              <given-names>I. K.</given-names>
            </name>
            <name>
              <surname>Uyaver</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1155/2022/8904768.</pub-id>
          <article-title>Classification of breast cancer histopathological images using DenseNet and transfer learning</article-title>
          <source>Comput. Intell. Neurosci.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>