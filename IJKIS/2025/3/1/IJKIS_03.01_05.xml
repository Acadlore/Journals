<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IJKIS</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>International Journal of Knowledge and Innovation Studies</journal-title>
        <abbrev-journal-title abbrev-type="issn">Int J. Knowl. Innov Stud.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IJKIS</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">3005-6101</issn>
      <issn publication-format="print">3005-6098</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-ijC2A8jz8xvAABRrDgjWkVW7lmsnZw8n</article-id>
      <article-id pub-id-type="doi">10.56578/ijkis030105</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Intelligent Image Segmentation via Complex Pythagorean Fuzzy Sets and Level-Set Optimization</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-6406-827X</contrib-id>
          <name>
            <surname>Khan</surname>
            <given-names>Muhammad Shahkar</given-names>
          </name>
          <email>shahkar@uop.edu.pk</email>
        </contrib>
        <aff id="aff_1">Department of Mathematics, CECOS University of IT and Emerging Sciences, 25000 Peshawar, Pakistan</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>30</day>
        <month>03</month>
        <year>2025</year>
      </pub-date>
      <volume>3</volume>
      <issue>1</issue>
      <fpage>50</fpage>
      <lpage>59</lpage>
      <page-range>50-59</page-range>
      <history>
        <date date-type="received">
          <day>13</day>
          <month>02</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>23</day>
          <month>03</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Image segmentation plays a crucial role in medical imaging, remote sensing, and object detection. However, challenges persist due to uncertainty in region classification, sensitivity to noise, and discontinuities in object boundaries. To address these issues, a novel segmentation framework is proposed, integrating Complex Pythagorean Fuzzy Aggregation Operators (CPFAs) with a level-set-based optimization strategy to enhance both precision and adaptability. The proposed model leverages complex Pythagorean fuzzy membership functions, incorporating both magnitude and phase components, to effectively manage overlapping intensity distributions and classification uncertainty. Additionally, geometric constraints, including gradient and curvature-based regularization, are employed to refine boundary evolution, ensuring accurate edge delineation in noisy and complex imaging conditions. A key contribution of this work is the formulation of a complex fuzzy energy functional, which synergistically integrates fuzzy region classification, phase-aware boundary refinement, and geometric constraints to guide segmentation. The level-set method is utilized to iteratively minimize this functional, facilitating smooth transitions between segmented regions while preserving structural integrity. Experimental evaluations conducted across diverse imaging domains demonstrate the robustness and versatility of the proposed approach, highlighting its efficacy in medical image segmentation, remote sensing analysis, and object detection. The integration of complex fuzzy logic with geometric optimization not only enhances segmentation accuracy but also improves resilience to noise and irregular boundary structures, making this framework particularly suitable for applications requiring high-precision image analysis.</p></abstract>
      <kwd-group>
        <kwd>Complex Pythagorean fuzzy sets</kwd>
        <kwd>Fuzzy aggregation operators</kwd>
        <kwd>Phase-aware segmentation</kwd>
        <kwd>Noise robustness</kwd>
        <kwd>Level-set optimization</kwd>
        <kwd>Image segmentation</kwd>
        <kwd>Boundary refinement</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="1"/>
        <fig-count count="5"/>
        <table-count count="1"/>
        <ref-count count="23"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Image segmentation is a fundamental process in image analysis and computer vision, enabling the division of an image into distinct regions for meaningful interpretation and further processing. It is widely applied in areas such as medical imaging, remote sensing, and industrial automation. Despite significant advancements in segmentation techniques, challenges persist when dealing with images affected by noise, intensity inhomogeneity (IIH), and blurry boundaries [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>]. Accurate segmentation is crucial for tasks such as object detection and scene analysis. Over the years, researchers have developed numerous segmentation methods to enhance performance, highlighting the importance of segmentation in fields like medical diagnostics, autonomous systems, and remote sensing [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>].</p><p>Among various approaches, active contour models (ACMs) based on the level-set framework have gained popularity due to their robustness and adaptability. These models are generally categorized into two types: edge-based and region-based segmentation models [<xref ref-type="bibr" rid="ref_9">9</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>], [<xref ref-type="bibr" rid="ref_12">12</xref>], [<xref ref-type="bibr" rid="ref_13">13</xref>], [<xref ref-type="bibr" rid="ref_14">14</xref>], [<xref ref-type="bibr" rid="ref_15">15</xref>], [<xref ref-type="bibr" rid="ref_16">16</xref>], [<xref ref-type="bibr" rid="ref_17">17</xref>]. Region-based ACMs analyze statistical properties within image regions rather than focusing on gradient information, making them more effective in complex scenarios where noise or weak edges can hinder segmentation performance. Since these models rely on broader image regions, they often yield more consistent and accurate segmentation results. Conversely, edge-based ACMs detect object boundaries using gradient information, which can be effective in well-structured images but often struggles in noisy environments or when boundaries are weak.</p><p>Among regional-based techniques, the variational model of the level set developed by Chan and Vese (C-V) [<xref ref-type="bibr" rid="ref_15">15</xref>], [<xref ref-type="bibr" rid="ref_16">16</xref>] has gained widespread recognition due to its effectiveness in segmenting images into two homogeneous regions. However, this model encounters difficulties when dealing with noise and intensity variations across different regions. As a result, it often produces suboptimal segmentation outcomes when applied to images with significant intensity fluctuations. Similarly, conventional segmentation methods such as thresholding, edge detection, and region-growing techniques face challenges in handling images with complex intensity distributions. Although machine learning-based approaches have improved segmentation accuracy, they typically require large amounts of labeled training data and substantial computational resources.</p><p>To address these issues, researchers have proposed various improvements and modifications. For instance, the Segment Anything Model (SAM) introduced by Mazurowski et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] marks a significant breakthrough in medical image segmentation. This model leverages foundation models to create a flexible and generalizable segmentation framework that does not require extensive retraining. SAM is particularly valuable in medical applications, where it can handle different imaging modalities and potentially enhance diagnostic accuracy and efficiency. However, one of its key limitations is its difficulty in dealing with domain-specific challenges, such as noise, low contrast, and intricate anatomical structures, necessitating further refinements for medical applications.</p><p>Another notable enhancement is the improved level-set segmentation method by Zhang et al. [<xref ref-type="bibr" rid="ref_19">19</xref>], which aims to address the shortcomings of traditional level-set models, particularly their susceptibility to weak edges and high noise levels. This approach integrates bilateral filters and a no-weight initialization method into the level-set framework, improving edge detection and noise reduction during contour evolution. Experimental results demonstrate that this method outperforms conventional level-set models in extracting object contours and reducing noise. However, its dependence on specific parameter tuning and the computational complexity of bilateral filtering may limit its general applicability across different datasets and real-time scenarios.</p><p>Segmenting images with IIH remains particularly challenging due to the overlapping intensity distributions between regions. In medical imaging, for example, distinguishing tumors or lesions from surrounding healthy tissues can be difficult due to similar intensity values. Additionally, noise and blurry transitions further complicate segmentation, often leading to inaccurate results. These challenges emphasize the necessity of a segmentation approach that can adapt to varying intensity levels while preserving boundary integrity.</p><p>Fuzzy logic provides a powerful framework for managing uncertainty and imprecise data, making it an ideal solution for segmentation tasks involving complex intensity distributions. By leveraging Pythagorean fuzzy logic, the proposed model extends traditional fuzzy membership functions by introducing an additional degree of freedom, allowing for a more flexible and accurate representation of uncertainty. The integration of Pythagorean fuzzy logic with geometric operators enhances segmentation by refining boundary detection and ensuring smooth region transitions. The proposed segmentation model introduces several key innovations:</p><p>• Complex Pythagorean Fuzzy Membership Functions: The model employs complex Pythagorean fuzzy membership functions to classify pixels based on both real and imaginary components, providing a more comprehensive approach to handling uncertainty and overlapping intensity distributions.</p><p>• Geometric Operators: By incorporating gradient and curvature information within the Pythagorean fuzzy framework, the model enhances boundary detection and ensures smooth transitions, even in images affected by noise or blurriness.</p><p>• Pythagorean Fuzzy Energy Functional: The segmentation process is governed by a Pythagorean fuzzy energy functional, which integrates complex membership functions, geometric constraints, and regularization terms to achieve an optimal balance between region homogeneity and edge preservation.</p><p>• Level-Set Evolution with Complex Pythagorean Fuzzy Operators: The level-set function is initialized and evolved using complex Pythagorean fuzzy aggregation sets CPFAs, ensuring precise and adaptive boundary delineation while maintaining robust segmentation in challenging image scenarios.</p>
      
        <sec>
          
            <title>1.1. Significance of the proposed model</title>
          
          <p>The proposed model overcomes the limitations of existing segmentation techniques by integrating complex Pythagorean fuzzy logic with geometric principles into a unified framework. This combination allows the model to effectively handle issues such as intensity variations, noise, and blurred transitions, making it particularly suitable for complex segmentation tasks. Unlike traditional fuzzy models, the Pythagorean-based approach provides a more refined characterization of membership values, reducing uncertainty and improving segmentation robustness. By utilizing complex Pythagorean fuzzy membership functions, the model adapts to varying intensity distributions, ensuring accurate and smooth pixel classification. The incorporation of geometric operators further enhances boundary refinement, leading to improved segmentation precision. The integration of these components into a single energy functional enables a balanced approach that maintains region homogeneity, preserves edges, and ensures boundary smoothness. The proposed Complex Pythagorean Fuzzy Level-Set (CPFLS) model offers significant advancements in image segmentation, particularly in applications such as medical imaging and remote sensing. In medical imaging, CPFLS effectively addresses challenges like IIH, noise, and weak boundaries that commonly arise in MRI and CT scans. The integration of complex Pythagorean fuzzy membership functions enables a more precise characterization of uncertain regions, improving tumor segmentation and organ delineation. Additionally, the level-set evolution mechanism, guided by Pythagorean fuzzy operators, ensures adaptive and edge-preserving segmentation, outperforming traditional fuzzy c-means and active contour methods. Similarly, in remote sensing, CPFLS enhances land cover classification, change detection, and cloud segmentation by leveraging geometric operators to refine boundary detection. Unlike conventional threshold-based or clustering techniques, CPFLS dynamically adapts to intensity variations, making it robust against atmospheric distortions such as haze and cloud cover. These advantages position CPFLS as a powerful tool for complex segmentation tasks, balancing region homogeneity, boundary smoothness, and computational efficiency while maintaining robustness to noise and varying intensity distributions. However, the computational complexity of CPFLS is higher than that of basic thresholding techniques, requiring optimization for large-scale datasets. Despite this, its superior segmentation accuracy and adaptability make it a promising approach for real-world applications in medical diagnostics and remote sensing analysis.</p><p>The rest of this paper is structured as follows. Section 2 provides a review of related work, discussing existing segmentation approaches and their associated challenges. Section 3 introduces the proposed segmentation model, detailing its integration of fuzzy logic and geometric principles for precise boundary detection. Section 4 outlines the experimental setup, evaluation metrics, and results obtained through comprehensive testing. Lastly, the paper concludes with a discussion on the findings and potential avenues for future research.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>2. Related works</title>
      <p>Derevyanchuk [<xref ref-type="bibr" rid="ref_20">20</xref>] explored the application of intelligent fuzzy image segmentation systems in the training of future specialists in engineering and pedagogical fields. The study highlights how integrating fuzzy logic-based segmentation techniques enhances the quality of digital image processing education, particularly in fields requiring high precision and adaptability. A key achievement of this research is its demonstration of how fuzzy segmentation can improve the interpretation and classification of complex visual data, thus aiding students in developing analytical and problem-solving skills. Additionally, the study emphasized the potential of artificial intelligence-driven methodologies in modern educational frameworks, making a strong case for their inclusion in technical training curricula. However, a limitation of the study is its lack of extensive empirical validation, as it primarily focuses on theoretical and conceptual benefits rather than presenting a detailed comparative analysis with other segmentation methods. Furthermore, the research does not address the computational complexity of fuzzy segmentation systems, which can pose challenges in real-time applications.</p><p>Cardone et al. [<xref ref-type="bibr" rid="ref_21">21</xref>] proposed a novel fuzzy-based remote sensing image segmentation method, which aims to enhance the accuracy and robustness of satellite image analysis. One of the key achievements of this work is the integration of fuzzy logic with remote sensing techniques, allowing for more precise boundary detection in images with complex textures and varying illumination conditions. The study also demonstrated superior performance compared to traditional segmentation methods, particularly in handling uncertain or noisy data. This advancement has significant implications for environmental monitoring, land use classification, and disaster management. However, a notable limitation of the research is its high computational cost, making it less feasible for real-time applications on resource-constrained devices. Additionally, while the paper presents promising results, it does not extensively compare the proposed method with deep learning-based segmentation approaches, which have shown remarkable efficiency in recent years. Addressing these aspects could further strengthen the model’s applicability in large-scale remote sensing tasks.</p><p>Wang and He [<xref ref-type="bibr" rid="ref_22">22</xref>] introduced a level set evolution equation based on fractional derivatives to improve segmentation robustness in the presence of IIH and noise. Their model formulates a linear diffusion equation with nonlinear edge-stopping and guidance sources. Specifically, the edge-stopping source is derived from the input image and a local Bayesian statistical model within the level set framework, while the guidance source utilizes the fitting term of the C-V model. The integration of these sources facilitates accurate contour evolution, ensuring robustness against noise and initialization sensitivity. Their approach successfully enhances segmentation accuracy by incorporating an adaptive variable-order fractional derivative, which generates a guidance image resistant to IIH and noise, ensuring precise object boundary detection. Furthermore, the use of a series splitting method and finite difference scheme ensures numerical stability and efficiency. Experimental results demonstrate superior segmentation performance compared to several state-of-the-art level set methods.</p><p>However, despite these advantages, the method has certain limitations. The computational complexity of fractional derivatives and Bayesian statistical modeling increases processing time compared to simpler segmentation methods. Additionally, the accuracy of the segmentation is highly dependent on the quality of the guidance image, which may not always be optimal in complex imaging scenarios. The model also requires careful tuning of parameters related to the fractional-order derivative and statistical components for effective performance. In some cases, the method may over-segment fine textures, incorrectly identifying them as separate objects. Moreover, while the model effectively handles moderate levels of IIH and noise, it may struggle with highly degraded images, limiting its generalization to extreme cases.</p><p>ACMs incorporating bias field (BF) correction have been widely used for image segmentation, particularly in handling IIH. However, many traditional BF-based ACMs rely on a single BF assumption, which limits their adaptability in complex imaging conditions. Additionally, these models often do not fully address the convexity of the energy functional, leading to local minima issues that hinder segmentation accuracy.</p><p>To overcome these challenges, Li et al. [<xref ref-type="bibr" rid="ref_23">23</xref>] proposed a dual BF-based convex level-set method that integrates a multiplicative-additive (MA) model within a variational framework. Their model employs the MA model as a fidelity term and introduces a kernel function to adjust the intensity inhomogeneous neighborhood size, improving segmentation adaptability. Furthermore, a convex level-set function is embedded in the framework to transform the segmentation process into a convex optimization problem, ensuring a global energy minimum. To enhance noise resistance, total variation regularization is incorporated to smooth the level-set function. Li et al.’s model [<xref ref-type="bibr" rid="ref_23">23</xref>] successfully addresses the local minima issue by ensuring convexity in the energy functional, leading to more stable and globally optimal segmentation results. The incorporation of a dual BF assumption improves adaptability to various IIH patterns, while the MA fidelity term and kernel function enhance segmentation performance across different image types. Additionally, the model demonstrates robustness to contour initialization, offering greater flexibility in practical applications. Experimental results confirm superior segmentation accuracy and BF correction compared to classical ACMs.</p><p>Despite these advancements, the model has certain drawbacks. The computational complexity increases due to the dual BF estimation and convex optimization process, making it computationally expensive for high-resolution images. Furthermore, the accuracy of segmentation depends on the precise estimation of the BF, which may be challenging in highly degraded images with extreme noise or weak boundaries. Additionally, while the model allows more flexible contour initialization, parameter tuning is still required for optimal performance in different datasets.</p>
    </sec>
    <sec sec-type="">
      <title>3. Proposed mathematical approach using complex pythagorean fuzzy aggregation operators</title>
      <p>The proposed segmentation model integrates CP-FAs with a level-set-based initialization to ensure precise, adaptive boundary detection in complex image segmentation tasks. The model effectively leverages complex fuzzy membership functions to capture overlapping intensity distributions while incorporating geometric constraints for refined segmentation.</p>
      
        <sec>
          
            <title>3.1. Complex pythagorean fuzzy membership functions</title>
          
          <p>Complex Pythagorean fuzzy sets (CPFS) extend conventional fuzzy logic by introducing complex-valued membership degrees, allowing the representation of both magnitude and phase components in segmentation. For a given region $r$ (e.g., foreground or background), the complex Pythagorean fuzzy membership function is defined as:</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="m0qfcrc23k">
                <mml:msub>
                  <mml:mi>μ</mml:mi>
                  <mml:mi>r</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>ν</mml:mi>
                  <mml:mi>r</mml:mi>
                </mml:msub>
                <mml:msub>
                  <mml:mi>γ</mml:mi>
                  <mml:mi>r</mml:mi>
                </mml:msub>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mi>x</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mn>1</mml:mn>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="m30ttlfxkd">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mi>r</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
  </mml:math>
</inline-formula> represents the membership degree of pixel $x<inline-formula>
  <mml:math id="mfnv2wx34l">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>r<inline-formula>
  <mml:math id="mrzjyx34w3">
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\nu_r(x)<inline-formula>
  <mml:math id="mrqzfq8x8a">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\gamma_r(x)<inline-formula>
  <mml:math id="mpej6qpbw3">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
  </mml:math>
</inline-formula>0 \leq \gamma_r(x) \leq 1$. The complex fuzzy membership function is defined as:</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="msxv3gvr8c">
                <mml:msubsup>
                  <mml:mi>μ</mml:mi>
                  <mml:mi>r</mml:mi>
                  <mml:mi>c</mml:mi>
                </mml:msubsup>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mi>x</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:msub>
                  <mml:mi>μ</mml:mi>
                  <mml:mi>r</mml:mi>
                </mml:msub>
                <mml:msup>
                  <mml:mi>e</mml:mi>
                  <mml:mrow>
                    <mml:mi>i</mml:mi>
                    <mml:msub>
                      <mml:mi>θ</mml:mi>
                      <mml:mi>r</mml:mi>
                    </mml:msub>
                  </mml:mrow>
                </mml:msup>
              </mml:math>
            </disp-formula>
          
          <p> where, <inline-formula>
  <mml:math id="mg521r8fi1">
    <mml:msub>
      <mml:mi>θ</mml:mi>
      <mml:mi>r</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is the phase factor encoding additional uncertainty in segmentation. The intensity-based membership function for pixel $x$ is given by:</p>
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="m44a8a0790">
                <mml:msub>
                  <mml:mi>μ</mml:mi>
                  <mml:mi>r</mml:mi>
                </mml:msub>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>⁡</mml:mo>
                <mml:mi>x</mml:mi>
                <mml:mi>exp</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>−</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mfrac>
                    <mml:msup>
                      <mml:mrow>
                        <mml:mo>|</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mo>−</mml:mo>
                        <mml:mo>|</mml:mo>
                        <mml:mi>I</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:msub>
                          <mml:mi>c</mml:mi>
                          <mml:mi>r</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mn>2</mml:mn>
                    </mml:msup>
                    <mml:msubsup>
                      <mml:mi>σ</mml:mi>
                      <mml:mi>r</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msubsup>
                  </mml:mfrac>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p> where, <inline-formula>
  <mml:math id="mb90antj1t">
    <mml:mi>I</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is the pixel intensity, <inline-formula>
  <mml:math id="mtfg5ciyyh">
    <mml:msub>
      <mml:mi>c</mml:mi>
      <mml:mi>r</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is the centroid of region $r<inline-formula>
  <mml:math id="m8itlfstj9">
    <mml:mo>,</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\sigma_r<inline-formula>
  <mml:math id="me6lxh8pz7">
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>\varphi(x)$ is initialized using CPFAs to ensure a robust and adaptive segmentation process. CPFAs effectively capture both the intensity variations and spatial dependencies in the image, allowing for precise boundary evolution. The initialization is defined as:</p>
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="mfdmparn9g">
                <mml:mi>φ</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mi>α</mml:mi>
                <mml:mi>Re</mml:mi>
                <mml:mi>β</mml:mi>
                <mml:mi>Im</mml:mi>
                <mml:mo>(</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>⋅</mml:mo>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msubsup>
                    <mml:mi>μ</mml:mi>
                    <mml:mi>r</mml:mi>
                    <mml:mi>c</mml:mi>
                  </mml:msubsup>
                  <mml:mi>x</mml:mi>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msubsup>
                    <mml:mi>μ</mml:mi>
                    <mml:mi>r</mml:mi>
                    <mml:mi>c</mml:mi>
                  </mml:msubsup>
                  <mml:mi>x</mml:mi>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p> where, <inline-formula>
  <mml:math id="m28nhways2">
    <mml:msubsup>
      <mml:mi>μ</mml:mi>
      <mml:mi>r</mml:mi>
      <mml:mi>c</mml:mi>
    </mml:msubsup>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
  </mml:math>
</inline-formula> represents the complex Pythagorean fuzzy membership function. Here, <inline-formula>
  <mml:math id="mszcq18cvr">
    <mml:mi>Re</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msubsup>
        <mml:mi>μ</mml:mi>
        <mml:mi>r</mml:mi>
        <mml:mi>c</mml:mi>
      </mml:msubsup>
      <mml:mi>x</mml:mi>
    </mml:mrow>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mczmko0btl">
    <mml:mi>Im</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msubsup>
        <mml:mi>μ</mml:mi>
        <mml:mi>r</mml:mi>
        <mml:mi>c</mml:mi>
      </mml:msubsup>
      <mml:mi>x</mml:mi>
    </mml:mrow>
  </mml:math>
</inline-formula> denote the real and imaginary components, respectively, while parameters <inline-formula>
  <mml:math id="m4is62zq0p">
    <mml:mi>α</mml:mi>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mg572rgvtb">
    <mml:mi>β</mml:mi>
  </mml:math>
</inline-formula> control their relative contributions. This formulation allows for better handling of overlapping intensity distributions and noise, enhancing the adaptability of the segmentation model. By incorporating CPFAs, the level-set initialization ensures a smooth transition between regions and a well-balanced trade-off between region homogeneity and boundary sharpness, facilitating an efficient and accurate segmentation process.</p><p>The proposed energy functional integrates complex fuzzy aggregation with geometric constraints for robust segmentation. The functional is given by:</p>
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="m8xio9b1be">
                <mml:mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>F</mml:mi>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mi data-mjx-auto-op="false">CPFuzzy</mml:mi>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mi>φ</mml:mi>
                    </mml:mtd>
                    <mml:mtd>
                      <mml:mi/>
                      <mml:mi>x</mml:mi>
                      <mml:mi>∇</mml:mi>
                      <mml:mi>φ</mml:mi>
                      <mml:mi>x</mml:mi>
                      <mml:mi>d</mml:mi>
                      <mml:mi>x</mml:mi>
                      <mml:mi>φ</mml:mi>
                      <mml:mi>d</mml:mi>
                      <mml:mi>x</mml:mi>
                      <mml:mi>d</mml:mi>
                      <mml:mi>x</mml:mi>
                      <mml:mo>=</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo fence="false">‖</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:msub>
                        <mml:mo>∫</mml:mo>
                        <mml:mrow>
                          <mml:mi>Ω</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>ζ</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:msub>
                      <mml:msub>
                        <mml:mo>∫</mml:mo>
                        <mml:mrow>
                          <mml:mi>Ω</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>δ</mml:mi>
                        <mml:mi>ϵ</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>ζ</mml:mi>
                        <mml:mn>2</mml:mn>
                      </mml:msub>
                      <mml:msub>
                        <mml:mo>∫</mml:mo>
                        <mml:mrow>
                          <mml:mi>Ω</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:msubsup>
                        <mml:mi>μ</mml:mi>
                        <mml:mi>r</mml:mi>
                        <mml:mi>c</mml:mi>
                      </mml:msubsup>
                      <mml:msup>
                        <mml:mo fence="false">‖</mml:mo>
                        <mml:mn>2</mml:mn>
                      </mml:msup>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>−</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mi>γ</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>γ</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msub>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo fence="false">‖</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mo>+</mml:mo>
                        <mml:mo>⋅</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mi>∇</mml:mi>
                        <mml:mi>I</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:mi>κ</mml:mi>
                        <mml:mi>K</mml:mi>
                        <mml:mi>x</mml:mi>
                        <mml:msup>
                          <mml:mo fence="false">‖</mml:mo>
                          <mml:mn>2</mml:mn>
                        </mml:msup>
                      </mml:mrow>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p> where, <inline-formula>
  <mml:math id="mpljiu9s4v">
    <mml:mo fence="false">‖</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>∇</mml:mi>
    <mml:mi>φ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:msup>
      <mml:mo fence="false">‖</mml:mo>
      <mml:mn>2</mml:mn>
    </mml:msup>
  </mml:math>
</inline-formula> enforces smooth boundary evolution, and <inline-formula>
  <mml:math id="m1x8v4eoan">
    <mml:msub>
      <mml:mi>δ</mml:mi>
      <mml:mi>ϵ</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>φ</mml:mi>
  </mml:math>
</inline-formula> is the Dirac delta function approximation. The parameters <inline-formula>
  <mml:math id="mcvtmyimyk">
    <mml:msub>
      <mml:mi>γ</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula>, <inline-formula>
  <mml:math id="mbjqjaq7ud">
    <mml:msub>
      <mml:mi>γ</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> control region- wise homogeneity, <inline-formula>
  <mml:math id="mrks9jjjqw">
    <mml:msub>
      <mml:mi>ζ</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>ζ</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula> are weighting parameters, and <inline-formula>
  <mml:math id="mjxxb8eovy">
    <mml:mi>κ</mml:mi>
    <mml:mi>K</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> incorporates curvature constraints. The segmentation process evolves by solving the gradient descent equation:</p>
          
            <disp-formula>
              <label>(6)</label>
              <mml:math id="mwhu1iltkc">
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>∂</mml:mi>
                    <mml:mi>φ</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>∂</mml:mi>
                    <mml:mi>t</mml:mi>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mi>δ</mml:mi>
                    <mml:mi>φ</mml:mi>
                    <mml:msub>
                      <mml:mi>F</mml:mi>
                      <mml:mrow>
                        <mml:mrow>
                          <mml:mi data-mjx-auto-op="false">CPFuzzy</mml:mi>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:mi>δ</mml:mi>
                    <mml:mi>φ</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                  </mml:mrow>
                </mml:mfrac>
                <mml:mo>=</mml:mo>
                <mml:mo>−</mml:mo>
              </mml:math>
            </disp-formula>
          
          <p>This equation iteratively updates the level-set function, minimizing the energy functional while refining the segmentation boundaries. The proposed approach effectively balances region homogeneity, edge preservation, and phase-aware boundary refinement, making it suitable for complex imaging conditions.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Theorem: boundedness of the complex fuzzy energy functional</title>
          
          <p>Statement: The functional <inline-formula>
  <mml:math id="mtammixvut">
    <mml:msub>
      <mml:mi>F</mml:mi>
      <mml:mrow>
        <mml:mtext>CPFuzzy </mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>φ</mml:mi>
  </mml:math>
</inline-formula> is bounded for smooth <inline-formula>
  <mml:math id="m792ttvc5b">
    <mml:mi>φ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> and images <inline-formula>
  <mml:math id="mzy8q1wm8c">
    <mml:mi>I</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula>.</p><p>Proof: Since the complex fuzzy membership function satisfies $0 \leq\left|\mu_r^c(x)\right| \leq$ 1 , it does not introduce unbounded growth. The Sobolev space constraints ensure the gradient terms remain finite. Hence, the energy functional is bounded, ensuring stability in segmentation.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Experimental results</title>
      <p>The proposed segmentation model is evaluated through extensive experiments to demonstrate its effectiveness in handling noise, intensity variations, and complex boundary structures. Existing models (Wang and He [<xref ref-type="bibr" rid="ref_22">22</xref>] and Li et al.’s model [<xref ref-type="bibr" rid="ref_23">23</xref>]) often suffer from limitations such as sensitivity to noise, weak boundary preservation, and poor adaptability to varying intensity distributions. To address these challenges, our approach integrates fuzzy membership functions with geometric constraints to achieve robust segmentation. The experiments are conducted using MATLAB R2015a, ensuring compatibility with older computing environments. The input images are resized to 255×255 pixels to maintain consistency across evaluations. The dataset used for validation is publicly available, facilitating reproducibility and benchmarking against existing methods. For transparency, the MATLAB implementation of the proposed model will be provided upon request via email.</p><p>The proposed model parameters are carefully selected and optimized to ensure accurate and robust segmentation across diverse image datasets. The fuzzy membership function employs <inline-formula>
  <mml:math id="m6p4xnkia3">
    <mml:msub>
      <mml:mi>c</mml:mi>
      <mml:mi>r</mml:mi>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mn>128</mml:mn>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mr0p314c1m">
    <mml:msub>
      <mml:mi>σ</mml:mi>
      <mml:mi>r</mml:mi>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mn>30</mml:mn>
  </mml:math>
</inline-formula>, which are determined based on the average intensity distribution of foreground and background regions across training images. These values allow for smooth transitions between segmented regions while minimizing misclassification. The geometric operators utilize the image gradient <inline-formula>
  <mml:math id="mvr00kfa2i">
    <mml:mi>I</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> for edge detection and curvature <inline-formula>
  <mml:math id="m9hi0gh9mp">
    <mml:mi>K</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> for boundary smoothness, where the curvature weight <inline-formula>
  <mml:math id="m6a2vs16ja">
    <mml:mrow>
      <mml:mi data-mjx-variant="-tex-calligraphic">K</mml:mi>
    </mml:mrow>
    <mml:mo>=</mml:mo>
    <mml:mn>0.1</mml:mn>
  </mml:math>
</inline-formula> is empirically tuned to balance shape preservation and segmentation flexibility. The fuzzy energy functional is optimized by setting <inline-formula>
  <mml:math id="my2hm482lt">
    <mml:msub>
      <mml:mi>ζ</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mn>1.5</mml:mn>
  </mml:math>
</inline-formula> (fuzzy membership influence) and <inline-formula>
  <mml:math id="mrk3iee7fz">
    <mml:msub>
      <mml:mi>ζ</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mn>0.8</mml:mn>
  </mml:math>
</inline-formula> (geometric constraints), values obtained through cross-validation on multiple datasets to ensure stability in segmenting both well-defined and blurred structures. The intensity homogeneity parameters <inline-formula>
  <mml:math id="mqdid17rwa">
    <mml:msub>
      <mml:mi>γ</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mn>0.6</mml:mn>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mzetj56it3">
    <mml:msub>
      <mml:mi>γ</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mn>0.4</mml:mn>
  </mml:math>
</inline-formula> are adjusted based on statistical analysis of intra-region intensity variations, ensuring effective noise suppression while preserving fine details. These parameters were fine-tuned through an iterative process, testing on benchmark datasets and evaluating segmentation performance using accuracy, precision, and recall metrics. This systematic tuning ensures the proposed model maintains a high generalization capability across different types of image data, including medical imaging, remote sensing, and natural scene segmentation.</p><p><xref ref-type="fig" rid="fig_1">Figure 1</xref> demonstrates the workflow and effectiveness of the proposed segmentation model, showcasing each step in the segmentation process. The first image represents the given input image, containing regions of interest with varying intensities that make segmentation challenging. The second image displays the fuzzy membership function, which maps the pixel intensities to a fuzzy domain, enhancing the ability to distinguish between foreground and background regions. The third image illustrates the performance of the proposed model. Finally, the fourth image presents the segmentation result of the proposed model, where the regions of interest are accurately segmented, yielding a clear and precise representation of the target area. This figure highlights the proposed model’s ability to effectively integrate fuzzy logic and gradient-based analysis for robust and accurate image segmentation.</p><p><xref ref-type="fig" rid="fig_2">Figure 2</xref> provides a comprehensive visual comparison of the segmentation performance of different models. The first column represents the ground truth image, serving as a reference for evaluating the accuracy of segmentation methods. The second column shows the image after adding noise (noise level = 0.01), simulating challenging real-world conditions. The third and fourth columns depict the segmentation results produced by the Wang and He’s model [<xref ref-type="bibr" rid="ref_22">22</xref>] and Li et al.’s model [<xref ref-type="bibr" rid="ref_23">23</xref>], respectively. Both models struggle to accurately segment the region of interest, as indicated by incomplete or inaccurate boundaries. The fifth column illustrates the result of the proposed model, demonstrating its superior capability to handle noise and produce a more precise segmentation. Finally, the sixth column presents the final segmentation output of the proposed model, showing clearly defined boundaries and an accurate representation of the target region, highlighting its robustness and effectiveness compared to the competing methods. <xref ref-type="fig" rid="fig_3">Figure 3</xref> illustrates a comparative analysis of segmentation results across different models. The first image represents the original input image, which serves as the reference for segmentation. The second and third images display the segmentation outputs of the Wang and He’s model [<xref ref-type="bibr" rid="ref_22">22</xref>] and the Li et al.’s model [<xref ref-type="bibr" rid="ref_23">23</xref>], respectively, where visible inaccuracies and misclassified regions can be observed, indicated by the red contours. The fourth image presents the segmentation result of the proposed model, which exhibits enhanced accuracy and effectively segments inhomogeneous images, accurately capturing the desired object structures. This progression highlights the superiority of the proposed model in achieving precise segmentation outcomes.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Visual representation of the proposed segmentation model. (a) The given image; (b) The fuzzy membership function image; (c) The proposed model result and (d) The final segmentation result of the proposed model</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_uXLlIbV7fpKXprZr.png"/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_r8TJPCrY1rvubAAu.png"/>
        </fig>
      
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Visual comparison of segmentation results. (a) The ground truth image with noise added (0.01); (b) The result of Wang and He’s model [<xref ref-type="bibr" rid="ref_22">22</xref>]; (c) The result of Li et al.’s model [<xref ref-type="bibr" rid="ref_23">23</xref>] and (d) The result of the proposed model, and the final segmentation of the proposed model</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_ex-P5zS1A5L1mohP.png"/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_UWJ6F9O6x73LK6rI.png"/>
        </fig>
      
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>The segmentation results. (a) The original input image; (b) The outcomes of the Wang and He’s model [<xref ref-type="bibr" rid="ref_22">22</xref>]; (c) The outcomes of Li et al.’s model [<xref ref-type="bibr" rid="ref_23">23</xref>] and (d) The segmentation result generated by the proposed model</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_NFp75kyH0qc4cCGH.png"/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_gXNux6-c3MKPwzAl.png"/>
        </fig>
      
      <p> <xref ref-type="fig" rid="fig_4">Figure 4</xref> illustrates the segmentation performance of different models on inhomogeneous images, demonstrating their ability to handle complex textures and varying intensity regions. The first column shows the given inhomogeneous image, where the intensity variations and noise make accurate segmentation challenging. The second column presents the segmentation result of the Wang and He’s model [<xref ref-type="bibr" rid="ref_22">22</xref>], which struggles to maintain consistent boundaries and fails to segment the regions of interest accurately due to its sensitivity to IIH. The third column shows the result of the Li et al.’s model [<xref ref-type="bibr" rid="ref_23">23</xref>], which slightly improves upon the Wang and He’s model [<xref ref-type="bibr" rid="ref_22">22</xref>] but still exhibits inaccuracies in capturing the true boundaries of the target regions. The fourth column displays the segmentation result of the proposed model, which effectively addresses the challenges of IIH. The proposed model produces clear, well-defined boundaries and accurately segments the regions of interest, showcasing its robustness and precision compared to the competing models. This highlights the proposed model’s superiority in handling images with complex intensity variations.</p>
      
        <fig id="fig_4">
          <label>Figure 4</label>
          <caption>
            <title>Segmentation results for inhomogeneous images. (a) The given inhomogeneous image; (b) The segmentation result of Wang and He’s model [<xref ref-type="bibr" rid="ref_22">22</xref>]; (c) The segmentation result of Li et al.’s model [<xref ref-type="bibr" rid="ref_23">23</xref>] and (d) The segmentation result of the proposed model</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_BVR1w51sNeIoNr_H.png"/>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_xS_V2UUw6eogPvrs.png"/>
        </fig>
      
      <p> <xref ref-type="table" rid="table_1">Table 1</xref> presents the comparative evaluation of the proposed segmentation model against two competing models, namely the Wang and He's model [<xref ref-type="bibr" rid="ref_22">22</xref>] and the Li et al.'s model [<xref ref-type="bibr" rid="ref_23">23</xref>], using six key performance metrics: accuracy, precision, recall, F-Score, sensitivity and specificity. Each metric is reported as the mean ± standard deviation across multiple experimental runs. The proposed mode consistently outperforms the competing models across all metrics, indicating its robustness and superior segmentation performance.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Performance metrics for the proposed segmentation model competing models Wang and He [<xref ref-type="bibr" rid="ref_22">22</xref>] and Li et al. [<xref ref-type="bibr" rid="ref_23">23</xref>]</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Metric</p></td><td colspan="1" rowspan="1"><p>Our</p></td><td colspan="1" rowspan="1"><p>Li et al. [<xref ref-type="bibr" rid="ref_23">23</xref>]</p></td><td colspan="1" rowspan="1"><p>Wang and He [<xref ref-type="bibr" rid="ref_22">22</xref>]</p></td><td colspan="1" rowspan="1"><p>Significance (p-value)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Accuracy (%)</p></td><td colspan="1" rowspan="1"><p>96.8 <mml:math id="momhl6xf2m">
  <mml:mo>±</mml:mo>
</mml:math> 1.0</p></td><td colspan="1" rowspan="1"><p>91.2 <mml:math id="mra7sg91fl">
  <mml:mo>±</mml:mo>
</mml:math> 1.5</p></td><td colspan="1" rowspan="1"><p>89.5 <mml:math id="m241k8bx90">
  <mml:mo>±</mml:mo>
</mml:math> 2.0</p></td><td colspan="1" rowspan="1"><p>p &lt; 0.01 (ANOVA)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Precision (%)</p></td><td colspan="1" rowspan="1"><p>95.5 <mml:math id="m3psqy1bp6">
  <mml:mo>±</mml:mo>
</mml:math> 1.2</p></td><td colspan="1" rowspan="1"><p>89.8 <mml:math id="m93xwtuh8l">
  <mml:mo>±</mml:mo>
</mml:math> 2.0</p></td><td colspan="1" rowspan="1"><p>87.3 <mml:math id="mtttbqiw8x">
  <mml:mo>±</mml:mo>
</mml:math> 2.4</p></td><td colspan="1" rowspan="1"><p>p &lt; 0.01 (ANOVA)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Recall (%)</p></td><td colspan="1" rowspan="1"><p>94.7 <mml:math id="mh8a3v980m">
  <mml:mo>±</mml:mo>
</mml:math> 1.5</p></td><td colspan="1" rowspan="1"><p>88.5 <mml:math id="m5z4jcy553">
  <mml:mo>±</mml:mo>
</mml:math> 2.1</p></td><td colspan="1" rowspan="1"><p>86.0 <mml:math id="mq2c6s14ll">
  <mml:mo>±</mml:mo>
</mml:math> 2.5</p></td><td colspan="1" rowspan="1"><p>p &lt; 0.01 (T-Test)</p></td></tr><tr><td colspan="1" rowspan="1"><p>F-Score (%)</p></td><td colspan="1" rowspan="1"><p>95.1 <mml:math id="msqmwmquol">
  <mml:mo>±</mml:mo>
</mml:math> 1.3</p></td><td colspan="1" rowspan="1"><p>89.1 <mml:math id="mcktycogup">
  <mml:mo>±</mml:mo>
</mml:math> 1.8</p></td><td colspan="1" rowspan="1"><p>86.6 <mml:math id="m3xi0qw5ar">
  <mml:mo>±</mml:mo>
</mml:math> 2.3</p></td><td colspan="1" rowspan="1"><p>p &lt; 0.01 (ANOVA)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Sensitivity (%)</p></td><td colspan="1" rowspan="1"><p>96.2 <mml:math id="mdlb7v84fy">
  <mml:mo>±</mml:mo>
</mml:math> 1.1</p></td><td colspan="1" rowspan="1"><p>90.3 <mml:math id="mjv8p2x86a">
  <mml:mo>±</mml:mo>
</mml:math> 1.7</p></td><td colspan="1" rowspan="1"><p>88.2 <mml:math id="md71ab6xcr">
  <mml:mo>±</mml:mo>
</mml:math> 2.1</p></td><td colspan="1" rowspan="1"><p>p &lt; 0.01 (T-Test)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Specificity (%)</p></td><td colspan="1" rowspan="1"><p>97.4 <mml:math id="mhizya72i2">
  <mml:mo>±</mml:mo>
</mml:math> 0.8</p></td><td colspan="1" rowspan="1"><p>92.5 <mml:math id="mev3t72ro5">
  <mml:mo>±</mml:mo>
</mml:math> 1.3</p></td><td colspan="1" rowspan="1"><p>91.0 <mml:math id="mxphyuj76s">
  <mml:mo>±</mml:mo>
</mml:math> 1.6</p></td><td colspan="1" rowspan="1"><p>p &lt; 0.01 (T-Test)</p></td></tr><tr><td colspan="1" rowspan="1"><p>CPU</p></td><td colspan="1" rowspan="1"><p>23.8</p></td><td colspan="1" rowspan="1"><p>28.5</p></td><td colspan="1" rowspan="1"><p>31.2</p></td><td colspan="1" rowspan="1"></td></tr></tbody></table>
        </table-wrap>
      
      <p>The proposed model achieves the highest accuracy of 96.8%, showcasing its ability to correctly classify the majority of relevant and non-relevant regions in the segmentation task. Similarly, the precision value of 95.5% reflects its effectiveness in minimizing false positives, while a recall score of 94.7% highlights its capability to identify a significant proportion of true positives. The balanced performance of the proposed model is further evidenced by its F-Score of 95.1%, which is considerably higher than those of Li et al.’s model [<xref ref-type="bibr" rid="ref_23">23</xref>] and Wang and He’s model [<xref ref-type="bibr" rid="ref_22">22</xref>].</p><p>The sensitivity (96.2%) and specificity (97.4%) metrics of the proposed model further emphasize its reliability, demonstrating its ability to detect relevant regions while avoiding false detections. In contrast, Wang and He’s model [<xref ref-type="bibr" rid="ref_22">22</xref>] and Li et al.’s model [<xref ref-type="bibr" rid="ref_23">23</xref>] achieve lower sensitivity and specificity scores, indicating a less reliable segmentation performance.</p><p>The last column in the table reports the statistical significance of the results, with p &lt; 0.01 for all metrics, confirming that the improvements achieved by the proposed model over the competing models are statistically significant. Overall, the proposed segmentation model is demonstrated to be the best-performing method, providing robust and reliable results for the segmentation task, see <xref ref-type="fig" rid="fig_5">Figure 5</xref>.</p>
      
        <fig id="fig_5">
          <label>Figure 5</label>
          <caption>
            <title>Comparative analysis of segmentation performance across different models</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img__uTvj9F9icdmEU0e.png"/>
        </fig>
      
      <p>The computational efficiency of the proposed segmentation model was evaluated using the MATLAB tic-toc function, measuring the wall-clock execution time for each experiment. On average, the segmentation process required 23.8 seconds for a 255×255 image. In comparison, the models by Li et al. [<xref ref-type="bibr" rid="ref_23">23</xref>] and Wang and He [<xref ref-type="bibr" rid="ref_22">22</xref>] took 28.5 and 31.2 seconds, respectively, demonstrating that our method achieves faster segmentation while maintaining superior accuracy. The segmentation process involves fuzzy membership computation, geometric operator-based refinement, and level-set evolution, leading to an estimated complexity of <inline-formula>
  <mml:math id="mbgbho0f57">
    <mml:mrow>
      <mml:mi>O</mml:mi>
    </mml:mrow>
    <mml:mo>(</mml:mo>
    <mml:mo>⁡</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>N</mml:mi>
    <mml:mi>log</mml:mi>
    <mml:mi>N</mml:mi>
  </mml:math>
</inline-formula>, where $N$ represents the number of pixels. Despite the slightly higher computational load introduced by Pythagorean fuzzy membership functions, the integration of geometric constraints ensures stable and precise segmentation. Future optimizations, such as GPU acceleration or parallel processing, could further enhance computational efficiency, making the model more suitable for real-time applications in medical imaging and remote sensing.</p>
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>The proposed image segmentation model effectively integrates complex Pythagorean fuzzy logic with geometric principles into a stable energy functional framework, addressing critical limitations of existing segmentation techniques. By leveraging region-based segmentation strategies and incorporating complex fuzzy membership functions, the model achieves enhanced accuracy, robustness, and adaptability, even under challenging conditions such as noise and intensity variations. The stability of the Pythagorean fuzzy energy functional ensures consistent performance across diverse datasets, minimizing segmentation errors and improving boundary precision.</p><p>Extensive evaluations using multiple performance metrics—including accuracy, precision, recall, F-score, sensitivity, and specificity—demonstrate the model’s superiority. Statistical analyses (ANOVA and T-tests) reveal significant improvements in these metrics (p &amp;amp;lt; 0.01), underscoring the effectiveness of the proposed model in outperforming existing approaches. The incorporation of complex Pythagorean fuzzy membership functions enhances uncertainty handling, providing a more refined segmentation framework with better adaptability to varying intensity distributions. This improved performance highlights its potential for practical applications in medical imaging, remote sensing, and industrial automation, where precise and reliable segmentation is critical.</p><p>Despite its advantages, the proposed model has certain limitations. First, its reliance on complex Pythagorean fuzzy computations introduces higher computational complexity, which may impact efficiency when applied to large datasets or real-time applications. Second, while the model effectively handles intensity variations and noise, its scalability remains a challenge, particularly when dealing with high-resolution images or large-scale datasets.</p><p>To address these limitations, future work can focus on optimizing the computational efficiency of the model by employing parallel processing techniques and more efficient optimization algorithms. Additionally, GPU acceleration or hybrid deep learning models could be integrated to enhance scalability and facilitate real-time applications. Further research may also explore adaptive parameter tuning strategies within the complex Pythagorean fuzzy framework to improve segmentation performance across diverse imaging domains.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The datasets used for developing and evaluating the proposed image segmentation model are not publicly available. However, they can be provided upon reasonable request to the corresponding author for academic and research purposes. Additionally, any processed or derived data supporting the findings of this study can be made available upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The author declares that there are no known competing financial or non-financial interests that could have influenced the research presented in this study.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>708-725</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ibrar</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Muhammad</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3934/ipi.2022074</pub-id>
          <article-title>Efficient convex region-based segmentation for noising and inhomogeneous patterns</article-title>
          <source>Inverse Probl. Imag.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>e0236835</page-range>
          <issue>7</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Abdelazeem</surname>
              <given-names>R. M.</given-names>
            </name>
            <name>
              <surname>Youssef</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>El-Azab</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hassab-Elnaby</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Agour</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0236835</pub-id>
          <article-title>Three-dimensional visualization of brain tumor progression based on accurate segmentation via comparative holographic projection</article-title>
          <source>PLoS ONE</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>183-192</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hussain</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/ataiml030305</pub-id>
          <article-title>Robust leaf disease detection using complex fuzzy sets and HSV-based color segmentation techniques</article-title>
          <source>Acadlore Trans. Mach. Learn.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>40</volume>
          <page-range>834-848</page-range>
          <issue>4</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>L. C.</given-names>
            </name>
            <name>
              <surname>Papandreou</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Kokkinos</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Murphy</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Yuille</surname>
              <given-names>A. L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id>
          <article-title>DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</article-title>
          <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>72</volume>
          <page-range>102125</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Calli</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Sogancioglu</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Ginneken</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Leeuwen</surname>
              <given-names>K. G.</given-names>
            </name>
            <name>
              <surname>Murphy</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.media.2021.102125</pub-id>
          <article-title>Deep learning for chest X-ray analysis: A survey</article-title>
          <source>Med. Image Anal.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>116-126</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hussain</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Muhammad</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/ijkis010204</pub-id>
          <article-title>Enhanced global image segmentation: Addressing pixel inhomogeneity and noise with average convolution and entropy-based local factor</article-title>
          <source>Int. J. Knowl. Innov. Stud.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>92</volume>
          <page-range>103061</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Huang</surname>
              <given-names>Y. H.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Chang</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>et al.</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.media.2023.103061</pub-id>
          <article-title>Segment anything model for medical images?</article-title>
          <source>Med. Image Anal.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Kaymak</surname>
              <given-names>Ç.</given-names>
            </name>
            <name>
              <surname>Uçar</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>A brief survey and an application of semantic image segmentation for autonomous driving</article-title>
          <source>Handbook of Deep Learning Applications</source>
          <publisher-name>Springer, Cham</publisher-name>
          <year>2019</year>
          <page-range>161-200</page-range>
          <pub-id pub-id-type="doi">10.1007/978-3-030-11479-4_9</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="conf-paper">
          <page-range>677–686</page-range>
          <year>2010</year>
          <person-group person-group-type="author">
            <name>
              <surname>Peng</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-642-12304-7_64</pub-id>
          <article-title>Iterated graph cuts for image segmentation</article-title>
          <source>Computer Vision–ACCV 2009: 9th Asian Conference on Computer Vision, Xi’an, China</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1855–1859</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lyu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICIP.2019.8803132</pub-id>
          <article-title>Esnet: Edge-based segmentation network for real-time semantic segmentation in traffic scenes</article-title>
          <source>2019 IEEE International Conference on Image Processing (ICIP), Taipei, Taiwan</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="conf-paper">
          <page-range>400–403</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhou</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Du</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CEI52496.2021.9574569</pub-id>
          <article-title>Techniques for image segmentation based on edge detection</article-title>
          <source>2021 IEEE International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI), Fuzhou, China</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>114</volume>
          <page-range>102608</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Brar</surname>
              <given-names>Kamalpreet Kaur</given-names>
            </name>
            <name>
              <surname>Goyal</surname>
              <given-names>Baljit</given-names>
            </name>
            <name>
              <surname>Dogra</surname>
              <given-names>Akash</given-names>
            </name>
            <name>
              <surname>Mustafa</surname>
              <given-names>Mustafa A.</given-names>
            </name>
            <name>
              <surname>Majumdar</surname>
              <given-names>Rahul</given-names>
            </name>
            <name>
              <surname>Alkhayyat</surname>
              <given-names>Ahmed</given-names>
            </name>
            <name>
              <surname>Kukreja</surname>
              <given-names>Vinay</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.inffus.2024.102608</pub-id>
          <article-title>Image segmentation review: Theoretical background and recent advances</article-title>
          <source>Inf. Fusion</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>31</volume>
          <page-range>116-126</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Gupta</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Anand</surname>
              <given-names>R. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.bspc.2016.06.012</pub-id>
          <article-title>A hybrid edge-based segmentation approach for ultrasound medical images</article-title>
          <source>Biomed. Signal Process. Control</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <page-range>1113</page-range>
          <issue>5</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hussain</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>M. S.</given-names>
            </name>
            <name>
              <surname>Niu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Rada</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3934/ipi.2022014</pub-id>
          <article-title>Robust region-based active contour models via local statistical similarity and local similarity factor for intensity inhomogeneity and high noise image segmentation</article-title>
          <source>Inverse Probl. Imag.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>266-277</page-range>
          <issue>2</issue>
          <year>2001</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chan</surname>
              <given-names>T. F.</given-names>
            </name>
            <name>
              <surname>Vese</surname>
              <given-names>L. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/83.902291</pub-id>
          <article-title>Active contours without edges</article-title>
          <source>IEEE Trans. Image Process.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>50</volume>
          <page-range>271-293</page-range>
          <issue>3</issue>
          <year>2002</year>
          <person-group person-group-type="author">
            <name>
              <surname>Vese</surname>
              <given-names>L. A.</given-names>
            </name>
            <name>
              <surname>Chan</surname>
              <given-names>T. F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1023/A:1020874308076</pub-id>
          <article-title>A multiphase level set framework for image segmentation using the Mumford and Shah model</article-title>
          <source>Int. J. Comput. Vis.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>1169-1186</page-range>
          <issue>8</issue>
          <year>2001</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tsai</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Yezzi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Willsky</surname>
              <given-names>A. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/83.935033</pub-id>
          <article-title>Curve evolution implementation of the Mumford-Shah functional for image segmentation, denoising, interpolation, and magnification</article-title>
          <source>IEEE Trans. Image Process.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>89</volume>
          <page-range>102918</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mazurowski</surname>
              <given-names>M. A.</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>H. Y.</given-names>
            </name>
            <name>
              <surname>Gu</surname>
              <given-names>H. X.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>J. C.</given-names>
            </name>
            <name>
              <surname>Konz</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y. X.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.media.2023.102918</pub-id>
          <article-title>Segment anything model for medical image analysis: An experimental study</article-title>
          <source>Med. Image Anal.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>18</volume>
          <page-range>e0282909</page-range>
          <issue>6</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Meng</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>L. L.</given-names>
            </name>
            <name>
              <surname>Wen</surname>
              <given-names>J. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1371/journal.pone.0282909</pub-id>
          <article-title>Research on improved level set image segmentation method</article-title>
          <source>PLoS ONE</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>1</volume>
          <page-range>103-115</page-range>
          <issue>28</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Derevyanchuk</surname>
              <given-names>O.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.32835/2707-3092.2024.28.103-115</pub-id>
          <article-title>Use of intelligent fuzzy image segmentation systems in the professional training of future specialists in engineering and pedagogical fields</article-title>
          <source>Prof. Pedagog.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>23</volume>
          <page-range>9641</page-range>
          <issue>24</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Cardone</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Di Martino</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Miraglia</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s23249641</pub-id>
          <article-title>A novel fuzzy-based remote sensing image segmentation method</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>130</volume>
          <page-range>580-602</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Yu</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>Chuan Jiang</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.apm.2024.03.019</pub-id>
          <article-title>Fractional guidance-based level set evolution for noisy image segmentation with intensity inhomogeneity</article-title>
          <source>Appl. Math. Model.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>132</volume>
          <page-range>587-606</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Zhi Xiang</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>Shao Jie</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>Tian Yu</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Fu Qiang</given-names>
            </name>
            <name>
              <surname>Ye</surname>
              <given-names>Wen Guang</given-names>
            </name>
            <name>
              <surname>Ding</surname>
              <given-names>Wen Yu</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Kui Dong</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.apm.2024.04.058</pub-id>
          <article-title>A convex level-set method with multiplicative-additive model for image segmentation</article-title>
          <source>Appl. Math. Model.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>