<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">MITS</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Mechatronics and Intelligent Transportation Systems</journal-title>
        <abbrev-journal-title abbrev-type="issn">Mechatron. Intell Transp. Syst.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">MITS</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-0218</issn>
      <issn publication-format="print">2958-020X</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-qHPikXxu9wcH672Ib2hi-LY6i7NUNiMm</article-id>
      <article-id pub-id-type="doi">10.56578/mits040201</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>A Hybrid Soft Computing Framework for Robust Classification of Heavy Transport Vehicles in Visual Traffic Surveillance</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-7540-6265</contrib-id>
          <name>
            <surname>Hussain</surname>
            <given-names>Ibrar</given-names>
          </name>
          <email>ibrar786@uop.edu.pk</email>
        </contrib>
        <aff id="aff_1">Department of Mathematics, University of Peshawar, 25120 Peshawar, Pakistan</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>05</day>
        <month>05</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>2</issue>
      <fpage>61</fpage>
      <lpage>71</lpage>
      <page-range>61-71</page-range>
      <history>
        <date date-type="received">
          <day>09</day>
          <month>03</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>27</day>
          <month>04</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>The efficient classification of transport vehicles is critical to the optimization of modern transportation systems, yet significant challenges persist, particularly in distinguishing Heavy Transport Vehicles (HTVs) from Light Transport Vehicles (LTVs). These challenges arise due to considerable variations in vehicle size, shape, orientation, and external factors such as camera perspective, lighting conditions, and occlusions. In this study, a novel classification framework is proposed, integrating geometric feature extraction with a soft computing approach based on fuzzy logic. Key geometric attributes, including bounding box length, width, area, and aspect ratio, are extracted through image processing techniques. Initial classification is performed via threshold-based rules to eliminate non-HTV instances using predefined feature thresholds. To address uncertainties inherent in real-world surveillance conditions, fuzzy logic inference is subsequently applied, enabling flexible and robust decision-making in the presence of imprecise or noisy data. This hybrid methodology, combining deterministic thresholding and soft computing principles, enhances classification reliability and adaptability under diverse environmental and operational conditions. Extensive real-world experiments have been conducted to validate the proposed framework, demonstrating superior performance in terms of accuracy, robustness, and computational efficiency when compared with conventional classification methods. The results underscore the potential of the framework for deployment in intelligent traffic monitoring systems where precise vehicle categorization is essential for traffic management, infrastructure planning, and safety enforcement.</p></abstract>
      <kwd-group>
        <kwd>Image processing</kwd>
        <kwd>Geometric feature extraction</kwd>
        <kwd>Fuzzy logic</kwd>
        <kwd>Threshold-based classification</kwd>
        <kwd>Vehicle detection</kwd>
        <kwd>Transportation systems</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="1"/>
        <fig-count count="4"/>
        <table-count count="2"/>
        <ref-count count="23"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Image processing techniques play a crucial role in isolating and analyzing specific objects within complex visual environments, aiding tasks such as object recognition, tracking, and scene understanding [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>]. The transport system is a critical component of modern infrastructure, facilitating the movement of people, goods, and services across different geographical locations. It encompasses a variety of modes, including road, rail, air, and water transport, each designed to meet specific needs based on distance, speed, and capacity [<xref ref-type="bibr" rid="ref_4">4</xref>], [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>]. Efficient transport systems are essential for economic development, enabling the exchange of resources, promoting trade, and improving accessibility to education, healthcare, and employment. They play a vital role in shaping urbanization, social integration, and environmental sustainability [<xref ref-type="bibr" rid="ref_7">7</xref>], [<xref ref-type="bibr" rid="ref_8">8</xref>], [<xref ref-type="bibr" rid="ref_9">9</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>]. The evolution of transportation technologies and associated infrastructure continues to shape the functioning of societies, influencing settlement patterns, commercial activities, and global interactions. In recent years, there has been an increasing focus on sustainable transport solutions, aiming to reduce congestion, emissions, and energy consumption, while improving safety and accessibility for all users.</p><p>The transport system has witnessed significant advancements with the integration of intelligent technologies aimed at improving vehicle identification, classification, and traffic management. Several studies have focused on utilizing machine learning and deep learning techniques to automate these processes. For example, Rajput et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] developed an automatic vehicle identification and classification model using the YOLOv3 algorithm for a toll management system, demonstrating its effectiveness in real-time vehicle detection and classification for smoother toll operations. Similarly, Sharma et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] proposed a modified region-based convolution network for vehicle identification, enhancing the accuracy and efficiency of intelligent transportation systems. In the realm of traffic management, Tippannavar and SD [<xref ref-type="bibr" rid="ref_13">13</xref>] provided a comprehensive review of real-time vehicle identification techniques aimed at improving traffic flow and reducing congestion. Additionally, Ni et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] introduced a novel moving vehicle identification framework based on structural vibration response and deep learning algorithms, offering a promising solution for dynamic vehicle tracking. Nasr et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] explored the design of an intelligent QR code-based mobile application for vehicle identification and authentication, presenting a new approach to enhancing security and streamlining vehicle registration processes. These advancements, through innovative identification and classification models, contribute significantly to optimizing the efficiency, safety, and sustainability of modern transport systems.</p><p>Recent advancements in vehicle detection and recognition systems have significantly enhanced the functionality of transport systems, especially in urban settings where efficient traffic management is crucial. Wang et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] explored a fusion algorithm for target vehicle detection and recognition, providing a robust solution for identifying vehicles under varying environmental conditions. This method, integrating multiple sources of data, helps improve the accuracy and reliability of vehicle recognition systems. However, the approach still faces challenges in handling real-time processing and the complexity of dynamic environments, where rapid changes in lighting, weather, and occlusions can impact performance. Similarly, Zohaib et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] proposed a deep learning approach with multimodal fusion to enhance emergency vehicle detection, offering a solution that improves the response times and prioritization of emergency vehicles in traffic. While the method demonstrates effectiveness, its reliance on large datasets and high computational power for training deep learning models can be a limitation, especially in low-resource environments. Additionally, the fusion of multiple data sources requires complex data preprocessing, which can introduce delays in real-time applications. In line with this, de Córdova et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] introduced a methodology for non-invasive identification of vehicle suspension parameters using synthetic data analysis, expanding the scope of vehicle identification beyond mere visual recognition and integrating performance-based parameters for better overall vehicle analysis. Although this approach improves the depth of vehicle analysis, the reliance on synthetic data may limit its applicability to real-world scenarios, where actual vehicle performance data might differ due to various external factors such as road conditions or vehicle wear and tear. Furthermore, Moussaoui et al. [<xref ref-type="bibr" rid="ref_19">19</xref>] integrated YOLOv8 and optical character recognition (OCR) techniques for high-precision license plate detection, marking an important step toward enhancing automated vehicle identification. While this combination of advanced detection techniques ensures greater accuracy in recognizing and classifying vehicles, even in complex traffic environments, the system has notable limitations. It relies heavily on the clear visibility of license plates, making it vulnerable to issues such as occlusions, dirty or worn-out plates, and varying plate designs across regions. Additionally, the integration of YOLOv8 and OCR significantly increases computational complexity, which may hinder real-time performance and scalability in large-scale or resource-constrained deployments.</p><p>These innovations, along with the previously mentioned advancements in intelligent transport systems, significantly contribute to improving safety, efficiency, and security within modern transportation infrastructure. However, challenges related to real-time processing, data preprocessing, reliance on synthetic data, and environmental conditions still persist, limiting the full potential of these systems in practical deployment. The fusion of deep learning, multimodal data, and advanced recognition techniques continues to evolve, offering promising solutions to enhance the overall functionality of transport systems worldwide.</p><p>In this paper, we propose a novel and robust vehicle classification model that differentiates between HTVs and LTVs. The model leverages a combination of geometric features, threshold-based criteria, and fuzzy logic to achieve high accuracy in real-world vehicle classification scenarios (see <xref ref-type="fig" rid="fig_1">Figure 1</xref>). The classification process starts with the extraction of key geometric features from the bounding box surrounding the detected vehicle. These features include length, width, area, and aspect ratio, all of which play a crucial role in identifying HTVs, which are typically larger than regular vehicles.</p><p>The classification process is implemented in the following steps:</p><p><italic>Feature extraction</italic>: The necessary geometric features—length, width, area, and aspect ratio—are computed from the vehicle's bounding box. These features form the foundation for distinguishing between HTVs and LTVs.</p><p><italic>Threshold-based filtering</italic>: Predefined threshold rules are applied to filter out vehicles that do not meet the criteria for HTVs based on the extracted features. This step ensures that only vehicles likely to be HTVs proceed for further classification.</p><p><italic>Fuzzy logic application</italic>: To account for uncertainties and variability in the measurements (such as changes in camera angle, vehicle orientation, or environmental factors), fuzzy logic is introduced. The fuzzy logic system models the imprecise nature of vehicle classification, providing flexibility and robustness in decision-making.</p><p><italic>Defuzzification</italic>: The fuzzy outputs, representing the uncertainty in classification, are defuzzified to generate a crisp decision. This final output classifies the vehicle as either an HTV or LTV.</p><p>The integration of both crisp threshold rules and fuzzy logic offers the advantages of accuracy and robustness. The threshold-based approach ensures that clear-cut decisions are made when possible, while the fuzzy logic handles the uncertainties that arise in real-world conditions, such as variation in vehicle orientation and camera angles. This combination of techniques allows the system to adapt and make more reliable decisions in complex and dynamic environments. The key contribution of the proposed model lies in its innovative approach to vehicle classification, which combines the precision of threshold rules with the adaptability of fuzzy logic. By incorporating fuzzy logic, our model effectively addresses the uncertainties inherent in vehicle measurements and classification, improving the system’s robustness to real-world conditions. This hybrid approach enables the model to be highly effective in traffic management systems, where accurate and reliable vehicle classification is critical. The proposed model offers a significant advancement over traditional methods, providing a more flexible, adaptive, and accurate solution for HTV detection.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Flowchart of the proposed vehicle classification method using geometric features, threshold-based criteria, and fuzzy logic</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_uERl-SyhQIWLfC0n.png"/>
        </fig>
      
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p>In recent years, vehicle detection and classification have emerged as critical components in intelligent transportation systems, enabling a wide range of applications such as traffic monitoring, congestion management, toll collection, and road safety enforcement. Numerous studies have explored various computational techniques, ranging from traditional image processing methods to advanced deep learning and fuzzy logic-based frameworks, to improve the accuracy and robustness of vehicle classification systems. In particular, distinguishing between HTVs and non-HTVs is essential for optimizing road usage and enforcing regulatory compliance. This section reviews recent advancements in vehicle detection and classification, highlighting their methodologies, key contributions, and existing limitations to establish the context for the proposed model.</p><p>Kanagamalliga et al. [<xref ref-type="bibr" rid="ref_20">20</xref>] developed a traffic management framework integrating advanced vehicle detection, recognition, and tracking using machine learning and computer vision. Their approach enhances real-time monitoring and traffic flow by maintaining vehicle identities across frames, reducing data redundancy. Although effective in structured environments, the system depends on high-resolution cameras and stable lighting, limiting its performance in poor conditions. Moreover, it lacks detailed strategies for distinguishing between vehicle types, which is important for specific transportation policies.</p><p>Islam et al. [<xref ref-type="bibr" rid="ref_21">21</xref>] investigated vehicle classification and detection using the YOLOv8 model. They evaluated YOLOv8’s performance for real-time highway monitoring in Bangladesh, highlighting its ability to process images quickly with minimal accuracy loss. The model, trained on diverse vehicle types, showed high classification accuracy for cars, motorcycles, and trucks. However, it struggled in congested scenes, particularly with closely packed or partially occluded heavy vehicles. Despite its speed and accuracy, YOLOv8's computational complexity can cause bottlenecks in large datasets or resource-limited environments. Additionally, the study did not address the interpretability of the model's outputs, a key factor for intelligent transportation systems.</p><p>El Mallahi et al. [<xref ref-type="bibr" rid="ref_22">22</xref>] proposed a hybrid model combining multiple machine learning algorithms to enhance road safety and vehicle identification. Their system uses supervised and unsupervised learning to detect vehicles, pedestrians, and dynamic road objects in real-time, adapting to traffic behavior and environmental changes. A key feature is the addition of intelligent identification layers that provide contextual information such as speed, distance, and lane behavior. However, the framework lacks extensive cross-validation across diverse datasets, raising concerns about its generalizability and robustness under varying conditions. It also prioritizes detection over precise vehicle classification, limiting its applicability in areas like tolling, load regulation, and environmental compliance.</p>
    </sec>
    <sec sec-type="">
      <title>3. Methodology</title>
      <p>To achieve accurate classification of vehicles into HTVs and LTVs, we propose a multi-stage methodology that integrates geometric feature extraction, threshold-based filtering, and fuzzy logic inference. The process begins by detecting vehicles in the input frames and computing key geometric features from their corresponding bounding boxes—specifically, length, width, area, and aspect ratio. These features are selected due to their strong correlation with the physical dimensions of HTVs, which are generally larger than standard vehicles. Following feature extraction, a set of predefined threshold rules is employed to filter out non-HTVs based on dimensional constraints. To address the inherent uncertainties and variability in real-world conditions—such as camera perspective, occlusion, and environmental noise—a fuzzy logic system is incorporated. This system interprets the geometric data within a flexible framework that tolerates imprecision and ambiguity. The final classification decision is obtained through defuzzification, converting the fuzzy output into a crisp label that determines whether the vehicle belongs to the HTV or LTV category. The entire classification pipeline is outlined in the following subsections.</p>
      
        <sec>
          
            <title>3.1. Feature extraction</title>
          
          <p>To classify a vehicle as an HTV or otherwise, it is essential to extract meaningful geometric features from the input image. These features are derived from the bounding box that tightly encloses the detected vehicle object. The primary features considered in this work include the vehicle’s length, width, area, and aspect ratio, all of which provide significant indicators for distinguishing between LTVs and HTVs. However, it is important to note that feature extraction in this approach is based on two-dimensional pixel dimensions, and does not account for practical scene limitations such as camera calibration, perspective distortion, or depth estimation. These limitations may cause variations in the appearance of vehicles depending on their distance from the camera or their positioning in the scene. As a result, perspective distortion can affect the accuracy of the feature extraction, particularly for distant vehicles.</p>
          
            <sec>
              
                <title>3.1.1 Length, width, and area</title>
              
              <p>The bounding box around the detected vehicle provides two key spatial measurements: the vertical extent (height) and the horizontal extent (width). The length of the vehicle, denoted by L, is defined as the vertical height of the bounding box in pixels. Similarly, the width, denoted by W, is the horizontal width of the bounding box in pixels.</p><p>Once the height and width are obtained, the area A covered by the vehicle is computed as the product of these two dimensions. This area represents the overall spatial footprint of the vehicle and serves as a key discriminative feature, particularly since HTVs typically occupy a larger image region than lighter vehicles. The area is calculated using the following equation:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mnklc5ifff">
    <mml:mi>A</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>W</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mo>×</mml:mo>
  </mml:math>
</inline-formula></p><p>However, due to potential scale variations caused by camera placement, perspective distortion, or the vehicle’s distance from the camera, this feature is combined with other characteristics to improve classification robustness. For more accurate classification, depth estimation or camera calibration would be necessary to compensate for the effects of perspective distortion, which can cause distant vehicles to appear smaller than they are.</p>
            </sec>
          
          
            <sec>
              
                <title>3.1.2 Aspect ratio</title>
              
              <p>Another important feature is the aspect ratio, denoted by AR, which is the ratio of the vehicle’s height to its width. It is defined as:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mvh4de2qx3">
    <mml:mrow>
      <mml:mi data-mjx-auto-op="false">AR</mml:mi>
    </mml:mrow>
    <mml:mo>=</mml:mo>
    <mml:mfrac>
      <mml:mi>L</mml:mi>
      <mml:mi>W</mml:mi>
    </mml:mfrac>
  </mml:math>
</inline-formula></p><p>This ratio captures the structural profile of the vehicle. For instance, HTVs such as trucks or buses typically exhibit a lower aspect ratio, as they are wider relative to their height. In contrast, smaller vehicles may appear taller and narrower, leading to a higher aspect ratio.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Threshold-based mathematical classification</title>
          
          <p>In the initial stage of classification, threshold values are applied to geometric features extracted from labeled vehicle image datasets. These thresholds were empirically determined based on statistical observations and distribution analysis of features across multiple annotated vehicle categories. A preliminary dataset exploration revealed distinct separations in size and shape between HTVs and non-HTVs. These insights guided the setting of appropriate boundary values for effective filtering. Additionally, sensitivity testing was conducted by slightly adjusting each threshold to verify classification stability and robustness.</p><p>The following threshold-based conditions are defined for classifying a vehicle as HTV based on its extracted features (area, aspect ratio, length, and width).</p>
          
            <sec>
              
                <title>3.2.1 Threshold criteria</title>
              
              <p>Area (A): The area of the bounding box correlates directly with vehicle size. Data distribution showed that HTVs typically exceed 9000 px² in image space, while smaller vehicles remain below this threshold. A sensitivity range of 8500–9500 px² was tested, with 9000 px² yielding the best trade-off between precision and recall. Therefore:</p><p style="text-align: center"><inline-formula>
  <mml:math id="ms3g6ewga9">
    <mml:mi>A</mml:mi>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mn>9000</mml:mn>
    <mml:msup>
      <mml:mrow>
        <mml:mi data-mjx-auto-op="false">px</mml:mi>
      </mml:mrow>
      <mml:mn>2</mml:mn>
    </mml:msup>
  </mml:math>
</inline-formula></p><p>Aspect ratio (AR): The ratio of height to width (AR) was analyzed across different vehicle classes. HTVs generally exhibit broader and flatter profiles, resulting in lower AR values. Histograms revealed that the majority of HTVs fall below an AR of 1.5, which was thus selected after validation:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mvkd31ra7f">
    <mml:mrow>
      <mml:mi data-mjx-auto-op="false">AR</mml:mi>
    </mml:mrow>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mn>1.5</mml:mn>
  </mml:math>
</inline-formula></p>
              <p>Length (L): Vehicle height was also examined, and vehicles categorized as HTVs consistently showed vertical lengths above 100 px in the image dataset. This threshold was confirmed through iterative adjustments in the range of 90–120 px, with 100 px providing a balanced cutoff:</p><p style="text-align: center"><inline-formula>
  <mml:math id="muxqpzhkqt">
    <mml:mi>L</mml:mi>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mn>100</mml:mn>
    <mml:mrow>
      <mml:mi data-mjx-auto-op="false">px</mml:mi>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p>Width (W): Similarly, HTVs demonstrated consistently higher width values, typically exceeding 80 px. Below this value, the majority of vehicles were non-HTVs such as cars or bikes. Thus, 80 px was established as a reliable threshold after testing several values from 70 to 90 px:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mcxyqlf8h2">
    <mml:mi>W</mml:mi>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mn>80</mml:mn>
    <mml:mrow>
      <mml:mi data-mjx-auto-op="false">px</mml:mi>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p>These empirically derived thresholds provide a reliable initial filtering layer before applying fuzzy logic. They are not arbitrarily chosen but rather supported by both feature distribution analysis and sensitivity tuning on the labeled dataset, ensuring robustness and adaptability to real-world classification scenarios.</p><p><xref ref-type="fig" rid="fig_2">Figure 2</xref> shows the feature distributions for HTVs and non-HTVs, demonstrating the effectiveness of the proposed model in distinguishing between the two categories. Key features, including Area (px²), Length (px), and Width (px), are analyzed. Empirically determined thresholds—9000 px² for Area, 1.5 for the Length-to-Width ratio, and 80 px for Width—are indicated with dashed lines to highlight optimal class separation. The histograms reveal distinct patterns, with HTVs generally exhibiting larger dimensions than non-HTVs. These results validate the model’s feature extraction process and suggest strong potential for accurate HTV detection in real-world applications.</p>
              
                <fig id="fig_2">
                  <label>Figure 2</label>
                  <caption>
                    <title>Feature distributions for HTV and Non-HTV classes</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_ZV1TvgaM7DW6J-RC.jpeg"/>
                </fig>
              
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. Classification rule</title>
          
          <p>The overall classification decision is made by applying all the above threshold conditions simultaneously. A vehicle is classified as an HTV if it satisfies all the following conditions:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mktrysa8da">
    <mml:mtext> IF </mml:mtext>
    <mml:mtext> AND AR </mml:mtext>
    <mml:mtext> AND </mml:mtext>
    <mml:mtext> AND </mml:mtext>
    <mml:mtext> Vehicle is an HTV </mml:mtext>
    <mml:mi>A</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>W</mml:mi>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>⇒</mml:mo>
    <mml:mn>9000</mml:mn>
    <mml:mn>1.5</mml:mn>
    <mml:mn>100</mml:mn>
    <mml:mn>80</mml:mn>
  </mml:math>
</inline-formula></p><p>This rule is based on the combination of the four extracted features: area, aspect ratio, length, and width. Vehicles meeting all the specified conditions are classified as HTVs. If any condition is not satisfied (e.g., if the vehicle has a high aspect ratio or a small area), it is classified as a non-HTV (LTV or other types of vehicles).</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Fuzzy input variables and membership functions</title>
          
          <p>To classify vehicles into HTVs or non-HTVs, fuzzy logic is used to model the uncertainty and imprecision associated with visual input features. The primary features considered for classification are Length (L), Width (W), Area (A), and Aspect Ratio (AR). These features are fuzzified using linguistic terms such as Short, Medium, Long, etc., and corresponding membership functions (see <xref ref-type="table" rid="table_1">Table 1</xref>). The membership functions were constructed using a combination of trapezoidal and Gaussian curves, with parameters derived from empirical analysis of the dataset.</p><p><italic>Membership function parameters</italic>: The trapezoidal membership functions are defined using four parameters: a, b, c, and d, where [a, b] is the rising edge, [b, c] is the plateau, and [c, d] is the falling edge. These parameters were chosen based on percentile thresholds calculated from the training dataset to reflect typical ranges for each class of vehicle. For Gaussian functions, the mean (<inline-formula>
  <mml:math id="mppqb2djbc">
    <mml:mi>μ</mml:mi>
  </mml:math>
</inline-formula>) and standard deviation (<inline-formula>
  <mml:math id="m7g35w05cw">
    <mml:mi>σ</mml:mi>
  </mml:math>
</inline-formula>) were similarly estimated using statistical analysis of the feature distribution.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Fuzzy input variables and membership functions</title>
              </caption>
              <table><tr><th >Feature</th><th >Linguistic Terms</th><th >Membership Function</th></tr><tr><td >Length</td><td >Short, Medium, Long</td><td >Trapezoidal</td></tr><tr><td >Width</td><td >Narrow, Medium, Wide</td><td >Trapezoidal</td></tr><tr><td >Area</td><td >Small, Moderate, Large</td><td >Gaussian</td></tr><tr><td >Aspect Ratio</td><td >Low, Medium, High</td><td >Gaussian</td></tr></table>
            </table-wrap>
          
          <p>The trapezoidal membership function is defined as:</p><p style="text-align: center"><inline-formula>
  <mml:math id="myounrtsv8">
    <mml:mi>μ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mrow>
      <mml:mo>{</mml:mo>
      <mml:mo fence="true"/>
      <mml:mtable columnalign="left left" columnspacing="1em" rowspacing=".2em">
        <mml:mtr>
          <mml:mtd>
            <mml:mn>0</mml:mn>
          </mml:mtd>
          <mml:mtd>
            <mml:mi>a</mml:mi>
            <mml:mi>m</mml:mi>
            <mml:mi>p</mml:mi>
            <mml:mi>x</mml:mi>
            <mml:mi>a</mml:mi>
            <mml:mo>;</mml:mo>
            <mml:mo>≤</mml:mo>
          </mml:mtd>
        </mml:mtr>
        <mml:mtr>
          <mml:mtd>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>x</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mo>−</mml:mo>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>b</mml:mi>
                <mml:mi>a</mml:mi>
                <mml:mo>−</mml:mo>
              </mml:mrow>
            </mml:mfrac>
          </mml:mtd>
          <mml:mtd>
            <mml:mi>a</mml:mi>
            <mml:mi>m</mml:mi>
            <mml:mi>p</mml:mi>
            <mml:mi>a</mml:mi>
            <mml:mi>x</mml:mi>
            <mml:mi>b</mml:mi>
            <mml:mo>;</mml:mo>
            <mml:mo>&amp;lt;</mml:mo>
            <mml:mo>≤</mml:mo>
          </mml:mtd>
        </mml:mtr>
        <mml:mtr>
          <mml:mtd>
            <mml:mn>1</mml:mn>
          </mml:mtd>
          <mml:mtd>
            <mml:mi>a</mml:mi>
            <mml:mi>m</mml:mi>
            <mml:mi>p</mml:mi>
            <mml:mi>b</mml:mi>
            <mml:mi>x</mml:mi>
            <mml:mi>c</mml:mi>
            <mml:mo>;</mml:mo>
            <mml:mo>&amp;lt;</mml:mo>
            <mml:mo>≤</mml:mo>
          </mml:mtd>
        </mml:mtr>
        <mml:mtr>
          <mml:mtd>
            <mml:mfrac>
              <mml:mrow>
                <mml:mi>d</mml:mi>
                <mml:mi>x</mml:mi>
                <mml:mo>−</mml:mo>
              </mml:mrow>
              <mml:mrow>
                <mml:mi>d</mml:mi>
                <mml:mi>c</mml:mi>
                <mml:mo>−</mml:mo>
              </mml:mrow>
            </mml:mfrac>
          </mml:mtd>
          <mml:mtd>
            <mml:mi>a</mml:mi>
            <mml:mi>m</mml:mi>
            <mml:mi>p</mml:mi>
            <mml:mi>c</mml:mi>
            <mml:mi>x</mml:mi>
            <mml:mi>d</mml:mi>
            <mml:mo>;</mml:mo>
            <mml:mo>&amp;lt;</mml:mo>
            <mml:mo>≤</mml:mo>
          </mml:mtd>
        </mml:mtr>
        <mml:mtr>
          <mml:mtd>
            <mml:mn>0</mml:mn>
          </mml:mtd>
          <mml:mtd>
            <mml:mi>a</mml:mi>
            <mml:mi>m</mml:mi>
            <mml:mi>p</mml:mi>
            <mml:mi>x</mml:mi>
            <mml:mi>d</mml:mi>
            <mml:mo>;</mml:mo>
            <mml:mo>&amp;gt;</mml:mo>
          </mml:mtd>
        </mml:mtr>
      </mml:mtable>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p>And the Gaussian membership function is given by:</p><p style="text-align: center"><inline-formula>
  <mml:math id="m4tz8hg7fv">
    <mml:mi>μ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:msup>
      <mml:mi>e</mml:mi>
      <mml:mrow>
        <mml:mo>−</mml:mo>
        <mml:mfrac>
          <mml:mrow>
            <mml:mo>(</mml:mo>
            <mml:mo>−</mml:mo>
            <mml:mi>x</mml:mi>
            <mml:mi>μ</mml:mi>
            <mml:msup>
              <mml:mo>)</mml:mo>
              <mml:mn>2</mml:mn>
            </mml:msup>
          </mml:mrow>
          <mml:mrow>
            <mml:mn>2</mml:mn>
            <mml:msup>
              <mml:mi>σ</mml:mi>
              <mml:mn>2</mml:mn>
            </mml:msup>
          </mml:mrow>
        </mml:mfrac>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula></p><p>These functions allow for smooth transitions between linguistic categories and enable robust handling of noisy or imprecise inputs.</p>
        </sec>
      
      
        <sec>
          
            <title>3.5. Fuzzy rule base</title>
          
          <p>A comprehensive rule base was developed to cover a wider range of vehicle configurations. Each rule uses combinations of input linguistic terms to infer the vehicle class. Below is an extended set of representative rules:</p><p>Rule 1: If <italic>L</italic> is <italic>Long</italic> AND <italic>W</italic> is <italic>Wide</italic> AND <italic>A</italic> is <italic>Large</italic> AND <italic>AR</italic> is <italic>Low</italic> THEN Vehicle is HTV.</p><p>Rule 2: If <italic>L</italic> is <italic>Medium</italic> AND <italic>W</italic> is <italic>Wide</italic> AND <italic>A</italic> is <italic>Moderate</italic> AND <italic>AR</italic> is <italic>Medium</italic> THEN Vehicle is LTV.</p><p>Rule 3: If <italic>L</italic> is <italic>Short</italic> AND <italic>W</italic> is <italic>Narrow</italic> AND <italic>A</italic> is <italic>Small</italic> AND <italic>AR</italic> is <italic>High</italic> THEN Vehicle is Two-Wheeler.</p><p>Rule 4: If <italic>L</italic> is <italic>Medium</italic> AND <italic>W</italic> is <italic>Medium</italic> AND <italic>A</italic> is <italic>Moderate</italic> AND <italic>AR</italic> is <italic>Medium</italic> THEN Vehicle is Car.</p><p>Rule 5: If <italic>L</italic> is <italic>Long</italic> AND <italic>W</italic> is <italic>Medium</italic> AND <italic>A</italic> is <italic>Large</italic> AND <italic>AR</italic> is <italic>Low</italic> THEN Vehicle is Mini-Bus.</p><p>This expanded rule base ensures coverage across a broad range of real-world vehicle geometries and enhances the classification accuracy of the fuzzy inference system. All rules were defined using expert knowledge and validated against annotated samples from the dataset.</p>
        </sec>
      
      
        <sec>
          
            <title>3.6. Rule firing strength</title>
          
          <p>In fuzzy logic, rule firing strength represents the degree to which a rule’s conditions are satisfied. For each fuzzy rule, the firing strength is calculated by taking the minimum of the membership values of the involved fuzzy sets. This represents the idea that a rule can only be as strong as the weakest condition it is based on.</p><p>For Rule 1, the firing strength <inline-formula>
  <mml:math id="m6g5sy8wpm">
    <mml:mi>α</mml:mi>
  </mml:math>
</inline-formula> is computed as follows:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mkmoj4d9a4">
    <mml:mi>α</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mo>min</mml:mo>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>μ</mml:mi>
        <mml:mrow>
          <mml:mtext>Long</mml:mtext>
        </mml:mrow>
      </mml:msub>
      <mml:msub>
        <mml:mi>μ</mml:mi>
        <mml:mrow>
          <mml:mtext>wide</mml:mtext>
        </mml:mrow>
      </mml:msub>
      <mml:msub>
        <mml:mi>μ</mml:mi>
        <mml:mrow>
          <mml:mtext>Large</mml:mtext>
        </mml:mrow>
      </mml:msub>
      <mml:msub>
        <mml:mi>μ</mml:mi>
        <mml:mrow>
          <mml:mtext>LowaR</mml:mtext>
        </mml:mrow>
      </mml:msub>
      <mml:mi>L</mml:mi>
      <mml:mi>W</mml:mi>
      <mml:mi>A</mml:mi>
      <mml:mi>A</mml:mi>
      <mml:mi>R</mml:mi>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p>where,</p><p><inline-formula>
  <mml:math id="m98qtky849">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:mtext>Long</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>L</mml:mi>
  </mml:math>
</inline-formula>: The membership value of the length feature $L<inline-formula>
  <mml:math id="mcuo57e9y6">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mo>“</mml:mo>
    <mml:mo>”</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>\mu_{\text {Wide}}(W)<inline-formula>
  <mml:math id="mmoa1xb9x9">
    <mml:mo>:</mml:mo>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>W<inline-formula>
  <mml:math id="m46p9v6vwv">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>W</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mo>“</mml:mo>
    <mml:mo>”</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>\mu_{\text {Large}}(A)<inline-formula>
  <mml:math id="mdkyybc3ru">
    <mml:mo>:</mml:mo>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>A<inline-formula>
  <mml:math id="mm18ih0ou1">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mo>“</mml:mo>
    <mml:mo>”</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>\mu_{\text {LowAR}}(A R)<inline-formula>
  <mml:math id="mvbfg0skj1">
    <mml:mo>:</mml:mo>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>A R<inline-formula>
  <mml:math id="m1ryr5t9rh">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mo>“</mml:mo>
    <mml:mo>”</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>\alpha$ reflects how well the given vehicle satisfies the conditions of Rule 1. A lower firing strength indicates that the vehicle does not satisfy the rule as strongly. This same calculation is repeated for other rules to assess their individual firing strengths.</p>
        </sec>
      
      
        <sec>
          
            <title>3.7. Defuzzification and decision</title>
          
          <p>Defuzzification is the process of converting the fuzzy output of the inference system into a single crisp value that can be used for decision-making. One of the most common methods for defuzzification is the “centroid method”(also known as the center of gravity method).</p><p>The centroid defuzzification method calculates the crisp output as the weighted average of the possible output values, where the weights are determined by the membership degrees of the fuzzy sets in the output domain.</p><p>The formula for centroid defuzzification is:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mv56qa6k0g">
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mrow>
        <mml:mtext>crisp </mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mo>=</mml:mo>
    <mml:mfrac>
      <mml:mrow>
        <mml:mo>∫</mml:mo>
        <mml:mo>⋅</mml:mo>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>x</mml:mi>
        <mml:mi>x</mml:mi>
        <mml:mi>d</mml:mi>
        <mml:mi>x</mml:mi>
        <mml:msub>
          <mml:mi>μ</mml:mi>
          <mml:mrow>
            <mml:mtext>Output </mml:mtext>
          </mml:mrow>
        </mml:msub>
      </mml:mrow>
      <mml:mrow>
        <mml:mo>∫</mml:mo>
        <mml:mo>(</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:msub>
          <mml:mi>μ</mml:mi>
          <mml:mrow>
            <mml:mtext>Output </mml:mtext>
          </mml:mrow>
        </mml:msub>
        <mml:mi>x</mml:mi>
        <mml:mi>d</mml:mi>
        <mml:mi>x</mml:mi>
      </mml:mrow>
    </mml:mfrac>
  </mml:math>
</inline-formula></p><p>where, $x<inline-formula>
  <mml:math id="m3upmx1ixg">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>\mu_{\text {Output}}(x)<inline-formula>
  <mml:math id="mthvr024m7">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>x<inline-formula>
  <mml:math id="my4fkvmycg">
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>y_{\text {crisp}}<inline-formula>
  <mml:math id="mjedlddn58">
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>O</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>y_{\text {crisp}}&gt;0.6<inline-formula>
  <mml:math id="m9vgetszdl">
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>H</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>V</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
  </mml:math>
</inline-formula>y_{\text {crisp}} \leq 0.6$, the vehicle is classified as a non-HTV or classified differently based on the system's logic.</p>
        </sec>
      
    </sec>
    <sec sec-type="discussion">
      <title>4. Discussion</title>
      <p>The proposed vehicle classification model demonstrates an effective approach for detecting and classifying HTVs in real-world traffic scenarios. By integrating geometric feature extraction with fuzzy logic, the model efficiently handles challenges such as vehicle occlusions, congestion, and varying vehicle types. The dataset used in this study is the Stanford Cars Dataset, a publicly available benchmark that contains 16,185 images of 196 classes of vehicles, including both LTVs and HTVs. For this study, a relevant subset comprising 500 images was selected and labeled accordingly. All images were resized to 255×255 pixels to maintain uniformity and enable efficient processing. The use of a standard public dataset supports reproducibility and benchmarking of the proposed model. The MATLAB functions employed in the development of the model are tailored for image processing tasks, utilizing built-in tools such as region props for feature extraction and custom fuzzy inference systems for decision-making. The MATLAB environment offers a flexible platform for implementing and fine-tuning the model, providing an intuitive interface for debugging and optimization. The code utilizes functions for bounding box extraction, thresholding, and fuzzy logic evaluation, creating a seamless integration of geometric and fuzzy approaches.</p><p>In the proposed model, several key parameters are defined for the fuzzy membership functions used to classify vehicle features such as Length (L), Width (W), Area (A), and Aspect Ratio (AR). These parameters were selected based on real-world traffic data to ensure accurate classification of HTVs. For Length (L), the trapezoidal membership function is defined with parameters: $a<inline-formula>
  <mml:math id="mldsmwezka">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>4.5</mml:mn>
  </mml:math>
</inline-formula>b<inline-formula>
  <mml:math id="mlexlr0g1g">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>6.5</mml:mn>
  </mml:math>
</inline-formula>c<inline-formula>
  <mml:math id="mlbguy5rm6">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>10.5</mml:mn>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>d<inline-formula>
  <mml:math id="mzdif4abv3">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>“</mml:mo>
    <mml:mo>”</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>“</mml:mo>
    <mml:mo>”</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>“</mml:mo>
    <mml:mo>”</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>12.5</mml:mn>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>W</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>W</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
  </mml:math>
</inline-formula>e<inline-formula>
  <mml:math id="mx5ygchp5k">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>2</mml:mn>
  </mml:math>
</inline-formula>f<inline-formula>
  <mml:math id="m22rf3sa0k">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>4</mml:mn>
  </mml:math>
</inline-formula>g<inline-formula>
  <mml:math id="m2n8gsuwbo">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>6</mml:mn>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>h<inline-formula>
  <mml:math id="mpj0yhzno8">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>“</mml:mo>
    <mml:mo>”</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>“</mml:mo>
    <mml:mo>”</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>“</mml:mo>
    <mml:mo>”</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>8</mml:mn>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>N</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>W</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>\mu_{\text {Small}}=1000, \mu_{\text {Large}}=4000, \sigma_{\text {Small}}<inline-formula>
  <mml:math id="mxukah77g8">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>200</mml:mn>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\sigma_{\text {Large}}<inline-formula>
  <mml:math id="mw7wolfloa">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>“</mml:mo>
    <mml:mo>”</mml:mo>
    <mml:mo>“</mml:mo>
    <mml:mo>”</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>500</mml:mn>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>R</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>G</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>\mu_{\text {Low}}<inline-formula>
  <mml:math id="mmdovly7uc">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula>\mu_{\text {High}}<inline-formula>
  <mml:math id="mnts3lv0mv">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>3</mml:mn>
  </mml:math>
</inline-formula>\sigma_{\text {Low}}<inline-formula>
  <mml:math id="m9b5mzykep">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>0.5</mml:mn>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\sigma_{\text {High}}<inline-formula>
  <mml:math id="mflvc088dq">
    <mml:mo>=</mml:mo>
    <mml:mo>“</mml:mo>
    <mml:mo>”</mml:mo>
    <mml:mo>“</mml:mo>
    <mml:mo>”</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>1.0</mml:mn>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>H</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>H</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>V</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
  </mml:math>
</inline-formula>y_{\text {crisp}}$, is greater than 0.6 . These parameter values ensure that the model remains robust and accurate across various realworld traffic conditions.</p><p>The MATLAB code for the proposed vehicle classification model will be made available for research purposes. Interested researchers can request the code by sending an email to the corresponding author. This open access to the code is intended to facilitate further research and allow other practitioners to apply and build upon the proposed methodology for vehicle classification in intelligent traffic monitoring systems.</p><p> <xref ref-type="fig" rid="fig_3">Figure 3</xref> illustrates the step-by-step process of the proposed vehicle classification model, which operates in two main stages: feature extraction and classification. In the feature extraction stage, geometric attributes—including length, width, area, and aspect ratio—are obtained from each vehicle’s bounding box. These features serve as key indicators for distinguishing HTVs from other vehicle types. By focusing on fundamental geometric properties, the model maintains high accuracy even when vehicles are partially occluded or overlapping.</p>
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>Overview of the proposed HTV detection framework</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_67oIkrnlLUTk0G9Z.png"/>
        </fig>
      
      <p>After feature extraction, threshold-based classification is performed, followed by entropy and fuzzy logic-based reasoning to account for image variations and enhance detection robustness. The final output identifies the detected HTVs within the input traffic scene, demonstrating the model’s effectiveness under diverse and challenging conditions.</p><p>In the second stage, the model applies a threshold-based classification system. The predefined thresholds are set based on empirical observations and statistical analysis of HTV dimensions, ensuring that only vehicles with characteristics above these limits are classified as HTVs. This step effectively filters out non-HTVs, which may include smaller cars, motorcycles, or light trucks that fall outside the size range typically associated with heavy vehicles. By leveraging these geometric thresholds, the system is able to make rapid and accurate decisions regarding vehicle classification.</p><p>To address the challenges posed by uncertain or noisy data, fuzzy logic is incorporated into the model. Traditional threshold-based methods may struggle when dealing with imprecise measurements, such as those caused by low-quality images, occlusions, or ambiguous vehicle shapes. Fuzzy logic provides a mechanism to handle these uncertainties, allowing the model to make more flexible and adaptive decisions. Specifically, fuzzy inference is used to assess the degree to which a vehicle’s features match the expected characteristics of an HTV, enabling the system to classify vehicles even in edge cases where strict geometric thresholds might not be met.</p><p>This combination of crisp thresholding and fuzzy inference enhances the robustness of the detection framework. The model becomes less sensitive to noise, occlusions, and other variations in traffic conditions, which are common in real-world environments. By integrating these two approaches, the model achieves a higher level of flexibility, allowing it to handle a broader range of traffic scenarios while maintaining high classification accuracy.</p><p> <xref ref-type="fig" rid="fig_4">Figure 4</xref> demonstrates the performance of the proposed vehicle classification model in detecting and classifying HTVs across a variety of real-world traffic scenarios. The top row shows the original traffic images captured under diverse conditions, including congestion, occlusion, and the presence of multiple vehicle types. These challenging environments often involve overlapping vehicles, obstructed views, and varying angles of observation.</p><p>The bottom row presents the detection outputs, where accurately identified HTVs are highlighted with red bounding boxes. The results illustrate the model’s robustness and reliability across different traffic conditions, confirming its effectiveness in complex, real-world environments.</p>
      
        <fig id="fig_4">
          <label>Figure 4</label>
          <caption>
            <title>HTV detection results in real-world traffic scenes</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/4/img_Dp33O2OlKvkhQosy.png"/>
        </fig>
      
      <p>Overall, the proposed vehicle classification model not only demonstrates superior performance in identifying HTVs but also showcases its adaptability in complex, dynamic traffic environments. The fusion of geometric feature extraction, threshold-based classification, and fuzzy logic results in a robust system capable of providing reliable vehicle classification even under challenging conditions, making it highly suitable for intelligent traffic monitoring and management systems. <xref ref-type="table" rid="table_2">Table 2</xref> presents a comprehensive evaluation of the proposed model's performance in detecting HTVs under varying lighting and angle conditions. The performance metrics are based on standard image quality assessment techniques as described by Bovik [<xref ref-type="bibr" rid="ref_23">23</xref>]. The results demonstrate that the model maintains high accuracy across diverse scenarios, highlighting its robustness and adaptability.</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>Performance of proposed model under varying light and angle conditions</title>
          </caption>
          <table><tbody><tr><th colspan="1" rowspan="1"><p>Condition</p></th><th colspan="1" rowspan="1"><p>Precision (%)</p></th><th colspan="1" rowspan="1"><p>Recall (%)</p></th><th colspan="1" rowspan="1"><p>F1-Score (%)</p></th><th colspan="1" rowspan="1"><p>IoU</p></th></tr><tr><td colspan="1" rowspan="1"><p>Bright Light (Day)</p></td><td colspan="1" rowspan="1"><p>92.5</p></td><td colspan="1" rowspan="1"><p>91.1</p></td><td colspan="1" rowspan="1"><p>91.8</p></td><td colspan="1" rowspan="1"><p>0.80</p></td></tr><tr><td colspan="1" rowspan="1"><p>Low Light (Night)</p></td><td colspan="1" rowspan="1"><p>88.6</p></td><td colspan="1" rowspan="1"><p>85.3</p></td><td colspan="1" rowspan="1"><p>86.9</p></td><td colspan="1" rowspan="1"><p>0.74</p></td></tr><tr><td colspan="1" rowspan="1"><p>Shadows/Overcast</p></td><td colspan="1" rowspan="1"><p>89.7</p></td><td colspan="1" rowspan="1"><p>86.8</p></td><td colspan="1" rowspan="1"><p>88.2</p></td><td colspan="1" rowspan="1"><p>0.76</p></td></tr><tr><td colspan="1" rowspan="1"><p>Front View (0°)</p></td><td colspan="1" rowspan="1"><p>93.1</p></td><td colspan="1" rowspan="1"><p>91.0</p></td><td colspan="1" rowspan="1"><p>92.0</p></td><td colspan="1" rowspan="1"><p>0.81</p></td></tr><tr><td colspan="1" rowspan="1"><p>Side View (90°)</p></td><td colspan="1" rowspan="1"><p>90.2</p></td><td colspan="1" rowspan="1"><p>88.5</p></td><td colspan="1" rowspan="1"><p>89.3</p></td><td colspan="1" rowspan="1"><p>0.77</p></td></tr><tr><td colspan="1" rowspan="1"><p>Diagonal View (45°/135°)</p></td><td colspan="1" rowspan="1"><p>87.9</p></td><td colspan="1" rowspan="1"><p>85.6</p></td><td colspan="1" rowspan="1"><p>86.7</p></td><td colspan="1" rowspan="1"><p>0.73</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>Under bright daylight conditions, the model achieves the highest precision of 92.5%, with a recall of 91.1%, an F1-score of 91.8%, and an Intersection over Union (IoU) score of 0.80, indicating strong detection capability in well-lit environments. Even in low-light conditions, such as nighttime, the model sustains reasonable performance with an F1-score of 86.9% and an IoU of 0.74, reflecting its ability to handle challenging visual settings.</p><p>In scenarios involving shadows or overcast lighting, the model performs reliably, achieving an F1-score of 88.2% and IoU of 0.76, demonstrating effective generalization. Regarding vehicle orientation, the model performs best when vehicles are in the front view (0°), yielding an F1-score of 92.0% and an IoU of 0.81. However, performance slightly decreases for side views (90°) and diagonal views (45°/135°), with F1-scores of 89.3% and 86.7%, and IoU values of 0.77 and 0.73, respectively.</p><p>This decline can be attributed to the geometric distortion and reduced visibility of key distinguishing features (such as frontal grilles, headlights, and bumper structures) when vehicles are viewed from non-frontal angles. These features play a significant role in fuzzy rule-based classification; thus, their partial occlusion leads to a slight drop in detection accuracy. Moreover, some false detections were observed, particularly with large SUVs occasionally being misclassified as HTVs due to similar dimensional features.</p>
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>The proposed fuzzy logic-based model provides an efficient and interpretable approach for the classification of HTVs using key geometric features—Length, Width, Area, and Aspect Ratio—extracted from traffic surveillance images. The use of trapezoidal and Gaussian membership functions enables the system to manage uncertainties and gradual transitions between vehicle categories. Implemented in MATLAB R2015a and tested on 255×255-pixel images, the model achieves robust performance in distinguishing HTVs from other vehicle types, making it suitable for real-time intelligent transport systems and traffic analysis.</p><p>Despite its effectiveness, the model has a couple of minor limitations. First, it shows reduced accuracy in detecting HTVs that appear far from the camera, due to smaller and less distinct bounding boxes which affect the membership strength. Second, the model relies solely on geometric features, which may limit its performance in crowded scenes where vehicles are partially occluded or overlapping.</p><p>To address these limitations, future work will aim to incorporate distance-aware normalization techniques or depth estimation to better handle faraway vehicles. Moreover, integrating additional features such as motion patterns, texture information, or deep feature embeddings can enhance the model's capability in complex environments with occlusions or dense traffic. These improvements will help increase the model's robustness while preserving its interpretability and computational efficiency.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The author declares that there are no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>654-671</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Shi</surname>
              <given-names>Ming</given-names>
            </name>
            <name>
              <surname>Hussain</surname>
              <given-names>Ibrar</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3934/math.2025029</pub-id>
          <article-title>Improved region-based active contour segmentation through divergence and convolution techniques</article-title>
          <source>AIMS Math.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>57</volume>
          <page-range>11</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Archana</surname>
              <given-names>R</given-names>
            </name>
            <name>
              <surname>Jeevaraj</surname>
              <given-names>P S Eliahim</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10462-023-10631-z</pub-id>
          <article-title>Deep learning models for digital image processing: A review</article-title>
          <source>Artif. Intell. Rev.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>708-725</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hussain</surname>
              <given-names>Ibrar</given-names>
            </name>
            <name>
              <surname>Muhammad</surname>
              <given-names>Junaid</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3934/ipi.2022074</pub-id>
          <article-title>Efficient convex region-based segmentation for noising and inhomogeneous patterns</article-title>
          <source>Inverse Probl. Imaging</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>20995-21031</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jabbar</surname>
              <given-names>Reham</given-names>
            </name>
            <name>
              <surname>Dhib</surname>
              <given-names>Emna</given-names>
            </name>
            <name>
              <surname>Said</surname>
              <given-names>Ahmad Bou</given-names>
            </name>
            <name>
              <surname>Krichen</surname>
              <given-names>Moez</given-names>
            </name>
            <name>
              <surname>Fetais</surname>
              <given-names>Noora</given-names>
            </name>
            <name>
              <surname>Zaidan</surname>
              <given-names>Eiman</given-names>
            </name>
            <name>
              <surname>Barkaoui</surname>
              <given-names>Kamel</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2022.3149958</pub-id>
          <article-title>Blockchain technology for intelligent transportation systems: A systematic literature review</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>33</volume>
          <page-range>e4427</page-range>
          <issue>4</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yuan</surname>
              <given-names>Tao</given-names>
            </name>
            <name>
              <surname>Rocha Neto</surname>
              <given-names>Wilson</given-names>
            </name>
            <name>
              <surname>Rothenberg</surname>
              <given-names>Christian Esteve</given-names>
            </name>
            <name>
              <surname>Obraczka</surname>
              <given-names>Katia</given-names>
            </name>
            <name>
              <surname>Barakat</surname>
              <given-names>Chadi</given-names>
            </name>
            <name>
              <surname>Turletti</surname>
              <given-names>Thierry</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1002/ett.4427</pub-id>
          <article-title>Machine learning for next-generation intelligent transportation systems: A survey</article-title>
          <source>Trans. Emerg. Telecommun. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>9156</page-range>
          <issue>18</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Behrooz</surname>
              <given-names>Hojat</given-names>
            </name>
            <name>
              <surname>Hayeri</surname>
              <given-names>Yeganeh M</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app12189156</pub-id>
          <article-title>Machine learning applications in surface transportation systems: A literature review</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>515</volume>
          <page-range>01015</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Fokina</surname>
              <given-names>Olga</given-names>
            </name>
            <name>
              <surname>Mottaeva</surname>
              <given-names>Angela</given-names>
            </name>
            <name>
              <surname>Mottaeva</surname>
              <given-names>Asiiat</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1051/e3sconf/202451501015</pub-id>
          <article-title>Transport infrastructure in the system of environmental projects for sustainable development of the region</article-title>
          <source>E3S Web Conf.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>13754</page-range>
          <issue>18</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Milewicz</surname>
              <given-names>Julia</given-names>
            </name>
            <name>
              <surname>Mokrzan</surname>
              <given-names>Daniel</given-names>
            </name>
            <name>
              <surname>Szymański</surname>
              <given-names>Grzegorz M</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/su151813754</pub-id>
          <article-title>Environmental impact evaluation as a key element in ensuring sustainable development of rail transport</article-title>
          <source>Sustainability</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>15372</page-range>
          <issue>21</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hmamed</surname>
              <given-names>Hala</given-names>
            </name>
            <name>
              <surname>Benghabrit</surname>
              <given-names>Asmaa</given-names>
            </name>
            <name>
              <surname>Cherrafi</surname>
              <given-names>Anass</given-names>
            </name>
            <name>
              <surname>Hamani</surname>
              <given-names>Nadia</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/su152115372</pub-id>
          <article-title>Achieving a sustainable transportation system via economic, environmental, and social optimization: A comprehensive AHP-DEA approach from the waste transportation sector</article-title>
          <source>Sustainability</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>100217</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kwilinski</surname>
              <given-names>Aleksy</given-names>
            </name>
            <name>
              <surname>Lyulyov</surname>
              <given-names>Oleksii</given-names>
            </name>
            <name>
              <surname>Pimonenko</surname>
              <given-names>Tetyana</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.joitmc.2024.100217</pub-id>
          <article-title>Reducing transport sector CO2 emissions patterns: Environmental technologies and renewable energy</article-title>
          <source>J. Open Innov. Technol. Mark. Complex.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>9163</page-range>
          <issue>15</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Rajput</surname>
              <given-names>Saroj Kumar</given-names>
            </name>
            <name>
              <surname>Patni</surname>
              <given-names>Jagdish Chand</given-names>
            </name>
            <name>
              <surname>Alshamrani</surname>
              <given-names>Sami S</given-names>
            </name>
            <name>
              <surname>others</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/su14159163</pub-id>
          <article-title>Automatic vehicle identification and classification model using the YOLOv3 algorithm for a toll management system</article-title>
          <source>Sustainability</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>81</volume>
          <page-range>34893-34917</page-range>
          <issue>24</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sharma</surname>
              <given-names>Poonam</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>Akansha</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>Krishna Kant</given-names>
            </name>
            <name>
              <surname>Dhull</surname>
              <given-names>Anuradha</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11042-020-10366-x</pub-id>
          <article-title>Vehicle identification using modified region based convolution network for intelligent transportation system</article-title>
          <source>Multimed. Tools Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>5</volume>
          <page-range>323-342</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tippannavar</surname>
              <given-names>Sanjay S</given-names>
            </name>
            <name>
              <surname>SD</surname>
              <given-names>Yashwanth</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.36548/jtcsst.2023.3.007</pub-id>
          <article-title>Real-time vehicle identification for improving the traffic management system—A review</article-title>
          <source>J. Trends Comput. Sci. Smart Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>201</volume>
          <page-range>110667</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ni</surname>
              <given-names>Fuchun</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Jun</given-names>
            </name>
            <name>
              <surname>Taciroglu</surname>
              <given-names>Ertugrul</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.ymssp.2023.110667</pub-id>
          <article-title>Development of a moving vehicle identification framework using structural vibration response and deep learning algorithms</article-title>
          <source>Mech. Syst. Signal Process.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <page-range>3139-3147</page-range>
          <issue>37</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Nasr</surname>
              <given-names>Omar</given-names>
            </name>
            <name>
              <surname>Alsisi</surname>
              <given-names>Elazab</given-names>
            </name>
            <name>
              <surname>Mohiuddin</surname>
              <given-names>Khaja</given-names>
            </name>
            <name>
              <surname>Alqahtani</surname>
              <given-names>Areej</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.17485/IJST/v16i37.1389</pub-id>
          <article-title>Designing an intelligent QR code-based mobile application: A novel approach for vehicle identification and authentication</article-title>
          <source>Indian J. Sci. Technol.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>2</volume>
          <page-range>2760</page-range>
          <issue>2</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Huai</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Meng</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.54517/mss.v2i2.2760</pub-id>
          <article-title>Research on the detection and recognition system of target vehicles based on fusion algorithm</article-title>
          <source>Math. Syst. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>1514</page-range>
          <issue>10</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zohaib</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Asim</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>ELAffendi</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/math12101514</pub-id>
          <article-title>Enhancing emergency vehicle detection: A deep learning approach with multimodal fusion</article-title>
          <source>Mathematics</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>397</page-range>
          <issue>3</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Córdova</surname>
              <given-names>A.H.F.</given-names>
            </name>
            <name>
              <surname>Olazagoitia</surname>
              <given-names>J.L.</given-names>
            </name>
            <name>
              <surname>Gijón-Rivera</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/math12030397</pub-id>
          <article-title>Non-invasive identification of vehicle suspension parameters: A methodology based on synthetic data analysis</article-title>
          <source>Mathematics</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>14389</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Moussaoui</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Akkad</surname>
              <given-names>N.E.</given-names>
            </name>
            <name>
              <surname>Benslimane</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>El-Shafai</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Baihan</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Hewage</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Rathore</surname>
              <given-names>R.S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-024-65272-1</pub-id>
          <article-title>Enhancing automated vehicle identification by integrating YOLOv8 and OCR techniques for high-precision license plate detection and recognition</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>233</volume>
          <page-range>793-800</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kanagamalliga</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Kovalan</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Kiran</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Rajalingam</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.procs.2024.03.268</pub-id>
          <article-title>Traffic management through cutting-edge vehicle detection, recognition, and tracking innovations</article-title>
          <source>Procedia Comput. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1–4</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Islam</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Ray</surname>
              <given-names>S. K.</given-names>
            </name>
            <name>
              <surname>Hossain</surname>
              <given-names>M. A.</given-names>
            </name>
            <name>
              <surname>M. A. R. Hasan</surname>
            </name>
            <name>
              <surname>Shammo</surname>
              <given-names>M. B. A. Z.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICRPSET64863.2024.10955913</pub-id>
          <article-title>Vehicle classification and detection using YOLOv8: A study on highway traffic analysis</article-title>
          <source>2024 International Conference on Recent Progresses in Science, Engineering and Technology (ICRPSET), Rajshahi, Bangladesh</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>El Mallahi</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Riffi</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>H. Tairi</surname>
            </name>
            <name>
              <surname>Mahraz</surname>
              <given-names>M. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.21203/rs.3.rs-4987140/v1</pub-id>
          <article-title>Enhancing traffic safety with advanced machine learning techniques and intelligent identification</article-title>
          <source>Res. Sq.</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Bovik</surname>
              <given-names>A. C.</given-names>
            </name>
          </person-group>
          <source>The Essential Guide to Image Processing</source>
          <publisher-name>Academic Press</publisher-name>
          <year>2009</year>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>