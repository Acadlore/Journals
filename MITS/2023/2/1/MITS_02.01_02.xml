<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">MITS</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Mechatronics and Intelligent Transportation Systems</journal-title>
        <abbrev-journal-title abbrev-type="issn">Mechatron. Intell Transp. Syst.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">MITS</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-0218</issn>
      <issn publication-format="print">2958-020X</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-oFGX5N19nEVEAD_ihsz0Y78O3F6mrzCV</article-id>
      <article-id pub-id-type="doi">10.56578/mits020102</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Development and Verification of an Autonomous and Controllable Mobile Robot Platform</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1,2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5039-6192</contrib-id>
          <name>
            <surname>Jiang</surname>
            <given-names>Tianyu</given-names>
          </name>
          <email>jiangtianyu218@163.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1544-2552</contrib-id>
          <name>
            <surname>Zhang</surname>
            <given-names>Shaolin</given-names>
          </name>
          <email>zhangshaolin2015@ia.ac.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0003-3172-3167</contrib-id>
          <name>
            <surname>Wang</surname>
            <given-names>Rui</given-names>
          </name>
          <email>rwang5212@ia.ac.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-1390-9219</contrib-id>
          <name>
            <surname>Wang</surname>
            <given-names>Shuo</given-names>
          </name>
          <email>shuo.wang@ia.ac.cn</email>
        </contrib>
        <aff id="aff_1">State Key Laboratory of Multimodal Artificial Intelligence Systems, Institute of Automation, Chinese Academy of Sciences, 100190 Beijing, China</aff>
        <aff id="aff_2">Institute of Advanced Engineers, University of Science and Technology Beijing, 100083 Beijing, China</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>19</day>
        <month>03</month>
        <year>2023</year>
      </pub-date>
      <volume>2</volume>
      <issue>1</issue>
      <fpage>11</fpage>
      <lpage>19</lpage>
      <page-range>11-19</page-range>
      <history>
        <date date-type="received">
          <day>09</day>
          <month>01</month>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <day>04</day>
          <month>03</month>
          <year>2023</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2023 by the author(s)</copyright-statement>
        <copyright-year>2023</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>In this paper, we design a mobile robot platform, which employs a fully autonomous mechanical structure and electrical control system. Two driving wheels realize flexible steering movement with four universal wheels. A variety of sensors are built on the mobile robot platform, including the Inertial Measurement Unit (IMU) used to establish the inertial navigation coordinate system and the Velodyne’s Puck lidar sensor (VLP-16) used to obtain the three-dimensional (3D) point cloud information of the environment. Then, we build a software control architecture based on the Robot Operating System (ROS), using multi-node communication to perform positioning, environment perception, dynamic obstacle avoidance, path planning and motion control. Furthermore, a method of actively exploring the environment and constructing a map is proposed, using multi-path evaluation for real-time path planning and obstacle avoidance. In the end, we conduct autonomous exploration experiments to verify the performance of the designed mobile robot platform in indoor multi-obstacle scenes.</p></abstract>
      <kwd-group>
        <kwd>Mobile robot platform</kwd>
        <kwd>Autonomous exploration</kwd>
        <kwd>Relative positioning</kwd>
        <kwd>Obstacle avoidance</kwd>
        <kwd>Map construction</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="4"/>
        <fig-count count="7"/>
        <table-count count="2"/>
        <ref-count count="20"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Mobile robot is an important branch of robotics field, which has the characteristics of simple mechanical structure, strong scalability, stable motion control, and easy operation. It can replace humans to perform specific tasks in complex and harsh environments. Mobile robots are widely used in industrial production, home service and other scenarios, such as building patrol, ground cleaning, warehousing, and freight transportation. Autonomous positioning and navigation, path planning, and map construction constitute the core of the mobile robot system, which is the premise for mobile robots to realize intelligence and autonomy [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>].</p><p>So far, many researchers and institutions have built experimental platforms for mobile robots. National Key Lab of Intelligent Technology and System in Tsinghua University developed an effective and robust high-speed system called THMR-V (Tsinghua Mobile Robot V), which realized automatic control by analyzing lane lines in grayscale images [<xref ref-type="bibr" rid="ref_4">4</xref>]. Nguyen et al. [<xref ref-type="bibr" rid="ref_5">5</xref>] made Segway Robotic Mobility Platform, which was based on the self-balancing Segway Human Transporter (HT) and capable of about 13km/h indoor. Amsters and Slaets [<xref ref-type="bibr" rid="ref_6">6</xref>] built a robotics education platform using turtlebot3, which realized autonomous navigation by equipped with a 360° laser scanner. Oltean [<xref ref-type="bibr" rid="ref_7">7</xref>] made a mobile robot platform commended by Raspberry Pi and Arduino Uno interfaces, which had the ability to move as line follower robot with mapping, navigation, and obstacle avoidance features. Wu et al. [<xref ref-type="bibr" rid="ref_8">8</xref>] designed an omnidirectional mobile robot platform named Savvy, which was equipped with a variety of unified I/O interfaces and used the ROS framework. Existing mobile robot platforms have poor scalability, which are difficult to meet the various interfaces of sensors on the market. In addition, they are often oriented to fixed scenarios without the advantage of expanding other functions.</p><p>Autonomous exploration is an important application of robotics, where robot platforms explore unknown environments actively. Batinovic et al. [<xref ref-type="bibr" rid="ref_9">9</xref>] proposed a planner based on the detection of a boundary between the explored and unknown part of the environment, which used the properties of the Octree environment representation to cluster the frontier points. Bircher et al. [<xref ref-type="bibr" rid="ref_10">10</xref>] employed a receding horizon “next-best-vie” scheme in the proposed planner, where it found the best branch in an online computed random tree determined by the amount of unmapped space that can be explored and only the first edge of this branch was executed at every planning step. Selin et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] used Frontier Exploration planning (FEP) as a global exploration planner to explore a large environment consisting of separate regions with ease and applied Receding Horizon Next-Best-View planning (RH-NBVP) for local exploration to explore individual regions. Dai et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] presented an exploration strategy for the reduction of map entropy regarding occupancy probabilities, which exploited the implicit grouping of frontier voxels in the underlying octree map representation. Chen et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] proposed an approach, which combined graph neural networks and deep reinforcement learning, enabling decision-making over graphs containing exploration information to predict a robot’s optimal sensing action in belief space. The above algorithms have high computational complexity and rarely build 3D maps of the indoor environment.</p><p>Mobile robot positioning refers to the judgment of its own pose and the position relative to the world coordinate system when the robot is running in the environment. Accurate positioning helps the robot to better perceive the surrounding environment information. Zheng et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] used nonlinear graph optimization to achieve precise position estimation of the robot by combining lidar, IMU and Global Positioning System (GPS). Based on the Markov positioning algorithm, Wu and David [<xref ref-type="bibr" rid="ref_15">15</xref>] used sensor data and kinematic models to complete the positioning of mobile robots. Li et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] proposed a fusion positioning algorithm based on ultra-wideband and lidar, which used a particle filter algorithm to perform complementary fusion calculations on the positioning data of the sensor, achieving good accuracy. At present, most researchers mainly use single-line lidar and cameras for environment perception and robot localization, while there are relatively few research works on multi-line lidar for active exploration in indoor environments.</p><p>The main contributions of this paper are summarized as follows:</p><p>(1) A mobile robot platform is designed with an autonomously controllable mechanical structure and control system, which is made of iron. The differential speed is controlled by the two driving wheels in the middle, and four universal wheels around are used as supporting weights. We write a set of basic controller algorithms that can achieve more flexible steering movements than other mobile robot platforms.</p><p>(2) A map construction method for active exploration of the environment is proposed. Under the premise that the environment is unknown, the robot reaches the artificially set global target point through the multi-path evaluation system. The environment map is constructed from the 3D point cloud data collected during the movement. Effective environment exploration and autonomous obstacle avoidance can be achieved without prior maps.</p><p>(3) Based on the designed mobile robot platform, an active exploration physical experiment is carried out to verify the stability of the mechanical structure and control system.</p><p>In the rest of this paper, the second chapter introduces the mechanical structure and electrical control system design of the mobile robot platform, the third chapter introduces the software architecture design of the mobile robot platform, and the fourth chapter explains the map construction method for the active exploration of the environment. The fifth chapter shows the active exploration of physical experiments, and the sixth chapter summarizes this paper and proposes prospects.</p>
    </sec>
    <sec sec-type="">
      <title>2. System design of mobile robot platform</title>
      
        <sec>
          
            <title>2.1. Mechanical design</title>
          
          <p>The overall structure of the designed mobile robot platform is shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>. The platform consists of two parts, the mobile chassis, and the bracket. The VLP-16 is installed on top of the bracket with the ability to detect a wide range of surrounding environmental areas for obstacle detection and map building. Laptop is placed in the hollow position in the middle of the bracket, which has a good fixing effect and is convenient for connecting various devices. The four corners of the mobile chassis are equipped with universal wheels, which play a supporting role and carry large loads. The two differential drive wheels are installed in the middle of the mobile chassis and pressed against the ground by springs, which enhances the adaptability to the ground and makes the driving more stable. At the same time, the spring structure effectively avoids idling. The mobile chassis shell is a 3D printed part that houses the electrical equipment. The fan is used for ventilation and heat dissipation. Some key parameters of the mobile robot platform are shown in <xref ref-type="table" rid="table_1">Table 1</xref>.</p><p>The mobile robot platform has a stronger load bearing capacity. The combination of universal wheels and driving wheels provides the possibility for flexible steering functions. In addition, the setting of the emergency stop switch fully considers the safety factor, allowing people to cut off the power supply in time. Easy expansion of peripherals is another important advantage of the mobile robot platform. It provides a wealth of commonly used interfaces such as Universal Serial Bus (USB), Controller Area Network (CAN), Transistor-transistor logic (TTL), Recommended Standard 232 (RS232), Recommended Standard 485 (RS485) and Ethernet, which are used to add other peripheral devices that may be needed.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>
                <title>Structural design of mobile robot platform</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/2/img_A0x1XQlX_68cAJWW.png"/>
            </fig>
          
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Key parameters of mobile robot platform</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Name</p></td><td colspan="1" rowspan="1"><p>Parameter</p></td></tr><tr><td colspan="1" rowspan="1"><p>Installation height of VLP-16</p></td><td colspan="1" rowspan="1"><p>0.75<italic>m</italic></p></td></tr><tr><td colspan="1" rowspan="1"><p>Body width</p></td><td colspan="1" rowspan="1"><p>0.53<italic>m</italic></p></td></tr><tr><td colspan="1" rowspan="1"><p>Body length</p></td><td colspan="1" rowspan="1"><p><italic>0.86</italic>m</p></td></tr><tr><td colspan="1" rowspan="1"><p>Maximum travel thrust of drive wheels</p></td><td colspan="1" rowspan="1"><p>429<italic>N</italic></p></td></tr><tr><td colspan="1" rowspan="1"><p>Maximum travel speed</p></td><td colspan="1" rowspan="1"><p>4<italic>m/s</italic></p></td></tr><tr><td colspan="1" rowspan="1"><p>Maximum load</p></td><td colspan="1" rowspan="1"><p>248<italic>N</italic></p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>2.2. Mechanical design</title>
          
          <p>In order to facilitate timely debugging, the laptop is directly used to control in the research and development stage. The electrical control system is shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Control system configuration of mobile robot platform</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/2/img_x5MIrugirCFdkyzU.png"/>
            </fig>
          
          <p>The laptop is connected to the motor driver through the RS-485 communication protocol. The motor control mode selection and control parameter configuration are performed through the host computer software. After the configuration is completed, the speed command is sent through the CAN bus to control the two driving wheels, realizing functions such as forward, backward, and differential speed turning. The laptop is connected to VLP-16 through the Ethernet interface, and obtains the original point cloud information, which is used to describe the surrounding environment after digital filtering and coordinate transformation. In addition, the relay module is connected to the laptop through the Ethernet interface, and directly controls the power supply of the motor driver in the form of a switch. When encountering an unexpected situation, the mobile robot platform performs emergency braking by the relay module, avoiding the control delay of the motor driver. The IMU is connected to the laptop through the USB interface to provide the heading angle information of the mobile robot platform. The main control system device parameters used by the mobile robot platform are shown in <xref ref-type="table" rid="table_2">Table 2</xref>.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Device parameters of mobile robot platform control system</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Name</p></td><td colspan="1" rowspan="1"><p>Parameter</p></td></tr><tr><td colspan="1" rowspan="1"><p>Laptop</p></td><td colspan="1" rowspan="1"><p>Lenovo Legion Y7000P 2019 PG0</p></td></tr><tr><td colspan="1" rowspan="1"><p>16-line lidar</p></td><td colspan="1" rowspan="1"><p>VLP-16</p></td></tr><tr><td colspan="1" rowspan="1"><p>Motor Controller</p></td><td colspan="1" rowspan="1"><p>DS20270C</p></td></tr><tr><td colspan="1" rowspan="1"><p>IMU</p></td><td colspan="1" rowspan="1"><p>TL740D</p></td></tr><tr><td colspan="1" rowspan="1"><p>Relay module</p></td><td colspan="1" rowspan="1"><p>SRND-WM-RY1</p></td></tr><tr><td colspan="1" rowspan="1"><p>USB to CAN module</p></td><td colspan="1" rowspan="1"><p>CANalyst-II</p></td></tr></tbody></table>
            </table-wrap>
          
          <p>The electrical control system uses a simplified communication protocol conversion module to unify the data transmission protocols required by different peripherals to the USB port, which means that it is friendly to the choice of the main controller. The modular design makes equipment replacement more convenient without affecting the normal operation of other functions.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>3. Software architecture design of mobile robot platform</title>
      <p>The software framework of the system is shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>. The laptop sits at the heart, managing all inputs and outputs with extended USB ports and switches. The ROS runs on the laptop to divide the program into different modules and transmit data by publishing and receiving topics, realizing the active exploration and real-time obstacle avoidance of the mobile robot platform. In the algorithm, the environment detection results are obtained periodically to judge the rationality of multiple planned paths. Then, by giving the motion sequence points of the mobile robot platform, the left and right driving wheels are differentially controlled.</p>
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>Software framework of mobile robot platform</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/2/img_yNvcYpfIhxopo2ai.png"/>
        </fig>
      
      <p>The sensing module is divided into two parts, which are used to process the data of IMU and VLP-16 respectively. The base controller module is used for positioning and motion control of the mobile robot platform. The active exploration module sends speed commands to the basic controller module through the related topic, which is used for obstacle detection and path selection. A data packet of the IMU is 14 bytes, and the data transmission is set at a baud rate of 115200, which means that 822 valid data can be transmitted per second. The data of IMU is published to other nodes through its topic. The basic controller module obtains the attitude angle information of the mobile robot platform by subscribing to the topic of the IMU. Then, velocity information is published by the topic of the basic controller module. The dead reckoning method is used to estimate the position of the mobile robot platform, and the attitude information of the robot is released to other nodes through the related topic. The measurement frequency of VLP-16 is 5-20Hz, and the original point cloud information is obtained through the official driver, which is relative to the base coordinate system of VLP-16. We set the starting position of the mobile robot platform as the origin of the world coordinate system. By combining the real-time estimated pose of the mobile robot platform, the original point cloud is transformed into the world coordinate system as the basis for obstacle detection and path selection. The point cloud data during the exploration process is stored for offline mapping.</p>
    </sec>
    <sec sec-type="">
      <title>4. Active exploration map construction method</title>
      
        <sec>
          
            <title>4.1. Mobile robot positioning</title>
          
          <p>The basic principle of relative positioning is to calculate the pose change between adjacent moments through the internal sensor information on the premise that the initial pose of the mobile robot is given, realizing the real-time estimation of the pose. This method is also called dead reckoning, which predicts the trajectory of the robot based on the kinematics model [<xref ref-type="bibr" rid="ref_17">17</xref>]. The internal sensors commonly used in relative positioning methods mainly include odometers, gyroscopes, and inertial sensors. In this paper, our mobile robot platform adopts the IMU. We wrote a set of basic controller algorithms to drive the operation of the mobile robot platform. The code environment adopts C++ and ROS. The algorithm receives velocity and corner information through topics. In the information, the linear velocity in the x direction is always kept as the direction of the vehicle head, and the unit is m/s. The z-axis angular velocity is expressed as the spin angular velocity in rad/s, assuming that the counterclockwise direction is positive. That is, we control the forward, backward and turning of the robot. Assume that the traveling speed is <italic>v</italic>, the steering angular velocity is <italic>ω</italic>, and the robot's turning radius is <italic>r</italic>. Then the left wheel speed <italic>v<sub>l</sub></italic> and the right wheel speed <italic>v<sub>r</sub></italic> on the robot chassis can be expressed as Eq. (1).</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="mpfqru49hv">
                <mml:mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd/>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>v</mml:mi>
                        <mml:mi>l</mml:mi>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>∗</mml:mo>
                      <mml:mi>v</mml:mi>
                      <mml:mi>ω</mml:mi>
                      <mml:mi>r</mml:mi>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd/>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>v</mml:mi>
                        <mml:mi>r</mml:mi>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mo>+</mml:mo>
                      <mml:mo>∗</mml:mo>
                      <mml:mi>v</mml:mi>
                      <mml:mi>ω</mml:mi>
                      <mml:mi>r</mml:mi>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>After that, the speed command is sent to the motor driver DS20270C through the CAN bus communication protocol, realizing the movement of the mobile robot platform.</p><p>In terms of dead reckoning, we use the moment obtained by the ROS timestamp as the integration time reference. Assume that the surrounding environment is in the same horizontal plane, the current pose of the robot is<span style="font-family: 等线"> <italic>P</italic>(<italic>x</italic>,<italic>y</italic>,<italic>z</italic>,<italic>r<sub>x</sub></italic>,<italic>r<sub>y</sub></italic>,<italic>r<sub>z</sub></italic>), the initial pose is <italic>P</italic><sub>0</sub>(<italic>x</italic><sub>0</sub>,<italic>y</italic><sub>0</sub>,<italic>z</italic><sub>0</sub>,<italic>r<sub>x</sub></italic><sub>0</sub>,r<sub>y0</sub>,<italic>r<sub>z</sub></italic><sub>0</sub>), the current moment provided by the ROS is <italic>t<sub>now</sub></italic>, the previous moment is <italic>t<sub>before</sub></italic>, the heading angle provided by the IMU in real time is <italic>ψ</italic>, and the value range is [-<italic>π</italic>,<italic>π</italic>]. Then in the algorithm cycle, the robot pose can be estimated Expressed as Eq. (2).</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="m5py65f7ej">
                <mml:mtable displaystyle="true" columnspacing="1em" rowspacing="3pt">
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mi>x</mml:mi>
                      <mml:mi>cos</mml:mi>
                      <mml:mi>ψ</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mo>⁡</mml:mo>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mo stretchy="false">)</mml:mo>
                      <mml:mo>∗</mml:mo>
                      <mml:mo>∗</mml:mo>
                      <mml:msup>
                        <mml:mi>v</mml:mi>
                        <mml:mo>∗</mml:mo>
                      </mml:msup>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>−</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mi>t</mml:mi>
                          <mml:mrow>
                            <mml:mtext>now </mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>t</mml:mi>
                          <mml:mrow>
                            <mml:mtext>before </mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:msub>
                        <mml:mi>c</mml:mi>
                        <mml:mi>x</mml:mi>
                      </mml:msub>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd>
                      <mml:mi>y</mml:mi>
                      <mml:mi>sin</mml:mi>
                      <mml:mi>ψ</mml:mi>
                      <mml:mo>+</mml:mo>
                      <mml:mo>=</mml:mo>
                      <mml:mo>⁡</mml:mo>
                      <mml:mo stretchy="false">(</mml:mo>
                      <mml:mo stretchy="false">)</mml:mo>
                      <mml:mo>∗</mml:mo>
                      <mml:mo>∗</mml:mo>
                      <mml:msup>
                        <mml:mi>v</mml:mi>
                        <mml:mo>∗</mml:mo>
                      </mml:msup>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>−</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mi>t</mml:mi>
                          <mml:mrow>
                            <mml:mtext>now </mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>t</mml:mi>
                          <mml:mrow>
                            <mml:mtext>before </mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                      <mml:msub>
                        <mml:mi>c</mml:mi>
                        <mml:mi>y</mml:mi>
                      </mml:msub>
                    </mml:mtd>
                  </mml:mtr>
                  <mml:mtr>
                    <mml:mtd>
                      <mml:msub>
                        <mml:mi>r</mml:mi>
                        <mml:mi>z</mml:mi>
                      </mml:msub>
                      <mml:mo>=</mml:mo>
                      <mml:mi>ψ</mml:mi>
                    </mml:mtd>
                  </mml:mtr>
                </mml:mtable>
              </mml:math>
            </disp-formula>
          
          <p>Among them, <italic>c<sub>x</sub></italic> and <italic>c<sub>y</sub></italic> are calibration coefficients respectively, which are generated by system error and measurement error.</p><p>The positioning model above is streamlined and has the accuracy to meet the needs of task scenarios. It is undeniable that there will be cumulative errors in the dead reckoning method. However, when the moving speed is relatively fixed and the speed change frequency is low, the cumulative error is also relatively slow. Under the condition of short running time, there is an approximate linear relationship between the cumulative error and the actual distance, so the cumulative error is reduced to a certain extent by the calibration coefficient. We make the mobile robot move arbitrarily within a certain range for a period by means of remote control, and make it return to the initial position to obtain the cumulative error of dead reckoning. During the running process, the total distance change of the mobile robot platform in the x and y directions is recorded at the same time. By comparing the cumulative error with the total distance, the calibration coefficient is obtained.</p>
        </sec>
      
      
        <sec>
          
            <title>4.2. Active exploration method</title>
          
          <p>The proposed method uses multi-path evaluation for active exploration, and the multiple paths generated offline are shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>.</p><p>The path points contained in it are stored in the form of data packets, which are directly called in the algorithm, saving computing resources greatly. The mobile robot platform obtains the environment point cloud information in real time through the VLP-16. Combined with its own length, width, and height, the mobile robot platform judges whether the planned path to be selected collides with environmental obstacles and obtains one or more effective paths. Then, according to the position of the end point, a path closer to the target is selected. Compared with reference [<xref ref-type="bibr" rid="ref_18">18</xref>] using intensive small-scale search and reference [<xref ref-type="bibr" rid="ref_19">19</xref>] using a search scheme based on likelihood estimation, the path adopted in this paper is sparser, which effectively reduce the amount of calculation. In addition, we add multiple paths on the left side of <italic>y</italic>=<italic>x</italic> and <italic>y</italic>=-<italic>x</italic>, which prevent the mobile robot from getting stuck at local points. The algorithm process is as follows.</p>
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>Multiple planning paths</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/2/img_wPcbn8hAlC1elswM.png"/>
            </fig>
          
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Algorithm 1: Active Exploration Method Based on Multi-path Evaluation</p></td></tr><tr><td colspan="1" rowspan="1"><p>1. Initialize multiple communication protocol configurations, mobile robot posture, and parameters of each node</p></td></tr><tr><td colspan="1" rowspan="1"><p>2. Obtain the 3D point cloud, and transform to the world coordinate system after performing voxel grid filtering, obtaining the point cloud set <italic>P</italic>. The number of points in it is set to <italic>Np</italic></p></td></tr><tr><td colspan="1" rowspan="1"><p>3. Read the offline path file and get <italic>N</italic> pieces of path information, including path number, path point coordinates and tangent slope of the point</p></td></tr><tr><td colspan="1" rowspan="1"><p>IMU</p></td></tr><tr><td colspan="1" rowspan="1"><p>4. Obtain artificially set target point information</p></td></tr><tr><td colspan="1" rowspan="1"><p>5. for <italic>i</italic> = 1 ... <italic>N</italic> do</p></td></tr><tr><td colspan="1" rowspan="1"><p>6. for <italic>j</italic> = 1 … <italic>Np</italic> do</p></td></tr><tr><td colspan="1" rowspan="1"><p>7. According to the position of the path point, the slope of the tangent line at this point, and the length, width, and height of the mobile robot platform, calculate whether the <italic>i</italic>-th path collides with the <italic>j</italic>-th environment point, and determine the feasibility of the path</p></td></tr><tr><td colspan="1" rowspan="1"><p>8. end for</p></td></tr><tr><td colspan="1" rowspan="1"><p>9. If the <italic>i</italic>-th path is feasible, obtain the distance <italic>Di</italic> between the end position of the path and the target point; if it is not feasible, set <italic>Di</italic> to a larger number</p></td></tr><tr><td colspan="1" rowspan="1"><p>10. end for</p></td></tr><tr><td colspan="1" rowspan="1"><p>11. According to <italic>D1</italic>...<italic>Dn</italic>, select the best path to drive the mobile robot platform to explore</p></td></tr></tbody></table><p>The point cloud set <italic>P</italic> is the 3D point cloud of the environment after filtering and coordinate transformation, including obstacle information. <italic>N</italic> represents 25 paths generated offline, and the path information is stored in other arrays. The set <italic>D</italic> represents the distance between the end point and the end position of the selected path, where we use the Euclidean distance.</p><p>In the process of active exploration, whether the environment point cloud accurately reflect the environmental information directly determines the accuracy of path selection in active exploration. Since VLP-16 has a blind area of one meter, the use of real-time environment point cloud for multipath evaluation often ignores important obstacle information. Therefore, in this paper, we unify the coordinates of the environment point cloud into the world coordinate system, and store a frame of environment point cloud at intervals to ensure that the scope of multipath evaluation is the environment information within the previous 3-5 seconds.</p>
        </sec>
      
      
        <sec>
          
            <title>4.3. Offline map creation method</title>
          
          <p>During the active exploration process of the mobile robot platform, the 3D point cloud information of the environment is stored to generate an offline data package. Afterwards, the algorithm [<xref ref-type="bibr" rid="ref_20">20</xref>] is used for offline map building. It combines lidar and IMU data for mapping. By optimizing the ground points, high-precision 3D reconstruction of the environment is achieved. Since lidar data and IMU data are loosely coupled, it is also possible to use only lidar data for mapping. Constructing the environment map offline saves computing resources in the active exploration process of the mobile robot. At the same time, the offline method has the advantage of reviewing the exploration path selected by the mobile robot during operation and providing data support for the subsequent improvement of the algorithm. After the offline map is obtained, the effectiveness of the offline map is verified by measuring the positional relationship between obstacles and comparing it with the obstacle relationship in the real world. In addition, we draw a visual path for easy observation by subscribing to the odometer information.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>5. Experiment</title>
      <p>We conduct an indoor active exploration experiment, as shown in <xref ref-type="fig" rid="fig_5">Figure 5</xref>.</p>
      
        <fig id="fig_5">
          <label>Figure 5</label>
          <caption>
            <title>Description diagram of autonomous exploration experiment for mobile robot platform</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/2/img_anunfQYLDZgoyx66.png"/>
        </fig>
      
      <p>The starting point of the mobile robot platform is taken as the origin of the world coordinate system called /map. By artificially setting the target point, the mobile robot platform is driven to explore autonomously. The experiment is carried out in a 6<italic>m</italic>*6<italic>m</italic> space with multiple obstacles, including chairs and cardboard boxes. They are scattered throughout the environment to ensure that multiple paths are valid, avoiding artificially blocked and unsolvable situations. The 6<italic>m</italic>*6<italic>m</italic> space belongs to the small environment experiment scene. Compared with the real environment, the placement of four chairs and two cardboard boxes increases the complexity of the environment, which is more challenging for autonomous exploration.</p><p>At the beginning of the experiment, set an exploration target point through artificial estimation. After receiving the coordinates of the target point, the mobile robot platform starts active exploration. The experimental diagram is shown in <xref ref-type="fig" rid="fig_6">Figure 6</xref>.</p>
      
        <fig id="fig_6">
          <label>Figure 6</label>
          <caption>
            <title>Autonomous exploration experiment of mobile robot platform</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/2/img_ZB996G4MfKw3Jl4h.png"/>
        </fig>
      
      <p>After that, we save the point cloud data during the autonomous exploration process and use it to build the environment map. The picture is shown in <xref ref-type="fig" rid="fig_7">Figure 7</xref>.</p><p>After the mobile robot platform starts along the positive direction of the red x-axis, it judges the position of obstacles according to the VLP-16. When it encounters Carton 1 at time 0 second, the mobile robot platform adjusts its course to the left and chooses a path that avoids Chair 1. At time 4 second, the mobile robot platform is in the middle of Chair 1 and Chair 2, choosing to move on. Encountering the obstacle Chair 3 at time 8 second, the mobile robot platform adjusts its course to the right and chooses to avoid the obstacle Chair 4. At time 12 second, it reaches the artificially set target point.</p>
      
        <fig id="fig_7">
          <label>Figure 7</label>
          <caption>
            <title>The environment map and exploration path obtained from the experiment</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/2/img_ssjcjFJf56S4ndwR.png"/>
        </fig>
      
      <p>The experimental effect is quantitatively evaluated from two aspects. One is the exploration efficiency, which is evaluated by the number of explorations and the length of the exploration path. The number of explorations indicates how many times the mobile robot needs to choose a path to reach the target position, and the length of the exploration path indicates how much distance the mobile robot needs to travel to reach the target position. In the above experiment, the mobile robot selects a total of 4 paths. The second is information gain, which is mainly manifested in the quantity and quality of point cloud modeled on environmental objects. From the experimental picture, we see that this method has performed high-quality modeling on the surrounding walls and tables, but due to the influence of the sensor field of view, the modeling effect on chairs and cardboard boxes is poor. Only a small number of point cloud is obtained. Therefore, setting the starting point of the mobile robot is an important thing. There should be as few obstacles as possible within a certain range around the starting point, so that the final effect will be better.</p>
    </sec>
    <sec sec-type="conclusions">
      <title>6. Conclusions</title>
      <p>In this paper, we design a mobile robot platform with autonomous controllable mechanical structure and control system, which has good mobility and scalability. In addition, we build a software control framework based on the ROS, using multi-node communication to realize the cooperation of various sensor data and control instructions. Finally, we propose an environmental active exploration map construction method, completing an active exploration experiment in the indoor environment by using multi-path evaluation. In the future, we will study active exploration methods that adapt to various complex environments and improve the operating efficiency of the algorithm.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p>This research was funded by the National Key Research and Development Program (Grant No.: 2020YFC1512102); the 2035 Innovation Task of the Institute of Artificial Intelligence Innovation, Chinese Academy of Sciences.</p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data supporting our research results are included within the article or supplementary material.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Overview of multi-robot collaborative navigation technology</article-title>
          <source>Unmanned Syst. Technol.</source>
          <year>2020</year>
          <volume>3</volume>
          <issue>2</issue>
          <page-range>1-8</page-range>
          <fpage>1</fpage>
          <lpage>8</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.19942/j.issn.2096-5915.2020.02.001</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Design and implementation of low-cost autonomous navigation system for mobile robot platform</article-title>
          <source>Unmanned Syst. Technol.</source>
          <year>2022</year>
          <volume>5</volume>
          <issue>4</issue>
          <page-range>50-62</page-range>
          <fpage>50</fpage>
          <lpage>62</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.19942/j.issn.2096-5915.2022.4.038</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>Research on SLAM road sign observation based on particle filter</article-title>
          <source>Comput Intel. Neurosc.</source>
          <year>2022</year>
          <volume>2022</volume>
          <pub-id pub-id-type="doi">https://doi.org/10.1155/2022/4478978</pub-id>
          <pub-id pub-id-type="publisher-id">4478978</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Ni</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>THMR-V: An effective and robust high-speed system in structured road</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>SMC'03 Conference Proceedings, 2003 IEEE International Conference on Systems, Man and Cybernetics, Conference Theme - System Security and Assurance, Washington, DC</conf-name>
          <conf-loc>USA</conf-loc>
          <conf-date>2003</conf-date>
          <year>2003</year>
          <page-range>4370-4374</page-range>
          <fpage>4370</fpage>
          <lpage>4374</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICSMC.2003.1245672.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Nguyen</surname>
              <given-names>H. G.</given-names>
            </name>
            <name>
              <surname>Morrell</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Mullens</surname>
              <given-names>K. D.</given-names>
            </name>
            <name>
              <surname>Burmeister</surname>
              <given-names>A. B.</given-names>
            </name>
            <name>
              <surname>Miles</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Farrington</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Thomas</surname>
              <given-names>K. M.</given-names>
            </name>
            <name>
              <surname>Gage</surname>
              <given-names>D. W.</given-names>
            </name>
          </person-group>
          <article-title>Segway robotic mobility platform</article-title>
          <source>Mobile Robots XVII</source>
          <year>2004</year>
          <volume>5609</volume>
          <page-range>207-220</page-range>
          <fpage>207</fpage>
          <lpage>220</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1117/12.571750</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Amsters</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Slaets</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Turtlebot 3 as a robotics education platform</article-title>
          <source>International Conference on Robotics in Education, Vienna</source>
          <publisher-name>Austria</publisher-name>
          <year>2019</year>
          <page-range>170-181</page-range>
          <fpage>170</fpage>
          <lpage>181</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1007/978-3-030-26945-6_16</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Oltean</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Mobile robot platform with arduino uno and raspberry pi for autonomous navigation</article-title>
          <source>Procedia Manuf.</source>
          <year>2019</year>
          <volume>32</volume>
          <page-range>572-577</page-range>
          <fpage>572</fpage>
          <lpage>577</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1016/j.promfg.2019.02.254</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lv</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Design and implementation of an omnidirectional mobile robot platform with unified I/O interfaces</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2017 IEEE International Conference on Mechatronics and Automation, Takamatsu</conf-name>
          <conf-loc>Japan</conf-loc>
          <conf-date>2017</conf-date>
          <year>2017</year>
          <page-range>410-415</page-range>
          <fpage>410</fpage>
          <lpage>415</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICMA.2017.8015852.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Batinovic</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Petrovic</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Ivanovic</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Petric</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Bogdan</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>A multi-resolution frontier-based planner for autonomous 3D exploration</article-title>
          <source>IEEE Robot. Autom. Lett.</source>
          <year>2021</year>
          <volume>6</volume>
          <issue>3</issue>
          <page-range>4528-4535</page-range>
          <fpage>4528</fpage>
          <lpage>4535</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/LRA.2021.3068923</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Bircher</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kamel</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Alexis</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Oleynikova</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Siegwart</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Receding horizon ‘next-best-view’ planner for 3d exploration</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2016 IEEE International Conference on Robotics and Automation, Stockholm</conf-name>
          <conf-loc>Sweden</conf-loc>
          <conf-date>2016</conf-date>
          <year>2016</year>
          <page-range>1462-1468</page-range>
          <fpage>1462</fpage>
          <lpage>1468</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICRA.2016.7487281.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Selin</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Tiger</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Duberg</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Heintz</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Jensfelt</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Efficient autonomous exploration planning of large-scale 3D environments</article-title>
          <source>IEEE Robot. Autom. Lett.</source>
          <year>2019</year>
          <volume>4</volume>
          <issue>2</issue>
          <page-range>1699-1706</page-range>
          <fpage>1699</fpage>
          <lpage>1706</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/LRA.2019.2897343</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Dai</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Papatheodorou</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Funk</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Tzoumanikas</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Leutenegger</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Fast frontier-based information-driven autonomous exploration with an MAV</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>P 2020 IEEE International Conference on Robotics and Automation, Paris</conf-name>
          <conf-loc>France</conf-loc>
          <conf-date>2020</conf-date>
          <year>2020</year>
          <page-range>9570-9576</page-range>
          <fpage>9570</fpage>
          <lpage>9576</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICRA40945.2020.9196707.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Martin</surname>
              <given-names>J. D.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Englot</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Autonomous exploration under uncertainty via deep reinforcement learning on graphs</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2020 IEEE/RSJ International Conference on Intelligent Robots and Systems, Las Vegas, NV</conf-name>
          <conf-loc>USA</conf-loc>
          <conf-date>2020</conf-date>
          <year>2020</year>
          <page-range>6140-6147</page-range>
          <fpage>6140</fpage>
          <lpage>6147</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/IROS45743.2020.9341657.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Zheng</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Xue</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Fan</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Low-cost GPS-aided lidar state estimation and map building</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2019 IEEE International Conference on Imaging Systems and Techniques, Abu Dhabi</conf-name>
          <conf-loc>United Arab Emirates</conf-loc>
          <conf-date>2019</conf-date>
          <year>2019</year>
          <page-range>1-6</page-range>
          <fpage>1</fpage>
          <lpage>6</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/IST48021.2019.9010530.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>David</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>A study on Markov localization for mobile robot</article-title>
          <source>Acta Automatica Sinica</source>
          <year>2003</year>
          <volume>29</volume>
          <issue>1</issue>
          <page-range>154-160</page-range>
          <fpage>154</fpage>
          <lpage>160</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.16383/j.aas.2003.01.021</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Chang</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Research on UWB and LiDAR fusion positioning algorithm in indoor environment</article-title>
          <source>Comput. Eng. Appl.</source>
          <year>2021</year>
          <volume>57</volume>
          <issue>6</issue>
          <page-range>260-266</page-range>
          <fpage>260</fpage>
          <lpage>266</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.3778/j.issn.1002-8331.2005-0435</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lu</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Gong</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>A SLAM method based on multi-robot cooperation for pipeline environments underground</article-title>
          <source>Sustainability</source>
          <year>2022</year>
          <volume>14</volume>
          <issue>0</issue>
          <pub-id pub-id-type="doi">https://doi.org/10.3390/su142012995</pub-id>
          <pub-id pub-id-type="publisher-id">12995</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Cao</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Xia</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Choset</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Oh</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Autonomous exploration development environment and the planning algorithms</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2022 International Conference on Robotics and Automation, Philadelphia, PA</conf-name>
          <conf-loc>USA</conf-loc>
          <conf-date>2022</conf-date>
          <year>2022</year>
          <page-range>8921-8928</page-range>
          <fpage>8921</fpage>
          <lpage>8928</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICRA46639.2022.9812330.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Chadha</surname>
              <given-names>R. G.</given-names>
            </name>
            <name>
              <surname>Singh</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Falco: Fast likelihood-based collision avoidance with extension to human-guided navigation</article-title>
          <source>J. Field Robot.</source>
          <year>2020</year>
          <volume>37</volume>
          <issue>8</issue>
          <page-range>1300-1313</page-range>
          <fpage>1300</fpage>
          <lpage>1313</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1002/rob.21952</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Shan</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Englot</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>LeGO-LOAM: Lightweight and ground-optimized lidar odometry and mapping on variable terrain</article-title>
          <publisher-name>IEEE</publisher-name>
          <conf-name>2018 IEEE/RSJ International Conference on Intelligent Robots and Systems, Madrid</conf-name>
          <conf-loc>Spain</conf-loc>
          <conf-date>2018</conf-date>
          <year>2018</year>
          <page-range>4758-4765</page-range>
          <fpage>4758</fpage>
          <lpage>4765</lpage>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/IROS.2018.8594299.</pub-id>
          <pub-id pub-id-type="publisher-id"/>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>