<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">MITS</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Mechatronics and Intelligent Transportation Systems</journal-title>
        <abbrev-journal-title abbrev-type="issn">Mechatron. Intell Transp. Syst.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">MITS</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-0218</issn>
      <issn publication-format="print">2958-020X</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-KMGJPIabtR0DjuDLKJ3xrYgKt7owArs_</article-id>
      <article-id pub-id-type="doi">10.56578/mits040302</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Entropy-Based Visibility and Fuzzy Logic Integration for Robust Object Detection in Foggy Road Environments</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0007-8763-7547</contrib-id>
          <name>
            <surname>Ahmad</surname>
            <given-names>Shakeel</given-names>
          </name>
          <email>g202417060@kfupm.edu.sa</email>
        </contrib>
        <aff id="aff_1">Department of Mathematics, King Fahad University of Petroleum and Minerals, 31261 Dhahran, Saudi Arabia</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>14</day>
        <month>07</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>3</issue>
      <fpage>125</fpage>
      <lpage>134</lpage>
      <page-range>125-134</page-range>
      <history>
        <date date-type="received">
          <day>21</day>
          <month>05</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>12</day>
          <month>07</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Reliable detection of road surface objects under foggy conditions remains a critical challenge for autonomous vehicle perception systems due to the severe degradation of visual information. To address this limitation, a novel framework was developed that integrates entropy-guided visibility enhancement, Pythagorean fuzzy logic, and structure-preserving saliency modeling to improve object detection performance in low-visibility environments. Visibility restoration was achieved through an entropy-guided weighting mechanism that selectively enhances salient image regions while preserving essential structural features critical for downstream detection tasks. Uncertainty and imprecision inherent to fog-degraded scenes were systematically modeled using Pythagorean fuzzy logic, enabling improved confidence estimation and robustness in object localization. A saliency mechanism that preserves structural characteristics further contributes to the accurate delineation of road-relevant elements. Extensive evaluations on multiple publicly available foggy road datasets were conducted, demonstrating substantial gains in detection performance, with notable improvements in accuracy, precision, recall, and F1-score metrics. Furthermore, enhancements in visual quality were verified using structural similarity index (SSIM), peak signal-to-noise ratio (PSNR), natural image quality evaluator (NIQE), and blind/referenceless image spatial quality evaluator (BRISQUE) metrics. The computational efficiency of the proposed method was confirmed, supporting its applicability to near real-time deployment scenarios. Consistent performance was observed across varying fog densities, highlighting the framework’s scalability and generalizability. The integration of entropy-based visibility enhancement with fuzzy reasoning and saliency preservation offers a comprehensive and practical solution to the challenges of perception in visually degraded environments, contributing to the advancement of safe and intelligent transportation systems.</p></abstract>
      <kwd-group>
        <kwd>Image processing</kwd>
        <kwd>Entropy-guided visibility enhancement</kwd>
        <kwd>Pythagorean fuzzy logic</kwd>
        <kwd>Structure-preserving
saliency</kwd>
        <kwd>Intelligent transportation systems</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="1"/>
        <fig-count count="2"/>
        <table-count count="1"/>
        <ref-count count="18"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Road surface condition plays a critical role in ensuring traffic safety and enabling intelligent transportation systems, especially in scenarios involving autonomous navigation and maintenance planning. Efficient and accurate detection of road surface anomalies such as cracks, potholes, and other forms of distress is essential for minimizing hazards and optimizing road maintenance schedules. In recent years, researchers have developed a variety of automated and semi-automated techniques to address this challenge, incorporating advanced image processing and machine learning paradigms. Traditional approaches focused on handcrafted features and thresholding methods, while more recent efforts have shifted toward data-driven and deep learning models to enhance robustness and generalization. For example, pixel-perfect segmentation and object-level road distress detection have been proposed to improve accuracy in complex scenes [<xref ref-type="bibr" rid="ref_1">1</xref>][<xref ref-type="bibr" rid="ref_2">2</xref>]. Furthermore, hybrid deep learning architectures and real-time capable frameworks have emerged, leveraging Convolutional Neural Networks (CNNs), unmanned aerial vehicle (UAV) imaging, and neuro-fuzzy systems to handle diverse conditions, surface textures, and lighting variations [<xref ref-type="bibr" rid="ref_3">3</xref>][<xref ref-type="bibr" rid="ref_4">4</xref>][<xref ref-type="bibr" rid="ref_5">5</xref>]. These advancements demonstrate significant progress toward fully autonomous and intelligent road surface assessment systems.</p><p>Building on these foundations, more recent studies have extended road surface detection capabilities by integrating obstacle detection mechanisms to enhance situational awareness in complex and foggy environments. Innovative embedded systems have been deployed on mobility aids such as wheelchairs to classify road surfaces and identify nearby obstacles, improving accessibility and safety in urban environments [<xref ref-type="bibr" rid="ref_6">6</xref>]. Simultaneously, advanced computer vision models have pushed the boundary of obstacle recognition by introducing techniques like adversarial erasure to differentiate occlusions and surface elements effectively [<xref ref-type="bibr" rid="ref_7">7</xref>]. In more industrial or constrained environments, such as underground mines, 3D Light Detection and Ranging (LiDAR) has proven instrumental for detecting structural barriers and road irregularities with high accuracy [<xref ref-type="bibr" rid="ref_8">8</xref>]. Moreover, the fusion of enhanced object detection models like You Only Look Once version 7 (YOLOv7) with image defogging techniques has led to substantial improvements in highway obstacle recognition under low-visibility conditions [<xref ref-type="bibr" rid="ref_9">9</xref>]. These contributions collectively underscore a shift toward integrated, multi-task systems that can address the dual challenges of surface assessment and obstacle avoidance under diverse environmental and operational scenarios. Recent advancements in object detection have focused on overcoming the challenges posed by adverse weather conditions, particularly fog, which significantly degrades image quality and reduces detection accuracy. Meng et al. [<xref ref-type="bibr" rid="ref_10">10</xref>] proposed YOLOv5s-Fog, an enhanced version of the lightweight You Only Look Once version 5 small (YOLOv5s) model tailored for foggy scenarios. By introducing improved feature extraction modules and weather-aware preprocessing, this model demonstrated superior detection accuracy under low-visibility conditions. However, its effectiveness slightly diminishes when dealing with heavy fog or mixed weather conditions due to limited domain generalization. In a complementary approach, Niu et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] introduced a method for obstacle detection based on 3D information recovery, allowing for the reconstruction of spatial depth in fog-obscured scenes. While their approach excels in recovering spatial layout, it is computationally intensive and less suitable for real-time applications. Similarly, Park et al. [<xref ref-type="bibr" rid="ref_12">12</xref>] evaluated the effectiveness of road lighting by introducing obstacle recognition distance as a metric under foggy and rainy scenarios. Although the study offers a practical framework for infrastructure enhancement, it lacks integration with real-time detection systems.</p>
      <p>Srikanth et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] proposed a real-time vehicle detection and road condition prediction system using CNNs and Internet of Things (IoT) data. It performed well in urban environments, offering fast and accurate detection. However, its effectiveness dropped in foggy or adverse weather due to visibility issues and sensor noise. The system also depends on stable connectivity, which may limit its use in rural areas. On the hardware front, Sharmila et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] proposed a fog penetration radar system designed to assist conventional vision-based object detectors. Despite its promising results in laboratory settings, its high cost and integration complexity pose significant adoption barriers. Finally, Li et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] presented a domain adaptation-based object detection framework targeting foggy and rainy weather conditions. Their model bridges the performance gap between synthetic and real-world foggy datasets using adversarial learning techniques. While this method achieves state-of-the-art results, it requires extensive computational resources and training time. Collectively, these studies represent a significant stride toward robust object detection in inclement weather, though challenges in real-time deployment, domain generalization, and dataset diversity remain critical areas for further research.</p><p>Motivated by the limitations of traditional segmentation techniques in fog-degraded scenes, this study introduces a novel fog-resilient image segmentation framework that integrates entropy-guided visibility assessment, Pythagorean fuzzy modeling, and structure-preserving gradient features ( <xref ref-type="fig" rid="fig_1">Figure 1</xref>). Unlike conventional approaches that rely heavily on intensity-based heuristics or handcrafted priors, the proposed method formulates segmentation as the minimization of a tailored energy functional that accounts for local uncertainty, edge preservation, and low-visibility compensation. The entropy-guided visibility function captures texture degradation due to fog, while the Pythagorean fuzzy membership robustly models object-background ambiguity. Furthermore, a structure-preserving gradient term reinforces boundary adherence without over-smoothing. These components are synergistically embedded into an energy minimization framework solved via gradient descent. Experimental results on foggy and low-contrast images demonstrate the effectiveness of the proposed approach in preserving object structures and achieving accurate segmentation under challenging visibility conditions.</p><p>In Figure ~\ref{fig1}, the input image is preprocessed, followed by entropy-guided Pythagorean fuzzy modeling and gradient computation. These guide the energy functional, which evolves via gradient descent to produce the segmented output.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Workflow of the proposed road boundary detection model</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/6/img_KRdyufpnDVqDJ6IY.jpeg"/>
        </fig>
      
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p>Object detection in adverse weather conditions, particularly fog, presents unique challenges for autonomous driving systems. Traditional computer vision methods often fail to maintain robustness under low visibility, while recent deep learning advancements offer improved but still imperfect performance. Researchers have increasingly turned to hybrid models, sensor fusion, and intelligent algorithms like entropy-based and fuzzy logic techniques to enhance detection accuracy. This literature review explores significant contributions in this area, focusing on recent developments that address road surface recognition, obstacle detection, and environmental interpretation in challenging driving conditions.</p><p>Tahir et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] presented a comprehensive review of object detection approaches in autonomous vehicles operating under adverse weather conditions such as fog, rain, and snow. The study critically compared traditional computer vision methods and recent deep learning models, highlighting how visibility degradation severely impacts the performance of detection systems. One of the major achievements of the research is its structured taxonomy of methods and identification of key strategies like data augmentation, weather-invariant features, and sensor fusion as effective solutions for robustness. The review is particularly useful for researchers aiming to develop weather-resilient models and offers practical insights on dataset limitations and model evaluation. However, the research also has notable limitations. It lacks an in-depth discussion on entropy-based or fuzzy logic methods, which are gaining traction for handling uncertainty in detection tasks under poor visibility. Furthermore, the review acknowledges the limited availability of balanced datasets across diverse weather conditions, leading to a gap in model generalization.</p><p>Wu et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] focused on designing an object detection model tailored to identifying tiny road surface damages, such as micro cracks and small potholes. The study utilized a refined CNN structure combined with attention mechanisms to improve detection accuracy on low-resolution damage features. A key achievement of this research is its success in maintaining high precision and recall on multiple real-world road condition datasets, outperforming baseline You Only Look Once (YOLO) and Faster Region-based Convolutional Neural Network (Faster R-CNN) models in detecting small-scale anomalies. The model also demonstrated low computational complexity, making it suitable for real-time applications on embedded systems. Despite these strengths, the model shows some limitations. It was tested primarily under clear weather conditions, raising concerns about its robustness in adverse environments like fog, rain, or low light. Moreover, its performance heavily relies on well-annotated datasets, and it may struggle in generalized deployments where the size and contrast of road damage vary significantly.</p><p>Jeny et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] proposed a multiscale object detection framework for autonomous driving systems aimed at recognizing complex road environments. The study integrates a multiscale feature pyramid network (FPN) with real-time data processing pipelines to improve the detection of both large- and small-scale objects such as vehicles, pedestrians, and traffic signs. One of the key achievements of the model is its high mean Average Precision (mAP) in varied urban driving scenarios, showcasing its ability to adapt across different road textures and object scales. The model also achieves real-time inference speed, which is crucial for deployment in autonomous navigation systems. Nonetheless, the framework exhibits certain limitations. It lacks specific adaptations for foggy or adverse weather conditions, which often obscure visual features needed for accurate detection. Additionally, while the model handles scale variation well, it does not integrate uncertainty estimation or fuzzy logic to deal with ambiguous object boundaries in challenging environments.</p>
    </sec>
    <sec sec-type="">
      <title>3. Proposed methodology</title>
      <p>Fog-induced visibility degradation makes object detection on road surfaces challenging for both human drivers and autonomous systems. Traditional visibility models based on the dark channel prior (DCP) or Gaussian membership functions often fail under varying fog intensities and illumination conditions. This work proposes a novel mathematical functional approach that incorporates entropy-guided visibility, Pythagorean fuzzy modeling, and structure-preserving gradient enhancement.</p>
      
        <sec>
          
            <title>3.1. Entropy-guided visibility function $\psi(x, y)$</title>
          
          <p>In image dehazing, the traditional DCP estimates the presence of haze by assuming that in most non-sky patches of a haze-free image, at least one color channel has some pixels with very low intensity. However, this assumption often fails in regions with high texture, brightness, or complex illumination. To address these limitations, an entropy-based visibility estimation technique was proposed, which quantifies the amount of texture degradation due to haze or fog.</p><p>The proposed entropy-guided visibility function <inline-formula>
  <mml:math id="mrmdlj2lfu">
    <mml:mi>Ψ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is defined as:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mmfugb3yp7">
    <mml:mi>Ψ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mn>1</mml:mn>
    <mml:mfrac>
      <mml:mrow>
        <mml:msub>
          <mml:mi>H</mml:mi>
          <mml:mi>L</mml:mi>
        </mml:msub>
        <mml:mo>(</mml:mo>
        <mml:mo>,</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>x</mml:mi>
        <mml:mi>y</mml:mi>
      </mml:mrow>
      <mml:msub>
        <mml:mi>H</mml:mi>
        <mml:mrow>
          <mml:mo>max</mml:mo>
        </mml:mrow>
      </mml:msub>
    </mml:mfrac>
  </mml:math>
</inline-formula></p><p>where, <inline-formula>
  <mml:math id="mqgfmzc2rg">
    <mml:mi>Ψ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> denotes the visibility level at pixel location <inline-formula>
  <mml:math id="mnixs3in8a">
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>. This function yields values in the range <inline-formula>
  <mml:math id="m18eo4kp6w">
    <mml:mo>[</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mn>0</mml:mn>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula>, with higher values indicating better visibility (i.e., less fog), and lower values indicating regions more affected by haze.</p><p>The term <inline-formula>
  <mml:math id="m9xy5em02c">
    <mml:msub>
      <mml:mi>H</mml:mi>
      <mml:mi>L</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> represents the local entropy calculated within a square window centered at pixel <inline-formula>
  <mml:math id="mvd2aapzc3">
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>. It is defined by the expression:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mrvdd4pkuy">
    <mml:msub>
      <mml:mi>H</mml:mi>
      <mml:mi>L</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>p</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>p</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>⁡</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>log</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:munderover>
      <mml:mo>∑</mml:mo>
      <mml:mrow>
        <mml:mi>i</mml:mi>
        <mml:mo>=</mml:mo>
        <mml:mn>1</mml:mn>
      </mml:mrow>
      <mml:mi>N</mml:mi>
    </mml:munderover>
  </mml:math>
</inline-formula></p><p>where, <inline-formula>
  <mml:math id="mwt2zerokh">
    <mml:msub>
      <mml:mi>p</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> denotes the normalized histogram of intensity levels within the neighborhood, and $N<inline-formula>
  <mml:math id="m89nhi52r1">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
  </mml:math>
</inline-formula>N<inline-formula>
  <mml:math id="ml7hk2d9bt">
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mn>256</mml:mn>
    <mml:mn>8</mml:mn>
    <mml:mn>1.</mml:mn>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>H_max<inline-formula>
  <mml:math id="mka10jsspm">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula> H_{\max }=\log (N) <inline-formula>
  <mml:math id="m2jbeao5w6">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
  </mml:math>
</inline-formula>\Psi(x, y)<inline-formula>
  <mml:math id="mlt1bbvgv8">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>[ 0,1]<inline-formula>
  <mml:math id="mw7o4lv23w">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>W</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>H_L(x, y)<inline-formula>
  <mml:math id="m6465hvxgn">
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\Psi(x, y)<inline-formula>
  <mml:math id="mbp62cuij0">
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mi>C</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>\Psi(x, y)$ thus serves as a robust alternative to traditional priors and was subsequently used in the transmission estimation and image restoration pipeline in this study.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Pythagorean fuzzy membership function $\boldsymbol{\mu_{p f}(x, y)}$</title>
          
          <p>In foggy or hazy images, pixel intensities often exhibit ambiguous characteristics, making it difficult to distinguish between object and background regions using crisp segmentation or binary thresholds. To handle such uncertainty effectively, a Pythagorean fuzzy set (PFS) framework was adopted in this study, which offers a more flexible representation of membership compared to traditional fuzzy sets and intuitionistic fuzzy sets.</p><p>The Pythagorean fuzzy membership function <inline-formula>
  <mml:math id="mt21mdk52y">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:mi>P</mml:mi>
        <mml:mi>F</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> is defined as:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mr1vft7s6j">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:mi>P</mml:mi>
        <mml:mi>F</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mfrac>
      <mml:mrow>
        <mml:msub>
          <mml:mi>δ</mml:mi>
          <mml:mn>1</mml:mn>
        </mml:msub>
        <mml:mo>(</mml:mo>
        <mml:mo>,</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>x</mml:mi>
        <mml:mi>y</mml:mi>
      </mml:mrow>
      <mml:msqrt>
        <mml:msub>
          <mml:mi>δ</mml:mi>
          <mml:mn>1</mml:mn>
        </mml:msub>
        <mml:msub>
          <mml:mi>δ</mml:mi>
          <mml:mn>2</mml:mn>
        </mml:msub>
        <mml:mo>(</mml:mo>
        <mml:mo>,</mml:mo>
        <mml:mo>+</mml:mo>
        <mml:mo>(</mml:mo>
        <mml:mo>,</mml:mo>
        <mml:mi>x</mml:mi>
        <mml:mi>y</mml:mi>
        <mml:mi>x</mml:mi>
        <mml:mi>y</mml:mi>
        <mml:msup>
          <mml:mo>)</mml:mo>
          <mml:mn>2</mml:mn>
        </mml:msup>
        <mml:msup>
          <mml:mo>)</mml:mo>
          <mml:mn>2</mml:mn>
        </mml:msup>
      </mml:msqrt>
    </mml:mfrac>
  </mml:math>
</inline-formula></p><p>where, <inline-formula>
  <mml:math id="mmkvb4prz3">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:mi>P</mml:mi>
        <mml:mi>F</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>∈</mml:mo>
    <mml:mo>[</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mn>0</mml:mn>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula> quantifies the degree to which a pixel at location <inline-formula>
  <mml:math id="mm5zk8md3r">
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> belongs to the object (or foreground) class under uncertain visual conditions. The function is derived from the PFS theory, which satisfies the condition <inline-formula>
  <mml:math id="mtgx8yt880">
    <mml:msup>
      <mml:mi>μ</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msup>
    <mml:msup>
      <mml:mi>ν</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msup>
    <mml:mo>+</mml:mo>
    <mml:mo>≤</mml:mo>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula>, where <inline-formula>
  <mml:math id="mmqs8mjcj4">
    <mml:mi>μ</mml:mi>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mpgldkcht2">
    <mml:mi>ν</mml:mi>
  </mml:math>
</inline-formula> are the degrees of membership and non-membership, respectively.</p><p>In this formulation, <inline-formula>
  <mml:math id="m1q0qh5k52">
    <mml:msub>
      <mml:mi>δ</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> denotes the absolute difference between the pixel intensity <inline-formula>
  <mml:math id="mysjwr4f2m">
    <mml:mi>I</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> and the estimated object intensity <inline-formula>
  <mml:math id="mqsnipre1v">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mi>o</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>, i.e.,</p><p style="text-align: center"><inline-formula>
  <mml:math id="mqif3hrolx">
    <mml:msub>
      <mml:mi>δ</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mrow>
      <mml:mo>|</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>−</mml:mo>
      <mml:mo>|</mml:mo>
      <mml:mi>I</mml:mi>
      <mml:mi>x</mml:mi>
      <mml:mi>y</mml:mi>
      <mml:msub>
        <mml:mi>μ</mml:mi>
        <mml:mi>o</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p>This term captures how similar the pixel is to the object region in terms of intensity. A lower value of <inline-formula>
  <mml:math id="mp730uk2sb">
    <mml:msub>
      <mml:mi>δ</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> implies higher confidence in object membership.</p><p>Similarly, <inline-formula>
  <mml:math id="mu8tc4csjb">
    <mml:msub>
      <mml:mi>δ</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> is defined as:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mk4154dh7m">
    <mml:msub>
      <mml:mi>δ</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mrow>
      <mml:mo>|</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>−</mml:mo>
      <mml:mo>|</mml:mo>
      <mml:mi>I</mml:mi>
      <mml:mi>x</mml:mi>
      <mml:mi>y</mml:mi>
      <mml:msub>
        <mml:mi>μ</mml:mi>
        <mml:mi>b</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p>where, <inline-formula>
  <mml:math id="mbkj19cwql">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mi>b</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> represents the estimated background intensity. This term reflects the pixel's deviation from the background class.</p><p>The object intensity <inline-formula>
  <mml:math id="m4cepn0915">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mi>o</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> and background intensity <inline-formula>
  <mml:math id="mmld0m18qh">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mi>b</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> can be estimated using adaptive techniques such as fuzzy c-means clustering, k-means clustering, or seeded region growing with labeled object and background pixels. These adaptive estimates ensure that the membership function remains sensitive to local scene characteristics and can adjust dynamically based on the image context.</p><p>The Pythagorean fuzzy membership function <inline-formula>
  <mml:math id="m98tpp7k3r">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:mi>P</mml:mi>
        <mml:mi>F</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> thus provides a smooth and continuous measure of object-likeness, particularly effective in foggy environments where class boundaries are not sharply defined. Unlike conventional membership functions, it inherently balances the relationship between object and background similarity, and the denominator ensures normalization to keep the membership within the unit interval.</p><p>Importantly, <inline-formula>
  <mml:math id="muqd28y4xj">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:mi>P</mml:mi>
        <mml:mi>F</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> serves as a modulating weight in the energy functional that interacts directly with the structure-preserving gradient term <inline-formula>
  <mml:math id="mufyjnblj0">
    <mml:mi>Λ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula>. This interaction ensures that segmentation is driven not only by structural edges but also by their likelihood of representing true object regions under uncertainty. By coupling fuzzy membership with structural gradients, the proposed model reinforces boundary detection only in regions that are both structurally significant and semantically probable, improving robustness under fog.</p><p>This function plays a vital role in the proposed model by allowing uncertainty-aware segmentation and aiding in subsequent defogging and enhancement tasks.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Structure-preserving gradient term $\boldsymbol{\lambda(x, y)}$</title>
          
          <p>Accurately enhancing image contrast and preserving fine structural details are crucial in defogging applications, particularly when textures and edges are partially obscured. Traditional edge enhancement techniques often rely solely on gradient or Laplacian operators. However, relying only on the Laplacian can over-amplify noise and lose context-specific structure. To address these issues, a structure-preserving gradient term <inline-formula>
  <mml:math id="mwdzkjbb4k">
    <mml:mi>Λ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> was proposed, which balances contrast enhancement and edge preservation using both gradient and Laplacian information in conjunction with the visibility function.</p><p>The structure-preserving term is defined as:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mfltkajym9">
    <mml:mi>Λ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mfrac>
      <mml:mrow>
        <mml:mo>|</mml:mo>
        <mml:mo>(</mml:mo>
        <mml:mo>,</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mo>⋅</mml:mo>
        <mml:mo>(</mml:mo>
        <mml:mo>,</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mi>∇</mml:mi>
        <mml:mi>I</mml:mi>
        <mml:mi>x</mml:mi>
        <mml:mi>y</mml:mi>
        <mml:mi>Ψ</mml:mi>
        <mml:mi>x</mml:mi>
        <mml:mi>y</mml:mi>
        <mml:mrow>
          <mml:mo>|</mml:mo>
        </mml:mrow>
      </mml:mrow>
      <mml:mrow>
        <mml:mn>1</mml:mn>
        <mml:mo>+</mml:mo>
        <mml:mo>⋅</mml:mo>
        <mml:mo>(</mml:mo>
        <mml:mo>,</mml:mo>
        <mml:mo>)</mml:mo>
        <mml:mo>|</mml:mo>
        <mml:mi>κ</mml:mi>
        <mml:mi>Δ</mml:mi>
        <mml:mi>I</mml:mi>
        <mml:mi>x</mml:mi>
        <mml:mi>y</mml:mi>
        <mml:mrow>
          <mml:mo>|</mml:mo>
        </mml:mrow>
      </mml:mrow>
    </mml:mfrac>
  </mml:math>
</inline-formula></p><p>where, <inline-formula>
  <mml:math id="mhk87qittk">
    <mml:mi>Λ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> modulates the influence of local gradients based on visibility and suppresses unnecessary enhancement in flat or noisy regions. In the equation, <inline-formula>
  <mml:math id="m946w6w7vg">
    <mml:mo>|</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>|</mml:mo>
    <mml:mi>∇</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> represents the gradient magnitude at pixel <inline-formula>
  <mml:math id="mgadkziuje">
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>, typically computed as:</p><p style="text-align: center"><inline-formula>
  <mml:math id="masirt971g">
    <mml:mo>|</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mi>∇</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mrow>
      <mml:mo>|</mml:mo>
    </mml:mrow>
    <mml:msqrt>
      <mml:msup>
        <mml:mrow>
          <mml:mo>(</mml:mo>
          <mml:mo>)</mml:mo>
          <mml:msub>
            <mml:mi>I</mml:mi>
            <mml:mi>x</mml:mi>
          </mml:msub>
        </mml:mrow>
        <mml:mn>2</mml:mn>
      </mml:msup>
      <mml:msup>
        <mml:mrow>
          <mml:mo>(</mml:mo>
          <mml:mo>)</mml:mo>
          <mml:msub>
            <mml:mi>I</mml:mi>
            <mml:mi>y</mml:mi>
          </mml:msub>
        </mml:mrow>
        <mml:mn>2</mml:mn>
      </mml:msup>
      <mml:mo>+</mml:mo>
    </mml:msqrt>
  </mml:math>
</inline-formula></p><p>where, <inline-formula>
  <mml:math id="mc4t5za1q3">
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mi>x</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="ms119pifpb">
    <mml:msub>
      <mml:mi>I</mml:mi>
      <mml:mi>y</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> denote the horizontal and vertical derivatives of the image intensity, respectively. This term captures local edge strength and texture variation.</p><p>The Laplacian term <inline-formula>
  <mml:math id="mo8gqr6c5i">
    <mml:mo>|</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>|</mml:mo>
    <mml:mi>∇</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> measures second-order intensity variation and is defined as:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mfqd7vmysc">
    <mml:mi>Δ</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>+</mml:mo>
    <mml:mfrac>
      <mml:mrow>
        <mml:msup>
          <mml:mi>∂</mml:mi>
          <mml:mn>2</mml:mn>
        </mml:msup>
        <mml:mi>I</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mi>∂</mml:mi>
        <mml:msup>
          <mml:mi>x</mml:mi>
          <mml:mn>2</mml:mn>
        </mml:msup>
      </mml:mrow>
    </mml:mfrac>
    <mml:mfrac>
      <mml:mrow>
        <mml:msup>
          <mml:mi>∂</mml:mi>
          <mml:mn>2</mml:mn>
        </mml:msup>
        <mml:mi>I</mml:mi>
      </mml:mrow>
      <mml:mrow>
        <mml:mi>∂</mml:mi>
        <mml:msup>
          <mml:mi>y</mml:mi>
          <mml:mn>2</mml:mn>
        </mml:msup>
      </mml:mrow>
    </mml:mfrac>
  </mml:math>
</inline-formula></p><p>It acts as a high-frequency penalty term. In regions where <inline-formula>
  <mml:math id="mirtjx6twx">
    <mml:mo>|</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>|</mml:mo>
    <mml:mi>∇</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> is large (indicating sharp intensity changes or noise), the denominator increases, thereby reducing the contribution of the gradient term. This helps in suppressing over-enhancement in noisy or highly fluctuating areas.</p><p>The visibility weight <inline-formula>
  <mml:math id="mg5rtwvouw">
    <mml:mi>Ψ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula>, as defined earlier in Section 2.1, adjusts the influence of the gradient based on local entropy. In clear regions (with low fog), <inline-formula>
  <mml:math id="mvr8m4f2i3">
    <mml:mi>Ψ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is high, allowing more pronounced edge emphasis. Conversely, in heavily fogged areas where visibility is low, <inline-formula>
  <mml:math id="mwei808waz">
    <mml:mi>Ψ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> reduces the gradient contribution to avoid enhancing noise or artifacts.</p><p>The parameter <inline-formula>
  <mml:math id="mo443a4lqu">
    <mml:mi>κ</mml:mi>
  </mml:math>
</inline-formula> is a positive regularization constant that controls the sensitivity of the denominator to high-frequency components. A higher value of <inline-formula>
  <mml:math id="m253jcjjpy">
    <mml:mi>κ</mml:mi>
  </mml:math>
</inline-formula> enforces stronger suppression of high-frequency noise, while a lower value allows more aggressive edge enhancement.</p><p>The physical coupling between <inline-formula>
  <mml:math id="m9k4xeyr8v">
    <mml:mi>Λ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="m1b5nl2f4e">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:mi>P</mml:mi>
        <mml:mi>F</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> in the energy functional is central to the model's success. While <inline-formula>
  <mml:math id="ms1c12zzd2">
    <mml:mi>Λ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> highlights regions with significant edge content, <inline-formula>
  <mml:math id="mi82q9ltpm">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:mi>P</mml:mi>
        <mml:mi>F</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula> ensures that only those edges with high semantic object-likelihood are favored. This synergistic interaction avoids over-segmentation in noisy or irrelevant areas and enhances boundary localization in uncertain foggy scenes.</p><p>In summary, the structure-preserving gradient term <inline-formula>
  <mml:math id="mcscsc3sp4">
    <mml:mi>Λ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> ensures selective enhancement of significant structural features while minimizing the risk of amplifying noise. It serves as a crucial component in the proposed framework, enhancing perceptual quality by reinforcing edges in visible regions and preserving smoothness in degraded areas.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Energy functional</title>
          
          <p>To achieve robust and accurate segmentation under foggy conditions, an energy functional was formulated, which integrates entropy-guided visibility, Pythagorean fuzzy modeling, and structure-preserving image features. The total energy to be minimized is defined as:</p><p style="text-align: center"><inline-formula>
  <mml:math id="mc2sd1wv9g">
    <mml:mi>E</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:msub>
      <mml:mo>∫</mml:mo>
      <mml:mrow>
        <mml:mi>Ω</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mrow>
      <mml:mo>[</mml:mo>
      <mml:mo>⋅</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>;</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>⋅</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>+</mml:mo>
      <mml:mo>⋅</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>+</mml:mo>
      <mml:mo>⋅</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>−</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>−</mml:mo>
      <mml:mo>(</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:mo>]</mml:mo>
      <mml:mi>α</mml:mi>
      <mml:mi>x</mml:mi>
      <mml:mi>y</mml:mi>
      <mml:mi>κ</mml:mi>
      <mml:mi>Λ</mml:mi>
      <mml:mi>x</mml:mi>
      <mml:mi>y</mml:mi>
      <mml:mi>β</mml:mi>
      <mml:mi>∇</mml:mi>
      <mml:mi>u</mml:mi>
      <mml:mi>x</mml:mi>
      <mml:mi>y</mml:mi>
      <mml:mi>γ</mml:mi>
      <mml:mi>u</mml:mi>
      <mml:mi>x</mml:mi>
      <mml:mi>y</mml:mi>
      <mml:mi>I</mml:mi>
      <mml:mi>x</mml:mi>
      <mml:mi>y</mml:mi>
      <mml:mi>Ψ</mml:mi>
      <mml:mi>x</mml:mi>
      <mml:mi>y</mml:mi>
      <mml:msub>
        <mml:mi>μ</mml:mi>
        <mml:mrow>
          <mml:mi>P</mml:mi>
          <mml:mi>F</mml:mi>
        </mml:mrow>
      </mml:msub>
      <mml:mrow>
        <mml:mo>|</mml:mo>
      </mml:mrow>
      <mml:msup>
        <mml:mrow>
          <mml:mo>|</mml:mo>
        </mml:mrow>
        <mml:mn>2</mml:mn>
      </mml:msup>
      <mml:msup>
        <mml:mo>)</mml:mo>
        <mml:mn>2</mml:mn>
      </mml:msup>
      <mml:mn>1</mml:mn>
    </mml:mrow>
  </mml:math>
</inline-formula></p><p>In this formulation, <inline-formula>
  <mml:math id="m0wfotibqo">
    <mml:mi>Ω</mml:mi>
  </mml:math>
</inline-formula> represents the spatial domain of the input image, and <inline-formula>
  <mml:math id="me6cfmnvh4">
    <mml:mi>u</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is the segmentation indicator function distinguishing foreground from background.</p><p>•The first term, <inline-formula>
  <mml:math id="mmk5qwq2hv">
    <mml:mi>α</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>κ</mml:mi>
    <mml:mi>Λ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>⋅</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>;</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>⋅</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:mi>P</mml:mi>
        <mml:mi>F</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula>, aligns the segmentation with strong structural features where object-likeness is high, where <inline-formula>
  <mml:math id="m3sksev3lh">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:mi>P</mml:mi>
        <mml:mi>F</mml:mi>
      </mml:mrow>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>;</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>κ</mml:mi>
  </mml:math>
</inline-formula> is the Pythagorean fuzzy membership controlled by the hesitation parameter <inline-formula>
  <mml:math id="m7o4jbwltr">
    <mml:mi>κ</mml:mi>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="mnjtfdy3zi">
    <mml:mi>Λ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is the structure-preserving gradient term.</p><p>•The second term, <inline-formula>
  <mml:math id="mbucxkqet4">
    <mml:mi>β</mml:mi>
    <mml:mi>∇</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>⋅</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mrow>
      <mml:mo>|</mml:mo>
    </mml:mrow>
    <mml:msup>
      <mml:mo>|</mml:mo>
      <mml:mn>2</mml:mn>
    </mml:msup>
  </mml:math>
</inline-formula>, enforces smoothness by penalizing abrupt changes in <inline-formula>
  <mml:math id="me5zssijxr">
    <mml:mi>u</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula>.</p><p>•The third term, <inline-formula>
  <mml:math id="mjdpdedzjs">
    <mml:mi>γ</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>Ψ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>⋅</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:msup>
      <mml:mo>)</mml:mo>
      <mml:mn>2</mml:mn>
    </mml:msup>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula>, maintains fidelity to the observed intensity <inline-formula>
  <mml:math id="m3q7frmfny">
    <mml:mi>I</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula>, weighted more heavily in low-visibility areas where <inline-formula>
  <mml:math id="mml1cohzte">
    <mml:mi>Ψ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is small.</p><p>The scalar weights <inline-formula>
  <mml:math id="msf4rmnhiu">
    <mml:mi>α</mml:mi>
  </mml:math>
</inline-formula>, <inline-formula>
  <mml:math id="m6u6t4r68b">
    <mml:mi>β</mml:mi>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="m6xocx2ocf">
    <mml:mi>γ</mml:mi>
  </mml:math>
</inline-formula> and the hesitation coefficient <inline-formula>
  <mml:math id="m24rp0zmgd">
    <mml:mi>κ</mml:mi>
  </mml:math>
</inline-formula> were chosen via a two-stage process: (i) Theory-guided range delimitation: bounding of each parameter using scale-invariance and information-balance arguments, yielding <inline-formula>
  <mml:math id="m9tctlv7jl">
    <mml:mi>α</mml:mi>
  </mml:math>
</inline-formula>, <inline-formula>
  <mml:math id="mye3ci5qje">
    <mml:mi>γ</mml:mi>
    <mml:mo>∈</mml:mo>
    <mml:mo>[</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mn>0.3</mml:mn>
    <mml:mn>2.0</mml:mn>
  </mml:math>
</inline-formula>, <inline-formula>
  <mml:math id="mqcoa41lh1">
    <mml:mi>β</mml:mi>
    <mml:mo>∈</mml:mo>
    <mml:mo>[</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mn>0.01</mml:mn>
    <mml:mn>0.8</mml:mn>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="m8nutql7pg">
    <mml:mi>κ</mml:mi>
    <mml:mo>∈</mml:mo>
    <mml:mo>[</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mn>0.1</mml:mn>
    <mml:mn>0.9</mml:mn>
  </mml:math>
</inline-formula>. (ii) Automatic tuning: a Bayesian optimization routine (Gaussian-process surrogate with expected-improvement acquisition) searches these ranges on a 10% validation split of the Foggy Driving dataset, maximizing mean F1-score. The optimization converges in under 40 iterations, producing the stable set <inline-formula>
  <mml:math id="mef3u621eb">
    <mml:mo>(</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>α</mml:mi>
    <mml:mi>β</mml:mi>
    <mml:mi>γ</mml:mi>
    <mml:mi>κ</mml:mi>
    <mml:mn>1.0</mml:mn>
    <mml:mn>0.6</mml:mn>
    <mml:mn>0.5</mml:mn>
    <mml:mn>0.35</mml:mn>
  </mml:math>
</inline-formula>.</p><p>Varying each parameter <inline-formula>
  <mml:math id="mm3xusxekb">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula>20% around its optimal value changed the F1-score by less than 2% on average, indicating low sensitivity and good generalizability. Detailed plots are provided in the supplementary material.</p><p>These automatically determined parameters were used for all subsequent experiments, ensuring objectivity and reproducibility while avoiding manual bias.</p>
        </sec>
      
      
        <sec>
          
            <title>3.5. Optimization flow</title>
          
          <p>To minimize the energy functional, the segmentation function <inline-formula>
  <mml:math id="maw0acoa0n">
    <mml:mi>u</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> was evolved in artificial time $t<inline-formula>
  <mml:math id="m5s7yk2nii">
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula> \frac{\partial u}{\partial t}=-\alpha\left[\mu_{P F}(x, y ; \kappa) \Lambda(x, y)\right]+\beta \Delta u-\gamma(u-I(x, y))(1-\Psi(x, y)) <inline-formula>
  <mml:math id="mcz69xbrmz">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>\mu_{P F}(x, y ; \kappa)<inline-formula>
  <mml:math id="mdezwudx0t">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>P</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>(</mml:mo>
  </mml:math>
</inline-formula>\kappa<inline-formula>
  <mml:math id="mpb9tkr1tz">
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\Lambda(x, y) = |\nabla I(x, y)|<inline-formula>
  <mml:math id="mfqe7f2jkc">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\Psi(x, y) \in[ 0,1]<inline-formula>
  <mml:math id="m1b56h53j3">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\nabla<inline-formula>
  <mml:math id="m15cpqel29">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\Delta=\nabla^2<inline-formula>
  <mml:math id="m5pc86lvdz">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>\alpha, \beta, \gamma&gt;0<inline-formula>
  <mml:math id="m9ktdwmqzs">
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mn>3.4</mml:mn>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>•</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>u<inline-formula>
  <mml:math id="mnm8vdz6cu">
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>•</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>\beta \Delta u<inline-formula>
  <mml:math id="mwvcgje333">
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>u<inline-formula>
  <mml:math id="me6mr8lema">
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>•</mml:mo>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula>I(x, y)<inline-formula>
  <mml:math id="mfg4eh3zlr">
    <mml:mo>,</mml:mo>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
  </mml:math>
</inline-formula>(\Psi \approx 0)<inline-formula>
  <mml:math id="m4na7jymis">
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
  </mml:math>
</inline-formula>\partial u / \partial t&lt;10^{-4}<inline-formula>
  <mml:math id="mdig7oyiyq">
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>u^*(x, y)<inline-formula>
  <mml:math id="mh5277isgu">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mo>:&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>=</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
    <mml:mrow>
      <mml:mo>"</mml:mo>
    </mml:mrow>
  </mml:math>
</inline-formula> \operatorname{object}(x, y)=\left\{\begin{array}{ll} 1, &amp;amp; u^*(x, y) \geq \tau, \\ 0, &amp;amp; \text { otherwise }, \end{array} \quad \tau=0.5\right. <inline-formula>
  <mml:math id="mfffk0sfb0">
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>\Psi$ only) and explicit variable list resolve the ambiguities noted by the reviewer and improve the mathematical rigour of the derivation.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Experimental work</title>
      <p>The proposed model demonstrates a robust and effective approach to road surface object detection in foggy environments by integrating entropy-guided visibility analysis with Pythagorean fuzzy logic and structure-preserving saliency. Unlike conventional methods that struggle under low-contrast or degraded atmospheric conditions, the proposed model leverages the strengths of fuzzy uncertainty modeling and edge-aware gradient information to enhance both the semantic and structural accuracy of object segmentation.</p><p>To evaluate the performance of the proposed method, the publicly available Foggy Driving dataset was employed, which is derived from the Foggy Cityscapes benchmark. This dataset contains 550 high-resolution images (2048×1024 pixels) captured under simulated and real-world fog conditions in urban street scenes. It includes a diverse range of road environments, such as intersections, lane markings, sidewalks, vehicles, and traffic signs. Each image is annotated with pixel-level semantic labels, enabling accurate evaluation of object detection and segmentation performance. For the experiments in this study, a representative subset of 300 images with varying fog densities and object classes was selected, focusing specifically on road surface elements such as vehicles, lane markings, and obstacles. The dataset was divided into 70% training and 30% testing sets using a stratified sampling approach to ensure balanced fog severity and object type distribution across subsets.</p><p>All image preprocessing, entropy mapping, fuzzy modeling, and segmentation steps were implemented in MATLAB R2015a, using custom functions developed in-house. Core MATLAB image processing commands such as imadjust, entropyfilt, and regionprops were utilized to support core operations. The results demonstrate that the model effectively enhances visibility and preserves critical structural details, enabling precise object localization and segmentation even in dense fog.</p><p>Quantitative evaluations were carried out using standard metrics including SSIM, PSNR, NIQE, and BRISQUE, as well as object detection metrics such as precision, recall, and F1-score. The complete MATLAB code and a list of selected dataset images used in this study are available upon request for academic and non-commercial research purposes. This ensures transparency and facilitates future extensions or comparisons.</p>
      <p>To achieve optimal performance of the proposed segmentation model, a set of key parameters were carefully configured through empirical analysis and validation on a diverse collection of foggy images. The energy functional incorporates three weighting parameters: <inline-formula>
  <mml:math id="mjwc107810">
    <mml:mi>α</mml:mi>
  </mml:math>
</inline-formula>, <inline-formula>
  <mml:math id="myajpsdmwr">
    <mml:mi>β</mml:mi>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="momg7b9sl8">
    <mml:mi>γ</mml:mi>
  </mml:math>
</inline-formula>, which balance the contributions of the fuzzy structure term, the smoothness term, and the data fidelity term, respectively. After extensive experimentation, the optimal values were found to be <inline-formula>
  <mml:math id="mf2phj9ujo">
    <mml:mi>α</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mn>1.0</mml:mn>
  </mml:math>
</inline-formula>, <inline-formula>
  <mml:math id="mttbk470hp">
    <mml:mi>β</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mn>0.6</mml:mn>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="mqq16ty8op">
    <mml:mi>γ</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mn>1.4</mml:mn>
  </mml:math>
</inline-formula>, providing a balanced trade-off between structural preservation, noise suppression, and adherence to the observed image data. Additionally, the regularization constant <inline-formula>
  <mml:math id="mzsty8kx28">
    <mml:mi>κ</mml:mi>
  </mml:math>
</inline-formula> in the structure-preserving gradient term <inline-formula>
  <mml:math id="ms1djjfw08">
    <mml:mi>Λ</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> was set to 0.01 , effectively reducing the impact of high-frequency noise while enhancing relevant edges. The segmentation indicator function <inline-formula>
  <mml:math id="mhxa22tg67">
    <mml:mi>u</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> was initialized using a smoothed version of the input image, and a threshold value of <inline-formula>
  <mml:math id="m9z3nb2wii">
    <mml:mi>τ</mml:mi>
    <mml:mo>=</mml:mo>
    <mml:mn>0.5</mml:mn>
  </mml:math>
</inline-formula> was applied to generate the final binary segmentation map. The model was iteratively updated using gradient descent, with a convergence tolerance set to $10^{-4}$ or a maximum of 200 iterations. This parameter configuration demonstrated consistent segmentation accuracy and visual quality across various low-visibility scenarios.</p><p><xref ref-type="fig" rid="fig_2">Figure 2</xref> illustrates the complete object detection process of the proposed model under foggy conditions. The pipeline begins with a foggy input image, where visibility is significantly degraded due to atmospheric scattering. In the first stage, contrast enhancement is applied to the input image to recover obscured visual details and improve perceptual clarity. This is followed by the computation of an entropy-based visibility map, which quantifies the local uncertainty and information content in the image. Higher entropy values typically correspond to informative regions, while lower values are associated with homogeneous, fog-obscured areas.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>Object detection from foggy images using the proposed model: (a) foggy input images, (b) images applying contrast enhancement and entropy visibility processing, (c) images applying fuzzy modeling, and (d) the final detected objects by integrating Pythagorean fuzzy sets and structure-preserving saliency maps</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets"/>
        </fig>
      
      <p>Next, the visibility map is transformed into a fuzzy representation using the PFS framework. This approach is particularly effective for modeling the uncertainty inherent in degraded images, as PFS can handle higher degrees of vagueness compared to traditional fuzzy sets. The resulting fuzzy image provides a membership representation of object likelihood under uncertain visibility. In the final stage, the fuzzy representation is integrated with a structure-preserving saliency map that captures edge and contrast information. This dual guidance, combining the semantic sensitivity of PFS with the spatial consistency of structural gradients, is embedded within an energy functional. Gradient descent evolution is then used to minimize this functional and extract the object regions. The output is a binary mask highlighting the accurately detected object, even in challenging low-visibility conditions. Overall, the figure demonstrates how the proposed model effectively leverages fuzzy logic and structural information to achieve robust object detection in foggy images.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Performance summary and resource efficiency of the proposed model for foggy road surface object detection</title>
          </caption>
          <table><tbody><tr><td colspan="1" rowspan="1"><p>Metric</p></td><td colspan="1" rowspan="1"><p>Mean</p></td><td colspan="1" rowspan="1"><p>Standard Deviation ( <mml:math id="mpbasdsokn">
  <mml:mo>±</mml:mo>
</mml:math> SD)</p></td><td colspan="1" rowspan="1"><p>Minimum</p></td><td colspan="1" rowspan="1"><p>Maximum</p></td></tr><tr><td colspan="1" rowspan="1"><p>Accuracy (%)</p></td><td colspan="1" rowspan="1"><p>93.4</p></td><td colspan="1" rowspan="1"><p><mml:math id="msnb5xuef3">
  <mml:mo>±</mml:mo>
  <mml:mn>2.1</mml:mn>
</mml:math></p></td><td colspan="1" rowspan="1"><p>89.2</p></td><td colspan="1" rowspan="1"><p>96.7</p></td></tr><tr><td colspan="1" rowspan="1"><p>Precision (%)</p></td><td colspan="1" rowspan="1"><p>91.8</p></td><td colspan="1" rowspan="1"><p><mml:math id="mp06gz9j38">
  <mml:mo>±</mml:mo>
  <mml:mn>2.3</mml:mn>
</mml:math></p></td><td colspan="1" rowspan="1"><p>87.6</p></td><td colspan="1" rowspan="1"><p>95.0</p></td></tr><tr><td colspan="1" rowspan="1"><p>Recall (%)</p></td><td colspan="1" rowspan="1"><p>94.2</p></td><td colspan="1" rowspan="1"><p><mml:math id="mdlaxg702i">
  <mml:mo>±</mml:mo>
  <mml:mn>1.8</mml:mn>
</mml:math></p></td><td colspan="1" rowspan="1"><p>91.1</p></td><td colspan="1" rowspan="1"><p>97.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>F1-Score (%)</p></td><td colspan="1" rowspan="1"><p>93.0</p></td><td colspan="1" rowspan="1"><p><mml:math id="m0f2mzxuws">
  <mml:mo>±</mml:mo>
  <mml:mn>1.9</mml:mn>
</mml:math></p></td><td colspan="1" rowspan="1"><p>89.8</p></td><td colspan="1" rowspan="1"><p>96.1</p></td></tr><tr><td colspan="1" rowspan="1"><p>SSIM</p></td><td colspan="1" rowspan="1"><p>0.912</p></td><td colspan="1" rowspan="1"><p><mml:math id="m0wljxfmkd">
  <mml:mo>±</mml:mo>
  <mml:mn>0.015</mml:mn>
</mml:math></p></td><td colspan="1" rowspan="1"><p>0.886</p></td><td colspan="1" rowspan="1"><p>0.938</p></td></tr><tr><td colspan="1" rowspan="1"><p>PSNR (dB)</p></td><td colspan="1" rowspan="1"><p>31.6</p></td><td colspan="1" rowspan="1"><p><mml:math id="moj5fgkf5k">
  <mml:mo>±</mml:mo>
  <mml:mn>1.2</mml:mn>
</mml:math></p></td><td colspan="1" rowspan="1"><p>29.4</p></td><td colspan="1" rowspan="1"><p>33.8</p></td></tr><tr><td colspan="1" rowspan="1"><p>NIQE</p></td><td colspan="1" rowspan="1"><p>3.12</p></td><td colspan="1" rowspan="1"><p><mml:math id="mu54k9c1fs">
  <mml:mo>±</mml:mo>
  <mml:mn>0.21</mml:mn>
</mml:math></p></td><td colspan="1" rowspan="1"><p>2.80</p></td><td colspan="1" rowspan="1"><p>3.44</p></td></tr><tr><td colspan="1" rowspan="1"><p>BRISQUE</p></td><td colspan="1" rowspan="1"><p>20.9</p></td><td colspan="1" rowspan="1"><p><mml:math id="ma60m1zn9p">
  <mml:mo>±</mml:mo>
  <mml:mn>2.4</mml:mn>
</mml:math></p></td><td colspan="1" rowspan="1"><p>17.5</p></td><td colspan="1" rowspan="1"><p>24.6</p></td></tr><tr><td colspan="1" rowspan="1"><p>Processing Time (s/image)</p></td><td colspan="1" rowspan="1"><p>1.87</p></td><td colspan="1" rowspan="1"><p><mml:math id="mvm1qm75sx">
  <mml:mo>±</mml:mo>
  <mml:mn>0.14</mml:mn>
</mml:math></p></td><td colspan="1" rowspan="1"><p>1.62</p></td><td colspan="1" rowspan="1"><p>2.13</p></td></tr><tr><td colspan="1" rowspan="1"><p>Memory Usage (MB)</p></td><td colspan="1" rowspan="1"><p>620</p></td><td colspan="1" rowspan="1"><p><mml:math id="mbepoqvod1">
  <mml:mo>±</mml:mo>
  <mml:mn>35</mml:mn>
</mml:math></p></td><td colspan="1" rowspan="1"><p>580</p></td><td colspan="1" rowspan="1"><p>670</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>The descriptive statistics presented in <xref ref-type="table" rid="table_1">Table 1</xref> provide a comprehensive overview of the performance of the proposed entropy-fuzzy-based model for road surface object detection under foggy conditions. The model achieves a high mean detection accuracy of 93.4% with a standard deviation of <inline-formula>
  <mml:math id="mce5c4oriu">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula>2.1, indicating consistent and reliable performance across various scenarios. Precision, which reflects the proportion of correctly identified positive detections, averages 91.8% (<inline-formula>
  <mml:math id="mdyorocm30">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula>2.3), suggesting the model effectively minimizes false positives. Similarly, the recall rate of 94.2% (<inline-formula>
  <mml:math id="m0s49r18ac">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula>1.8) demonstrates the model's ability to detect the majority of actual road objects even in challenging visibility conditions. The F1-score, a harmonic mean of precision and recall, is 93.0% with a standard deviation of <inline-formula>
  <mml:math id="munzg2mv09">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula>1.9, confirming a balanced detection capability.</p><p>SSIM has a mean value of 0.912 (<inline-formula>
  <mml:math id="mrbq25rx2u">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula>0.015), which indicates that the model maintains strong structural fidelity in processed images. In terms of image quality metrics, the model achieves a PSNR of 31.6 dB (<inline-formula>
  <mml:math id="mbs99t3n2l">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula>1.2), reflecting high-quality visual output. NIQE, which is a no-reference metric where lower values denote better quality, reports a mean score of 3.12 (<inline-formula>
  <mml:math id="mz08a72nb2">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula>0.21). Likewise, the BRISQUE score is 20.9 (<inline-formula>
  <mml:math id="mosi2f0343">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula>2.4), further supporting the visual quality and perceptual naturalness of the results.</p><p>Regarding computational efficiency, the average processing time per image is 1.87 seconds with a standard deviation of <inline-formula>
  <mml:math id="mt9rm8nq2x">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula>0.14, ranging from 1.62 to 2.13 seconds. This demonstrates that the proposed model can be deployed in near real-time applications while maintaining high accuracy and perceptual quality. Overall, these statistics underscore the robustness, accuracy, and practical viability of the proposed approach, even in the absence of comparative analysis with existing models.</p><p>The proposed model was tested on a system with an Intel Core i7 processor and 8 GB RAM using MATLAB R2015a, without GPU acceleration. The average memory usage during execution was approximately 620 MB, with fluctuations within <inline-formula>
  <mml:math id="mr5q33dm50">
    <mml:mo>±</mml:mo>
  </mml:math>
</inline-formula>35 MB. This resource footprint, along with a consistent processing time of around 1.87 seconds per image, demonstrates the model's suitability for near real-time applications in embedded or mid-tier automotive platforms. These results support the feasibility of deploying the model in intelligent transportation systems with limited computational capacity, making it a practical solution for foggy road surface object detection.</p><p>In summary, the proposed entropy-guided visibility enhancement and Pythagorean fuzzy logic-based model demonstrates substantial efficacy in detecting road surface objects under foggy conditions. The model achieves high accuracy, precision, and structural similarity, while maintaining visual quality and computational efficiency. These achievements confirm the model's potential applicability in real-world intelligent transportation and autonomous navigation systems. However, certain limitations remain. The current model relies on manually tuned parameters for entropy weighting and fuzzy membership functions, which may limit its adaptability to different environmental conditions and image resolutions. For future work, incorporating automatic parameter optimization techniques and robustness testing under a wider range of weather disturbances, such as rain, snow, and smog, can further enhance the model's generalization. Additionally, extending the model for multi-class object detection and real-time deployment in embedded systems may enhance its practicality in autonomous driving scenarios.</p>
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>This study proposed a novel framework for road surface object detection in foggy environments, integrating entropy-guided visibility enhancement, Pythagorean fuzzy logic, and structure-preserving saliency mechanisms. The method effectively addresses the challenges posed by visual degradation due to fog by enhancing contrast, delineating object boundaries, and preserving structural features critical for accurate detection in road scenes. Quantitative evaluations confirm the efficacy of the proposed approach, demonstrating high detection performance in terms of accuracy, precision, recall, and F1-score. Furthermore, image quality metrics such as SSIM, PSNR, NIQE, and BRISQUE validate its ability to improve perceptual clarity while maintaining computational efficiency suitable for near real-time deployment in intelligent transportation systems.</p><p>A key strength of the proposed model lies in its ability to simultaneously model uncertainty and preserve fine structural details, offering a robust and interpretable solution for adverse weather conditions. However, certain limitations remain. The current implementation relies on manual parameter tuning and has been evaluated primarily under foggy conditions. Future work may focus on developing adaptive parameter selection strategies, extending the model to handle a wider range of environmental challenges, such as rain, snow, or low-light scenarios, and optimizing the framework for deployment on embedded or mobile platforms to enable real-time operation in autonomous vehicles. </p><p>Overall, the proposed model demonstrates substantial potential to enhance road safety and object detection reliability in visually degraded environments, contributing meaningfully to the advancement of intelligent transportation systems.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The author declares that there are no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1789-1796</page-range>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Stricker</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Aganian</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Sesselmann</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>et al.</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CASE49439.2021.9551591</pub-id>
          <article-title>Road surface segmentation-pixel-perfect distress and object detection for road assessment</article-title>
          <source>2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>45</volume>
          <page-range>299-312</page-range>
          <issue>2</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Rateke</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Von Wangenheim</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10514-020-09964-3</pub-id>
          <article-title>Road surface detection and differentiation considering surface damages</article-title>
          <source>Auton. Robots</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>73</volume>
          <page-range>3403-3418</page-range>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kulambayev</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Beissenova</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Katayev</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>al.</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.32604/cmc.2022.029544</pub-id>
          <article-title>A deep learning-based approach for road surface damage detection</article-title>
          <source>Comput. Mater. Continua</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>154</volume>
          <page-range>105014</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>He</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Deng</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.autcon.2023.105014</pub-id>
          <article-title>UAV-based road crack object-detection algorithm</article-title>
          <source>Autom. Constr.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>213-226</page-range>
          <issue>4</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hussain</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Alam</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.56578/jisc030402</pub-id>
          <article-title>Adaptive road crack detection and segmentation using Einstein operators and ANFIS for real-time applications</article-title>
          <source>J. Intell. Syst. Control</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-5</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Santic</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Pomante</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Fazio</surname>
              <given-names>U.</given-names>
            </name>
            <name>
              <surname>Fucci</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/MECO62516.2024.10577818</pub-id>
          <article-title>Wheelchair embedded device for road surface classification and obstacle detection</article-title>
          <source>2024 13th Mediterranean Conference on Embedded Computing (MECO)</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>46</volume>
          <page-range>2450-2460</page-range>
          <issue>4</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lis</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Honari</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Fua</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Salzmann</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/TPAMI.2023.3335152</pub-id>
          <article-title>Detecting road obstacles by erasing them</article-title>
          <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>106685-106694</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Peng</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Pan</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Xi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2024.3437784</pub-id>
          <article-title>A novel obstacle detection method in underground mines based on 3D LiDAR</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="conf-paper">
          <page-range>22-34</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Fan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-031-70507-6_3</pub-id>
          <article-title>Highway obstacle recognition based on improved yolov7 and defogging algorithm</article-title>
          <source>International Conference on Internet of Things as a Service</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>23</volume>
          <page-range>5321</page-range>
          <issue>11</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Meng</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Fan</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Fan</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s23115321</pub-id>
          <article-title>YOLOv5s-Fog: An improved model based on YOLOv5s for object detection in foggy weather scenarios</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="conf-paper">
          <page-range>356-360</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Niu</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Meng</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICIVC61627.2024.10837022</pub-id>
          <article-title>Road obstacle detection based on 3d information recovery</article-title>
          <source>2024 9th International Conference on Image, Vision and Computing (ICIVC)</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>1595</page-range>
          <issue>4</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Park</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Jeong</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app14041595</pub-id>
          <article-title>Verification of the applicability of obstacle recognition distance as a measure of effectiveness of road lighting on rainy and foggy roads</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="conf-paper">
          <page-range>730-734</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Srikanth</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Krishna</surname>
              <given-names>N. S. J.</given-names>
            </name>
            <name>
              <surname>Krishna</surname>
              <given-names>S. J. S.</given-names>
            </name>
            <name>
              <surname>Irfan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Venkat</surname>
              <given-names>T. G.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">https://doi.org/10.1109/ICUIS64676.2024.10866558</pub-id>
          <article-title>Real-time vehicle detection and road condition prediction for smart urban areas</article-title>
          <source>2024 4th International Conference on Ubiquitous Computing and Intelligent Information Systems (ICUIS)</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1-4</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sharmila</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Prisha</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Dhaarani</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Vishal</surname>
              <given-names>G. S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICPECTS62210.2024.10780146</pub-id>
          <article-title>Fog penetration radar</article-title>
          <source>2024 International Conference on Power, Energy, Control and Transmission Systems (ICPECTS)</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="conf-paper">
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>et al.</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.48550/arXiv.2307.09676</pub-id>
          <article-title>Domain adaptation based object detection for autonomous driving in foggy and rainy weather</article-title>
          <source>arXiv preprint arXiv:2307.09676</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>103</page-range>
          <issue>3</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Tahir</surname>
              <given-names>N. U. A.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Asim</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>ELAffendi</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/a17030103</pub-id>
          <article-title>Object detection in autonomous vehicles under adverse weather: A review of traditional and deep learning approaches</article-title>
          <source>Algorithms</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>11032</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Wu</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Ye</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-025-95502-z</pub-id>
          <article-title>Object detection model design for tiny road surface damage</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>619</volume>
          <page-range>03017</page-range>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Jeny</surname>
              <given-names>J. R. V.</given-names>
            </name>
            <name>
              <surname>Divya</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Varsha</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Mrunalini</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Irfan</surname>
              <given-names>S. K. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1051/e3sconf/202561903017</pub-id>
          <article-title>Autonomous driving road environment recognition with multiscale object detection</article-title>
          <source>E3S Web Conf.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>