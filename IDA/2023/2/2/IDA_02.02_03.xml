<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">IDA</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Information Dynamics and Applications</journal-title>
        <abbrev-journal-title abbrev-type="issn">Inf. Dyn. Appl.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">IDA</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-1494</issn>
      <issn publication-format="print">2958-1486</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-6XHE686V8DUiZAjuo6b23PpkPwo8gE6E</article-id>
      <article-id pub-id-type="doi">10.56578/ida020203</article-id>
      <title-group>
        <article-title>A Cervical Lesion Recognition Method Based on ShuffleNetV2-CA</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Liu</surname>
            <given-names>Chunhui</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9457-5563</contrib-id>
          <email>liuchs@hbu.edu.cn</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="2">2</xref>
          <name>
            <surname>Yang</surname>
            <given-names>Jiahui</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-2009-5158</contrib-id>
          <email>y3116531637@163.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="3">3</xref>
          <name>
            <surname>Liu</surname>
            <given-names>Ying</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0001-8735-3612</contrib-id>
          <email>yliu@bournemouth.ac.uk</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="2">2</xref>
          <name>
            <surname>Zhang</surname>
            <given-names>Ying</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0009-8945-7595</contrib-id>
          <email>18236913293@163.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="2">2</xref>
          <name>
            <surname>Liu</surname>
            <given-names>Shuang</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-9552-1364</contrib-id>
          <email>whlius@hbu.edu.cn</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="4">4</xref>
          <name>
            <surname>Chaikovska</surname>
            <given-names>Tetiana</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0000-2008-4270</contrib-id>
          <email>TetianaChaikovs@hotmail.com</email>
        </contrib>
        <contrib contrib-type="author">
          <xref ref-type="aff" rid="1">1</xref>
          <name>
            <surname>Liu</surname>
            <given-names>Chan</given-names>
          </name>
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-4882-7100</contrib-id>
          <email>15930242026@126.com</email>
        </contrib>
        <aff id="1">Affiliated Hospital of Hebei University, 071002 Baoding, China</aff>
        <aff id="2">College of Quality and Technical Supervision, Hebei University, 071002 Baoding, China</aff>
        <aff id="3">Bournemouth University, BH12 5BB Bournemouth, United Kingdom</aff>
        <aff id="4">Department of Medical Imaging, Clinical Infectious Diseases Hospital N1, 02154 Kryvyi Rih, Ukraine</aff>
      </contrib-group>
      <year>2023</year>
      <volume>2</volume>
      <issue>2</issue>
      <fpage>77</fpage>
      <lpage>89</lpage>
      <page-range>77-89</page-range>
      <history>
        <date date-type="received">
          <month>02</month>
          <day>01</day>
          <year>2023</year>
        </date>
        <date date-type="accepted">
          <month>04</month>
          <day>15</day>
          <year>2023</year>
        </date>
        <date date-type="pub">
          <month>05</month>
          <day>24</day>
          <year>2023</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>Â©2023 by the authors</copyright-statement>
        <copyright-year>2023</copyright-year>
        <license>. Licensee Acadlore Publishing Services Limited, Hong Kong. This article can be downloaded for free, and reused and quoted with a citation of the original published version, under the <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 license</ext-link>.</license>
      </permissions>
      <abstract>Cervical cancer is the second most common cancer among women globally. Colposcopy plays a vital role in assessing cervical intraepithelial neoplasia (CIN) and screening for cervical cancer. However, existing colposcopy methods mainly rely on physician experience, leading to misdiagnosis and limited medical resources. This study proposes a cervical lesion recognition method based on ShuffleNetV2-CA. A dataset of 6,996 cervical images was created from Hebei University Affiliated Hospital, including normal, cervical cancer, low-grade squamous intraepithelial lesions (LSIL, CIN 1), high-grade squamous intraepithelial lesions (HSIL, CIN 2/CIN 3), and cervical tumor data. Images were preprocessed using data augmentation, and the dataset was divided into training and validation sets at a 9:1 ratio during the training phase. This study introduces a coordinate attention mechanism (CA) to the original ShuffleNetV2 model, enabling the model to focus on larger areas during the image feature extraction process. Experimental results show that compared to other classic networks, the ShuffleNetV2-CA network achieves higher recognition accuracy with smaller model parameters and computation, making it suitable for resource-limited embedded devices such as mobile terminals and offering high clinical applicability.</abstract>
      <kwd-group>
        <kwd>Cervical cancer screening</kwd>
        <kwd>Colposcopy</kwd>
        <kwd>Data augmentation</kwd>
        <kwd>ShuffleNetV2</kwd>
        <kwd>Attention mechanism</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors">7</count>
        <fig-count>11</fig-count>
        <table-count>3</table-count>
        <ref-count>23</ref-count>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec disp-level="level1" sec-type="intro">
      <title>1. Introduction</title>
      <p>Cervical cancer is the fourth leading cause of death among women worldwide [<xref ref-type="bibr" rid="ref_1">1</xref>]. Statistics from the World Health Organization (WHO) demonstrate that in 2020, there were approximately 600,000 new cases globally, accounting for 7.7% of all female cancer deaths. The cure rate for early cervical cancer is high, but the lack of signs and symptoms at this stage hinders early diagnosis. Thus, a mature cervical precancerous screening program can reduce the disease's incidence and prevent numerous cervical cancer-related deaths. In high-income countries, well-organized screening programs have significantly reduced the incidence of cervical cancer, which can be attributed to the effective prevention of cervical intraepithelial neoplasia (CIN) through initial screening, colposcopy, and treatment. Cervical cancer's precancerous lesions are cervical intraepithelial neoplasms (CINs), divided into low-grade squamous intraepithelial lesions (LSIL), such as CIN1, and high-grade squamous intraepithelial lesions (HSIL), such as CIN2 and CIN3 [<xref ref-type="bibr" rid="ref_2">2</xref>]. HSIL patients require surgical treatment, while LSIL patients need conservative observation. Presently, cervical cytology testing and human papillomavirus (HPV) testing are utilized for cervical cancer screening. If abnormalities are detected during screening tests, colposcopy using acetic acid and iodine solutions is performed to identify cervical lesions. Certain manifestations under colposcopy, such as dense acetowhite epithelium, coarse mosaicism, punctuation, and atypical vessels, are considered abnormal [<xref ref-type="bibr" rid="ref_3">3</xref>]. Since these symptoms often suggest HSIL or invasive cancer, colposcopy-guided biopsy is necessary. Colposcopy requires an experienced colposcopy physician. There are significant differences in the detection rates of lesions between different colposcopy physicians. Additionally, the lack of experienced medical personnel and insufficient funding for screening systems in developing countries contribute to a severe shortage of cervical cancer screening facilities. Therefore, digital colposcopy with high-definition imaging combined with deep learning for subjective diagnosis of images can provide opportunities for image-based automated diagnosis. However, the application of artificial intelligence currently faces a series of problems, such as the scarcity of high-quality clinical data and the lack of model promotion in clinical applications. Consequently, a lightweight cervical cancer screening system based on deep learning can not only reduce the cost of early cervical cancer screening but also is suitable for deployment on limited hardware devices, aligning more with clinical application.</p><p>Deep learning is defined as a machine learning algorithm that attempts higher-level abstractions through a composition of multiple nonlinear transforms [<xref ref-type="bibr" rid="ref_4">4</xref>]. Artificial neural networks are used to implement deep learning, with the convolutional neural network (CNN) being the most representative example. CNN comprises several hidden layers, such as convolutional layers, pooling layers, and fully connected layers. Through numerous hidden layers, CNN can extract the features of the data. Some studies have employed CNN to classify colposcopy images or cervical radiographs. Buiu et al. [<xref ref-type="bibr" rid="ref_5">5</xref>] proposed an automated colposcopy image analysis framework for classifying cervical precancerous lesions and cancerous lesions. The framework is based on an ensemble of MobileNetV2 networks. The experimental results demonstrate that the method achieves an accuracy of 83.33% and 91.66% on four-class and binary classification tasks, respectively. Saini et al. [<xref ref-type="bibr" rid="ref_6">6</xref>] proposed a deep learning-based ColpoNet network using colposcopy images for cervical cancer classification. It was tested and validated on the dataset released by the National Cancer Institute of the United States and compared with other deep learning models AlexNet, VGG16, ResNet50, LeNet, and GoogleNet. The experimental analysis reveals that, compared to other state-of-the-art deep technologies, ColpoNet achieves an accuracy of 81.353% and exhibits the highest performance. Luo et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] proposed a method using multiple CNN decision feature fusion to classify and diagnose cervical lesions. First, the k-means algorithm is utilized in the data preprocessing stage to aggregate the training data into specific classes. Then, DenseNet121 and ResNet50 are fine-tuned based on transfer learning, and the XGBoost algorithm is employed to integrate the decisions of different CNNs and optimize the final prediction. The experimental results indicate that the k-means data preprocessing method can improve the training effect of the neural network and the proposed multi-decision feature fusion strategy. Park et al. [<xref ref-type="bibr" rid="ref_8">8</xref>] compared the performance of machine learning and deep learning models, using the deep learning model ResNet-50 and machine learning models XGB, SVM, and RF. The final experimental results show that in identifying cervical cancer using cervical radiographs, the ResNet-50 deep learning algorithm can provide higher performance than current machine learning models. Liu et al. [<xref ref-type="bibr" rid="ref_9">9</xref>] proposed a new non-homogeneous bilinear pooling convolutional neural network model and combined it with an attention mechanism to further enhance the network's ability to extract specific features of images. The experimental results indicate thatthe proposed network model can significantly improve the prediction accuracy of the network while maintaining computational efficiency. In recent years, constructing deeper and larger neural networks has been the main trend in the development of major visual tasks [<xref ref-type="bibr" rid="ref_9">9</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>], which requires billions of FLOPs for computation. The high cost limits the actual deployment of cervical cancer classification models. He et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] proposed a lesion attention-aware convolutional neural network (CNN) model using ResNet-50 as the convolutional backbone and self-attention mechanism to locate lesion areas in WCE images. The experimental results demonstrate that their method accurately aggregates spatial features in the global context to locate lesion attention maps in WCE images. In this study, a lightweight deep learning-based cervical precancerous lesion classification method is proposed. The main contributions of this study are as follows:</p><p>A deep learning-based method for classifying cervical precancerous lesions is proposed, with the main contributions being the development and evaluation of a novel lightweight cervical cancer screening system that can be deployed on limited hardware devices, aligning more with clinical applications.</p><p>(1) This study investigates automatic analysis methods for colposcopy medical images based on deep learning to classify different levels of cervical precancerous lesions, cervical tumors and cervical cancer.</p><p>(2) An improved deep inverted residual network with additional coordinate attention based on ShuffleNetV2 is proposed. The deep inverted residual network automatically completes the self-learning of the data feature-to-expression relationship. By introducing the coordinate attention mechanism, a larger range of feature extraction is obtained. The improved network was trained on the self-made dataset, achieving an accuracy and precision of 82.48% and 82.53% respectively.</p><p>(3) Compared with traditional residual networks, the improved network can ensure automatic feature extraction of images while reducing computational complexity and improving the computing speed of the model.</p>
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>2. Methodology</title>
      <p>The overall process of this method primarily comprises image preprocessing, network architecture, and model evaluation. Specifically, the five class labels of the training set are initially processed using a single channel. Subsequently, the training samples are scaled proportionally. An improved network is then trained with the training images enhanced by various transformations. Finally, to evaluate the classification performance, the test samples are fed into the pre-trained model, and the performance of the network is analyzed and evaluated by comparing the evaluation metrics. Each step will be discussed in detail in the following sections.</p>
      
        <sec disp-level="level2">
          
            <title>2.1. Basic network architecture</title>
          
          <p>This section systematically introduces the structural model of ShuffleNetV2 and expands the description of the channel random mixing operation and the ShuffleNetV2 unit contained in the network structure. Due to the low contrast between different levels of cervical lesions in colposcopy images, especially HSIL and LSIL lesions, the five classifications of colposcopy images present a challenging task. In order to address these challenges, a high-performance network is required to extract rich features from the images. The front-end CNN networks include MobileNet [<xref ref-type="bibr" rid="ref_12">12</xref>], [<xref ref-type="bibr" rid="ref_13">13</xref>], ShuffleNet [<xref ref-type="bibr" rid="ref_14">14</xref>], [<xref ref-type="bibr" rid="ref_15">15</xref>], VGGNet [<xref ref-type="bibr" rid="ref_16">16</xref>], GoogleNet [<xref ref-type="bibr" rid="ref_17">17</xref>], DenseNet [<xref ref-type="bibr" rid="ref_18">18</xref>], and ResNet [<xref ref-type="bibr" rid="ref_19">19</xref>]. Among them, although deep convolutional networks such as ResNet and DenseNet can significantly improve the accuracy of image classification, they increase the computational complexity. The increase in parameters makes the network deployment in clinical applications difficult. Therefore, designing a lightweight and efficient classification network is essential. In view of the excellent performance of the ShuffleNetV2 [<xref ref-type="bibr" rid="ref_14">14</xref>] model in biomedical image classification, a lightweight feature attention network based on ShuffleNetV2 is proposed.</p><p>Based on the ShuffleNetV2 unit, the overall architecture of ShuffleNetV2 is shown in <xref ref-type="fig" rid="fig_1">Figure 1</xref>. The proposed network consists of a stack of ShuffleNetV2 units grouped into three levels. The first block in each stage applies stride = 2. Other hyperparameters remain unchanged for the first stage, while the number of output channels doubles for the next stage. The number of bottleneck channels is set to 1/4 of the output channels of each ShuffleNet unit. The ShuffleNet unit mainly adopts two technologies: depth separable convolution (Depthwise Separable Convolution) and channel random mixing (channel shuffle) to substantially reduce the computational overhead while retaining the precision of the model.</p>
          
            <fig id="fig_1">
              <label>Figure 1</label>
              <caption>ShuffleNetV2 structure model</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/4/img_HuL-x8uErFj1LlWP.png"/>
            </fig>
          
          
            <sec disp-level="level3">
              
                <title>2.1. 1 Channel random mixing</title>
              
              <p>Group convolution also leads to the problem that different groups cannot share information. Therefore, in order to achieve circulation in different groups, ShuffleNet performs channel reordering operations on the output elements. <xref ref-type="fig" rid="fig_2">Figure 2</xref>(a) shows the related situation of two stacked group convolutional layers. Obviously, the output of a group is only related to the input of that group. It prevents the transmission of information between different channel groups and weakens its representation. As shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>(b), group convolution (group convolution) can obtain input information from different groups, so the input and output channels will be fully related. This process can be efficiently achieved by the channel shuffling operation, as shown in <xref ref-type="fig" rid="fig_2">Figure 2</xref>(c): assuming a convolutional layer has g groups, its output is gÃn channels; the output channel dimension is first reorganized into (g, n), then exchanged and flattened as the input of the next layer.</p>
              
                <fig id="fig_2">
                  <label>Figure 2</label>
                  <caption>Channel random mixing (channel shuffling)</caption>
                  <abstract/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/4/img_BzyZ0pLAvHUWlToq.png"/>
                </fig>
              
            </sec>
          
          
            <sec disp-level="level3">
              
                <title>2.1. 2 Shufflenetv2 unit</title>
              
              <p>Using the channel random mixing operation, a new ShuffleNetV2 specifically designed for small networks is proposed, starting from the bottleneck unit, as shown in the ShuffleNetV2 unit in <xref ref-type="fig" rid="fig_1">Figure 1</xref>. It is a residual block (residual block). The two branches are concatenated by Concat to minimize arithmetic complexity (MAC) when the number of input and output feature channels of the convolutional layer is equal, at which point the model speed is fastest. When Stride=1, the left module is used. Since the residual edge has no convolution, the width and height remain unchanged, which is mainly used to deepen the network layers. When Stride=2, the right module is used. Since the residual edge has convolution, the width and height can change, which is mainly used to compress the width and height of the feature layer for downsampling.</p><p>In order to make the classification more accurate, attention mechanisms are added to the model. The attention mechanism is one of the ways to achieve network adaptive attention. When convolutional neural networks are used to process images, it is expected that convolutional neural networks will focus on the places that need attention. At this time, attention mechanisms are needed to adaptively pay attention to important attention objects. Four different attention mechanisms are added to the model to compare and analyze the effects of improving network performance.</p>
            </sec>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>2.2. Attention mechanism</title>
          
          <p>This section presents the structures of four attention mechanisms. The attention mechanism is a technique in artificial neural networks that emulates human cognitive attention. This mechanism can automatically allocate different weights based on the importance of various parts of the feature matrix, enabling the network to pay more attention to significant local features. Currently, attention mechanisms mainly comprise the (SE) attention mechanism, CBAM attention mechanism, ECA attention mechanism, and CA attention mechanism. The SE attention mechanism can learn adaptive channel weights, allowing the model to focus more on valuable channel information. However, the SE attention mechanism only takes into account the channel dimension's attention and cannot capture the spatial dimension's attention. It is suitable for scenarios with a higher number of channels but may not perform as well as other attention mechanisms in situations with fewer channels.</p><p>The CBAM attention mechanism combines convolution and attention mechanisms, enabling it to focus on images in both spatial and channel dimensions. Nevertheless, it demands more computing resources and has a higher computational complexity. The ECA attention mechanism considers the attention of both channel and spatial dimensions simultaneously. It offers high computational efficiency for feature maps with larger sizes. However, extra calculation is necessary, which results in considerable computational overhead for smaller feature maps. CA attention mechanism not only acquires information between channels but also takes into account direction-related position information, aiding the model in better locating and recognizing targets. Additionally, it is flexible and lightweight enough to be easily integrated into the core structure of mobile networks. Ultimately, the CA attention mechanism module is incorporated into the network to more effectively allocate resources and process crucial information in the feature map.</p>
          
            <sec disp-level="level3">
              
                <title>2.2. 1 Se attention mechanism</title>
              
              <p>The SE network [<xref ref-type="bibr" rid="ref_20">20</xref>] focuses on the relationship between channels and can automatically learn the importance of different features. The structure is depicted in <xref ref-type="fig" rid="fig_3">Figure 3</xref>. Before entering the SE attention mechanism (left figure C), the importance of each channel of the feature map is the same. After passing through SENet (right colored figure C), different colors represent different weights, making the importance of each feature channel distinct. This enables the neural network to concentrate on channels with larger weights.</p>
              
                <fig id="fig_3">
                  <label>Figure 3</label>
                  <caption>SE attention mechanism</caption>
                  <abstract/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/4/img_I3egPQVcYRWiZ2ZT.png"/>
                </fig>
              
              <p>CBAM [<xref ref-type="bibr" rid="ref_21">21</xref>] combines channel attention mechanism and spatial attention mechanism, which can achieve improved results compared to SENet, which only focuses on the channel attention mechanism. Its implementation is illustrated in the schematic diagram. CBAM processes the input feature layer separately with the channel attention mechanism and the spatial attention mechanism.</p><p>The specific implementation of the channel attention mechanism and spatial attention mechanism in the CBAM attention mechanism is shown in <xref ref-type="fig" rid="fig_4">Figure 4</xref>. The upper half of the image is the channel attention mechanism. The implementation of the channel attention mechanism can be divided into two parts. Global average pooling and global maximum pooling are performed on the input single feature layer respectively. After processing the results of average pooling and maximum pooling with a shared fully connected layer, the two results are added and a sigmoid is applied, obtaining the weight value of each channel of the input feature layer (between 0-1). After obtaining this weight, this weight is multiplied by the original input feature layer.</p><p>The lower half of the image is the spatial attention mechanism. The maximum value and average value of each channel at each feature point of the input feature layer are taken. Then these two results are stacked, the number of channels is adjusted with a 1-channel convolution, and a sigmoid is applied, obtaining the weight value of each feature point of the input feature layer (between 0-1). After obtaining this weight, this weight is multiplied by the original input feature layer.</p>
              
                <fig id="fig_4">
                  <label>Figure 4</label>
                  <caption>CBAM attention mechanism</caption>
                  <abstract/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/4/img_EdPcB4YyX3bZHb2j.png"/>
                </fig>
              
            </sec>
          
          
            <sec disp-level="level3">
              
                <title>2.2. 2 Eca feature extraction module</title>
              
              <p>ECA [<xref ref-type="bibr" rid="ref_22">22</xref>] is also an implementation of the channel attention mechanism, which can be considered as an improved version of SE. Its implementation is illustrated in <xref ref-type="fig" rid="fig_5">Figure 5</xref>. The ECA module replaces two fully connected layers with one-dimensional convolutions. The authors of ECA argue that capturing the dependence of all channels in the SE module is inefficient and unnecessary. Convolution has a strong ability to obtain cross-channel information. </p><p>Therefore, the ECA module removes the original fully connected layer in the SE module and directly learns on the globally averaged pooled features through a one-dimensional convolution. This module avoids dimension reduction and effectively captures cross-channel interactions with few parameters. The size of the convolution kernel is adaptively changed through a function, allowing layers with more channels to interact more across channels. The adaptive function is shown in Eq. (1), where <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>Î³</mi>
  </math>
</inline-formula>=2 and b=1.</p>
              
                <disp-formula>
                  <label>(1)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>K</mi>
                    <mo>=</mo>
                    <mrow data-mjx-texclass="INNER">
                      <mo data-mjx-texclass="OPEN">|</mo>
                      <mo>+</mo>
                      <mo data-mjx-texclass="CLOSE">|</mo>
                      <mfrac>
                        <mrow>
                          <msub>
                            <mi>log</mi>
                            <mn>2</mn>
                          </msub>
                          <mo data-mjx-texclass="NONE">â¡</mo>
                          <mo stretchy="false">(</mo>
                          <mo stretchy="false">)</mo>
                          <mi>c</mi>
                        </mrow>
                        <mi>Î³</mi>
                      </mfrac>
                      <mfrac>
                        <mi>b</mi>
                        <mi>Î³</mi>
                      </mfrac>
                    </mrow>
                  </math>
                </disp-formula>
              
              
                <fig id="fig_5">
                  <label>Figure 5</label>
                  <caption>ECA feature extraction module</caption>
                  <abstract/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/4/img_uG3boDdUHO_ykDA3.png"/>
                </fig>
              
            </sec>
          
          
            <sec disp-level="level3">
              
                <title>2.2. 3 Ca feature extraction module</title>
              
              <p>CA [<xref ref-type="bibr" rid="ref_23">23</xref>] is an efficient coordinate attention mechanism for mobile devices, which can consider both channel relationship and long-distance position information. It can effectively improve the accuracy of the model with little additional computation. The CA module encodes channel relationships and long-range dependencies through precise position information in two steps: coordinate information embedding and coordinate attention generation. Its specific structure is shown in <xref ref-type="fig" rid="fig_6">Figure 6</xref>.</p>
              
                <fig id="fig_6">
                  <label>Figure 6</label>
                  <caption> CA feature extraction module</caption>
                  <abstract/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/4/img_vJJ8WkqLZzhowF5k.png"/>
                </fig>
              
              <p>Coordinate information embedding: The input feature map of CÃHÃW shape is averaged pooled channel by channel. Using pooling kernels of (H,1) and (1,W) respectively along the X and Y axes for each channel encoding, generating CÃHÃ1 and CÃ1ÃW shaped feature maps. The output of the c-th channel at height h is shown in Eq. (2):</p>
              
                <disp-formula>
                  <label>(2)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msubsup>
                      <mi>Z</mi>
                      <mi>c</mi>
                      <mi>h</mi>
                    </msubsup>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo>,</mo>
                    <mo stretchy="false">)</mo>
                    <mi>h</mi>
                    <mi>h</mi>
                    <mi>i</mi>
                    <mfrac>
                      <mn>1</mn>
                      <mi>W</mi>
                    </mfrac>
                    <munder>
                      <mo data-mjx-texclass="OP">â</mo>
                      <mrow data-mjx-texclass="ORD">
                        <mn>0</mn>
                        <mo>â¤</mo>
                        <mo>â¤</mo>
                        <mi>i</mi>
                        <mi>w</mi>
                      </mrow>
                    </munder>
                    <msub>
                      <mi>x</mi>
                      <mi>c</mi>
                    </msub>
                  </math>
                </disp-formula>
              
              <p>The output of the c-th channel with width W is shown in Eq. (3):</p>
              
                <disp-formula>
                  <label>(3)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msubsup>
                      <mi>Z</mi>
                      <mi>c</mi>
                      <mi>w</mi>
                    </msubsup>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo>,</mo>
                    <mo stretchy="false">)</mo>
                    <mi>w</mi>
                    <mi>w</mi>
                    <mi>i</mi>
                    <mfrac>
                      <mn>1</mn>
                      <mi>H</mi>
                    </mfrac>
                    <munder>
                      <mo data-mjx-texclass="OP">â</mo>
                      <mrow data-mjx-texclass="ORD">
                        <mn>0</mn>
                        <mo>â¤</mo>
                        <mo>â¤</mo>
                        <mi>i</mi>
                        <mi>w</mi>
                      </mrow>
                    </munder>
                    <msub>
                      <mi>x</mi>
                      <mi>c</mi>
                    </msub>
                  </math>
                </disp-formula>
              
              <p>The above two transformations aggregate features along two different directions to obtain a pair of directional perception directional and position-sensitive feature maps. This is different from SE, which generates a single feature vector in the channel attention method. This can capture long-range dependencies along one spatial direction and retain precise position information along the other spatial direction. It helps the network accurately locate targets of interest.</p><p>The coordinate attention generation stage: After the transformation in the information embedding, this part performs a concatenate operation on the above transformations, and then uses a 1*1 convolution transformation function <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>F</mi>
      <mn>1</mn>
    </msub>
  </math>
</inline-formula>,</p>
              
                <disp-formula>
                  <label>(4)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mi>Î´</mi>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">[</mo>
                    <mo>,</mo>
                    <mo stretchy="false">]</mo>
                    <mo stretchy="false">)</mo>
                    <mo stretchy="false">)</mo>
                    <msub>
                      <mi>F</mi>
                      <mn>1</mn>
                    </msub>
                    <msup>
                      <mi>z</mi>
                      <mi>h</mi>
                    </msup>
                    <msup>
                      <mi>z</mi>
                      <mi>W</mi>
                    </msup>
                  </math>
                </disp-formula>
              
              <p>where, [,] is the concat operation along the spatial dimension, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <mi>Î´</mi>
  </math>
</inline-formula> is a nonlinear activation function, and f is an intermediate feature mapping that encodes spatial information in the horizontal and vertical directions. Using another two 1*1 convolution transformation functions <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>F</mi>
      <mn>1</mn>
    </msub>
  </math>
</inline-formula>, <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>f</mi>
      <mi>h</mi>
    </msub>
  </math>
</inline-formula> and <inline-formula>
  <math xmlns="http://www.w3.org/1998/Math/MathML">
    <msub>
      <mi>f</mi>
      <mi>w</mi>
    </msub>
  </math>
</inline-formula> are transformed into tensors $X$ with the same number of channels, to obtain:</p>
              
                <disp-formula>
                  <label>(5)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msup>
                      <mi>g</mi>
                      <mi>h</mi>
                    </msup>
                    <msup>
                      <mi>f</mi>
                      <mi>h</mi>
                    </msup>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo stretchy="false">)</mo>
                    <mi>Ï</mi>
                    <msub>
                      <mi>F</mi>
                      <mi>h</mi>
                    </msub>
                  </math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(6)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msup>
                      <mi>g</mi>
                      <mi>w</mi>
                    </msup>
                    <msup>
                      <mi>f</mi>
                      <mi>w</mi>
                    </msup>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo stretchy="false">)</mo>
                    <mi>Ï</mi>
                    <msub>
                      <mi>F</mi>
                      <mi>w</mi>
                    </msub>
                  </math>
                </disp-formula>
              
              <p>Output Y:</p>
              
                <disp-formula>
                  <label>(7)</label>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <msub>
                      <mi>y</mi>
                      <mi>c</mi>
                    </msub>
                    <msub>
                      <mi>x</mi>
                      <mi>c</mi>
                    </msub>
                    <mo stretchy="false">(</mo>
                    <mo>,</mo>
                    <mo stretchy="false">)</mo>
                    <mo>=</mo>
                    <mo stretchy="false">(</mo>
                    <mo>,</mo>
                    <mo stretchy="false">)</mo>
                    <mo>â</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mo>â</mo>
                    <mo stretchy="false">(</mo>
                    <mo stretchy="false">)</mo>
                    <mi>i</mi>
                    <mi>j</mi>
                    <mi>i</mi>
                    <mi>j</mi>
                    <mi>i</mi>
                    <mi>j</mi>
                    <msubsup>
                      <mi>g</mi>
                      <mi>c</mi>
                      <mi>h</mi>
                    </msubsup>
                    <msubsup>
                      <mi>g</mi>
                      <mi>c</mi>
                      <mi>w</mi>
                    </msubsup>
                  </math>
                </disp-formula>
              
              <p>After inserting SE, CBAM, ECA, and CA channel attention into the depth separable convolution of ShuffleNetV2 respectively, the effects of four feature attention mechanisms on network performance were compared and analyzed. The experimental structure shows that compared with the module integrated with SE, CBAM, and ECA attention mechanisms, the accuracy, precision, and floating point operations (FLOPs) of the network integrated with the CA module are 82.48%, 82.53%, and 0.157G, respectively. The accuracy is the highest while the FLOPs is the lowest. Finally, ShuffleNetV2-CA network is selected. The inverted residual network structure model integrated with the CA module is shown in <xref ref-type="fig" rid="fig_7">Figure 7</xref>.</p>
              
                <fig id="fig_7">
                  <label>Figure 7</label>
                  <caption>ShuffleNetV2-CA unit</caption>
                  <abstract/>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/4/img_JRQXhJc52kSfd_Qj.png"/>
                </fig>
              
            </sec>
          
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>3. Data source and processing</title>
      <p>High-quality datasets are crucial for cervical cancer identification tasks based on deep learning. To complete the five-category task of hysteroscopy, our experiment was conducted on a dataset of 1189 cases of vaginal speculum (Leisegang vaginal speculum 3ML) examinations from Hebei University Affiliated Hospital from July 2019 to February 2023. All hysteroscopy images were taken as part of routine clinical practice for patients. No exclusion criteria based on age or ethnicity were used. Each case contained 5 consecutive images of acetic acid white test with sequence labels, images taken with a green lens, and images taken after application of compound iodine solution. Vaginal speculum images 2 minutes after acetic acid application were selected, labeled by experienced gynecologists, and finally 6996 vaginal speculum images of different stages of precancerous lesions (normal, CIN 1 and CIN 2/3), cervical tumors and cervical cancer were generated using image enhancement techniques.</p><p>The steps to obtain preprocessed images of five categories are as follows: First, each original image was cleaned by manually cropping to retain information about the cervix. Second, the original images were classified into five categories according to the case report information, as shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>, namely cervical cancer, low-grade squamous intraepithelial lesion (LSIL), high-grade squamous intraepithelial lesion (HSIL) and cervical tumors. Then, the classified data were checked and adjusted by experienced doctors. Afterwards, as is well known, convolutional neural networks require the same image size to work properly. Therefore, we uniformly adjusted the training set to a size of 224 Ã 224 by affine transformation, which can keep the original aspect ratio of the input image unchanged and avoid changes in the size of the traditional convolutional neural network training set. Finally, due to the uneven distribution of the datasets provided in each category and the small number of samples, we used random cropping, random horizontal and vertical flipping data augmentation techniques to create more available data and improve the overall safety of the trained model.</p><p>For the five-category cervical dataset, we selected vaginal speculum images 2 minutes after saline application. Before training the proposed network, the dataset was split 9:1. Specifically, to ensure consistent results from multiple runs of our network, these images were grouped separately according to the five types in the dataset so that cervical images of all categories could be sampled for training and testing. The distribution of cervical image data is shown in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>Distribution of cervical dataset</caption>
          <abstract/>
          <table><tr><th text-center>Category</th><th text-center>Data amount</th><th text-center>Training set</th><th text-center>Validation set</th></tr><tr><td text-center>Normal</td><td text-center>2352</td><td text-center>2117</td><td text-center>235</td></tr><tr><td text-center>Low-grade squamous intraepithelial lesion (LSIL)</td><td text-center>780</td><td text-center>702</td><td text-center>78</td></tr><tr><td text-center>High-grade squamous intraepithelial lesion (HSIL)</td><td text-center>2532</td><td text-center>2279</td><td text-center>253</td></tr><tr><td text-center>Cervical cancer</td><td text-center>408</td><td text-center>367</td><td text-center>41</td></tr><tr><td text-center>Cervical tumor</td><td text-center>924</td><td text-center>832</td><td text-center>92</td></tr></table>
        </table-wrap>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>4. Experiments</title>
      <p>In this section, we performed extensive experiments to test and evaluate the feature representation ability of the proposed method. Specifically, the description of the dataset, evaluation indicators, implementation details and comparative experiments are the main contents discussed in the following sections.</p>
      
        <sec disp-level="level2">
          
            <title>4.1. Evaluation indicators</title>
          
          <p>To effectively evaluate the algorithm, training loss and model accuracy were used as metrics during the training phase. In the testing phase, this study introduces a confusion matrix as the basic evaluation criterion. The four parts of information in the confusion matrix include: true negatives (TN), indicating the number of negative samples predicted as negative; true positives (TP), indicating the number of positive samples predicted as positive; false negatives (FN), indicating the number of positive samples predicted as negative; and false positives (FP), indicating the number of negative samples predicted as positive.</p><p>For the multi-classification cervical dataset, accuracy, precision, recall, F1 score, and floating point operations (FLOPs) were used to evaluate the classification performance of the network. FLOPs was used to measure the complexity of the model. The lower the FLOPs, the easier it is to port the model to mobile devices. The formulas for classification accuracy, precision, recall, and F1 are Eqns.(8)-(11).</p>
          
            <disp-formula>
              <label>(8)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>Accurary</mi>
                <mi mathvariant="normal">%</mi>
                <mo stretchy="false">(</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <mo>Ã</mo>
                <mfrac>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                    <mi>T</mi>
                    <mi>N</mi>
                    <mo>+</mo>
                  </mrow>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                    <mi>F</mi>
                    <mi>P</mi>
                    <mi>T</mi>
                    <mi>N</mi>
                    <mi>F</mi>
                    <mi>N</mi>
                    <mo>+</mo>
                    <mo>+</mo>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
                <mn>100</mn>
              </math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(9)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>Precision</mi>
                <mi mathvariant="normal">%</mi>
                <mo stretchy="false">(</mo>
                <mo stretchy="false">)</mo>
                <mo>Ã</mo>
                <mfrac>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                  </mrow>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                    <mi>F</mi>
                    <mi>P</mi>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
                <mn>100</mn>
              </math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(10)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>Recall</mi>
                <mi mathvariant="normal">%</mi>
                <mo stretchy="false">(</mo>
                <mo stretchy="false">)</mo>
                <mo>Ã</mo>
                <mfrac>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                  </mrow>
                  <mrow>
                    <mi>T</mi>
                    <mi>P</mi>
                    <mi>F</mi>
                    <mi>N</mi>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
                <mn>100</mn>
              </math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(11)</label>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mi>F</mi>
                <mi>score</mi>
                <mi mathvariant="normal">%</mi>
                <mn>1</mn>
                <mn>100</mn>
                <mo>â</mo>
                <mo stretchy="false">(</mo>
                <mo stretchy="false">)</mo>
                <mo>=</mo>
                <mo>Ã</mo>
                <mfrac>
                  <mrow>
                    <mn>2</mn>
                    <mo>Ã</mo>
                    <mo>Ã</mo>
                    <mtext>Â RecallÂ </mtext>
                    <mtext>Â precisionÂ </mtext>
                  </mrow>
                  <mrow>
                    <mtext>Â RecallÂ </mtext>
                    <mtext>Â PrecisionÂ </mtext>
                    <mo>+</mo>
                  </mrow>
                </mfrac>
              </math>
            </disp-formula>
          
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>4.2. Implementation details</title>
          
          <p>To ensure iteration efficiency, improve model stability and generalization ability, the Stochastic Gradient Descent (SGD) algorithm was used to optimize network parameters. The Nesterov gradient descent method was used with a weight decay of 1e-4, a momentum of 0.9, and a batch size of 32. Each model was trained for 100 epochs, and the initial learning rate was set to 0.05. The CNN algorithm was implemented in the PyTorch coding framework. IntelÂ® XeonÂ® Gold 6240 CPU@2.60 GHz and NVIDIA RTX 2080 ti GPU were used for model training and evaluation. All programs were run on Ubuntu 18.04.5 LTS.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>4.3. The influence of attention mechanism on model performance</title>
          
          <p>To verify the improvement effect of the CA attention module compared with other attention modules on the model, comparative experiments were conducted under the same experimental conditions by replacing the CA module used in this study with the SE attention module and CBAM attention module. The network model parameters and identification results are shown in <xref ref-type="table" rid="table_2">Table 2</xref>. The performance comparison of different attention modules is shown in <xref ref-type="fig" rid="fig_8">Figure 8</xref>, and the floating point operations of different attention mechanisms are shown in <xref ref-type="fig" rid="fig_9">Figure 9</xref>.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>Performance verification test results of different attention modules</caption>
              <abstract/>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Attention mechanism</p></th><th colspan="1" rowspan="1"><p>Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Precision (%)</p></th><th colspan="1" rowspan="1"><p>Recall (%)</p></th><th colspan="1" rowspan="1"><p>F1- Score (%)</p></th><th colspan="1" rowspan="1"><p>FLOPs(G)</p></th></tr><tr><td colspan="1" rowspan="1"><p>ShuffleNetV2--SE</p></td><td colspan="1" rowspan="1"><p>81.38</p></td><td colspan="1" rowspan="1"><p>81.76</p></td><td colspan="1" rowspan="1"><p>80.74</p></td><td colspan="1" rowspan="1"><p>81.16</p></td><td colspan="1" rowspan="1"><p>0.1830</p></td></tr><tr><td colspan="1" rowspan="1"><p>ShuffleNetV2-ECA</p></td><td colspan="1" rowspan="1"><p>81.18</p></td><td colspan="1" rowspan="1"><p>81.46</p></td><td colspan="1" rowspan="1"><p>81.34</p></td><td colspan="1" rowspan="1"><p>81.39</p></td><td colspan="1" rowspan="1"><p>0.1642</p></td></tr><tr><td colspan="1" rowspan="1"><p>ShuffleNetV2-CBAM</p></td><td colspan="1" rowspan="1"><p>81.91</p></td><td colspan="1" rowspan="1"><p>81.91</p></td><td colspan="1" rowspan="1"><p>81.56</p></td><td colspan="1" rowspan="1"><p>81.73</p></td><td colspan="1" rowspan="1"><p>0.1597</p></td></tr><tr><td colspan="1" rowspan="1"><p>ShuffleNetV2-CA</p></td><td colspan="1" rowspan="1"><p>82.48</p></td><td colspan="1" rowspan="1"><p>82.53</p></td><td colspan="1" rowspan="1"><p>82.36</p></td><td colspan="1" rowspan="1"><p>82.44</p></td><td colspan="1" rowspan="1"><p>0.1570</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <fig id="fig_8">
              <label>Figure 8</label>
              <caption>Performance comparison of different attention modules</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/4/img__2BXNGaFITNpvl-Y.png"/>
            </fig>
          
          
            <fig id="fig_9">
              <label>Figure 9</label>
              <caption>Floating point operations of different attention mechanisms</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/4/img_dEy_SNW-8fPXiCIm.png"/>
            </fig>
          
          <p>As can be seen from <xref ref-type="table" rid="table_2">Table 2</xref>, compared with other attention modules, the CA attention module added in this study achieved an Accuracy, Precision, Recall, and F1-Score of 82.48%, 82.53%, 82.36%, and 82.44%, respectively. It can also be intuitively seen from <xref ref-type="fig" rid="fig_10">Figure 10</xref> that ShuffleNetV2-CA performs best. <xref ref-type="fig" rid="fig_11">Figure 11</xref> shows that it has the lowest floating point operations, indicating that the complexity of the model is the smallest and most suitable for embedding in mobile devices. The CA attention mechanism can not only consider attention in the channel dimension and spatial dimension at the same time, but also learn adaptive channel weights to make the model pay more attention to useful channel information. This helps the model learn the interaction and dependence between the information of the characteristic channels of cervical lesions, further improving the recognition performance of the model. However, it cannot capture long-range dependencies. These features make it simple and lightweight enough for practical use, while being able to utilize the extracted position information.</p>
        </sec>
      
      
        <sec disp-level="level2">
          
            <title>4.4. Model comparison</title>
          
          <p>To evaluate the effectiveness of the designed model in classifying cervical cancer, VGG-16, ResNet 34, GoogleNet, DenseNet 121, and ShuffleNet were selected for comparison based on the competitiveness of the model. To compare the results more confidently, all models used the dataset in this study and were trained in the same training environment. As shown in <xref ref-type="table" rid="table_3">Table 3</xref>, this study compared the accuracy, precision, recall, and F1 scores.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>Performance comparison of different network models</caption>
              <abstract/>
              <table><tbody><tr><th colspan="1" rowspan="1"><p>Model</p></th><th colspan="1" rowspan="1"><p>Accuracy (%)</p></th><th colspan="1" rowspan="1"><p>Precision (%)</p></th><th colspan="1" rowspan="1"><p>Recall (%)</p></th><th colspan="1" rowspan="1"><p>F1- Score (%)</p></th><th colspan="1" rowspan="1"><p>FLOPs(G)</p></th></tr><tr><td colspan="1" rowspan="1"><p>VGG-16</p></td><td colspan="1" rowspan="1"><p>50.72</p></td><td colspan="1" rowspan="1"><p>45.63</p></td><td colspan="1" rowspan="1"><p>45.67</p></td><td colspan="1" rowspan="1"><p>45.07</p></td><td colspan="1" rowspan="1"><p>15.47</p></td></tr><tr><td colspan="1" rowspan="1"><p>ResNet34</p></td><td colspan="1" rowspan="1"><p>83.95</p></td><td colspan="1" rowspan="1"><p>84.88</p></td><td colspan="1" rowspan="1"><p>81.28</p></td><td colspan="1" rowspan="1"><p>82.81</p></td><td colspan="1" rowspan="1"><p>6.428</p></td></tr><tr><td colspan="1" rowspan="1"><p>GoogleNet</p></td><td colspan="1" rowspan="1"><p>53.72</p></td><td colspan="1" rowspan="1"><p>47.43</p></td><td colspan="1" rowspan="1"><p>51.73</p></td><td colspan="1" rowspan="1"><p>45.09</p></td><td colspan="1" rowspan="1"><p>4.737</p></td></tr><tr><td colspan="1" rowspan="1"><p>DenseNet121</p></td><td colspan="1" rowspan="1"><p>86.39</p></td><td colspan="1" rowspan="1"><p>87.00</p></td><td colspan="1" rowspan="1"><p>83.95</p></td><td colspan="1" rowspan="1"><p>85.17</p></td><td colspan="1" rowspan="1"><p>2.869</p></td></tr><tr><td colspan="1" rowspan="1"><p>MobileNet</p></td><td colspan="1" rowspan="1"><p>54.30</p></td><td colspan="1" rowspan="1"><p>65.12</p></td><td colspan="1" rowspan="1"><p>44.60</p></td><td colspan="1" rowspan="1"><p>43.45</p></td><td colspan="1" rowspan="1"><p>0.1664</p></td></tr><tr><td colspan="1" rowspan="1"><p>ShuffleNetV2</p></td><td colspan="1" rowspan="1"><p>80.37</p></td><td colspan="1" rowspan="1"><p>79.90</p></td><td colspan="1" rowspan="1"><p>79.42</p></td><td colspan="1" rowspan="1"><p>79.60</p></td><td colspan="1" rowspan="1"><p>0.1568</p></td></tr><tr><td colspan="1" rowspan="1"><p>ShuffleNetV2-CA</p></td><td colspan="1" rowspan="1"><p>82.48</p></td><td colspan="1" rowspan="1"><p>82.53</p></td><td colspan="1" rowspan="1"><p>82.36</p></td><td colspan="1" rowspan="1"><p>82.44</p></td><td colspan="1" rowspan="1"><p>0.1570</p></td></tr></tbody></table>
            </table-wrap>
          
          
            <fig id="fig_10">
              <label>Figure 10</label>
              <caption>Accuracy comparison of different models</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/4/img_z9TBrynj4DCdtQl3.png"/>
            </fig>
          
          
            <fig id="fig_11">
              <label>Figure 11</label>
              <caption>FLOPs comparison of different models</caption>
              <abstract/>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2023/4/img_Smds4aiOwnLo5MwK.png"/>
            </fig>
          
          <p>As can be seen from <xref ref-type="table" rid="table_3">Table 3</xref>, different network architectures have different effects on the classification of the cervical dataset. The accuracy of the improved network ShuffleNetV2-CA reached 82.48%, slightly lower than ResNet34's 83.95% and DenseNet121's 86.39%, but the FLOPs of ShuffleNetV2-CA were much smaller than other networks. This is due to the use of grouped convolution in this network to reduce the number of parameters and channel reordering operations to enhance the interaction and fusion between different channels. Therefore, the network selected in this study achieved a good balance between recognition accuracy and model size, which can meet the requirements of mobile devices and other computing power limited devices for cervical lesion identification. <xref ref-type="fig" rid="fig_10">Figure 10</xref> and <xref ref-type="fig" rid="fig_11">Figure 11</xref> also visually show that the improved network FLOPs are greatly reduced compared with traditional classification networks, and the classification accuracy is relatively high. Compared with the unimproved ShuffleNet, FLOPs increased slightly, but network performance improved significantly, and Accuracy increased by 2.11%. Therefore, the network selected in this study achieved a good balance between recognition accuracy and model size, which can meet the requirements of mobile devices and other computing power limited devices for cervical lesion identification.</p>
        </sec>
      
    </sec>
    <sec disp-level="level1" sec-type="">
      <title>5. Conclusion</title>
      <p>For the identification of cervical lesions, this study took 5 different types of vaginal speculum images as the research object. A dataset of 6996 cervical lesion identification images was constructed from images collected from the hospital. On the basis of the ShuffleNetV2 model structure, this study embedded the CA attention module to adaptively extract channel information important for target identification and obtained the following conclusions: Compared with the modules embedded with SE, CBAM, and ECA attention mechanisms, the improved network in this study achieved an Accuracy, Precision, Recall, and F1-Score of 82.48%, 82.53%, 82.36%, and 82.44%, respectively, with the highest network performance and the lowest FLOPs of 0.157G. Compared with other classic VGG-16, ResNet 34, GoogleNet, DenseNet 121, and ShuffleNetV2 networks, ShuffleNetV2-CA obtained better classification effects while maintaining high resolution in feature extraction, smaller model parameters, and balanced the relationship between recognition accuracy and floating point operations. This is conducive to deploying the convolutional neural network model on mobile terminals and other embedded resource-constrained devices to meet real-time needs and has high clinical application value. In addition, this study has limitations in the use of data diversity in datasets and does not make full use of different types of data in each case. At the same time, hardware deployment has not been implemented. Therefore, in the future, while continuing to expand high-quality datasets, multimodal data can be used to improve the accuracy of the model, and mobile hardware deployment can be achieved.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      <p>This study was supported by Baoding Science and Technology Planning Project (Grant No.: 2141ZF306 and 2141ZF135) and Youth Foundation of Affiliated Hospital of Hebei University (Grant No.: 2022QC54).</p>
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>20</volume>
          <page-range>319-321</page-range>
          <issue>3</issue>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Marc</given-names>
              <surname>Brisson</surname>
            </name>
            <name>
              <given-names>MÃ©lanie</given-names>
              <surname>Drolet</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/S1470-2045(19)30072-5</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Global elimination of cervical cancer as a public health problem</article-title>
          <source>Lancet Oncol.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>130</volume>
          <page-range>104209</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Gengyou</given-names>
              <surname>Peng</surname>
            </name>
            <name>
              <given-names>Hua</given-names>
              <surname>Dong</surname>
            </name>
            <name>
              <given-names>Tong</given-names>
              <surname>Liang</surname>
            </name>
            <name>
              <given-names>Ling</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>Jun</given-names>
              <surname>Liu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compbiomed.2021.104209</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Diagnosis of cervical precancerous lesions based on multimodal feature changes</article-title>
          <source>Comput. Biol. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>468</page-range>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Seongmin</given-names>
              <surname>Kim</surname>
            </name>
            <name>
              <given-names>Hwajung</given-names>
              <surname>Lee</surname>
            </name>
            <name>
              <given-names>Sanghoon</given-names>
              <surname>Lee</surname>
            </name>
            <name>
              <given-names>Jae Yun</given-names>
              <surname>Song</surname>
            </name>
            <name>
              <given-names>Jae Kwan</given-names>
              <surname>Lee</surname>
            </name>
            <name>
              <given-names>Nak Woo</given-names>
              <surname>Lee</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/healthcare10030468</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Role of artificial intelligence interpretation of colposcopic images in cervical cancer screening</article-title>
          <source>Healthcare</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range/>
          <issue/>
          <year>2008</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Jerome D</given-names>
              <surname>Waye</surname>
            </name>
            <name>
              <given-names>Douglas K</given-names>
              <surname>Rex</surname>
            </name>
            <name>
              <given-names>Christopher B</given-names>
              <surname>Williams</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>Colonoscopy: Principles and Practice</article-title>
          <source/>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>595</page-range>
          <issue>5</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>CÄtÄlin</given-names>
              <surname>Buiu</surname>
            </name>
            <name>
              <given-names>Vlad RareÅ</given-names>
              <surname>DÄnÄilÄ</surname>
            </name>
            <name>
              <given-names>Cristina Nicoleta</given-names>
              <surname>RÄduÅ£Ä</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/pr8050595</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>MobileNetV2 ensemble for cervical precancerous lesions classification</article-title>
          <source>Processes</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>31</volume>
          <page-range>15</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Sumindar Kaur</given-names>
              <surname>Saini</surname>
            </name>
            <name>
              <given-names>Vasudha</given-names>
              <surname>Bansal</surname>
            </name>
            <name>
              <given-names>Ravinder</given-names>
              <surname>Kaur</surname>
            </name>
            <name>
              <given-names>Mamta</given-names>
              <surname>Juneja</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s00138-020-01063-8</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>ColpoNet for automated cervical cancer screening using colposcopy images</article-title>
          <source>Mach. Vision. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>8</volume>
          <page-range>29616-29626</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Yan Min</given-names>
              <surname>Luo</surname>
            </name>
            <name>
              <given-names>Tao</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>Ping</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>Pei Zhong</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>Pengming</given-names>
              <surname>Sun</surname>
            </name>
            <name>
              <given-names>Binhua</given-names>
              <surname>Dong</surname>
            </name>
            <name>
              <given-names>Guanyu</given-names>
              <surname>Ruan</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2020.2972610</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>MDFI: Multi-CNN decision feature integration for diagnosis of cervical precancerous lesions</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>16143</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Ye Rang</given-names>
              <surname>Park</surname>
            </name>
            <name>
              <given-names>Young Jae</given-names>
              <surname>Kim</surname>
            </name>
            <name>
              <given-names>Woong</given-names>
              <surname>Ju</surname>
            </name>
            <name>
              <given-names>Kyehyun</given-names>
              <surname>Nam</surname>
            </name>
            <name>
              <given-names>Soonyung</given-names>
              <surname>Kim</surname>
            </name>
            <name>
              <given-names>Kwang Gi</given-names>
              <surname>Kim</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-021-95748-3</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Comparison of machine and deep learning for the classification of cervical cancer based on cervicography images</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>23</volume>
          <page-range>816</page-range>
          <issue>7</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Pingping</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>Xiaokang</given-names>
              <surname>Yang</surname>
            </name>
            <name>
              <given-names>Baixin</given-names>
              <surname>Jin</surname>
            </name>
            <name>
              <given-names>Qiuzhan</given-names>
              <surname>Zhou</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/e23070816</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Diabetic retinal grading using attention-based bilinear convolutional neural network and complement cross entropy</article-title>
          <source>Entropy</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>42</volume>
          <page-range>157-168</page-range>
          <issue>2</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Prabhananthakumar</given-names>
              <surname>Muruganantham</surname>
            </name>
            <name>
              <given-names>Senthil Murugan</given-names>
              <surname>Balakrishnan</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s40846-022-00686-8</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Attention aware deep learning model for wireless capsule endoscopy lesion classification and localization</article-title>
          <source>J. Med. Biol. Eng.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range>770-778</page-range>
          <issue/>
          <year>2016</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Kaiming</given-names>
              <surname>He</surname>
            </name>
            <name>
              <given-names>Xiangyu</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>Shaoqing</given-names>
              <surname>Ren</surname>
            </name>
            <name>
              <given-names>Jian</given-names>
              <surname>Sun</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Deep residual learning for image recognition</article-title>
          <source>In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range/>
          <issue/>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Andrew G</given-names>
              <surname>Howard</surname>
            </name>
            <name>
              <given-names>Menglong</given-names>
              <surname>Zhu</surname>
            </name>
            <name>
              <given-names>Bo</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>Dmitry</given-names>
              <surname>Kalenichenko</surname>
            </name>
            <name>
              <given-names>Weijun</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>Tobias</given-names>
              <surname>Weyand</surname>
            </name>
            <name>
              <given-names>Marco</given-names>
              <surname>Andreetto</surname>
            </name>
            <name>
              <given-names>Hartwig</given-names>
              <surname>Adam</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title/>
          <source>arXiv preprint arXiv:1704.04861</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="conf-paper">
          <volume/>
          <page-range/>
          <issue/>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <given-names>A.</given-names>
              <surname>Howard</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Sandler</surname>
            </name>
            <name>
              <given-names>B.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>W.</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>L.C.</given-names>
              <surname>Chen</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Tan</surname>
            </name>
            <name>
              <given-names>G.</given-names>
              <surname>Chu</surname>
            </name>
            <name>
              <given-names>V.</given-names>
              <surname>Vasudevan</surname>
            </name>
            <name>
              <given-names>Y.</given-names>
              <surname>Zhu</surname>
            </name>
            <name>
              <given-names>R.</given-names>
              <surname>Pang</surname>
            </name>
            <name>
              <given-names>H.</given-names>
              <surname>Adam</surname>
            </name>
            <name>
              <given-names>Q.</given-names>
              <surname>Le</surname>
            </name>
          </person-group>
          <person-group person-group-type="editor"/>
          <pub-id pub-id-type="doi">10.1109/iccv.2019.00140</pub-id>
          <article-title>Searching for MobileNetV3</article-title>
          <source>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range>116-131</page-range>
          <issue/>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Ningning</given-names>
              <surname>Ma</surname>
            </name>
            <name>
              <given-names>Xiangyu</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>Hai-Tao</given-names>
              <surname>Zheng</surname>
            </name>
            <name>
              <given-names>Jian</given-names>
              <surname>Sun</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-030-01264-9_8</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Shufflenet v2: Practical guidelines for efficient cnn architecture design</article-title>
          <source>In Proceedings of the European conference on computer vision (ECCV), Munich, Germany</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range>6848-6856</page-range>
          <issue/>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>X.</given-names>
              <surname>Zhang</surname>
            </name>
            <name>
              <given-names>X.</given-names>
              <surname>Zhou</surname>
            </name>
            <name>
              <given-names>M.</given-names>
              <surname>Lin</surname>
            </name>
            <name>
              <given-names>J.</given-names>
              <surname>Sun</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CVPR.2018.00716</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>ShuffleNet: An extremely efficient convolutional neural network for mobile devices</article-title>
          <source>In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Lake City, UT, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range/>
          <issue/>
          <year>2014</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Karen</given-names>
              <surname>Simonyan</surname>
            </name>
            <name>
              <given-names>Andrew</given-names>
              <surname>Zisserman</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi"/>
          <pub-id pub-id-type="publisher"/>
          <article-title>Very deep convolutional networks for large-scale image recognition</article-title>
          <source>arXiv preprint arXiv:1409.1556</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range>1-9</page-range>
          <issue/>
          <year>2015</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Christian</given-names>
              <surname>Szegedy</surname>
            </name>
            <name>
              <given-names>Wei</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>Yangqing</given-names>
              <surname>Jia</surname>
            </name>
            <name>
              <given-names>Pierre</given-names>
              <surname>Sermanet</surname>
            </name>
            <name>
              <given-names>Scott</given-names>
              <surname>Reed</surname>
            </name>
            <name>
              <given-names>Dragomir</given-names>
              <surname>Anguelov</surname>
            </name>
            <name>
              <given-names>Dumitru</given-names>
              <surname>Erhan</surname>
            </name>
            <name>
              <given-names>Vincent</given-names>
              <surname>Vanhoucke</surname>
            </name>
            <name>
              <given-names>Andrew</given-names>
              <surname>Rabinovich</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CVPR.2015.7298594</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Going deeper with convolutions</article-title>
          <source>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Boston, MA, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range>4700-4708</page-range>
          <issue/>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Gao</given-names>
              <surname>Huang</surname>
            </name>
            <name>
              <given-names>Zhuang</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>Laurens</given-names>
              <surname>Van Der Maaten</surname>
            </name>
            <name>
              <given-names>Kilian Q</given-names>
              <surname>Weinberger</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CVPR.2017.243</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Densely connected convolutional networks</article-title>
          <source>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Honolulu, HI, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range>19-34</page-range>
          <issue/>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Chenxi</given-names>
              <surname>Liu</surname>
            </name>
            <name>
              <given-names>Barret</given-names>
              <surname>Zoph</surname>
            </name>
            <name>
              <given-names>Maxim</given-names>
              <surname>Neumann</surname>
            </name>
            <name>
              <given-names>Jonathon</given-names>
              <surname>Shlens</surname>
            </name>
            <name>
              <given-names>Wei</given-names>
              <surname>Hua</surname>
            </name>
            <name>
              <given-names>Li-Jia</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>Li</given-names>
              <surname>Fei-Fei</surname>
            </name>
            <name>
              <given-names>Alan</given-names>
              <surname>Yuille</surname>
            </name>
            <name>
              <given-names>Jonathan</given-names>
              <surname>Huang</surname>
            </name>
            <name>
              <given-names>Kevin</given-names>
              <surname>Murphy</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-030-01246-5_2</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Progressive neural architecture search</article-title>
          <source>In Proceedings of the European conference on computer vision (ECCV), Munich, Germany</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range>7132-7141</page-range>
          <issue/>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Jie</given-names>
              <surname>In Hu</surname>
            </name>
            <name>
              <given-names>Li</given-names>
              <surname>Shen</surname>
            </name>
            <name>
              <given-names>Gang</given-names>
              <surname>Sun</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CVPR.2018.00745</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Squeeze-and-excitation networks</article-title>
          <source>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Lake City, UT, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range>3-19</page-range>
          <issue/>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Sanghyun</given-names>
              <surname>Woo</surname>
            </name>
            <name>
              <given-names>Jongchan</given-names>
              <surname>Park</surname>
            </name>
            <name>
              <given-names>Joon-Young</given-names>
              <surname>Lee</surname>
            </name>
            <name>
              <given-names>In So</given-names>
              <surname>Kweon</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/978-3-030-01234-2_1</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Cbam: Convolutional block attention module</article-title>
          <source>In Proceedings of the European conference on computer vision (ECCV), Munich, Germany</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range>11534-11542</page-range>
          <issue/>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Qilong</given-names>
              <surname>Wang</surname>
            </name>
            <name>
              <given-names>Banggu</given-names>
              <surname>Wu</surname>
            </name>
            <name>
              <given-names>Pengfei</given-names>
              <surname>Zhu</surname>
            </name>
            <name>
              <given-names>Peihua</given-names>
              <surname>Li</surname>
            </name>
            <name>
              <given-names>Wangmeng</given-names>
              <surname>Zuo</surname>
            </name>
            <name>
              <given-names>Qinghua</given-names>
              <surname>Hu</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CVPR42600.2020.01155</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>ECA-Net: Efficient channel attention for deep convolutional neural networks</article-title>
          <source>In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume/>
          <page-range>13713-13722</page-range>
          <issue/>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <given-names>Qibin</given-names>
              <surname>Hou</surname>
            </name>
            <name>
              <given-names>Daquan</given-names>
              <surname>Zhou</surname>
            </name>
            <name>
              <given-names>Jiashi</given-names>
              <surname>Feng</surname>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/CVPR46437.2021.01350</pub-id>
          <pub-id pub-id-type="publisher"/>
          <article-title>Coordinate attention for efficient mobile network design</article-title>
          <source>In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Nashville, TN, USA</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>