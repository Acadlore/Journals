<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-vBdD9wbJWdZ52vnu-3EUXe-ns7TRYh8R</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml030203</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Advanced Dental Implant System Classification with Pre-trained CNN Models and Multi-branch Spectral Channel Attention Networks</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-5876-0117</contrib-id>
          <name>
            <surname>Vemula</surname>
            <given-names>Srinivasa Rao</given-names>
          </name>
          <email>srinivas.vemula@fisglobal.com</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0003-3503-7321</contrib-id>
          <name>
            <surname>Vemula</surname>
            <given-names>Maruthi</given-names>
          </name>
          <email>vemula24m@ncssm.edu</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_3">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2611-4925</contrib-id>
          <name>
            <surname>Vatambeti</surname>
            <given-names>Ramesh</given-names>
          </name>
          <email>amesh.v@vitap.ac.in</email>
        </contrib>
        <aff id="aff_1">FIS Management Services, 27705 Durham, North Carolina, USA</aff>
        <aff id="aff_2">North Carolina School of Science and Mathematics, 27705 Durham, North Carolina, USA</aff>
        <aff id="aff_3">School of Computer Science and Engineering, VIT-AP University, 522237 Vijayawada, India</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>15</day>
        <month>05</month>
        <year>2024</year>
      </pub-date>
      <volume>3</volume>
      <issue>2</issue>
      <fpage>94</fpage>
      <lpage>105</lpage>
      <page-range>94-105</page-range>
      <history>
        <date date-type="received">
          <day>21</day>
          <month>02</month>
          <year>2024</year>
        </date>
        <date date-type="accepted">
          <day>02</day>
          <month>05</month>
          <year>2024</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2024 by the author(s)</copyright-statement>
        <copyright-year>2024</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Dental implants (DIs) are prone to failure due to uncommon mechanical complications and fractures. Precise identification of implant fixture systems from periapical radiographs is imperative for accurate diagnosis and treatment, particularly in the absence of comprehensive medical records. Existing methods predominantly leverage spatial features derived from implant images using convolutional neural networks (CNNs). However, texture images exhibit distinctive patterns detectable as strong energy at specific frequencies in the frequency domain, a characteristic that motivates this study to employ frequency-domain analysis through a novel multi-branch spectral channel attention network (MBSCAN). High-frequency data obtained via a two-dimensional (2D) discrete cosine transform (DCT) are exploited to retain phase information and broaden the application of frequency-domain attention mechanisms. Fine-tuning of the multi-branch spectral channel attention (MBSCA) parameters is achieved through the modified aquila optimizer (MAO) algorithm, optimizing classification accuracy. Furthermore, pre-trained CNN architectures such as Visual Geometry Group (VGG) 16 and VGG19 are harnessed to extract features for classifying intact and fractured DIs from panoramic and periapical radiographs. The dataset comprises 251 radiographic images of intact DIs and 194 images of fractured DIs, meticulously selected from a pool of 21,398 DIs examined across two dental facilities. The proposed model has exhibited robust accuracy in detecting and classifying fractured DIs, particularly when relying exclusively on periapical images. The MBSCA-MAO scheme has demonstrated exceptional performance, achieving a classification accuracy of 95.7% with precision, recall, and F1-score values of 95.2%, 94.3%, and 95.6%, respectively. Comparative analysis indicates that the proposed model significantly surpasses existing methods, showcasing its superior efficacy in DI classification.</p></abstract>
      <kwd-group>
        <kwd>Dental implant</kwd>
        <kwd>Modified aquila optimizer algorithm</kwd>
        <kwd>Multi-branch spectral channel attention network</kwd>
        <kwd>Texture complication</kwd>
        <kwd>Convolutional neural network</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="3"/>
        <fig-count count="4"/>
        <table-count count="3"/>
        <ref-count count="30"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>The ability to place DIs has revolutionized dentistry and significantly benefited patients worldwide [<xref ref-type="bibr" rid="ref_1">1</xref>]. Technological advancements reduce the likelihood of adverse alveolar bone conditions and improve the long-term prognosis, leading to increased usage of DIs [<xref ref-type="bibr" rid="ref_2">2</xref>]. Before implant placement, surgeons address alveolar bone atrophy by designing implants with various shapes and textures, such as threading and platforms, to enhance alveolar ridge augmentation and sinus lift surgery. The demand for DIs has attracted numerous manufacturers, resulting in over 220 different brands of implants on the market since 2000, with this number continuing to rise [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>].</p><p>Categorizing implant brands is challenging due to the wide variety of styles, structures, and associated tools, including fasteners, abutments, and superstructures. Implant maintenance, such as retightening to prevent loosening, is directly influenced by the manufacturer's unique screws used to fix prostheses [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>]. Identifying the brand of implant is crucial, especially when different dentists may use different implants and screws for the same patient, and the types of implants used may vary over time [<xref ref-type="bibr" rid="ref_7">7</xref>]. Panoramic radiography can provide comprehensive data on the jawbone and teeth in a single image, offering the potential to identify the brand of a patient's implant. However, manual identification requires significant human skill and effort [<xref ref-type="bibr" rid="ref_8">8</xref>].</p><p>Recent advancements in deep learning (DL) and neural network technologies, particularly deep CNNs (DCNNs), have shown promise in detecting and classifying various medical conditions, including bone fractures. While studies on DI fractures are lacking, efforts have been made to increase accuracy in detecting other dental fractures using radiographs [<xref ref-type="bibr" rid="ref_9">9</xref>], [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>].</p><p>Recent advances in computer vision and machine learning (ML) have opened up new possibilities for enhancing the diagnostic capabilities of dental practitioners [<xref ref-type="bibr" rid="ref_12">12</xref>]. In particular, CNNs have shown remarkable success in various image classification tasks, including medical imaging. By leveraging the hierarchical representation learning capabilities of CNNs, it is possible to automatically extract relevant features from radiographic images of DIs, enabling more accurate and efficient classification of implant failures [<xref ref-type="bibr" rid="ref_13">13</xref>].</p><p>However, traditional CNN-based approaches typically focus on spatial domain characteristics, which may not fully capture the underlying structural and textural information present in DI images. Digital signal processing theory suggests that images contain valuable frequency domain information, which can provide additional insights into the underlying patterns and structures. By analyzing the frequency content of DI images, it may be possible to uncover hidden features that are not readily apparent in the spatial domain [<xref ref-type="bibr" rid="ref_14">14</xref>].</p><p>Therefore, this study aims to evaluate the validity and reliability of DI fracture identification and classification using panoramic and periapical radiography images with two distinct architectures. Features of the input images were extracted using VGG16 and VGG19 and classified using MBSCA. The MAO model was employed to properly adjust the MBSCA parameters. The remaining sections of the study include a review of relevant literature in Section 2, details of the data sources in Section 3, and a presentation of the proposed model in Section 4. Finally, Section 6 concludes the research, while Section 5 discusses the analysis of results.</p>
    </sec>
    <sec sec-type="">
      <title>2. Related works</title>
      <p>Yang et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] proposed a two-stream implant position regression network (MSPENet) to address the challenge of accurately classifying implant fixture systems using periapical radiographs. By augmenting initial annotations with supervisory data for implant region detection (IRD) training, richer characteristics were incorporated without additional labeling expenses. A multi-scale patch embedding module was developed within the MSPENet to adaptively extract features from images with varying tooth spacing. The MSPENet encoder, integrating transformer and convolution for enhanced feature representation, utilized a global-local feature interaction block. Additionally, the region of interest (RoI) mask obtained from the IRD was employed to enhance prediction outcomes. Experimental trials on a DI dataset using five-fold cross-validation demonstrated that the proposed TSIPR model outperformed state-of-the-art techniques.</p><p>Ramachandran et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] introduced a state-of-the-art prediction method utilizing ML models to classify DI materials and predict potential mechanical deterioration. Important parameters examined included corrosion potential and acoustic emission (AE) weight-loss estimations, with a particular focus on pure alloys. With ML prototype models achieving over 90% accuracy, the proposed approach validated its viability for predicting tribocorrosion, demonstrating its potential as a reliable predictive modeling method for DI monitoring.</p><p>Rekawek et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] developed a ML model to maximize the success rate of DIs by predicting implant failure and peri-implantitis development. Utilizing ensemble methods and logistic classifiers on retrospective data from 398 patients and 942 DIs, the random forest model outperformed others in predictive performance. Significant factors associated with implant failure included local anesthetic dose, implant length and diameter, antibiotic usage before surgery, and hygiene visit frequency. Similarly, factors correlated with peri-implantitis included diabetes mellitus, hygiene visit frequency, implant characteristics, and antibiotic usage.</p><p>Park et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] evaluated the effectiveness of an automated DL algorithm in classifying different DI systems (DIS) using a large-scale multicenter dataset. After analyzing panoramic and periapical radiographs from various facilities, the DL algorithm achieved high classification accuracy, demonstrating reliable performance in DIS classification across extensive datasets without noticeable variations between periapical and panoramic images.</p><p>Park et al. [<xref ref-type="bibr" rid="ref_19">19</xref>] compared two artificial intelligence (AI) algorithms for DI length categorization using periapical radiographs, employing DL and clustering analysis. Both AI models demonstrated reliable classification performance, with statistically significant improvements observed after fine-tuning. The study highlights the potential clinical utility of AI models in DI length categorization validated across multiple centers.</p><p>Chen et al. [<xref ref-type="bibr" rid="ref_20">20</xref>] proposed a novel method for assessing peri-implantitis damage utilizing periapical films (PA) and CNN models. With its high accuracy in implant localization and peri-implantitis damage assessment, the CNN-based method offers potential for precise evaluation of peri-implantitis damage, aiding in implant dentistry and patient care.</p><p>Park et al. [<xref ref-type="bibr" rid="ref_21">21</xref>] assessed the effectiveness of DL in identifying and classifying different DI schemes using a large dataset of panoramic and periapical radiographs. DL demonstrated reliable classification accuracy, outperforming both specialized and nonspecialized dental experts in categorizing DIS encountered in clinical practice. Additionally, DL showed efficiency in reading and categorization time compared to dental experts, suggesting its potential as an effective decision support tool in dental implantology.</p>
    </sec>
    <sec sec-type="">
      <title>3. Materials and methods</title>
      <p>The Institutional Review Boards reviewed and authorized the study design and protocol; informed or written consent was obtained [<xref ref-type="bibr" rid="ref_22">22</xref>]. All procedures in this study followed the Strengthening the Reporting of Observational Studies in Epidemiology (STROBE) recommendations for the journalism and execution of experimental studies [<xref ref-type="bibr" rid="ref_23">23</xref>], [<xref ref-type="bibr" rid="ref_24">24</xref>] and the updated Declaration of Helsinki.</p>
      
        <sec>
          
            <title>3.1. Dataset</title>
          
          <p>Retrospective data was collected at the Veterans Health Service Medical Center (VHSMC) starting in January 2019. This research involved the analysis of clinical photographs and digital radiography images of dental structures by two board-certified clinical practitioners. The aim was to identify a total of 21,398 DIs distributed across 7,281 patients. Three dental practitioners referenced in the study were responsible for excluding radiographic images exhibiting excessive noise, haziness, or distortion, particularly those obtained through the conventional paralleling technique in periapical radiography. Subsequently, a periodontist named JHL categorized the remaining images based on their respective anatomical regions, which were of two main types: panoramic (1,402 images) and periapical. Ultimately, 251 DIs were classified as intact, while 198 were identified as fractured, forming the core dataset for this investigation. A prior study examining fracture patterns facilitated the classification of fractured DIs into three distinct types, with Type I denoting horizontal fractures confined within and around the crestal module, Type II representing vertical fractures, and Type III encompassing horizontal fractures. However, due to the limited occurrence of Type III fractured DIs (n=4) within the datasets, this study exclusively focused on Type I and Type II fractures. Detailed information regarding all panoramic and periapical images of each DI, regardless of their fracture status, along with associated characteristics and quantities, is presented in <xref ref-type="table" rid="table_1">Table 1</xref>.</p>
          
            <table-wrap id="table_1">
              <label>Table 1</label>
              <caption>
                <title>Various panoramic and periapical images and DIs (Source: Daejeon Dental Hospital at Wonkwang University and VHSMC.)</title>
              </caption>
              <table><tbody><tr><td colspan="3" rowspan="1"><p>Dataset</p></td></tr><tr><td colspan="1" rowspan="1"></td><td colspan="1" rowspan="1"><p>Frequency</p></td><td colspan="1" rowspan="1"><p>Percentage (%)</p></td></tr><tr><td colspan="3" rowspan="1"><p>Intact DIs</p></td></tr><tr><td colspan="1" rowspan="1"><p>Panoramic imageries</p></td><td colspan="1" rowspan="1"><p>110</p></td><td colspan="1" rowspan="1"><p>43.9</p></td></tr><tr><td colspan="3" rowspan="1"><p>Fractured DIs, type I</p></td></tr><tr><td colspan="1" rowspan="1"><p>Panoramic imageries</p></td><td colspan="1" rowspan="1"><p>42</p></td><td colspan="1" rowspan="1"><p>48.9</p></td></tr><tr><td colspan="1" rowspan="1"><p>Periapical imageries</p></td><td colspan="1" rowspan="1"><p>43</p></td><td colspan="1" rowspan="1"><p>51.2</p></td></tr><tr><td colspan="3" rowspan="1"><p>Fractured DIs, type II</p></td></tr><tr><td colspan="1" rowspan="1"><p>Periapical images</p></td><td colspan="1" rowspan="1"><p>58</p></td><td colspan="1" rowspan="1"><p>52.7</p></td></tr></tbody></table>
            </table-wrap>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Pre-processing</title>
          
          <p>The radiographic pictures used in the study were scaled to 224×224 pixels for the VGG19 construction and 299×299 pixels for the VGG16 architecture. For model construction and accuracy performance predictions, the dataset was randomly divided into three parts: training, validation, and test. Each part received 60% of the total. Pixel normalization is part of the pre-processing, and one-hot encoding was used to decrease dataset abnormalities. 100 instances of rotation (with a range of 30°) were randomly added, along with width and height shifting (with a variety of 0.2), zooming and flipping to the training dataset. The validation and test datasets did not undergo any sort of augmentation operation.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Proposed methodology</title>
      
        <sec>
          
            <title>4.1. Feature extraction using pre-trained cnns</title>
          
          <p>The study employed an elementary CNN architecture featuring three instances of the VGG19 model. Leveraging transfer learning methodologies, both the VGG16 and VGG19 architectures underwent fine-tuning and training processes. Notably, the VGG16 model exhibits a profound layer depth, comprising 13 convolutional layers and three fully connected layers, as detailed in prior research [<xref ref-type="bibr" rid="ref_25">25</xref>]. Trained on an extensive dataset of over 1 million images spanning 1,000 classes, with iterative optimization exceeding 370,000 iterations, these models demonstrate a robust capacity for feature extraction and classification. VGG19, recognized for its prowess in image recognition tasks, notably secured first place in the classification and localization division of the 2014 Large Scale Visual Recognition Challenge. Comprising 19 weighted layers, 16 of which are convolutional layers, and organized into five distinct groups by max pooling layers [<xref ref-type="bibr" rid="ref_26">26</xref>], VGG19 encapsulates a deep hierarchical representation of image features. In the experimentation phase, the learning rates were meticulously configured at 0.0001 for the fundamental CNN model, while the fine-tuning process for the VGG16/19 models also adhered to the same learning rate setting. Convolutional layers apply filters (also known as kernels) to the input data to extract features. Each filter slides across the input image, computing the dot product between the filter weights and the values in the receptive field, generating feature maps that highlight specific patterns such as edges, textures, or shapes.</p><p>The research used a total of five CNN study groups:</p><p>• A CNN model at its most fundamental level, consisting of six convolutional layers.</p><p>• A VGG16 model that learned from its pre-trained weights (VGG16 transfer).</p><p>• A VGG16 model that learned from its pre-trained weights was then fine-tuned (VGG16 fine-tuning).</p><p>• VGG19 transfer, a method of learning a model using pre-trained weights.</p><p>• Fine-tuning the VGG19 model using pre-trained weights and transfer learning.</p><p>Following each convolutional layer, a rectified linear unit (ReLU) activation function is commonly applied element-wise to introduce non-linearity into the network. ReLU sets all negative values in the feature maps to zero, enabling the network to learn complex relationships between features. Momentum stochastic gradient descent (SGD) was utilized for optimizing the four VGG models, whereas Adam was employed for the basic CNN. Transfer learning was used for dataset training, with the training dataset randomly divided into 128 batches for each epoch, and a maximum of 700 iterations (epochs) determined by the validation loss behavior. The present method's efficacy was assessed using fourfold cross-validation. This cross-validation procedure ensures generalization and helps avoid overfitting. Every architecture, including the basic CNN, VGG16/19 transfer, and tuning, underwent this procedure for both training and evaluation. The Keras library (https://keras.io) and the TensorFlow engine were used for building, training, and predicting deep-learning representations.</p>
          
            <sec>
              
                <title>4.1.1 Model visualization</title>
              
              <p>The most important elements utilized for categorization may be better understood with the aid of CNN model visualization. Using gradient-weighted class activation [<xref ref-type="bibr" rid="ref_27">27</xref>], the most important pixels for classification in the picture were determined, which allowed for both finding possible accurate classifications based on wrong characteristics and obtaining a better understanding of the classification process. Map representations are heatmaps of the gradients, where the locations of higher relevance for feature extraction are represented by the “hotter” hues. This work used the last convolutional layer to rebuild the gradient-weighted class activation mapping (Grad-CAM) heat map.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>4.2. Classification using mbscan</title>
          
          <p>The MBSCAN refers to a sophisticated DL architecture designed for image analysis tasks. It incorporates multiple branches, each equipped with spectral channel attention mechanisms, which dynamically adapt the network's focus to relevant image features. Spectral channel attention enhances the network's ability to selectively attend to informative channels across different spectral frequencies, thereby facilitating robust feature extraction and representation learning. Through its multi-branch design, MBSCAN harnesses diverse pathways to capture intricate spatial and spectral dependencies within the input data, leading to enhanced performance in tasks such as image classification, object detection, and semantic segmentation. This advanced network architecture holds promise for various applications in computer vision and medical imaging, where precise feature discrimination and context-aware analysis are paramount.</p><p>For the dental image classification challenge, the study begins by presenting the overall structure of the proposed MBSCAN. Following this, a detailed illustration of the MBSCAN's main module of multi-branch spectral is provided.</p>
          
            <sec>
              
                <title>4.2.1 General structure</title>
              
              <p>MBSCAN, which can be trained effectively for rapid inference, is based on Residual Network (ResNet) 18, the fundamental network used in the study. The MBSCAN is composed of MBSCAs and ResNet18 fundamental blocks for each module. The topological structure of a basic block remains unchanged when the MBSCA is added into its end. Because there aren't enough dental photos to train a network from beginning, this approach lets us reuse ResNet18's weights. High- and low-frequency data interact more favorably with the network. The network can learn more important information and less useless information via the channel attention technique. It is suggested to utilize channel features of the input images, because texture images generally concentrate their primary information in the low-frequency domain. The attention module requires minimal additional parameters, offering the benefits of enhanced feature operations with a small increase in computational complexity. Furthermore, MBSCAN's modularity allows easy integration, simply by adjusting the number of output channels to incorporate the attention module while retaining channel structures consistent with preceding layers.</p>
            </sec>
          
          
            <sec>
              
                <title>4.2.2 Mbsca unit</title>
              
              <p>In reality, the feature extraction model ignores all data over a certain frequency and only uses data below that frequency. Despite the fact that the model delves into the many 2D DCT frequency components, each frequency component is only utilized to depict a portion of a channel in a feature map. There is no modeling of the more realistic and deserving individual channels represented by numerous frequency components. Hence, to address this restriction, the multi-attention module is suggested.</p><p>In the MBSCA, distinct branches pay attention to distinct aspects of the input based on their frequency of occurrence. Generally speaking, such a branch can do cross-latitude interactive computation in a variety of ways, including channel attention, spatial attention, and others. This study aims to accomplish spectral channel attention by repeating the model's comparable computing procedure. Any two branches can record distinct frequency components since each branch uses its own unique component. The multi-branch structure allows for the exploration of many frequency components, which solves the problem of partial use of the image's frequency information. This allows for the realization of interaction between the frequency components. Both structures are redesigned by the MBSCA. The input <inline-formula>
  <mml:math id="m3zreh0ym0">
    <mml:mi>X</mml:mi>
    <mml:mo>∈</mml:mo>
    <mml:msup>
      <mml:mi>R</mml:mi>
      <mml:mrow>
        <mml:mi>H</mml:mi>
        <mml:mi>W</mml:mi>
        <mml:mi>C</mml:mi>
        <mml:mo>×</mml:mo>
        <mml:mo>×</mml:mo>
      </mml:mrow>
    </mml:msup>
  </mml:math>
</inline-formula> is denoted as <inline-formula>
  <mml:math id="ml12ndsilb">
    <mml:mrow>
      <mml:mo>{</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>…</mml:mo>
      <mml:mo>,</mml:mo>
      <mml:mo>}</mml:mo>
      <mml:msub>
        <mml:mi>X</mml:mi>
        <mml:mn>0</mml:mn>
      </mml:msub>
      <mml:msub>
        <mml:mi>X</mml:mi>
        <mml:mrow>
          <mml:mi>K</mml:mi>
          <mml:mo>−</mml:mo>
          <mml:mn>1</mml:mn>
        </mml:mrow>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula>. In each subdivision, consistent 2D assignments for the input <inline-formula>
  <mml:math id="m4buqqfl65">
    <mml:msub>
      <mml:mi>X</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> are expressed as:</p>
              
                <disp-formula>
                  <label>(1)</label>
                  <mml:math id="m56q0vihav">
                    <mml:mrow>
                      <mml:mi>F</mml:mi>
                      <mml:mi>r</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:msub>
                        <mml:mi>q</mml:mi>
                        <mml:mi>k</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>X</mml:mi>
                        <mml:mi>k</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mn>2</mml:mn>
                    <mml:mi>D</mml:mi>
                    <mml:mi>D</mml:mi>
                    <mml:mi>C</mml:mi>
                    <mml:msub>
                      <mml:mi>T</mml:mi>
                      <mml:mrow>
                        <mml:mi>Ω</mml:mi>
                        <mml:mrow>
                          <mml:mi>k</mml:mi>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>X</mml:mi>
                      <mml:mi>k</mml:mi>
                    </mml:msub>
                    <mml:munderover>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mi>h</mml:mi>
                        <mml:mo>+</mml:mo>
                        <mml:mn>0</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>H</mml:mi>
                        <mml:mo>−</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:munderover>
                    <mml:munderover>
                      <mml:mo>∑</mml:mo>
                      <mml:mrow>
                        <mml:mi>w</mml:mi>
                        <mml:mo>=</mml:mo>
                        <mml:mn>0</mml:mn>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>W</mml:mi>
                        <mml:mo>−</mml:mo>
                        <mml:mn>1</mml:mn>
                      </mml:mrow>
                    </mml:munderover>
                    <mml:msubsup>
                      <mml:mi>B</mml:mi>
                      <mml:mrow>
                        <mml:mi>h</mml:mi>
                        <mml:mi>w</mml:mi>
                        <mml:mo>,</mml:mo>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>Ω</mml:mi>
                        <mml:mrow>
                          <mml:mi>k</mml:mi>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:msubsup>
                  </mml:math>
                </disp-formula>
              
              <p>s.t. <inline-formula>
  <mml:math id="mfreynyjwp">
    <mml:mrow>
      <mml:mi>k</mml:mi>
    </mml:mrow>
    <mml:mo>∈</mml:mo>
    <mml:mo fence="false">{</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>…</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>−</mml:mo>
    <mml:mo fence="false">}</mml:mo>
    <mml:mn>0</mml:mn>
    <mml:mn>1</mml:mn>
    <mml:mn>1</mml:mn>
    <mml:mi>K</mml:mi>
  </mml:math>
</inline-formula>, where, <inline-formula>
  <mml:math id="myu3h9y2dl">
    <mml:mi>F</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:msub>
      <mml:mi>q</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
    <mml:mo>∈</mml:mo>
    <mml:msup>
      <mml:mi>R</mml:mi>
      <mml:mi>c</mml:mi>
    </mml:msup>
  </mml:math>
</inline-formula> is the $k<inline-formula>
  <mml:math id="mmzzyjcujo">
    <mml:mo>−</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>Freq_k \in R^c<inline-formula>
  <mml:math id="mku8jskc5q">
    <mml:mo>=</mml:mo>
    <mml:mi>compression</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>X</mml:mi>
        <mml:mi>k</mml:mi>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula>. The 2D DCT corresponds to <inline-formula>
  <mml:math id="mwbagz9qjq">
    <mml:msub>
      <mml:mi>X</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>. <inline-formula>
  <mml:math id="mg7yfdyb88">
    <mml:mi>Ω</mml:mi>
    <mml:mrow>
      <mml:mi>k</mml:mi>
    </mml:mrow>
  </mml:math>
</inline-formula> represents the $2 \mathrm{D}<inline-formula>
  <mml:math id="muubnga9a4">
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>F r e q_k$ and were scaled subsequently.</p>
              
                <disp-formula>
                  <label>(2)</label>
                  <mml:math id="msc876716s">
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>a</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>m</mml:mi>
                      </mml:mrow>
                      <mml:mi>k</mml:mi>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mi>s</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>g</mml:mi>
                      <mml:mi>m</mml:mi>
                      <mml:mi>o</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>d</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo fence="true"/>
                      <mml:mi>f</mml:mi>
                      <mml:mi>c</mml:mi>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo fence="true"/>
                      </mml:mrow>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>F</mml:mi>
                      <mml:mi>r</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>q</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo fence="true"/>
                      <mml:mo>)</mml:mo>
                      <mml:mrow>
                        <mml:mo fence="true"/>
                        <mml:mo>)</mml:mo>
                        <mml:msub>
                          <mml:mrow/>
                          <mml:mi>k</mml:mi>
                        </mml:msub>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(3)</label>
                  <mml:math id="mwecyhqfk9">
                    <mml:msub>
                      <mml:mrow>
                        <mml:mover>
                          <mml:mi>X</mml:mi>
                          <mml:mo>¯</mml:mo>
                        </mml:mover>
                      </mml:mrow>
                      <mml:mi>k</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>a</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>t</mml:mi>
                        <mml:mi>m</mml:mi>
                      </mml:mrow>
                      <mml:mi>k</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>X</mml:mi>
                      <mml:mi>k</mml:mi>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                  </mml:math>
                </disp-formula>
              
              <p>Eqs. (2) and (3) can be used for predicting weights <inline-formula>
  <mml:math id="mpqky4p54y">
    <mml:msub>
      <mml:mi>a</mml:mi>
      <mml:mrow>
        <mml:mi>t</mml:mi>
        <mml:mi>t</mml:mi>
        <mml:msub>
          <mml:mi>m</mml:mi>
          <mml:mi>k</mml:mi>
        </mml:msub>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> and scaling input <inline-formula>
  <mml:math id="mmztx1xwax">
    <mml:msub>
      <mml:mi>X</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>, respectively.</p><p>Applying Eqs. (1) to (3) across all $K<inline-formula>
  <mml:math id="m4f4c77lfs">
    <mml:mi>b</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>B</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
  </mml:math>
</inline-formula>\left\{X_0, \ldots, X_{K-1}\right\}$. These branches are aggregated to form the final output of the MBSCA module:</p>
              
                <disp-formula>
                  <label>(4)</label>
                  <mml:math id="mpyp2h4bfo">
                    <mml:mi>Υ</mml:mi>
                    <mml:mi>A</mml:mi>
                    <mml:mi>V</mml:mi>
                    <mml:mi>G</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mrow>
                        <mml:mo>{</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>…</mml:mo>
                        <mml:mo>,</mml:mo>
                        <mml:mo>}</mml:mo>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mover>
                              <mml:mi>X</mml:mi>
                              <mml:mo>¯</mml:mo>
                            </mml:mover>
                          </mml:mrow>
                          <mml:mn>0</mml:mn>
                        </mml:msub>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mover>
                              <mml:mi>X</mml:mi>
                              <mml:mo>¯</mml:mo>
                            </mml:mover>
                          </mml:mrow>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                        <mml:msub>
                          <mml:mrow>
                            <mml:mover>
                              <mml:mi>X</mml:mi>
                              <mml:mo>¯</mml:mo>
                            </mml:mover>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mi>K</mml:mi>
                            <mml:mo>−</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mrow>
                        </mml:msub>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>Various components on each individual channel can interact, and <inline-formula>
  <mml:math id="m8yf800zc1">
    <mml:mi>A</mml:mi>
    <mml:mi>V</mml:mi>
    <mml:mi>G</mml:mi>
  </mml:math>
</inline-formula> is the average pooling to fuse <inline-formula>
  <mml:math id="moxe87xpal">
    <mml:msub>
      <mml:mi>X</mml:mi>
      <mml:mi>k</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>. The MBSCA can be easily integrated into different basic system without modifying its topology, thanks to the fact of using the shape as the input $X$. This allows for reusing the pretrained weights. Ablation experiments were conducted to determine the relative relevance of various frequency branches (using MAO), and then the top-k frequency components were chosen using the best solution.</p>
            </sec>
          
          
            <sec>
              
                <title>4.2.3 Finding k value using mao algorithm</title>
              
              <p>The MAO algorithm [<xref ref-type="bibr" rid="ref_28">28</xref>] was used to determine the recommended classifier's k-value. The ability of the aquila to swoop down and seize its prey is crucial to the aquila optimizer (AO). AO’s worth was quickly proven in the domain of complicated and nonlinear optimization, which is a population-based approach. The work on the search control factor (SCF) from the improved aquila optimizer (IAO) motivated MAO to make further changes to the AO. Nevertheless, the precision of the epochs in IAO is hindered by the convergence characteristics of SCF. Some of the difficulties encountered in seeking the best outcome could be attributable to these characteristics. In order to address these issues, a revised IAO was implemented, which has a modified search control factor (MSCF) tailored to the second and third search phases. The next part describes the MAO method in depth, focusing on the changes that were made and how they affected the optimization method. The aquila's movement was reduced in terms of epochs when the search range was controlled using the MSCF. This means that there is a much smaller search space than there was with the previous SCF. Also, compared to the previous method, the optimal answer was found much faster. The updated MSCF takes the following form:</p>
              
                <disp-formula>
                  <label>(5)</label>
                  <mml:math id="mq5nyp88k2">
                    <mml:mi>M</mml:mi>
                    <mml:mi>S</mml:mi>
                    <mml:mi>C</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mi>exp</mml:mi>
                    <mml:mo>−</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>×</mml:mo>
                    <mml:mo>⁡</mml:mo>
                    <mml:mo>×</mml:mo>
                    <mml:mn>2</mml:mn>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mn>1</mml:mn>
                      <mml:mrow>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mfrac>
                          <mml:mrow>
                            <mml:mi>t</mml:mi>
                            <mml:mi>t</mml:mi>
                            <mml:mo>×</mml:mo>
                            <mml:mo>(</mml:mo>
                            <mml:mo>×</mml:mo>
                            <mml:mo>)</mml:mo>
                            <mml:mn>0.1</mml:mn>
                          </mml:mrow>
                          <mml:mi>T</mml:mi>
                        </mml:mfrac>
                      </mml:mrow>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>d</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>r</mml:mi>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(6)</label>
                  <mml:math id="mgpwdievn2">
                    <mml:mrow>
                      <mml:mi>d</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>r</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>{</mml:mo>
                      <mml:mo fence="true"/>
                      <mml:mtable columnalign="left right" columnspacing="1em" rowspacing="4pt">
                        <mml:mtr>
                          <mml:mtd>
                            <mml:mn>1</mml:mn>
                          </mml:mtd>
                          <mml:mtd>
                            <mml:mtext> if </mml:mtext>
                            <mml:mi>r</mml:mi>
                            <mml:mo>&lt;</mml:mo>
                            <mml:mn>0.5</mml:mn>
                          </mml:mtd>
                        </mml:mtr>
                        <mml:mtr>
                          <mml:mtd>
                            <mml:mo>−</mml:mo>
                            <mml:mn>1</mml:mn>
                          </mml:mtd>
                          <mml:mtd>
                            <mml:mtext> else </mml:mtext>
                          </mml:mtd>
                        </mml:mtr>
                      </mml:mtable>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="m4x4xpn92y">
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula> is the direction control factor, and the r parameter is a random number between zero and one. These characteristics are crucial in determining the aquila's fighting style. By limiting the aquila's mobility, the MSCF function aims to achieve rapid convergence. In addition, optimization delays were reduced. The updated method outperformed the original AO because it quickly identified the best set of solutions. With 250 and 250 epochs, respectively, both optimization methods were run. The proposed method has the following four search phases, which were incorporated into the MSCF function:</p><p>Step 1: Vertical dive attack (<inline-formula>
  <mml:math id="muwtjr5at8">
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula>)</p><p>Before diving into its hunt, the aquila swoops down to survey the area it intends to prey upon and choose the best spot to perch. These kinds of assaults are known as “vertical dive attacks,” which can be expressed as follows:</p>
              
                <disp-formula>
                  <label>(7)</label>
                  <mml:math id="m45tfkkecz">
                    <mml:msub>
                      <mml:mi>S</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>S</mml:mi>
                      <mml:mrow>
                        <mml:mi>b</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>×</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mi>t</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mn>1</mml:mn>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mn>1</mml:mn>
                      <mml:mfrac>
                        <mml:mi>t</mml:mi>
                        <mml:mi>T</mml:mi>
                      </mml:mfrac>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>×</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>S</mml:mi>
                        <mml:mi>M</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>S</mml:mi>
                        <mml:mrow>
                          <mml:mi>b</mml:mi>
                          <mml:mi>e</mml:mi>
                          <mml:mi>s</mml:mi>
                          <mml:mi>t</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mi>t</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>r</mml:mi>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="my095yylu7">
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>+</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula> is the key contender of <inline-formula>
  <mml:math id="mxp21rgipp">
    <mml:mo>(</mml:mo>
    <mml:mo>+</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula> epochs, $r<inline-formula>
  <mml:math id="mv8pz6rdyy">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>[</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>]</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>0</mml:mn>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula>S_{best}(t)<inline-formula>
  <mml:math id="mowg8v9yyc">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>i<inline-formula>
  <mml:math id="m0qsrgexle">
    <mml:mo>−</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
  </mml:math>
</inline-formula>\left(1-\frac{t}{T}\right)<inline-formula>
  <mml:math id="m6vuxlu4je">
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mn>2</mml:mn>
  </mml:math>
</inline-formula>MS$)</p><p>This is known as a shorter glide assault, and it occurs before the aquila strikes its target. The aquila searches the key space using a variety of directions and speeds as follows:</p>
              
                <disp-formula>
                  <label>(8)</label>
                  <mml:math id="mftxaj1ysl">
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>M</mml:mi>
                        <mml:mi>S</mml:mi>
                      </mml:mrow>
                      <mml:mn>2</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>S</mml:mi>
                      <mml:mi>R</mml:mi>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>×</mml:mo>
                    <mml:mo>×</mml:mo>
                    <mml:mo>×</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>t</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mi>M</mml:mi>
                    <mml:mi>S</mml:mi>
                    <mml:mi>C</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mi>r</mml:mi>
                    <mml:mi>y</mml:mi>
                    <mml:mi>x</mml:mi>
                    <mml:mn>1</mml:mn>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>S</mml:mi>
                        <mml:mrow>
                          <mml:mi>b</mml:mi>
                          <mml:mi>e</mml:mi>
                          <mml:mi>s</mml:mi>
                          <mml:mi>t</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mi>t</mml:mi>
                      <mml:mi>S</mml:mi>
                      <mml:mi>t</mml:mi>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>The above equation describes the outcomes using the point that forms the spiral during stage ($x<inline-formula>
  <mml:math id="mrzha61q3l">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>y<inline-formula>
  <mml:math id="mukr2hj7wn">
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
  </mml:math>
</inline-formula>r<inline-formula>
  <mml:math id="mhw6g39sif">
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mn>0</mml:mn>
    <mml:mn>1</mml:mn>
  </mml:math>
</inline-formula>t<inline-formula>
  <mml:math id="myowetdv3n">
    <mml:mo>)</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mi>T</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>M</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>L</mml:mi>
    <mml:mi>F</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mn>3</mml:mn>
  </mml:math>
</inline-formula>MS<inline-formula>
  <mml:math id="mc6v1o6yk5">
    <mml:mo>)</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>MS_2$ search step is followed to correctly locate the prey's district. In what is known as an attack, the aquila carefully investigates the area surrounding the target and uses fake attacks to gauge the prey's reaction.</p>
              
                <disp-formula>
                  <label>(9)</label>
                  <mml:math id="mk5pgf93zh">
                    <mml:msub>
                      <mml:mrow>
                        <mml:mi>M</mml:mi>
                        <mml:mi>S</mml:mi>
                      </mml:mrow>
                      <mml:mn>3</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>b</mml:mi>
                      <mml:mi>j</mml:mi>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>,</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>×</mml:mo>
                    <mml:mo>×</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>×</mml:mo>
                    <mml:mi>i</mml:mi>
                    <mml:mi>j</mml:mi>
                    <mml:mi>l</mml:mi>
                    <mml:mi>r</mml:mi>
                    <mml:mi>r</mml:mi>
                    <mml:mi>M</mml:mi>
                    <mml:mi>S</mml:mi>
                    <mml:mi>C</mml:mi>
                    <mml:mi>F</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mi>u</mml:mi>
                      <mml:mi>l</mml:mi>
                      <mml:msub>
                        <mml:mi>b</mml:mi>
                        <mml:mi>j</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>b</mml:mi>
                        <mml:mi>j</mml:mi>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>S</mml:mi>
                        <mml:mi>R</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>S</mml:mi>
                        <mml:mrow>
                          <mml:mi>b</mml:mi>
                          <mml:mi>e</mml:mi>
                          <mml:mi>s</mml:mi>
                          <mml:mi>t</mml:mi>
                        </mml:mrow>
                      </mml:msub>
                      <mml:mi>j</mml:mi>
                      <mml:mi>j</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mn>1</mml:mn>
                      <mml:mfrac>
                        <mml:mi>t</mml:mi>
                        <mml:mi>T</mml:mi>
                      </mml:mfrac>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="madftdnuuy">
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mi>R</mml:mi>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>j</mml:mi>
  </mml:math>
</inline-formula> represents the accidental set of keys, and <inline-formula>
  <mml:math id="miw92zt5hq">
    <mml:mi>M</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mn>3</mml:mn>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> specifies the current key for $t<inline-formula>
  <mml:math id="mz6wx4kgqs">
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>W</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;&amp;lt;</mml:mo>
    <mml:mo>&amp;gt;</mml:mo>
    <mml:mo>:</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mrow>
      <mml:mo>/</mml:mo>
    </mml:mrow>
    <mml:mn>4</mml:mn>
  </mml:math>
</inline-formula>S$)</p><p>The last strategy for finding prey involves an aquila's aerial attack, which is triggered by the prey's motion. One way to describe this hunting technique is “walk and grab prey” as follows:</p>
              
                <disp-formula>
                  <label>(10)</label>
                  <mml:math id="mhyzhpxycd">
                    <mml:msub>
                      <mml:mi>S</mml:mi>
                      <mml:mn>4</mml:mn>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>Q</mml:mi>
                      <mml:mi>F</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>S</mml:mi>
                      <mml:mrow>
                        <mml:mi>b</mml:mi>
                        <mml:mi>e</mml:mi>
                        <mml:mi>s</mml:mi>
                        <mml:mi>t</mml:mi>
                      </mml:mrow>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>G</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msub>
                    <mml:mo>(</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>=</mml:mo>
                    <mml:mo>×</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mo>×</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>t</mml:mi>
                    <mml:mi>t</mml:mi>
                    <mml:mi>D</mml:mi>
                    <mml:mn>1</mml:mn>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>×</mml:mo>
                      <mml:mo>(</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mo>×</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>G</mml:mi>
                        <mml:mn>1</mml:mn>
                      </mml:msub>
                      <mml:mi>S</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>r</mml:mi>
                    </mml:mrow>
                    <mml:mrow>
                      <mml:mi>l</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>v</mml:mi>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(11)</label>
                  <mml:math id="mienm117o9">
                    <mml:msub>
                      <mml:mi>Q</mml:mi>
                      <mml:mi>F</mml:mi>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:msup>
                      <mml:mi>t</mml:mi>
                      <mml:mrow>
                        <mml:mfrac>
                          <mml:mrow>
                            <mml:mn>2</mml:mn>
                            <mml:mn>1</mml:mn>
                            <mml:mo>×</mml:mo>
                            <mml:mo>−</mml:mo>
                            <mml:mi>r</mml:mi>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:mo>(</mml:mo>
                            <mml:mo>−</mml:mo>
                            <mml:mn>1</mml:mn>
                            <mml:mi>T</mml:mi>
                            <mml:msup>
                              <mml:mo>)</mml:mo>
                              <mml:mn>2</mml:mn>
                            </mml:msup>
                          </mml:mrow>
                        </mml:mfrac>
                      </mml:mrow>
                    </mml:msup>
                  </mml:math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(12)</label>
                  <mml:math id="mtvwaaurff">
                    <mml:msub>
                      <mml:mi>G</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mo>×</mml:mo>
                    <mml:mo>−</mml:mo>
                    <mml:mn>2</mml:mn>
                    <mml:mn>1</mml:mn>
                    <mml:mtext> random </mml:mtext>
                  </mml:math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(13)</label>
                  <mml:math id="mazjr04nq6">
                    <mml:msub>
                      <mml:mi>G</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msub>
                    <mml:mo>=</mml:mo>
                    <mml:mo>×</mml:mo>
                    <mml:mn>2</mml:mn>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:mn>1</mml:mn>
                      <mml:mfrac>
                        <mml:mi>t</mml:mi>
                        <mml:mi>T</mml:mi>
                      </mml:mfrac>
                    </mml:mrow>
                  </mml:math>
                </disp-formula>
              
              <p>where, <inline-formula>
  <mml:math id="mq39x6yjia">
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mn>4</mml:mn>
    </mml:msub>
    <mml:mo>(</mml:mo>
    <mml:mo>+</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mi>t</mml:mi>
    <mml:mi>D</mml:mi>
    <mml:mn>1</mml:mn>
    <mml:mstyle scriptlevel="0">
      <mml:mspace width="0.167em"/>
    </mml:mstyle>
    <mml:mrow>
      <mml:mi>l</mml:mi>
      <mml:mi>e</mml:mi>
      <mml:mi>v</mml:mi>
    </mml:mrow>
  </mml:math>
</inline-formula> displays the Levy distribution for the $D<inline-formula>
  <mml:math id="moxmdj8ge1">
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>G_1<inline-formula>
  <mml:math id="ml4j8thouh">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>q</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>k</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>G_2$ is the hunting combat slope.</p>
              
                <disp-formula>
                  <label>(14)</label>
                  <mml:math id="moirzorrbw">
                    <mml:mrow>
                      <mml:mi>F</mml:mi>
                      <mml:mi>i</mml:mi>
                      <mml:mi>t</mml:mi>
                      <mml:mi>n</mml:mi>
                      <mml:mi>e</mml:mi>
                      <mml:mi>s</mml:mi>
                      <mml:mi>s</mml:mi>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:mo>max</mml:mo>
                    <mml:mo>(</mml:mo>
                    <mml:mo>)</mml:mo>
                    <mml:mi>P</mml:mi>
                  </mml:math>
                </disp-formula>
              
              
                <disp-formula>
                  <label>(15)</label>
                  <mml:math id="m48rrxuznt">
                    <mml:mi>P</mml:mi>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mi>T</mml:mi>
                        <mml:mi>P</mml:mi>
                        <mml:mi>F</mml:mi>
                        <mml:mi>P</mml:mi>
                        <mml:mo>+</mml:mo>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:math>
                </disp-formula>
              
              <p><inline-formula>
  <mml:math id="m8n6brj4hz">
    <mml:mi>T</mml:mi>
    <mml:mi>P</mml:mi>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="muxwaeww3w">
    <mml:mi>F</mml:mi>
    <mml:mi>P</mml:mi>
  </mml:math>
</inline-formula> stand for the values of true positive and false positive, respectively. An essential part of the MAO approach is the fitness option. Encoder performance was used to find the best possible option. At this point, the most important criterion used to create a fitness function (FF) is its performance value.</p><p>On the other hand, while the suggested model did use many frequency components to enhance characteristics, each component of the channel features only used one frequency component. The insufficient depiction of a single channel led to inadequate channel modeling, as it failed to account for the interplay between these components. Out of all of them, the proposed MBSCAN showed the most promising trial outcomes.</p><p>The MBSCAN is an advanced DL architecture specifically designed for image analysis tasks. At its core, MBSCA consists of multiple branches, each incorporating spectral channel attention mechanisms. These attention mechanisms enable the network to dynamically adjust its focus on relevant image features by selectively attending to informative channels across different spectral frequencies. This capability is particularly valuable in scenarios where images contain complex spatial and spectral characteristics, such as medical imaging or remote sensing applications.</p><p>The rationale behind leveraging pre-trained CNN models like VGG16 and VGG19 lies in their established efficacy in feature extraction and representation learning. VGG16 and VGG19 are renowned for their deep architectures, comprising multiple layers of convolutional and pooling operations, followed by fully connected layers for classification. These models have been pre-trained on large-scale image datasets, such as ImageNet, which contain millions of labeled images across thousands of classes. As a result, the learned features in these models capture a broad range of visual patterns and semantics, making them well-suited for transfer learning. In MBSCA, the pre-trained VGG models serve as feature extractors within each branch of the network. By leveraging the hierarchical representations learned by VGG16 and VGG19, MBSCA can effectively capture and encode complex image features across different spectral channels. This not only enhances the network's discriminative power but also enables it to generalize well to unseen data, especially in tasks where labeled data is limited.</p><p>Furthermore, the choice between VGG16 and VGG19 may depend on factors such as the complexity of the image dataset and the computational resources available. With more convolutional layers and a deeper architecture, VGG19 may be able to capture more complex features, but it also needs more computing power for training and inference than VGG16. Therefore, the selection between these models should be based on a trade-off between model complexity and computational efficiency, tailored to the specific requirements of the image analysis task at hand.</p>
            </sec>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>5. Results and discussion</title>
      <p>Accuracy, precision, recall, and F1-score are performance metrics used to evaluate diagnostic performance at the picture level, in line with the study's recommendations for processing binary balanced data [<xref ref-type="bibr" rid="ref_29">29</xref>]. Network training hyperparameters include an initial learning rate of 0.001 and a decay rate that halves the current rate after five iterations. By utilizing the MAO optimizer with a value of 0.9, the loss function was controlled towards a global minimum and prevented from reaching suboptimal solutions. All models underwent training using label smoothing and cosine learning rate decay over 100 epochs. The PyTorch [<xref ref-type="bibr" rid="ref_30">30</xref>] framework was employed to execute all experiments on a server equipped with an RTX 3090 GPU. <xref ref-type="fig" rid="fig_1">Figure 1</xref> illustrates the suggested model's accuracy on both the training and validation datasets.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Accuracy of the proposed model on training and validation datasets</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/4/img_F_0HNO9h6BGdXtbG.png"/>
        </fig>
      
      
        <sec>
          
            <title>5.1. Validation analysis of the proposed model</title>
          
          <p>The accuracy of the proposed model was evaluated across various images, categorised into different classes.</p>
          
            <table-wrap id="table_2">
              <label>Table 2</label>
              <caption>
                <title>Investigation of the proposed model in terms of accuracy</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Types</p></td><td colspan="1" rowspan="1"><p>Normal</p></td><td colspan="1" rowspan="1"><p>Intact DIs</p></td><td colspan="1" rowspan="1"><p>Fractured DIs (Type I)</p></td><td colspan="1" rowspan="1"><p>Fractured DIs (Type II)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image_1</p></td><td colspan="1" rowspan="1"><p>0.99756</p></td><td colspan="1" rowspan="1"><p>0.98193</p></td><td colspan="1" rowspan="1"><p>0.99514</p></td><td colspan="1" rowspan="1"><p>0.96451</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image_2</p></td><td colspan="1" rowspan="1"><p>0.99775</p></td><td colspan="1" rowspan="1"><p>0.98975</p></td><td colspan="1" rowspan="1"><p>0.99552</p></td><td colspan="1" rowspan="1"><p>0.97972</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image_3</p></td><td colspan="1" rowspan="1"><p>0.99758</p></td><td colspan="1" rowspan="1"><p>0.98710</p></td><td colspan="1" rowspan="1"><p>0.99518</p></td><td colspan="1" rowspan="1"><p>0.97454</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image_4</p></td><td colspan="1" rowspan="1"><p>0.99738</p></td><td colspan="1" rowspan="1"><p>0.98974</p></td><td colspan="1" rowspan="1"><p>0.99479</p></td><td colspan="1" rowspan="1"><p>0.97969</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image_5</p></td><td colspan="1" rowspan="1"><p>0.99777</p></td><td colspan="1" rowspan="1"><p>0.97334</p></td><td colspan="1" rowspan="1"><p>0.99556</p></td><td colspan="1" rowspan="1"><p>0.94807</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image_6</p></td><td colspan="1" rowspan="1"><p>0.99701</p></td><td colspan="1" rowspan="1"><p>0.98348</p></td><td colspan="1" rowspan="1"><p>0.99404</p></td><td colspan="1" rowspan="1"><p>0.96751</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image_7</p></td><td colspan="1" rowspan="1"><p>0.99761</p></td><td colspan="1" rowspan="1"><p>0.97269</p></td><td colspan="1" rowspan="1"><p>0.99523</p></td><td colspan="1" rowspan="1"><p>0.94684</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image_8</p></td><td colspan="1" rowspan="1"><p>0.99729</p></td><td colspan="1" rowspan="1"><p>0.99024</p></td><td colspan="1" rowspan="1"><p>0.99459</p></td><td colspan="1" rowspan="1"><p>0.98068</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image_9</p></td><td colspan="1" rowspan="1"><p>0.99895</p></td><td colspan="1" rowspan="1"><p>0.98740</p></td><td colspan="1" rowspan="1"><p>0.99790</p></td><td colspan="1" rowspan="1"><p>0.97512</p></td></tr><tr><td colspan="1" rowspan="1"><p>Image_10</p></td><td colspan="1" rowspan="1"><p>0.99882</p></td><td colspan="1" rowspan="1"><p>0.92817</p></td><td colspan="1" rowspan="1"><p>0.99765</p></td><td colspan="1" rowspan="1"><p>0.86597</p></td></tr></tbody></table>
            </table-wrap>
          
          <p><xref ref-type="table" rid="table_2">Table 2</xref> investigates the proposed model in terms of accuracy. In the analysis of Image_1, the proposed model demonstrated an accuracy of 0.99756 for the normal category, 0.98193 for intact DIs, and 0.99514 and 0.96451 for fractured DIs, Type I and Type II, respectively. Image_2 achieved accuracies of 0.99775 for the normal category, 0.98975 for intact DIs, and 0.99552 and 0.97972 for fractured DIs, Type I and Type II, respectively. In Image_3, the accuracy was 0.99758 for the normal category, 0.98710 for intact DIs, and 0.99518 and 0.97454 for fractured DIs, Type I and Type II, respectively. Image_4 displayed accuracies of 0.99738 for the normal category, 0.98974 for intact DIs, and 0.99479 and 0.97969 for fractured DIs, Type I and Type II, respectively. For Image_5, accuracies of 0.99777 were achieved for the normal category and 0.97334 for intact DIs, while for fractured DIs, Type II, the accuracy was 0.94807. Image_6 indicated accuracies of 0.99701 for the normal category, 0.98348 for intact DIs, and 0.99404 and 0.96751 for fractured DIs, Type I and Type II, respectively. In Image_7, the normal category had an accuracy of 0.99761, while intact DIs achieved 0.97269. Accuracies for fractured DIs, Type I and Type II were 0.99523 and 0.94684, respectively. For Image_8, the accuracy for the normal category was 0.99729, 0.99024 for intact DIs, and 0.98068 for fractured DIs, Type II. Image_9 showed an accuracy of 0.99895 for the normal category, 0.98740 for intact DIs, and 0.99790 and 0.97512 for fractured DIs, Type I and Type II, respectively. Finally, for Image_10, the accuracy of the normal category was 0.99882, while intact DIs had 0.92817, and fractured DIs, Type I and Type II showed accuracies of 0.99765 and 0.86597, respectively.</p><p><xref ref-type="fig" rid="fig_2">Figure 2</xref> shows a visual analysis of the proposed model.</p>
          
            <fig id="fig_2">
              <label>Figure 2</label>
              <caption>
                <title>Visual analysis of the proposed model</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/4/img_ADevJRWs1WL_3sS2.png"/>
            </fig>
          
        </sec>
      
      
        <sec>
          
            <title>5.2. Comparative analysis of the proposed model with existing procedures</title>
          
          <p>In terms of the effectiveness of the wished-for model, this study considered existing models from related works and tested them with a dataset. Researchers have not used the dataset extensively. Therefore, all models were implemented.</p><p><xref ref-type="table" rid="table_3">Table 3</xref> shows a relative analysis of the proposed model with existing techniques. Firstly, the MSPENet scheme [<xref ref-type="bibr" rid="ref_15">15</xref>] achieved an accuracy of 89.7%, with precision, recall, and F1-score values of 89.1%, 88.0%, and 89.4%, respectively. Next, the random forest scheme [<xref ref-type="bibr" rid="ref_16">16</xref>], [<xref ref-type="bibr" rid="ref_17">17</xref>] exhibited an accuracy of 92.7%, with precision, recall, and F1-score values of 92.4%, 91.5%, and 92.7%, respectively. The Support Vector Machine (SVM) scheme [<xref ref-type="bibr" rid="ref_17">17</xref>] demonstrated an accuracy of 94.3%, with precision, recall, and F1-score values of 94.1%, 93.2%, and 94.2%, respectively. The VGG16 scheme [<xref ref-type="bibr" rid="ref_19">19</xref>] yielded an accuracy of 91.6%, with recall and F1-score values of 90.5% and 91.4%, respectively. The CNN scheme [<xref ref-type="bibr" rid="ref_20">20</xref>], [<xref ref-type="bibr" rid="ref_21">21</xref>] displayed an accuracy of 87.2%, with precision, recall, and F1-score values of 87.3%, 86.7%, and 86.3%, respectively. Finally, the MBSCA-MAO scheme showcased an accuracy of 95.7%, with precision, recall, and F1-score values of 95.2%, 94.3%, and 95.6%, respectively.</p>
          
            <table-wrap id="table_3">
              <label>Table 3</label>
              <caption>
                <title>Comparative analysis of the proposed model with existing techniques</title>
              </caption>
              <table><tbody><tr><td colspan="1" rowspan="1"><p>Architectures</p></td><td colspan="1" rowspan="1"><p>Accuracy (%)</p></td><td colspan="1" rowspan="1"><p>Precision (%)</p></td><td colspan="1" rowspan="1"><p>Recall (%)</p></td><td colspan="1" rowspan="1"><p>Fl-score (%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>MSPENet [<xref ref-type="bibr" rid="ref_15">15</xref>]</p></td><td colspan="1" rowspan="1"><p>89.7</p></td><td colspan="1" rowspan="1"><p>89.1</p></td><td colspan="1" rowspan="1"><p>88.0</p></td><td colspan="1" rowspan="1"><p>89.4</p></td></tr><tr><td colspan="1" rowspan="1"><p>Random forest [<xref ref-type="bibr" rid="ref_16">16</xref>], [<xref ref-type="bibr" rid="ref_17">17</xref>]</p></td><td colspan="1" rowspan="1"><p>92.7</p></td><td colspan="1" rowspan="1"><p>92.4</p></td><td colspan="1" rowspan="1"><p>91.5</p></td><td colspan="1" rowspan="1"><p>92.7</p></td></tr><tr><td colspan="1" rowspan="1"><p>SVM [<xref ref-type="bibr" rid="ref_17">17</xref>]</p></td><td colspan="1" rowspan="1"><p>94.3</p></td><td colspan="1" rowspan="1"><p>94.1</p></td><td colspan="1" rowspan="1"><p>93.2</p></td><td colspan="1" rowspan="1"><p>94.2</p></td></tr><tr><td colspan="1" rowspan="1"><p>VGG16 [<xref ref-type="bibr" rid="ref_19">19</xref>]</p></td><td colspan="1" rowspan="1"><p>91.5</p></td><td colspan="1" rowspan="1"><p>91.6</p></td><td colspan="1" rowspan="1"><p>90.5</p></td><td colspan="1" rowspan="1"><p>91.4</p></td></tr><tr><td colspan="1" rowspan="1"><p>CNN [<xref ref-type="bibr" rid="ref_20">20</xref>], [<xref ref-type="bibr" rid="ref_21">21</xref>]</p></td><td colspan="1" rowspan="1"><p>87.2</p></td><td colspan="1" rowspan="1"><p>87.3</p></td><td colspan="1" rowspan="1"><p>86.7</p></td><td colspan="1" rowspan="1"><p>86.3</p></td></tr><tr><td colspan="1" rowspan="1"><p>MBSCA-MAO</p></td><td colspan="1" rowspan="1"><p>95.7</p></td><td colspan="1" rowspan="1"><p>95.2</p></td><td colspan="1" rowspan="1"><p>94.3</p></td><td colspan="1" rowspan="1"><p>95.6</p></td></tr></tbody></table>
            </table-wrap>
          
          <p> <xref ref-type="fig" rid="fig_3">Figure 3</xref> shows a visual representation of the proposed model. <xref ref-type="fig" rid="fig_4">Figure 4</xref> shows a graphical description of different models for DIS.</p>
          
            <fig id="fig_3">
              <label>Figure 3</label>
              <caption>
                <title>A visual representation of the proposed model</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/4/img_4sFTndT-oCm9dQzR.png"/>
            </fig>
          
          
            <fig id="fig_4">
              <label>Figure 4</label>
              <caption>
                <title>A graphical description of different models for DIS</title>
              </caption>
              <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2024/4/img_eqiEF0zs2ji3t1Gb.png"/>
            </fig>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>6. Conclusions and future work</title>
      <p>It was found that feature extraction architectures like VGG16, VGG19, and the MBSCA-MAO achieved satisfactory accuracy in identifying and categorizing fractured DIs. Notably, the automated DCNN architecture, utilizing input images, demonstrated the highest performance. The finely tuned CNNs, VGG16 and VGG19, excelled particularly in classification tasks. Moreover, Grad-CAM analysis revealed an understanding of each network's convolutional layers regarding implant fixtures, which holds significance in identifying DI brands from input images. However, further clinical and prospective evidence is necessary to validate the effectiveness of DCNN construction practices.</p><p>The multi-branch methodology of MBSCA-MAO enables effective integration of a wide range of frequency information, encompassing both low- and high-frequency components. While the model performs well on images with clear borders and strong contrast, recognizing low-resolution photos with fuzzy borders remains challenging.</p><p>Several caveats and potential avenues for further research were proposed in this study. A key challenge is the scarcity of datasets containing fractured DI imaging, owing to the infrequent occurrence of DI fractures. Despite analyzing nearly 20,000 radiographs from two dental clinics, the dataset only included 194 images of fractured DIs. To enhance clinical applicability in implant dentistry, collecting a larger, higher-quality dataset from diverse dental institutions is imperative.</p><p>Another limitation of this study is that it uses poor-resolution picture datasets for training and validation of the proposed architecture. Due to resource constraints, such as storage space and processing power, the study had to rely on cropped and downscaled low-resolution panoramic and periapical images. Further investigation is warranted to assess whether a high-resolution image dataset could enhance classification accuracy. Further research could encompass several avenues. For example, advanced imaging techniques, such as cone beam computed tomography (CBCT) or three-dimensional (3D) imaging, could be incorporated to enhance the classification accuracy of DIS. These techniques can provide more detailed information about the implant structure and surrounding anatomy, which may improve the performance of the classification model.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the research findings are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare no conflict of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>60</volume>
          <page-range>2951-2968</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Kohlakala</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Coetzer</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Bertels</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Vandermeulen</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11517-022-02642-9</pub-id>
          <article-title>Deep learning-based dental implant recognition using synthetic X-ray images</article-title>
          <source>Med. Biol. Eng. Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>51</volume>
          <page-range>20220244</page-range>
          <issue>8</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Çelik</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Çelik</surname>
              <given-names>M. E.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1259/dmfr.20220244</pub-id>
          <article-title>Automated detection of dental restorations using deep learning on panoramic radiographs</article-title>
          <source>Dentomaxillofac. Radiol.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>17</volume>
          <page-range>649-660</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Alzaid</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Wignall</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Dogramadzi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Pandit</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names>S. Q.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s11548-021-02552-5</pub-id>
          <article-title>Automatic detection and classification of peri-prosthetic femur fracture</article-title>
          <source>Int. J. Comput. Assist. Radiol. Surg.</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>53</volume>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chaurasia</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Namachivayam</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Koca-Ünsal</surname>
              <given-names>R. B.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>J. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.5051/jpis.2300160008</pub-id>
          <article-title>Deep-learning performance in identifying and classifying dental implant systems from dental imaging: A systematic review and meta-analysis</article-title>
          <source>J. Periodont. Implant Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <volume>52</volume>
          <page-range>220</page-range>
          <issue>3</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lee</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>Y. T.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>J. B.</given-names>
            </name>
            <name>
              <surname>Jeong</surname>
              <given-names>S. N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.5051/jpis.2104080204</pub-id>
          <article-title>Deep learning improves implant classification by dental professionals: A multi-center evaluation of accuracy and efficiency</article-title>
          <source>J. Periodont. Implant Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>36</volume>
          <page-range>1077-1082</page-range>
          <issue>6</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hadj Saïd</surname>
              <given-names>Mehdi</given-names>
            </name>
            <name>
              <surname>Le Roux</surname>
              <given-names>Marc Kévin</given-names>
            </name>
            <name>
              <surname>Catherine</surname>
              <given-names>Jean Hugues</given-names>
            </name>
            <name>
              <surname>Lan</surname>
              <given-names>Romain</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.11607/jomi.8060</pub-id>
          <article-title>Development of an artificial intelligence model to identify a dental implant from a radiograph</article-title>
          <source>Int. J. Oral Maxillofac. Implants</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <volume>167</volume>
          <page-range>107620</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Nie</surname>
              <given-names>Qian Qing</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Chen</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Jin Zhu</given-names>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names>Yu Dong</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>Hong Xan</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>Tao</given-names>
            </name>
            <name>
              <surname>Grzegorzek</surname>
              <given-names>Marcin</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Ao</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Haoyuan</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>Wei Ming</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Rui</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Jia Wei</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Dan Ning</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.compbiomed.2023.107620</pub-id>
          <article-title>OII-DS: A benchmark Oral Implant Image Dataset for object detection and image classification evaluation</article-title>
          <source>Comput. Biol. Med.</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>37</volume>
          <page-range>257-265</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Inamanamelluri</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Pulipati</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Pradhan</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Chintamaneni</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Manur</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Vatambeti</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/ria.370202</pub-id>
          <article-title>Classification of a new-born infant’s jaundice symptoms using a binary spring search algorithm with machine learning</article-title>
          <source>Rev. Intell. Artif.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>43</volume>
          <page-range>363-368</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hsiao</surname>
              <given-names>C. Y.</given-names>
            </name>
            <name>
              <surname>Bai</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Ling</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.11607/prd.5781</pub-id>
          <article-title>Artificial intelligence in identifying dental implant systems on radiographs</article-title>
          <source>Int. J. Periodont. Restor. Dent.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>19</volume>
          <page-range>15414</page-range>
          <issue>22</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Al-Sarem</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Al-Asali</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Ahmed  Alqutaibi</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Saeed</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/ijerph192215414</pub-id>
          <article-title>Enhanced tooth region detection using pretrained deep learning models</article-title>
          <source>Int. J. Environ. Res. Public Health</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <volume>51</volume>
          <page-range>20210197</page-range>
          <issue>1</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Putra</surname>
              <given-names>Rangga H</given-names>
            </name>
            <name>
              <surname>Doi</surname>
              <given-names>Chisato</given-names>
            </name>
            <name>
              <surname>Yoda</surname>
              <given-names>Nobuhiro</given-names>
            </name>
            <name>
              <surname>Astuti</surname>
              <given-names>Eka R</given-names>
            </name>
            <name>
              <surname>Sasaki</surname>
              <given-names>Kazunori</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1259/DMFR.20210197</pub-id>
          <article-title>Current applications and development of artificial intelligence for digital dental radiography</article-title>
          <source>Dentomaxillofac. Radiol.</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <volume>126</volume>
          <page-range>104301</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Bonfanti-Gris</surname>
              <given-names>Maria</given-names>
            </name>
            <name>
              <surname>Garcia-Cañas</surname>
              <given-names>Ana</given-names>
            </name>
            <name>
              <surname>Alonso-Calvo</surname>
              <given-names>Raúl</given-names>
            </name>
            <name>
              <surname>Rodriguez-Manzaneque</surname>
              <given-names>Maria P S</given-names>
            </name>
            <name>
              <surname>Ramiro</surname>
              <given-names>Guillermo P</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.jdent.2022.104301</pub-id>
          <article-title>Evaluation of an Artificial Intelligence web-based software to detect and classify dental structures and treatments in panoramic radiographs</article-title>
          <source>J. Dent.</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>57</volume>
          <page-range>942-951</page-range>
          <issue>5</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mohammad-Rahimi</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Motamedian</surname>
              <given-names>S. R.</given-names>
            </name>
            <name>
              <surname>A. Haiat Z. Pirayesh</surname>
              <given-names>A. Z.</given-names>
            </name>
            <name>
              <surname>Zahedrozegar</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Mahmoudinia</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Rohban</surname>
              <given-names>M. H.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Krois</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Schwendicke</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1111/jre.13037</pub-id>
          <article-title>Deep learning in periodontology and oral implantology: A scoping review</article-title>
          <source>J. Periodontal Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>49</volume>
          <page-range>872-883</page-range>
          <issue>9</issue>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Huang</surname>
              <given-names>Nan Nan</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Peng</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>You Long</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Ling</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Yuan Ding</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>Gang</given-names>
            </name>
            <name>
              <surname>Lan</surname>
              <given-names>Yi Qing</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Sheng</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>Jin Lin</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Yu Zhou</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1111/jcpe.13689</pub-id>
          <article-title>Predicting the risk of dental implant loss using deep learning</article-title>
          <source>J. Clin. Periodontol.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>235</volume>
          <page-range>121135</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>Xin Quan</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Xu Guang</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Xue Chen</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Wen Ting</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>Lin Lin</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Xin</given-names>
            </name>
            <name>
              <surname>Deng</surname>
              <given-names>Yong Qiang</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2023.121135</pub-id>
          <article-title>Two-stream regression network for dental implant position prediction</article-title>
          <source>Expert Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>187</volume>
          <page-range>108735</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ramachandran</surname>
              <given-names>Rajesh A</given-names>
            </name>
            <name>
              <surname>Barão</surname>
              <given-names>Valentim A</given-names>
            </name>
            <name>
              <surname>Ozevin</surname>
              <given-names>Didem</given-names>
            </name>
            <name>
              <surname>Sukotjo</surname>
              <given-names>Cortino</given-names>
            </name>
            <name>
              <surname>Mathew</surname>
              <given-names>Mathew</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.triboint.2023.108735</pub-id>
          <article-title>Early predicting tribocorrosion rate of dental implant titanium materials using random forest machine learning models</article-title>
          <source>Tribol. Int.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>38</volume>
          <page-range>576-582</page-range>
          <issue>3</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Rekawek</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Eliot  Herbst</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Suri</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Brian  Ford</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Chamith  Rajapakse</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Panchal</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.11607/jomi.9852</pub-id>
          <article-title>Machine learning and artificial intelligence: A web-based implant failure and peri-implantitis prediction model for clinicians</article-title>
          <source>Int. J. Oral Maxillofac. Implants</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>4862</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Park</surname>
              <given-names>Wonse</given-names>
            </name>
            <name>
              <surname>Huh</surname>
              <given-names>Jong Ki</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>Jae Hong</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-023-32118-1</pub-id>
          <article-title>Automated deep learning for classification of dental implant radiographs using a large multi-center dataset</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>13</volume>
          <page-range>16856</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Park</surname>
              <given-names>Jin Hong</given-names>
            </name>
            <name>
              <surname>Moon</surname>
              <given-names>Hyun Seok</given-names>
            </name>
            <name>
              <surname>Jung</surname>
              <given-names>H. I.</given-names>
            </name>
            <name>
              <surname>Hwang</surname>
              <given-names>Jiwoo</given-names>
            </name>
            <name>
              <surname>Choi</surname>
              <given-names>Young Ho</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>Ji Eun</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1038/s41598-023-42385-7</pub-id>
          <article-title>Deep learning and clustering approaches for dental implant size classification based on periapical radiographs</article-title>
          <source>Sci. Rep.</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>640</page-range>
          <issue>6</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>Yen Chen</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Ming Yu</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Tzu Ying</given-names>
            </name>
            <name>
              <surname>Chan</surname>
              <given-names>Ming Li</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Yue Yung</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Yu Ling</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>Pei Ting</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Guan Jhih</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Tai Feng</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Chiung An</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Shih Lun</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Kuo Chen</given-names>
            </name>
            <name>
              <surname>Abu</surname>
              <given-names>P. A. R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/bioengineering10060640</pub-id>
          <article-title>Improving dental implant outcomes: CNN-based system accurately measures degree of peri-implantitis damage on periapical film</article-title>
          <source>Bioengineering</source>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <volume>102</volume>
          <page-range>727-733</page-range>
          <issue>7</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Park</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Schwendicke</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Krois</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Huh</surname>
              <given-names>J. K.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>J. H.</given-names>
            </name>
          </person-group>
          <article-title>Identification of dental implant systems using a large-scale multicenter data set</article-title>
          <source>J. Dent. Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>233</page-range>
          <issue>2</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lee</surname>
              <given-names>D. W.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>S. Y.</given-names>
            </name>
            <name>
              <surname>Jeong</surname>
              <given-names>S. N.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>J. H.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/diagnostics11020233</pub-id>
          <article-title>Artificial intelligence in fractured dental implant detection and classification: Evaluation using dataset from two dental hospitals</article-title>
          <source>Diagnostics</source>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <volume>335</volume>
          <page-range>806-808</page-range>
          <issue>7624</issue>
          <year>2007</year>
          <person-group person-group-type="author">
            <name>
              <surname>Elm</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Altman</surname>
              <given-names>D.G.</given-names>
            </name>
            <name>
              <surname>Egger</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Pocock</surname>
              <given-names>S.J.</given-names>
            </name>
            <name>
              <surname>Gotzsche</surname>
              <given-names>P. C.</given-names>
            </name>
            <name>
              <surname>Vandenbroucke</surname>
              <given-names>J.P.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1136/bmj.39335.541782.AD</pub-id>
          <article-title>The strengthening the reporting of observational studies in epidemiology (strobe) statement: Guidelines for reporting observational studies</article-title>
          <source>J. Clin. Epidemiol.</source>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="journal">
          <volume>20</volume>
          <page-range>463-469</page-range>
          <issue>4</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Lee</surname>
              <given-names>J.H.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>Y.T.</given-names>
            </name>
            <name>
              <surname>Jeong</surname>
              <given-names>S.N.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>N.H.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>D.W.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1111/cid.12621</pub-id>
          <article-title>Incidence and pattern of implant fractures: A long-term follow-up multicenter study</article-title>
          <source>Clin. Implant. Dent. Relat. Res.</source>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <volume>28</volume>
          <page-range>433-441</page-range>
          <issue>2</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Macherla</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Kotapati</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Sunitha</surname>
              <given-names>M.T.</given-names>
            </name>
            <name>
              <surname>Chittipireddy</surname>
              <given-names>K.R.</given-names>
            </name>
            <name>
              <surname>Attuluri</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Vatambeti</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.18280/isi.280219</pub-id>
          <article-title>Deep learning framework-based chaotic hunger games search optimization algorithm for prediction of air quality index</article-title>
          <source>Ing. Syst. Inf.</source>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <volume>3</volume>
          <page-range>521-536</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Baswaraju</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Maheswari</surname>
              <given-names>V. U.</given-names>
            </name>
            <name>
              <surname>Chennam</surname>
              <given-names>K. K.</given-names>
            </name>
            <name>
              <surname>Thirumalraj</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kantipudi</surname>
              <given-names>M. P.</given-names>
            </name>
            <name>
              <surname>Aluvalu</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s44230-023-00046-y</pub-id>
          <article-title>Future food production prediction using AROA based hybrid deep learning model in agri-sector</article-title>
          <source>Hum-Cent. Intell. Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="conf-paper">
          <page-range>618–626</page-range>
          <year>2017</year>
          <person-group person-group-type="author">
            <name>
              <surname>Selvaraju</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Cogswell</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Das</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Vedantam</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Parikh</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Batra</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICCV.2017.74</pub-id>
          <article-title>Grad-CAM: Visual explanations from deep networks via gradient-based localization</article-title>
          <source>Proceedings of the International Conference on Computer Vision, Venice, Italy</source>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <volume>23</volume>
          <page-range>7037</page-range>
          <issue>16</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mumenin</surname>
              <given-names>K. M.</given-names>
            </name>
            <name>
              <surname>Biswas</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>M. A. M.</given-names>
            </name>
            <name>
              <surname>Alammary</surname>
              <given-names>A. S.</given-names>
            </name>
            <name>
              <surname>Nahid</surname>
              <given-names>A. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s23167037</pub-id>
          <article-title>A modified aquila-based optimized XGBoost framework for detecting probable seizure status in neonates</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <volume>45</volume>
          <page-range>8</page-range>
          <issue>1</issue>
          <year>2021</year>
          <person-group person-group-type="author">
            <name>
              <surname>Zerouaoui</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Idri</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1007/s10916-020-01689-1</pub-id>
          <article-title>Reviewing machine learning and image processing based decision-making systems for breast cancer imaging</article-title>
          <source>J. Med. Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="conf-paper">
          <page-range>3933–3944</page-range>
          <year>2019</year>
          <person-group person-group-type="author">
            <name>
              <surname>Paszke</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Gross</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Massa</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Lerer</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Bradbury</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Chanan</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Killeen</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Gimelshein</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Antiga</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Desmaison</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Köpf</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>DeVito</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Raison</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Tejani</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Chilamkurthy</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Steiner</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Fang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Bai</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Chintala</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>
          <source>the 33rd International Conference on Neural Information Processing Systems, Vancouver, Canada</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>