<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="research-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">MITS</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Mechatronics and Intelligent Transportation Systems</journal-title>
        <abbrev-journal-title abbrev-type="issn">Mechatron. Intell Transp. Syst.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">MITS</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2958-0218</issn>
      <issn publication-format="print">2958-020X</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-z3oYOTn-N9ZpipRXXtNGoujJe9Rqta1-</article-id>
      <article-id pub-id-type="doi">10.56578/mits040105</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Automated Vehicle Dent Detection Using Hybrid Image Processing and Fuzzy Decision Making</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0004-1254-7959</contrib-id>
          <name>
            <surname>Ullah</surname>
            <given-names>Ikram</given-names>
          </name>
          <email>ikramullah@csu.edu.cn</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_2">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-2526-6927</contrib-id>
          <name>
            <surname>Yow</surname>
            <given-names>Kai Siong</given-names>
          </name>
          <email>ksyow@upm.edu.my</email>
        </contrib>
        <aff id="aff_1">School of Mathematics and Statistics, Central South University, 410083 Changsha, China</aff>
        <aff id="aff_2">Department of Mathematics and Statistics, Faculty of Science, Universiti Putra Malaysia, 43400 UPM Serdang, Malaysia</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>30</day>
        <month>03</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>1</issue>
      <fpage>49</fpage>
      <lpage>60</lpage>
      <page-range>49-60</page-range>
      <history>
        <date date-type="received">
          <day>01</day>
          <month>02</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>20</day>
          <month>03</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Automated detection of vehicle dents remains a challenging task due to variability in lighting conditions, surface textures, and the presence of minor deformations that may mimic actual dents. This paper presents a novel hybrid framework that integrates color deviation analysis, fuzzy classification, and the Structural Similarity Index (SSI) to enhance detection robustness and accuracy. The proposed model employs an adaptive bounding box generation technique, optimized via morphological operations, for precise dent localization. A newly introduced Color Difference Metric (CDM), computed in the Hue, Saturation, and Value (HSV) color space, quantifies subtle color deviations induced by dents, improving the system’s sensitivity to minor deformations. Furthermore, a hybrid classification mechanism—merging step-function classification with fuzzy membership functions—ensures smoother transitions between dent severity levels, mitigating the risks of hard thresholding. SSI serves as a structural integrity validator, helping to differentiate true dents from surface irregularities and lighting artifacts. A Dent Confidence Score is computed as a weighted aggregation of the step-function output, fuzzy confidence levels, and SSI response, effectively balancing sensitivity and specificity. Dents are categorized into three interpretable classes: No Dent, Possible Dent, and Confirmed Dent. Evaluation on real-world datasets—encompassing diverse lighting conditions, vehicle colors, and camera angles—demonstrates the model’s superior performance. Compared to traditional approaches, our method significantly improves key metrics such as Intersection over Union (IoU), Dice Coefficient, Precision, Recall, and F1-Score, underscoring its applicability in real-world automated dent detection systems.</p></abstract>
      <kwd-group>
        <kwd>Dent detection</kwd>
        <kwd>Color deviation analysis</kwd>
        <kwd>Fuzzy classification</kwd>
        <kwd>SSI</kwd>
        <kwd>CDM</kwd>
        <kwd>Adaptive bounding box</kwd>
        <kwd>Image processing</kwd>
        <kwd>Deep learning alternative</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="2"/>
        <fig-count count="3"/>
        <table-count count="3"/>
        <ref-count count="20"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>Vehicle dent detection plays a vital role in ensuring quality control during automotive manufacturing and maintenance processes. Accurate identification of dents and deformations is essential for maintaining vehicle aesthetics, structural integrity, and customer satisfaction. Traditionally, this task has relied on manual visual inspection, which is often time-consuming, subjective, and inconsistent. To address these limitations, recent advancements have focused on automated approaches using computer vision, artificial intelligence (AI), and machine learning (ML). These intelligent systems, especially when combined with image processing techniques, offer efficient, reliable, and cost-effective solutions for real-time dent detection and classification in automotive applications [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>], [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>], [<xref ref-type="bibr" rid="ref_5">5</xref>].</p><p>Various approaches have been employed to tackle the problem of car dent detection, primarily leveraging deep learning and image processing techniques [<xref ref-type="bibr" rid="ref_6">6</xref>], [<xref ref-type="bibr" rid="ref_7">7</xref>]. For example, Parihar et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] proposed a machine intelligence-based model that integrates deep learning algorithms for the detection of dents and scratches on car surfaces. Their approach employs convolutional neural networks (CNNs) trained on annotated datasets, allowing the system to automatically identify and classify surface damages with high accuracy. The study demonstrated that AI-driven approaches significantly outperform traditional manual inspection methods, making them more efficient for large-scale automotive applications. Park et al. [<xref ref-type="bibr" rid="ref_8">8</xref>] introduced a region-based convolutional neural network (R-CNN) for precise dent localization. Their model utilizes selective search to generate region proposals, which are then processed by a CNN to classify and refine the dent boundaries. The R-CNN approach significantly improves detection accuracy over conventional image processing techniques by learning hierarchical features from raw image data. This model proved particularly effective in handling complex surface variations and identifying small dents that are often overlooked by traditional methods.</p><p>Martis et al. [<xref ref-type="bibr" rid="ref_9">9</xref>] advanced car dent detection by developing a car damage assessment recommendation system using neural networks. The system leverages neural networks for feature extraction and classification, enabling more accurate detection and assessment of car dents. This model improves performance, especially in cases where the contrast between the dent and the undamaged surface is minimal, and enhances robustness across varying lighting conditions and surface textures. Qaddour and Siddiqa [<xref ref-type="bibr" rid="ref_10">10</xref>] proposed a novel approach to vehicle damage estimation using an enhanced deep learning algorithm. The authors focused on improving the accuracy and efficiency of vehicle damage assessment by combining CNNs with a novel attention mechanism. This approach allowed the model to focus on critical areas of damage within the vehicle images, enhancing its ability to detect even subtle damage features. The proposed model demonstrated promising results, with a notable improvement in both the detection and classification of vehicle damage compared to traditional methods. Additionally, the study highlighted the ability of the model to estimate the extent of the damage, making it highly suitable for insurance claim processing and repair cost estimation.</p><p>Furthermore, Sikun et al. [<xref ref-type="bibr" rid="ref_11">11</xref>] introduced an image measurement system aimed at detecting dents and scratches on used car body parts. Their approach combines high-resolution imaging with AI-based classification, ensuring reliable defect assessment even for worn-out surfaces. The system was tested on a dataset of used vehicles and demonstrated improved precision in distinguishing between genuine dents and superficial scratches. This advancement is particularly useful for the used car industry, where accurate damage assessment is crucial for valuation and resale purposes. Banerjee [<xref ref-type="bibr" rid="ref_12">12</xref>] explored the integration of CNN and support vector machine (SVM) techniques to enhance dent classification. This model leverages CNNs for feature extraction, followed by SVM-based classification to improve robustness against noisy image artifacts. By this methodology, the system achieves higher detection accuracy while maintaining computational efficiency. Pérez-Zarate et al. [<xref ref-type="bibr" rid="ref_13">13</xref>] presented an automated car damage assessment system utilizing computer vision for an insurance company use case. Their approach employs advanced image processing techniques to identify and evaluate car damage, reducing the reliance on manual inspection. The system uses a combination of object detection algorithms and deep learning models to accurately detect damage such as dents, scratches, and other deformations. By automating the assessment process, the model enhances efficiency and consistency in damage evaluation. The proposed system demonstrates significant potential in improving the accuracy and speed of claims processing within the insurance industry.</p><p>Despite advancements in car dent detection, several challenges persist in developing highly reliable and generalized models. One of the primary obstacles is the sensitivity to environmental variations, including lighting conditions, viewing angles, and surface reflections, which can cause significant discrepancies in detection accuracy. These environmental factors often lead to false positives or missed detections, especially in real-world scenarios where the lighting may vary between daylight and low-light conditions or when the vehicle is viewed from different angles. Furthermore, existing models are often limited by surface texture complexity, as dents on certain materials or vehicle types can be difficult to distinguish from surface irregularities, resulting in inconsistent performance across diverse vehicle models. The use of real-time object detection models, such as YOLOv7 [<xref ref-type="bibr" rid="ref_10">10</xref>], has enhanced processing speed, but further optimization is needed to address the accuracy challenges posed by these environmental and textural variations. Additionally, many models are trained on specific datasets that may not generalize well across different vehicle makes or surface textures, highlighting the need for more robust training methods. Recent approaches have introduced novel methodologies to overcome these challenges.</p><p>He et al. [<xref ref-type="bibr" rid="ref_14">14</xref>] proposed AEGLR-Net, which integrates global and local features through an attention mechanism to better detect car body defects. While this approach significantly improves detection under challenging conditions, it requires large amounts of labeled data and substantial computational resources, which limits its real-time applicability. Hasan et al. [<xref ref-type="bibr" rid="ref_15">15</xref>] developed GroundingCarDD, a multimodal framework combining natural language processing (NLP) with visual data. While this enhances interpretability and aids in tasks like insurance claims processing, the reliance on high-quality textual annotations and domain-specific models introduces inconsistencies across linguistic variations and datasets. Similarly, Mohammed et al. [<xref ref-type="bibr" rid="ref_16">16</xref>] explored the use of deep learning techniques for classifying vehicle damage and estimating repair costs. The authors employ a Mask Region-based Convolutional Neural Network (Mask R-CNN) to identify and classify different types of car damage from images. Their model not only detects damage accurately but also provides a cost estimation for repairs, making it highly relevant for insurance and automotive industries. The study achieves an impressive classification accuracy of 98.5% in detecting damage areas, showcasing the effectiveness of deep learning in automating car damage assessment. However, the study has certain limitations. One major limitation is the reliance on high-quality, well-labeled training data, which may not always be available in real-world scenarios. Additionally, the model may struggle with classifying damage in cases of severe distortion or when damage is not clearly visible, such as in areas with low contrast.</p><p>In this paper, we propose an advanced dent detection framework that integrates multiple complementary features for robust and accurate surface deformation assessment. By combining color deviation analysis, which captures alterations in hue, saturation, and intensity, with fuzzy classification, which provides adaptive confidence levels based on deviation severity, the model effectively distinguishes dents from minor surface variations. Additionally, the SSI enhances detection reliability by quantifying local texture distortions. The final dent confidence score is computed through a weighted fusion of these features, ensuring a balanced trade-off between texture-based and intensity-based dent detection. The model’s decision rule categorizes detected regions into no dent, possible dent, or confirmed dent, enabling an automated and precise classification process. This comprehensive approach minimizes false positives caused by lighting variations and improves dent localization accuracy, making it highly suitable for practical automotive surface inspection applications.</p>
      
        <sec>
          
            <title>1.1. Novelty and contributions</title>
          
          <p>The proposed dent detection framework introduces a novel fusion of color deviation analysis, fuzzy classification, and SSI to enhance dent detection accuracy. The key novelties and contributions of the proposed model are summarized as follows:</p><p>•We introduce an adaptive bounding box formation method that refines dent localization by incorporating morphological operations, ensuring accurate contour-based region extraction.</p><p>•A robust color deviation analysis is formulated in the HSV color space, where a novel CDM quantifies dent-induced variations, improving sensitivity to subtle deformations.</p><p>•A hybrid classification scheme is proposed by integrating a step-function-based classification and a fuzzy membership function. This ensures a smooth transition between dent severity levels, minimizing abrupt decision boundaries.</p><p>•The SSI is incorporated to validate structural integrity, distinguishing genuine dents from minor surface irregularities or lighting artifacts.</p><p>•A novel dent confidence score is designed as a weighted combination of step-function classification, fuzzy confidence, and SSI, leading to an optimized trade-off between sensitivity and specificity.</p><p>•The proposed decision rule classifies detected dents into three categories: No Dent, Possible Dent, and Confirmed Dent, ensuring an interpretable and reliable assessment.</p><p>•Extensive evaluation on real-world datasets demonstrates the superiority of the proposed model over conventional methods, achieving higher robustness in challenging scenarios involving lighting variations and surface texture complexities.</p><p>Overall, the proposed approach enhances the precision and reliability of dent detection by leveraging fuzzy logic and structural validation, paving the way for more accurate and automated defect identification in industrial and automotive applications.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>2. Literature review</title>
      <p>The task of vehicle dent and damage detection has witnessed significant advancements through the integration of computer vision, image processing, and ML techniques. As the demand for accurate, fast, and automated inspection systems increases in automotive manufacturing, maintenance, and insurance assessments, researchers have proposed a variety of approaches tailored to different use cases and operational constraints. This section reviews recent notable contributions in the field, highlighting the strengths and limitations of each method.</p><p>Setyawan et al. [<xref ref-type="bibr" rid="ref_17">17</xref>] performed a comprehensive performance analysis of one-stage and two-stage object detection models for car damage detection. Specifically, they evaluated You Only Look Once (YOLO) as a representative of one-stage detectors and Faster R-CNN for the two-stage category. Their study demonstrated that YOLO is significantly faster in terms of inference time, making it highly suitable for real-time applications where low latency is essential. In contrast, Faster R-CNN achieved higher detection accuracy, particularly for complex and less distinguishable damages. The key achievement of this work lies in its side-by-side benchmarking of speed versus accuracy, offering valuable insights for developers in choosing the appropriate model for their specific needs. However, the study’s performance evaluation was based on a limited dataset, which may not encompass the full variability found in real-world automotive damage scenarios. Moreover, the models occasionally underperformed when dealing with reflective surfaces or in low-light conditions.</p><p>Hoang et al. [<xref ref-type="bibr" rid="ref_18">18</xref>] proposed a significant advancement in vehicle damage detection using deep learning methods. The authors introduced the VeHIDE (Vehicle Damage Identification and Estimation) dataset, which contains a large number of high-resolution images annotated across various damage categories. They applied various deep learning techniques, including Mask R-CNN and Salient Object Detection (SOD), to enhance the accuracy and robustness of car damage detection systems. The study demonstrated that these methods could effectively identify and localize vehicle damage with high precision, achieving impressive performance metrics in terms of both detection accuracy and damage classification. However, the study also acknowledges several limitations. One significant limitation is the dataset's reliance on a specific set of conditions and the diversity of vehicle types, which could affect generalization in real-world scenarios. Additionally, the proposed approach may face challenges in detecting subtle or small damages, especially those with minimal visual differences from the surrounding areas. Finally, the computational complexity of the model and the requirement for high-quality images may pose limitations in practical applications where image quality can vary.</p><p>Ramazhan et al. [<xref ref-type="bibr" rid="ref_19">19</xref>] proposed an enhanced YOLO-based framework coupled with image preprocessing techniques for smart car damage assessment. Their model incorporated image enhancement techniques such as histogram equalization and Gaussian filtering to improve image clarity and reduce noise before feeding the data into the YOLO detector. As a result, the modified YOLO architecture demonstrated superior performance in detecting a wide range of car damages, including dents, cracks, and surface scratches, under varying lighting and background conditions. The main achievement of this work is its robustness in real-world environments and its ability to maintain high detection accuracy in challenging visual conditions. The model also demonstrated strong potential for real-time applications due to its fast processing speed. Nonetheless, limitations include potential difficulty in handling overlapping or occluded damages, and its performance might degrade when exposed to highly textured or cluttered backgrounds that resemble defects.</p><p>Yang et al. [<xref ref-type="bibr" rid="ref_20">20</xref>] developed a ML-based framework for detecting and classifying automotive body surface defects using traditional image processing methods. Their workflow involved preprocessing techniques like edge detection (e.g., Canny) and morphological operations to segment candidate defect regions, followed by feature extraction (e.g., shape, texture) and classification using supervised learning algorithms such as k-NN and decision trees. This approach showed promising results in distinguishing between defect types such as rust, dents, and paint peeling. One notable achievement of this study is its low computational complexity, which makes it suitable for low-power or embedded systems in inspection environments. Additionally, the approach offers better interpretability compared to deep learning-based models. However, its reliance on handcrafted features limits adaptability to complex and unseen damage patterns. It may also suffer from reduced accuracy in noisy environments or when surface defects are subtle and do not exhibit clear edges or morphological features.</p><p>Based on the advancements discussed in the literature, we propose a novel dent detection framework that integrates multiple complementary features for robust and precise surface deformation assessment. By combining color deviation analysis, which captures changes in hue, saturation, and intensity, with fuzzy classification techniques that provide adaptive confidence levels based on the severity of the deviation, our model effectively differentiates between dents and minor surface imperfections. Furthermore, the incorporation of the SSI enhances the reliability of the detection process by quantifying local texture distortions. The final dent confidence score is obtained through a weighted fusion of these features, ensuring an optimal balance between texture-based and intensity-based dent detection. The decision-making rule categorizes detected regions into no dent, possible dent, or confirmed dent, offering an automated and accurate classification mechanism. This integrated approach significantly reduces false positives caused by lighting changes and improves dent localization accuracy, making it highly applicable for real-world automotive surface inspection.</p>
    </sec>
    <sec sec-type="">
      <title>3. Methodology</title>
      <p>The proposed approach for car dent detection consists of three main steps: Bounding Box Formation, Color Deviation Analysis, and Fuzzy Classification with a Step Function.</p><p>First, in Bounding Box Formation, the system identifies the Region of Interest (ROI) using edge detection and contour analysis. This isolates potential dents and encloses them within bounding boxes for further analysis. Next, Color Deviation Analysis is performed in the HSV color space, which better represents perceptual differences in shading and reflection. A CDM is computed by comparing the mean color values of the dented region with the surrounding car surface. Higher CDM values indicate stronger deviations caused by dents. Finally, Fuzzy Classification with a Step Function is applied to classify the region. A step function with two thresholds (T1 and T2) determines whether a region is smooth, possibly dented, or dented. A fuzzy membership function refines this decision, ensuring smooth classification. Additionally, a SSI measures texture and intensity variations, improving accuracy. The system integrates these metrics to produce a dent confidence score, confirming dent presence when it exceeds a set threshold. This approach provides an efficient and robust dent detection method under varying lighting and surface conditions.</p>
      
        <sec>
          
            <title>3.1. Bounding box formation</title>
          
          <p>Let the input grayscale image be <inline-formula>
  <mml:math id="md3jolpq7m">
    <mml:mi>I</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula>, where $x<inline-formula>
  <mml:math id="mk25usiven">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>y<inline-formula>
  <mml:math id="mdqyj19hek">
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>C</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>j</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>A</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>O</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>B$, defined as:</p>
          
            <disp-formula>
              <label>(1)</label>
              <mml:math id="mal72j7w8k">
                <mml:mi>B</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mrow>
                  <mml:mo>{</mml:mo>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:mo>∣</mml:mo>
                  <mml:mo>≤</mml:mo>
                  <mml:mo>≤</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>≤</mml:mo>
                  <mml:mo>≤</mml:mo>
                  <mml:mo>}</mml:mo>
                  <mml:mi>x</mml:mi>
                  <mml:mi>y</mml:mi>
                  <mml:mi>x</mml:mi>
                  <mml:mi>y</mml:mi>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mrow>
                      <mml:mo>min</mml:mo>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mrow>
                      <mml:mo>max</mml:mo>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mrow>
                      <mml:mo>min</mml:mo>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mrow>
                      <mml:mo>max</mml:mo>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, (<inline-formula>
  <mml:math id="m70f0sbrbb">
    <mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mrow>
        <mml:mi>m</mml:mi>
        <mml:mi>i</mml:mi>
        <mml:mi>x</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula>,<inline-formula>
  <mml:math id="murio7sxla">
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mrow>
        <mml:mi>m</mml:mi>
        <mml:mi>i</mml:mi>
        <mml:mi>n</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula>) and (<inline-formula>
  <mml:math id="md9pr1508e">
    <mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mrow>
        <mml:mi>m</mml:mi>
        <mml:mi>a</mml:mi>
        <mml:mi>x</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula>,<inline-formula>
  <mml:math id="m17me2mfdp">
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mrow>
        <mml:mi>m</mml:mi>
        <mml:mi>a</mml:mi>
        <mml:mi>x</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula>) represent the spatial limits of the detected dent region. These boundary values are computed as:</p>
          
            <disp-formula>
              <label>(2)</label>
              <mml:math id="mbqti9228l">
                <mml:msub>
                  <mml:mi>x</mml:mi>
                  <mml:mrow>
                    <mml:mi>m</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mi>n</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>x</mml:mi>
                  <mml:mrow>
                    <mml:mi>m</mml:mi>
                    <mml:mi>a</mml:mi>
                    <mml:mi>x</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>min</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>max</mml:mo>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>x</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          
            <disp-formula>
              <label>(3)</label>
              <mml:math id="mrtt13cm2s">
                <mml:msub>
                  <mml:mi>y</mml:mi>
                  <mml:mrow>
                    <mml:mi>m</mml:mi>
                    <mml:mi>i</mml:mi>
                    <mml:mi>n</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>y</mml:mi>
                  <mml:mrow>
                    <mml:mi>m</mml:mi>
                    <mml:mi>a</mml:mi>
                    <mml:mi>x</mml:mi>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mo>min</mml:mo>
                <mml:mo>,</mml:mo>
                <mml:mo>=</mml:mo>
                <mml:mo>max</mml:mo>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>y</mml:mi>
                    <mml:mi>i</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mk6r5qk7u2">
    <mml:mo>(</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:msub>
      <mml:mi>x</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
    <mml:msub>
      <mml:mi>y</mml:mi>
      <mml:mi>i</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> are the pixel coordinates belonging to the detected contour. The bounding box effectively isolates the dent region, allowing further analysis of its color and texture properties. Torefinethe bounding box, amorphological dilationoperation is applied to ensure that small gaps in the detected edges do not affect the bounding region. This is represented as:</p>
          
            <disp-formula>
              <label>(4)</label>
              <mml:math id="mtl7q68j67">
                <mml:msup>
                  <mml:mi>B</mml:mi>
                  <mml:mrow>
                    <mml:mi>′</mml:mi>
                  </mml:mrow>
                </mml:msup>
                <mml:mo>=</mml:mo>
                <mml:mo>⊕</mml:mo>
                <mml:mi>B</mml:mi>
                <mml:mi>S</mml:mi>
              </mml:math>
            </disp-formula>
          
          <p>where, $S<inline-formula>
  <mml:math id="m1w1gpx26w">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>B^{\prime}$ represents the refined bounding box. The detected bounding box is then used in subsequent stages for color deviation analysis and fuzzy classification to determine the severity and confidence of the detected dent. This approach ensures accurate localization of the dent while minimizing false detections caused by lighting variations or reflections.</p>
        </sec>
      
      
        <sec>
          
            <title>3.2. Color deviation analysis</title>
          
          <p>Car dents cause local distortions in the surface texture and color, which can be effectively analyzed in the HSV color space. Unlike RGB, the HSV color representation separates chromatic information (hue) from intensityvariations (value), making it more robust for detecting subtle changes caused by dents.</p><p>To quantify color deviations, two key color representations are defined:</p><p>Reference car surface mean color: The mean HSV color of anundamaged area near the detected bounding box, denoted as:</p>
          
            <disp-formula>
              <label>(5)</label>
              <mml:math id="mw76d1qmpt">
                <mml:msub>
                  <mml:mi>C</mml:mi>
                  <mml:mrow>
                    <mml:mrow>
                      <mml:mi data-mjx-auto-op="false">ref</mml:mi>
                    </mml:mrow>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>H</mml:mi>
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mi data-mjx-auto-op="false">ref</mml:mi>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>S</mml:mi>
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mi data-mjx-auto-op="false">ref</mml:mi>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>V</mml:mi>
                    <mml:mrow>
                      <mml:mrow>
                        <mml:mi data-mjx-auto-op="false">ref</mml:mi>
                      </mml:mrow>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="m7y152avjs">
    <mml:msub>
      <mml:mi>H</mml:mi>
      <mml:mrow>
        <mml:mrow>
          <mml:mi>r</mml:mi>
          <mml:mi>e</mml:mi>
          <mml:mi>f</mml:mi>
        </mml:mrow>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula>, <inline-formula>
  <mml:math id="mfmeumub9b">
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mrow>
        <mml:mi>r</mml:mi>
        <mml:mi>e</mml:mi>
        <mml:mi>f</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="mf988vc6t3">
    <mml:msub>
      <mml:mi>V</mml:mi>
      <mml:mrow>
        <mml:mi>r</mml:mi>
        <mml:mi>e</mml:mi>
        <mml:mi>f</mml:mi>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> represent the mean hue, saturation, and value of the reference surface.</p><p>Boundingboxmean color: The meanHSV color computed withinthe detected bounding box $B$, given by:</p>
          
            <disp-formula>
              <label>(6)</label>
              <mml:math id="mhng7ye1ni">
                <mml:msub>
                  <mml:mi>C</mml:mi>
                  <mml:mi>B</mml:mi>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>,</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>H</mml:mi>
                    <mml:mi>B</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>S</mml:mi>
                    <mml:mi>B</mml:mi>
                  </mml:msub>
                  <mml:msub>
                    <mml:mi>V</mml:mi>
                    <mml:mi>B</mml:mi>
                  </mml:msub>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mybs9d0o1a">
    <mml:msub>
      <mml:mi>H</mml:mi>
      <mml:mi>B</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>, <inline-formula>
  <mml:math id="mrv7r39ge5">
    <mml:msub>
      <mml:mi>S</mml:mi>
      <mml:mi>B</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula>, and <inline-formula>
  <mml:math id="m65l8bob9p">
    <mml:msub>
      <mml:mi>V</mml:mi>
      <mml:mi>B</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> correspond to the mean HSV values of the dented region.</p><p>To measure the degree of deviation between the dented and undamaged surfaces, a CDM is defined as:</p>
          
            <disp-formula>
              <label>(7)</label>
              <mml:math id="m7rvra65g4">
                <mml:msub>
                  <mml:mi>D</mml:mi>
                  <mml:mrow>
                    <mml:mtext>color</mml:mtext>
                  </mml:mrow>
                </mml:msub>
                <mml:mo>=</mml:mo>
                <mml:msqrt>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>H</mml:mi>
                        <mml:mi>B</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>H</mml:mi>
                        <mml:mrow>
                          <mml:mtext>ref</mml:mtext>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:msup>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>S</mml:mi>
                        <mml:mi>B</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>S</mml:mi>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mi data-mjx-auto-op="false">ref</mml:mi>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:msup>
                  <mml:msup>
                    <mml:mrow>
                      <mml:mo>(</mml:mo>
                      <mml:mo>−</mml:mo>
                      <mml:mo>)</mml:mo>
                      <mml:msub>
                        <mml:mi>V</mml:mi>
                        <mml:mi>B</mml:mi>
                      </mml:msub>
                      <mml:msub>
                        <mml:mi>V</mml:mi>
                        <mml:mrow>
                          <mml:mrow>
                            <mml:mi data-mjx-auto-op="false">ref</mml:mi>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:msub>
                    </mml:mrow>
                    <mml:mn>2</mml:mn>
                  </mml:msup>
                  <mml:mo>+</mml:mo>
                  <mml:mo>+</mml:mo>
                </mml:msqrt>
              </mml:math>
            </disp-formula>
          
          <p>where a higher <inline-formula>
  <mml:math id="mdgchu3wj0">
    <mml:msub>
      <mml:mi>D</mml:mi>
      <mml:mrow>
        <mml:mtext>color</mml:mtext>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula> value indicates a significant deviation incolor characteristics, which is often associated with dents due to light scattering, shadowing, or deformation.</p><p>By applying a threshold-based classification to <inline-formula>
  <mml:math id="myjpzi8b3k">
    <mml:msub>
      <mml:mi>D</mml:mi>
      <mml:mrow>
        <mml:mtext>color</mml:mtext>
      </mml:mrow>
    </mml:msub>
  </mml:math>
</inline-formula>, the dent confidence level is further analyzed using a fuzzy step function.</p>
        </sec>
      
      
        <sec>
          
            <title>3.3. Fuzzy step function for dent classification</title>
          
          <p>The detection of dents involves identifying subtle variations in the car's surface, particularly color deviations that may be influenced by lighting, viewing angle, and surface texture. Traditional classification methods based on hard thresholds can be overly rigid, potentially misclassifying areas with slight imperfections as dents or vice versa. Toovercome this limitation, the proposed approach utilizes a fuzzy step function to classify color deviations into dent categories.</p><p>The step function <inline-formula>
  <mml:math id="m7wc2j9kcb">
    <mml:mi>S</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
  </mml:math>
</inline-formula> is designed to provide a discrete classification of the detected color deviations, where different ranges correspond to different dent categories:</p>
          
            <disp-formula>
              <label>(8)</label>
              <mml:math id="mwr63gd2ch">
                <mml:mi>S</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>D</mml:mi>
                    <mml:mrow>
                      <mml:mtext>color</mml:mtext>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>{</mml:mo>
                  <mml:mo fence="true"/>
                  <mml:mtable columnalign="left left" columnspacing="1em" rowspacing=".2em">
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mn>0</mml:mn>
                        <mml:mo>,</mml:mo>
                      </mml:mtd>
                      <mml:mtd>
                        <mml:msub>
                          <mml:mi>D</mml:mi>
                          <mml:mrow>
                            <mml:mtext>color</mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>T</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                        <mml:mo>&lt;</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mstyle scriptlevel="0">
                          <mml:mspace width="1em"/>
                        </mml:mstyle>
                        <mml:mtext>No dent</mml:mtext>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mn>0.5</mml:mn>
                        <mml:mo>,</mml:mo>
                      </mml:mtd>
                      <mml:mtd>
                        <mml:msub>
                          <mml:mi>T</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>D</mml:mi>
                          <mml:mrow>
                            <mml:mtext>color</mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>T</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msub>
                        <mml:mo>≤</mml:mo>
                        <mml:mo>&lt;</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mstyle scriptlevel="0">
                          <mml:mspace width="1em"/>
                        </mml:mstyle>
                        <mml:mtext>Possible dent</mml:mtext>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mn>1</mml:mn>
                        <mml:mo>,</mml:mo>
                      </mml:mtd>
                      <mml:mtd>
                        <mml:msub>
                          <mml:mi>D</mml:mi>
                          <mml:mrow>
                            <mml:mtext>color</mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>T</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msub>
                        <mml:mo>≥</mml:mo>
                        <mml:mo>(</mml:mo>
                        <mml:mo>)</mml:mo>
                        <mml:mstyle scriptlevel="0">
                          <mml:mspace width="1em"/>
                        </mml:mstyle>
                        <mml:mtext>Confirmed dent</mml:mtext>
                      </mml:mtd>
                    </mml:mtr>
                  </mml:mtable>
                </mml:mrow>
                <mml:mo>=</mml:mo>
              </mml:math>
            </disp-formula>
          
          <p>In this formulation, <inline-formula>
  <mml:math id="mra8f69cuy">
    <mml:msub>
      <mml:mi>T</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> and <inline-formula>
  <mml:math id="m87v6w0kaf">
    <mml:msub>
      <mml:mi>T</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> represent thresholds empirically determined to delineate the three categories: “No Dent”, “Possible Dent”, and “Confirmed Dent”. While effective for basic classification, this step function can introduce abrupt boundaries between categories, which can be problematic in cases where color variations are subtle or influenced by external factors, such as lighting conditions or surface irregularities. Initially, <inline-formula>
  <mml:math id="mdp711c3wr">
    <mml:msub>
      <mml:mi>T</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> is set to capture minimal color deviations, while <inline-formula>
  <mml:math id="mb69gtcgvv">
    <mml:msub>
      <mml:mi>T</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> represents significant dent indicators. These thresholds are refined using data-driven techniques such as grid search or particle swarm optimization, with cross-validation to ensure robust performance across various conditions. This approach allows for dynamic adaptation to environmental variations, ensuring accuracy in diverse scenarios.</p><p>To address this issue, we introduce a fuzzy membership function, <inline-formula>
  <mml:math id="mqax2gco4i">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:mtext>dent</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>D</mml:mi>
        <mml:mrow>
          <mml:mtext>color</mml:mtext>
        </mml:mrow>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula>, to provide a smoother, continuous transition between categories:</p>
          
            <disp-formula>
              <label>(9)</label>
              <mml:math id="m67ajd1mi6">
                <mml:msub>
                  <mml:mi>μ</mml:mi>
                  <mml:mrow>
                    <mml:mtext>dent</mml:mtext>
                  </mml:mrow>
                </mml:msub>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>D</mml:mi>
                    <mml:mrow>
                      <mml:mtext>color</mml:mtext>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>{</mml:mo>
                  <mml:mo fence="true"/>
                  <mml:mtable columnalign="left left" columnspacing="1em" rowspacing=".2em">
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mn>0</mml:mn>
                        <mml:mo>,</mml:mo>
                      </mml:mtd>
                      <mml:mtd>
                        <mml:msub>
                          <mml:mi>D</mml:mi>
                          <mml:mrow>
                            <mml:mtext>color</mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>T</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                        <mml:mo>&lt;</mml:mo>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mfrac>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mi>D</mml:mi>
                              <mml:mrow>
                                <mml:mtext>color</mml:mtext>
                              </mml:mrow>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>T</mml:mi>
                              <mml:mn>1</mml:mn>
                            </mml:msub>
                            <mml:mo>−</mml:mo>
                          </mml:mrow>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mi>T</mml:mi>
                              <mml:mn>2</mml:mn>
                            </mml:msub>
                            <mml:msub>
                              <mml:mi>T</mml:mi>
                              <mml:mn>1</mml:mn>
                            </mml:msub>
                            <mml:mo>−</mml:mo>
                          </mml:mrow>
                        </mml:mfrac>
                        <mml:mo>,</mml:mo>
                      </mml:mtd>
                      <mml:mtd>
                        <mml:msub>
                          <mml:mi>T</mml:mi>
                          <mml:mn>1</mml:mn>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>D</mml:mi>
                          <mml:mrow>
                            <mml:mtext>color</mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>T</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msub>
                        <mml:mo>≤</mml:mo>
                        <mml:mo>&lt;</mml:mo>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mn>1</mml:mn>
                        <mml:mo>,</mml:mo>
                      </mml:mtd>
                      <mml:mtd>
                        <mml:msub>
                          <mml:mi>D</mml:mi>
                          <mml:mrow>
                            <mml:mtext>color</mml:mtext>
                          </mml:mrow>
                        </mml:msub>
                        <mml:msub>
                          <mml:mi>T</mml:mi>
                          <mml:mn>2</mml:mn>
                        </mml:msub>
                        <mml:mo>≥</mml:mo>
                      </mml:mtd>
                    </mml:mtr>
                  </mml:mtable>
                </mml:mrow>
                <mml:mo>=</mml:mo>
              </mml:math>
            </disp-formula>
          
          <p>The fuzzy membership function enhances the model’s ability to handle uncertainty and imprecision in color deviation measurements, ensuring that the dent classification adapts more naturally to variations in surface texture, lighting, and subtle imperfections. This smooth transition reduces the likelihood of false positives or negatives, as it accounts for the gradual nature of color changes and their impact on dent detection. The resulting dent confidence score—a weighted combination of fuzzy membership and step-function classification—ensures that the system can make more reliable, context-sensitive decisions.</p><p>The rationale for using fuzzy logic lies in its ability to handle ambiguity and gradual transitions between categories. In dent detection, where surface deformations may be slight and influenced by various external factors, fuzzy logic provides a robust framework for classifying these variations with greater accuracy. By leveraging fuzzy membership functions, the system can better distinguish between genuine dents and minor imperfections, even in challenging real-world conditions.</p>
        </sec>
      
      
        <sec>
          
            <title>3.4. Ssi</title>
          
          <p>Detecting dents also involves assessing structural distortions on the car surface, which can be subtle and vary in severity. These distortions are often not immediately apparent through color deviation alone, as reflections, lighting variations, and surface texture may obscure them. To address this challenge, we incorporate the SSI, a powerful metric that evaluates the similarity between a dented region and its surrounding undamaged surface.</p><p>The SSI measures structural similarities by considering luminance, contrast, and texture between the dented region and an undamaged reference region. It is mathematically defined as:</p>
          
            <disp-formula>
              <label>(10)</label>
              <mml:math id="my3pqny6a8">
                <mml:mi>S</mml:mi>
                <mml:mi>S</mml:mi>
                <mml:mi>I</mml:mi>
                <mml:mo>=</mml:mo>
                <mml:mfrac>
                  <mml:mrow>
                    <mml:mn>2</mml:mn>
                    <mml:msub>
                      <mml:mi>μ</mml:mi>
                      <mml:mi>A</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>μ</mml:mi>
                      <mml:mi>B</mml:mi>
                    </mml:msub>
                    <mml:msub>
                      <mml:mi>C</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msub>
                    <mml:mo>+</mml:mo>
                  </mml:mrow>
                  <mml:mrow>
                    <mml:msubsup>
                      <mml:mi>μ</mml:mi>
                      <mml:mi>A</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msubsup>
                    <mml:msubsup>
                      <mml:mi>μ</mml:mi>
                      <mml:mi>B</mml:mi>
                      <mml:mn>2</mml:mn>
                    </mml:msubsup>
                    <mml:mo>+</mml:mo>
                    <mml:mo>+</mml:mo>
                    <mml:msub>
                      <mml:mi>C</mml:mi>
                      <mml:mn>1</mml:mn>
                    </mml:msub>
                  </mml:mrow>
                </mml:mfrac>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mliju1derv">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mi>A</mml:mi>
    </mml:msub>
  </mml:math>
</inline-formula> is the mean intensity of pixels inside the detected bounding box $B<inline-formula>
  <mml:math id="m5juldwpgu">
    <mml:mo>(</mml:mo>
    <mml:mo>)</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>\mu_B<inline-formula>
  <mml:math id="mpp6lrvcoc">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>C_1<inline-formula>
  <mml:math id="mzleq5nctf">
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>C_1=(0.01 L)^2<inline-formula>
  <mml:math id="mnabbko87c">
    <mml:mo>,</mml:mo>
    <mml:mi>w</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
  </mml:math>
</inline-formula>L$ is the dynamic range of pixel values.</p><p>The primary advantage of using SSI is its ability to differentiate between genuine dents and other surface irregularities that may cause similar color deviations but lack the structural distortion associated with dents. SSI is less sensitive to changes in illumination and more focused on underlying texture and structural changes, making it a highly effective tool for dent detection in varying environmental conditions. As the severity of the dent increases, the structural differences between the dented and undamaged regions become more pronounced, resulting in lower SSI values. Therefore, SSI serves as a reliable indicator for distinguishing dents from other superficial anomalies. Incorporating SSI into the dent detection framework enhances the model’s sensitivity to subtle structural deformations, ensuring that dents are accurately detected even in complex scenarios where color deviation alone may not suffice. This combination of color deviation analysis and structural similarity provides a comprehensive approach that improves the robustness and reliability of the dent detection system.</p><p>The integration of fuzzy logic and the SSI is particularly effective for car dent detection due to the challenges posed by real-world conditions, such as varying lighting, surface textures, and minor deformations. Fuzzy logic offers a smooth, continuous transition between dent categories, which is crucial in handling the inherent uncertainty and gradual variations in surface deformations. By incorporating fuzzy membership functions, the model is able to adapt to subtle color changes, minimizing misclassifications and improving the robustness of the system. On the other hand, SSI provides an effective way to capture structural distortions that are characteristic of dents, distinguishing them from superficial surface imperfections. By combining these two techniques, the proposed model achieves a high level of accuracy and robustness, ensuring reliable dent detection across a wide range of conditions. The use of fuzzy logic and SSI together addresses the complexities of dent detection in a manner that is both adaptive and precise, making them ideal choices for the task at hand.</p>
        </sec>
      
      
        <sec>
          
            <title>3.5. Final dent score computation</title>
          
          <p>To achieve robust dent detection, we propose a weighted fusion model that integrates three key components. First, color deviation analysis captures the shift in color properties caused by dent-induced surface variations, allowing for the identification of irregularities in the car’s paint or texture. Second, fuzzy classification provides an adaptive confidence level based on the measured color deviation, ensuring a flexible and accurate assessment of potential dents. Finally, the SSI evaluates the degree of local structural distortion, distinguishing dents from normal surface variations. By combining these three components, the proposed model enhances the accuracy and reliability of dent detection.</p><p>The final dent confidence score is computed as:</p>
          
            <disp-formula>
              <label>(11)</label>
              <mml:math id="mv498vl650">
                <mml:mtext>Score</mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>+</mml:mo>
                <mml:mo>(</mml:mo>
                <mml:mo>−</mml:mo>
                <mml:mo>)</mml:mo>
                <mml:msub>
                  <mml:mi>w</mml:mi>
                  <mml:mn>1</mml:mn>
                </mml:msub>
                <mml:msub>
                  <mml:mi>w</mml:mi>
                  <mml:mn>2</mml:mn>
                </mml:msub>
                <mml:msub>
                  <mml:mi>μ</mml:mi>
                  <mml:mrow>
                    <mml:mtext>dent</mml:mtext>
                  </mml:mrow>
                </mml:msub>
                <mml:msub>
                  <mml:mi>w</mml:mi>
                  <mml:mn>3</mml:mn>
                </mml:msub>
                <mml:mi>S</mml:mi>
                <mml:mi>S</mml:mi>
                <mml:mi>S</mml:mi>
                <mml:mi>I</mml:mi>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>D</mml:mi>
                    <mml:mrow>
                      <mml:mtext>color</mml:mtext>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
                <mml:mrow>
                  <mml:mo>(</mml:mo>
                  <mml:mo>)</mml:mo>
                  <mml:msub>
                    <mml:mi>D</mml:mi>
                    <mml:mrow>
                      <mml:mtext>color</mml:mtext>
                    </mml:mrow>
                  </mml:msub>
                </mml:mrow>
                <mml:mn>1</mml:mn>
              </mml:math>
            </disp-formula>
          
          <p>where, <inline-formula>
  <mml:math id="mbsc5aefsu">
    <mml:mi>S</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>D</mml:mi>
        <mml:mrow>
          <mml:mtext>color</mml:mtext>
        </mml:mrow>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> is the step function-based classification score, capturing sharp deviations. <inline-formula>
  <mml:math id="m0besl8b8g">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:mtext>dent</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>D</mml:mi>
        <mml:mrow>
          <mml:mtext>color</mml:mtext>
        </mml:mrow>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> is the fuzzy membership function, providing a gradual confidence transition. (1-SSI) ensures that structural distortions are accounted for, where a lower SSI results in a higher dent confidence score. The weight parameters <inline-formula>
  <mml:math id="mdhdrb3fan">
    <mml:msub>
      <mml:mi>w</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>w</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
    <mml:msub>
      <mml:mi>w</mml:mi>
      <mml:mn>3</mml:mn>
    </mml:msub>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula> are empirically determined based on dataset characteristics and performance optimization. These weights reflect the relative importance of each feature in the final decision. <inline-formula>
  <mml:math id="mrmkxq7wau">
    <mml:msub>
      <mml:mi>w</mml:mi>
      <mml:mn>1</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> emphasizes color deviation, which plays a significant role in detecting obvious dents. <inline-formula>
  <mml:math id="mm5p9fen2w">
    <mml:msub>
      <mml:mi>w</mml:mi>
      <mml:mn>2</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> adjusts for the confidence in the fuzzy classification, balancing the sensitivity to slight color deviations. <inline-formula>
  <mml:math id="mhn0c1fe10">
    <mml:msub>
      <mml:mi>w</mml:mi>
      <mml:mn>3</mml:mn>
    </mml:msub>
  </mml:math>
</inline-formula> accounts for structural distortions through SSI, with a higher weight given when texture alterations are critical for distinguishing genuine dents. The weights are fine-tuned using cross-validation techniques to optimize the model's overall performance across varying environmental conditions and vehicle types.</p><p>By integrating these three features, the model achieves a balanced trade-off between texture-based and intensitybased dent detection, minimizing false positives caused by lighting variations.</p>
        </sec>
      
      
        <sec>
          
            <title>3.6. Decision rule for dent classification</title>
          
          <p>Using the computed dent score, the final classification is determined based on the following case-based decision rule:</p>
          
            <disp-formula>
              <label>(12)</label>
              <mml:math id="mpc4yd3ecb">
                <mml:mtext>Decision</mml:mtext>
                <mml:mo>=</mml:mo>
                <mml:mrow>
                  <mml:mo>{</mml:mo>
                  <mml:mo fence="true"/>
                  <mml:mtable columnalign="left left" columnspacing="1em" rowspacing=".2em">
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mtext>No Dent</mml:mtext>
                        <mml:mo>,</mml:mo>
                      </mml:mtd>
                      <mml:mtd>
                        <mml:mtext>if Score</mml:mtext>
                        <mml:mo>&lt;</mml:mo>
                        <mml:mn>0.3</mml:mn>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mtext>Possible Dent,</mml:mtext>
                      </mml:mtd>
                      <mml:mtd>
                        <mml:mtext>if 0.3</mml:mtext>
                        <mml:mtext>Score</mml:mtext>
                        <mml:mo>≤</mml:mo>
                        <mml:mo>&lt;</mml:mo>
                        <mml:mn>0.7</mml:mn>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd>
                        <mml:mtext>Confirmed Dent,</mml:mtext>
                      </mml:mtd>
                      <mml:mtd>
                        <mml:mtext>if Score</mml:mtext>
                        <mml:mo>≥</mml:mo>
                        <mml:mn>0.7</mml:mn>
                      </mml:mtd>
                    </mml:mtr>
                  </mml:mtable>
                </mml:mrow>
              </mml:math>
            </disp-formula>
          
          <p>The decision rule is based on the computed dent score, which integrates color deviation, fuzzy classification, and SSI. If the score is below 0.3, it indicates minimal deviation, suggesting that the surface is likely intact with no significant irregularities. A score between 0.3 and 0.7 represents a possible dent, where the variation in surface properties is moderate, requiring further verification to confirm the presence of a dent. Finally, a score equal to or greater than 0.7 signifies a confirmed dent, as the deviation is substantial, leading to a high confidence level in the presence of a dent. This structured classification ensures a reliable and adaptive approach for dent detection, balancing sensitivity and specificity effectively.</p>
        </sec>
      
      
        <sec>
          
            <title>3.7. Integration with fuzzy model</title>
          
          <p>The proposed fusion framework leverages the strengths of both step-function-based classification and fuzzy logic:</p><p>•The step function <inline-formula>
  <mml:math id="msqspzwmmt">
    <mml:mi>S</mml:mi>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>D</mml:mi>
        <mml:mrow>
          <mml:mtext>collos</mml:mtext>
        </mml:mrow>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> provides a discrete initial classification, which is refined by the fuzzy membership function.</p><p>•The fuzzy function <inline-formula>
  <mml:math id="m410uv43yg">
    <mml:msub>
      <mml:mi>μ</mml:mi>
      <mml:mrow>
        <mml:mtext>dent</mml:mtext>
      </mml:mrow>
    </mml:msub>
    <mml:mrow>
      <mml:mo>(</mml:mo>
      <mml:mo>)</mml:mo>
      <mml:msub>
        <mml:mi>D</mml:mi>
        <mml:mrow>
          <mml:mtext>coler</mml:mtext>
        </mml:mrow>
      </mml:msub>
    </mml:mrow>
  </mml:math>
</inline-formula> ensures smooth transitions in classification confidence.</p><p>•The SSI component introduces structural validation, reducing misclassification due to minor surface imperfections or varying lighting conditions.</p><p>By combining color deviation analysis, fuzzy logic, and structural similarity evaluation, this approach ensures a reliable and adaptable dent detection framework, making it well-suited for real-world applications.</p>
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Experimental results</title>
      <p>This section outlines the experimental setup, dataset description, and results analysis used to evaluate the proposed dent detection model. The effectiveness of the model is assessed using the Car Dent Dataset (e.g., DentedCar-2024), which contains a diverse collection of car images with real-world dented surfaces captured under various environmental conditions. This dataset includes high-resolution images of vehicles from different manufacturers, representing a variety of car colors, dent types, and surface textures. The images are captured under varying lighting conditions, including daylight and low-light environments, to ensure the model’s robustness across different illumination settings. In addition, the dataset contains images taken from multiple viewing angles, ensuring variability in perspectives, which is critical for assessing the model's ability to generalize. The experiments are conducted using MATLAB 2018, a powerful computational software, to implement the proposed dent detection model. MATLAB's advanced image processing toolbox, combined with its ML capabilities, facilitates the integration of color deviation analysis, fuzzy classification techniques, and the SSI. The model is designed and evaluated in MATLAB 2018, ensuring efficient handling of large image datasets and facilitating the customization of feature extraction and classification algorithms.</p><p>In the experimental setup, the proposed model is applied to segment and classify dented regions by combining bounding box formation, color deviation analysis, and fuzzy classification techniques. The model’s decision-making process integrates the SSI to quantify texture distortions and a fuzzy step function to refine confidence scores based on detected deformation severity. To quantify the segmentation performance, we employ standard evaluation metrics, including IoU, Dice Coefficient, Precision, Recall, and F1-Score. Specifically, we analyze the IoU under daylight and low-light conditions to assess the model’s adaptability to different lighting environments. Furthermore, the Dice Coefficient is computed separately for white and black cars, ensuring the model’s effectiveness across vehicles with varying reflectivity and surface contrast. The classification performance is further analyzed using F1-Scores for different viewing angles, including front view, tilted view, and side view, to examine the model’s robustness in handling perspective variations. The results demonstrate that the proposed model accurately localizes and classifies dents, achieving superior performance compared to traditional edge-based and thresholding approaches.</p><p>The effectiveness of the proposed car dent detection model depends on carefully chosen parameter values for accurate classification. The edge detection step utilizes the Canny operator with lower and upper threshold values set at 50 and 150 , respectively, to ensure robust contour detection while minimizing false edges. For bounding box refinement, a $3 \times 3<inline-formula>
  <mml:math id="my5x9ub4yk">
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>T_1<inline-formula>
  <mml:math id="m8mlbd6krb">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>10</mml:mn>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>T_2<inline-formula>
  <mml:math id="mfv0502oms">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>10</mml:mn>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
  </mml:math>
</inline-formula>T_1<inline-formula>
  <mml:math id="m5gzembyyv">
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>T_2<inline-formula>
  <mml:math id="metdaw2o67">
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>S</mml:mi>
    <mml:mi>I</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mo>.</mml:mo>
  </mml:math>
</inline-formula>11 \times 11<inline-formula>
  <mml:math id="mlmehdou3c">
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>x</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>T</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>b</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>z</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>m</mml:mi>
    <mml:mi>p</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>r</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>y</mml:mi>
    <mml:mi>c</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>w</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>i</mml:mi>
    <mml:mi>g</mml:mi>
    <mml:mi>h</mml:mi>
    <mml:mi>t</mml:mi>
    <mml:mi>v</mml:mi>
    <mml:mi>a</mml:mi>
    <mml:mi>l</mml:mi>
    <mml:mi>u</mml:mi>
    <mml:mi>e</mml:mi>
    <mml:mi>s</mml:mi>
    <mml:mi>o</mml:mi>
    <mml:mi>f</mml:mi>
    <mml:mo>.</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mo>,</mml:mo>
  </mml:math>
</inline-formula>w_1<inline-formula>
  <mml:math id="mjxdrwzmze">
    <mml:mo>=</mml:mo>
    <mml:mo>,</mml:mo>
    <mml:mn>0.4</mml:mn>
  </mml:math>
</inline-formula>w_2<inline-formula>
  <mml:math id="m6ifcwvyjc">
    <mml:mo>=</mml:mo>
    <mml:mn>0.4</mml:mn>
    <mml:mi>a</mml:mi>
    <mml:mi>n</mml:mi>
    <mml:mi>d</mml:mi>
  </mml:math>
</inline-formula>w_3$=0.2, prioritizing color-based deviation while considering structural distortions. These parameter values are optimized based on a dataset of car images under varying lighting and surface conditions, ensuring robust and adaptive dent detection.</p><p> <xref ref-type="fig" rid="fig_1">Figure 1</xref> presents a set of test images used for evaluating the proposed dent detection and segmentation model. The dataset includes a variety of dented cars with different colors, angles, and lighting conditions to ensure a comprehensive assessment of the model,s robustness. The images encompass diverse scenarios such as front, side, and tilted views, along with varying levels of damage severity. This variation allows for a thorough evaluation of the model's performance in terms of segmentation accuracy, IoU, Dice Coefficient, Precision, Recall, and F1-Score across different conditions.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Sample test images used in the proposed model</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_dYTXdAx0mGw_LBBg.png"/>
        </fig>
      
      <p><xref ref-type="fig" rid="fig_2">Figure 2</xref> illustrates the proposed dent detection framework, which consists of three main stages: input image acquisition, pre-processing and classification, and final result generation. In subgraph (a) of <xref ref-type="fig" rid="fig_2">Figure 2</xref> shows a given image of a car with visible dents, which serves as the input for the model. Subgraph (b) of <xref ref-type="fig" rid="fig_2">Figure 2</xref>, the image undergoes multiple preprocessing steps, including noise reduction, contrast enhancement, and edge detection, to enhance the visibility of dents. The stacked layers in the figure represent different feature extraction techniques applied to the image. These extracted features are then passed to a classification module labeled “Dent Score Classification”, which determines the severity of the detected dents. Subgraph (c) of <xref ref-type="fig" rid="fig_2">Figure 2</xref> presents the result, where the model successfully detects and highlights the damaged area with a red bounding box. This figure effectively demonstrates the workflow of the proposed method, from input processing to final dent localization.</p>
      
        <fig id="fig_2">
          <label>Figure 2</label>
          <caption>
            <title>The proposed dent detection framework. (a) The given input image of a dented car, (b) The pre-processing steps and proposed model techniques, including dent score classification, and (c) The final result highlighting the detected damaged area</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_q_SwKXZsGpgS5mzU.png"/>
        </fig>
      
      <p> <xref ref-type="fig" rid="fig_3">Figure 3</xref> further validates the performance of the proposed dent detection model by presenting a comparison between input images and model-generated results. The first row consists of original images of dented cars, showcasing various types and severities of damage. The second row displays the corresponding outputs generated by the model, where the detected dents are marked with red bounding boxes. Each pair of images in the first and second rows allows for a direct comparison, illustrating the model's ability to accurately localize different types of damage. The successful identification of dents in different scenarios demonstrates the reliability of the proposed approach in real-world applications, such as automated vehicle damage assessment and insurance claim processing.</p>
      
        <fig id="fig_3">
          <label>Figure 3</label>
          <caption>
            <title>Detection results: Original dented car images (top row) and model-identified damage areas (bottom row)</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/3/img_KmSSqkGKYPpOP4ua.png"/>
        </fig>
      
      <p><xref ref-type="table" rid="table_1">Table 1</xref> presents a performance analysis of the proposed dent car detection model based on key segmentation metrics across four experiments. The evaluation considers five essential performance measures: IoU, Dice Coefficient, Precision, Recall, and F1-Score, which collectively assess the model’s accuracy in detecting car dents. The IoU values range from 0.90 to 0.93, indicating a high degree of overlap between the predicted and actual dent regions. Similarly, the Dice Coefficient varies from 0.92 to 0.95, confirming strong segmentation consistency.</p><p>The Precision values remain consistently high (0.94) across all experiments, indicating that most of the detected dents are actual dents, minimizing false positives. However, the Recall values are slightly lower (<inline-formula>
  <mml:math id="mcjtctsaw3">
    <mml:mo>≥</mml:mo>
  </mml:math>
</inline-formula> 0.91), suggesting that while the model is highly precise, it may occasionally miss some dented areas. The F1-Score, which balances Precision and Recall, ranges from 0.92 to 0.95, demonstrating the model’s robustness in segmentation tasks. Among all experiments, Experiment 3 shows the best performance with an IoU of 0.93, a Dice Coefficient of 0.95, and a Precision of 0.97, making it the most effective scenario for dent detection. Experiment 4 has the lowest performance, but its metrics are still above 90%, ensuring reliability. Overall, the results show that the proposed approach achieves high precision in dent detection, effectively distinguishing between significant dents and minor surface variations. Moreover, the use of the SSI and fuzzy classification significantly reduces false positives, especially under challenging lighting conditions. The model's performance is also validated across different car colors and viewing angles, demonstrating its robustness and generalizability. This comprehensive evaluation confirms the model's suitability for real-world automotive surface inspection applications, offering a reliable solution for automatic dent detection in diverse conditions.</p>
      
        <table-wrap id="table_1">
          <label>Table 1</label>
          <caption>
            <title>Performance evaluation of the proposed dent car detection model across different experimental scenarios</title>
          </caption>
          <table><tr><th >Experiment</th><th >IoU</th><th >Dice Coefficient</th><th >Precision</th><th >Recall</th><th >F1-Score</th></tr><tr><td >Exp 1</td><td >0.92</td><td >0.94</td><td >0.96</td><td >0.93</td><td >0.94</td></tr><tr><td >Exp 2</td><td >0.91</td><td >0.93</td><td >0.95</td><td >0.92</td><td >0.93</td></tr><tr><td >Exp 3</td><td >0.93</td><td >0.95</td><td >0.97</td><td >0.94</td><td >0.95</td></tr><tr><td >Exp 4</td><td >0.90</td><td >0.92</td><td >0.94</td><td >0.91</td><td >0.92</td></tr></table>
        </table-wrap>
      
      <p>The last two tables present the performance evaluation of the proposed dent detection model for cars under varying conditions, focusing on lighting variations and different viewing angles. <xref ref-type="table" rid="table_2">Table 2</xref> assesses the model's robustness across different lighting environments and car colors using IoU and Dice coefficient as evaluation metrics. The results indicate that the model performs consistently well in both daylight and low-light conditions, with IoU values ranging from 0.90 to 0.94. This demonstrates the model's ability to detect dents reliably despite changes in illumination. Similarly, the Dice coefficient results for white and black cars show a high similarity between the predicted dent masks and ground truth, with values ranging from 0.92 to 0.95. While there is a slight drop in performance under low-light conditions, the impact is minimal, highlighting the model’s resilience to illumination changes and car color variations.</p>
      
        <table-wrap id="table_2">
          <label>Table 2</label>
          <caption>
            <title>Evaluation of the proposed dent car detection model under different lighting conditions and car colors</title>
          </caption>
          <table><tr><th >Daylight (IoU)</th><th >Low Light (IoU)</th><th >White Car (Dice)</th><th >Black Car (Dice)</th></tr><tr><td >0.93</td><td >0.91</td><td >0.94</td><td >0.93</td></tr><tr><td >0.92</td><td >0.90</td><td >0.93</td><td >0.92</td></tr><tr><td >0.94</td><td >0.92</td><td >0.95</td><td >0.94</td></tr></table>
        </table-wrap>
      
      <p>While <xref ref-type="table" rid="table_3">Table 3</xref> evaluates the model's accuracy across different viewpoints—front, tilted, and side—using the F1-score as the performance measure. The results indicate that the model achieves high detection accuracy in all three views, with F1-scores ranging from 0.90 to 0.96. The front and side views show slightly better performance (0.93–0.96) compared to the tilted view (0.90–0.92), which may be due to distortions or occlusions affecting dent visibility at angled perspectives. Despite this minor variation, the model demonstrates strong detection capabilities from multiple viewpoints, ensuring its applicability in real-world scenarios where vehicles are rarely positioned in a perfectly frontal or side-aligned manner.</p>
      
        <table-wrap id="table_3">
          <label>Table 3</label>
          <caption>
            <title>F1-score analysis of the proposed dent car detection model under various viewing angles</title>
          </caption>
          <table><tr><th >Front View (F1-Score)</th><th >Tilted View (F1-Score)</th><th >Side View (F1-Score)</th></tr><tr><td >0.95</td><td >0.91</td><td >0.93</td></tr><tr><td >0.94</td><td >0.90</td><td >0.92</td></tr><tr><td >0.96</td><td >0.92</td><td >0.94</td></tr></table>
        </table-wrap>
      
      <p>Overall, the findings suggest that the proposed dent detection model is highly effective in diverse conditions. It maintains strong performance across different lighting environments, car colors, and viewing angles, making it a robust solution for automated dent detection in real-world applications. The minimal drop in performance under challenging conditions further reinforces its reliability, making it suitable for practical deployment in automotive inspection systems.</p><p>The performance evaluation of the proposed dent detection model, based on the experiments conducted under varying conditions, reveals promising results. In terms of IoU, the model achieved values ranging from 0.90 to 0.94 across different experiments, indicating a high degree of overlap between predicted dent masks and ground truth. The Dice coefficient, which further corroborates the accuracy of segmentation, ranged from 0.92 to 0.95, showing strong consistency across different car colors and lighting conditions. The model demonstrated robustness even in low-light conditions, where the IoU and Dice coefficient only dropped slightly, from 0.93 to 0.91 and 0.94 to 0.92, respectively. For different viewing angles, the F1-Score varied from 0.90 to 0.96, with the tilted view showing the lowest score, suggesting that the model faces challenges when detecting dents from non-frontal perspectives.</p><p>In the error analysis, the performance degradation under low-light conditions and non-frontal views highlights areas where the model could be improved. While the model maintains high accuracy under daylight and front-facing views, slight errors occur due to reduced contrast in low-light scenarios and surface distortion in tilted angles. These issues point to the model’s reliance on visible surface details and its struggle with varying illumination and perspectives. Although the model performs consistently well across different car colors, slight discrepancies in the detection of dents on dark-colored cars (black) might be attributed to lower contrast in such surfaces. Future improvements could involve incorporating more diverse lighting conditions and viewing angles in the training dataset, as well as exploring image enhancement techniques for better performance under challenging lighting conditions.</p>
    </sec>
    <sec sec-type="">
      <title>5. Conclusion</title>
      <p>In this study, we proposed an advanced dent detection and segmentation model tailored for analyzing car dent images under diverse conditions. By integrating structural similarity measures, fuzzy classification, and adaptive segmentation, our model effectively detects dented regions with high precision and robustness. The experimental results demonstrate superior performance across different lighting conditions, car colors, and viewing angles, as evidenced by high IoU, Dice Coefficient, Precision, Recall, and F1Scores. The model's ability to accurately localize dents and minimize false positives makes it a promising approach for automated vehicle damage assessment in real-world scenarios.</p><p>Despite its effectiveness, the proposed model has certain limitations. First, its performance slightly declines in highly reflective or glossy surfaces, where lighting variations may introduce segmentation noise. Second, the model's accuracy is affected when analyzing extremely low-resolution images, as fine dent details become less distinguishable. Third, while the model performs well across different viewpoints, highly occluded or shadowed areas may still pose challenges for accurate dent detection.</p><p>To address these limitations, future research will focus on enhancing the model's robustness to reflection and glare by integrating adaptive illumination correction techniques. Additionally, incorporating deep learning-based feature extraction can further refine the segmentation of complex dent structures. Another promising direction is the development of a multi-view fusion framework to improve dent detection in occluded or shadowed regions by leveraging multiple camera perspectives. Furthermore, real-time implementation using edge computing or mobile-based applications could extend the model’s usability for practical automotive inspection and insurance claim processing.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <volume>25</volume>
          <page-range>1288</page-range>
          <issue>5</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Morales Matamoros</surname>
              <given-names>Oswaldo</given-names>
            </name>
            <name>
              <surname>Takeo Nava</surname>
              <given-names>José Guillermo</given-names>
            </name>
            <name>
              <surname>Moreno Escobar</surname>
              <given-names>Jesús Jaime</given-names>
            </name>
            <name>
              <surname>Ceballos Chávez</surname>
              <given-names>Blanca Alhely</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/s25051288</pub-id>
          <article-title>Artificial intelligence for quality defects in the automotive industry: A systemic review</article-title>
          <source>Sensors</source>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>100332</page-range>
          <year>2022</year>
          <person-group person-group-type="author">
            <name>
              <surname>Van Ruitenbeek</surname>
              <given-names>R. E.</given-names>
            </name>
            <name>
              <surname>Bhulai</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.mlwa.2022.100332</pub-id>
          <article-title>Convolutional neural networks for vehicle damage detection</article-title>
          <source>Mach. Learn. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <volume>11</volume>
          <page-range>981</page-range>
          <issue>10</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Traquinho</surname>
              <given-names>Nelson</given-names>
            </name>
            <name>
              <surname>Vale</surname>
              <given-names>Cecília</given-names>
            </name>
            <name>
              <surname>Ribeiro</surname>
              <given-names>Diogo</given-names>
            </name>
            <name>
              <surname>Meixedo</surname>
              <given-names>Andreia</given-names>
            </name>
            <name>
              <surname>Montenegro</surname>
              <given-names>Pedro</given-names>
            </name>
            <name>
              <surname>Mosleh</surname>
              <given-names>Araliya</given-names>
            </name>
            <name>
              <surname>Calçada</surname>
              <given-names>Rui</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/machines11100981</pub-id>
          <article-title>Damage identification for railway tracks using onboard monitoring systems in in-service vehicles and data science</article-title>
          <source>Machines</source>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <volume>136</volume>
          <page-range>110105</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mumbelli</surname>
              <given-names>J. D.</given-names>
            </name>
            <name>
              <surname>Guarneri</surname>
              <given-names>G. A.</given-names>
            </name>
            <name>
              <surname>Lopes</surname>
              <given-names>Y. K.</given-names>
            </name>
            <name>
              <surname>Casanova</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Teixeira</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.asoc.2023.110105</pub-id>
          <article-title>An application of generative adversarial networks to improve automatic inspection in automotive manufacturing</article-title>
          <source>Appl. Soft Comput.</source>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Rao</surname>
              <given-names>S. S.</given-names>
            </name>
            <name>
              <surname>Desai</surname>
              <given-names>S. R.</given-names>
            </name>
          </person-group>
          <article-title>Automatic dent detection in automobile using IR sensor</article-title>
          <source>Evolutionary Computing and Mobile Sustainable Networks: Proceedings of ICECMSN 2021</source>
          <year>2022</year>
          <page-range>501–511</page-range>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>1201-1209</page-range>
          <issue>10</issue>
          <year>2018</year>
          <person-group person-group-type="author">
            <name>
              <surname>Liang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>M. L.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Cai</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1049/iet-its.2018.5270</pub-id>
          <article-title>Car detection and classification using cascade model</article-title>
          <source>IET Intell. Transp. Syst.</source>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="conf-paper">
          <page-range>1–6</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Parihar</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Sachdeva</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Gupta</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/IC-SIT63503.2024.10861909</pub-id>
          <article-title>Detection and localization of car damages using deep learning</article-title>
          <source>2024 International Conference on Intelligent Computing and Sustainable Innovations in Technology (IC-SIT), Bhubaneswar, India</source>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <volume>10</volume>
          <page-range>1250</page-range>
          <issue>4</issue>
          <year>2020</year>
          <person-group person-group-type="author">
            <name>
              <surname>Park</surname>
              <given-names>S. H.</given-names>
            </name>
            <name>
              <surname>Tjolleng</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Chang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Cha</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Jung</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app10041250</pub-id>
          <article-title>Detecting and localizing dents on vehicle bodies using region-based convolutional neural network</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <volume>92</volume>
          <page-range>24–31</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Martis</surname>
              <given-names>J. E.</given-names>
            </name>
            <name>
              <surname>Sannidhan</surname>
              <given-names>M. S.</given-names>
            </name>
            <name>
              <surname>Aravinda</surname>
              <given-names>C.V.</given-names>
            </name>
            <name>
              <surname>Balasubramani</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.matpr.2023.03.259</pub-id>
          <article-title>Car damage assessment recommendation system using neural networks</article-title>
          <source>Mater. Today Proc.</source>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <volume>18</volume>
          <page-range>200192</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Qaddour</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Siddiqa</surname>
              <given-names>S. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.iswa.2023.200192</pub-id>
          <article-title>Automatic damaged vehicle estimator using enhanced deep learning algorithm</article-title>
          <source>Intell. Syst. Appl.</source>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="conf-paper">
          <page-range>543–548</page-range>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Sikun</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Ying</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Cunwei</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICBIR57571.2023.10147644</pub-id>
          <article-title>An image measurement system for detecting dents and scratches on the surface of used car body parts</article-title>
          <source>2023 8th International Conference on Business and Industrial Research (ICBIR), Bangkok, Thailand</source>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="conf-paper">
          <page-range>101–107</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Banerjee</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ICTACS62700.2024.10841110</pub-id>
          <article-title>Robust car damage identification through CNN and SVM techniques</article-title>
          <source>2024 4th International Conference on Technological Advancements in Computational Sciences (ICTACS), Tashkent, Uzbekistan</source>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <volume>14</volume>
          <page-range>9560</page-range>
          <issue>20</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Pérez-Zarate</surname>
              <given-names>S. A.</given-names>
            </name>
            <name>
              <surname>Corzo-García</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Pro-Martín</surname>
              <given-names>J. L.</given-names>
            </name>
            <name>
              <surname>Álvarez-García</surname>
              <given-names>J. A.</given-names>
            </name>
            <name>
              <surname>Martínez-del-Amor</surname>
              <given-names>M. A.</given-names>
            </name>
            <name>
              <surname>Fernández-Cabrera</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/app14209560</pub-id>
          <article-title>Automated car damage assessment using computer vision: Insurance company use case</article-title>
          <source>Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="journal">
          <volume>90</volume>
          <page-range>102806</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>He</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1016/j.rcim.2024.102806</pub-id>
          <article-title>AEGLR-Net: Attention enhanced global–local refined network for accurate detection of car body surface defects</article-title>
          <source>Robot. Comput. Integr. Manuf.</source>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <volume>12</volume>
          <page-range>179464-179477</page-range>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hasan</surname>
              <given-names>M. J.</given-names>
            </name>
            <name>
              <surname>Nalwan</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ong</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Jahani</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Boo</surname>
              <given-names>Y. L.</given-names>
            </name>
            <name>
              <surname>Nguyen</surname>
              <given-names>K. C.</given-names>
            </name>
            <name>
              <surname>Hasan</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2024.3506563</pub-id>
          <article-title>GroundingCarDD: Text-guided multimodal phrase grounding for car damage detection</article-title>
          <source>IEEE Access</source>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <volume>35</volume>
          <page-range>1-9</page-range>
          <issue>1</issue>
          <year>2023</year>
          <person-group person-group-type="author">
            <name>
              <surname>Mohammed</surname>
              <given-names>N. A.</given-names>
            </name>
            <name>
              <surname>Potrus</surname>
              <given-names>M. Y.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>A. M.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.21271/ZJPAS.35.1.1</pub-id>
          <article-title>Deep learning based car damage classification and cost estimation</article-title>
          <source>Zanco J. Pure Appl. Sci.</source>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <volume>15</volume>
          <page-range>1064–1073</page-range>
          <issue>7</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Setyawan</surname>
              <given-names>H. A.</given-names>
            </name>
            <name>
              <surname>Bustamam</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Buyung</surname>
              <given-names>R. A.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.14569/ijacsa.2024.01507103</pub-id>
          <article-title>Analysis performance of one-stage and two stage object detection method for car damage detection</article-title>
          <source>Int. J. Adv. Comput. Sci. Appl.,</source>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <volume>9</volume>
          <page-range>24-43</page-range>
          <issue>1</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Hoang</surname>
              <given-names>V. D.</given-names>
            </name>
            <name>
              <surname>Huynh</surname>
              <given-names>N. T.</given-names>
            </name>
            <name>
              <surname>Tran</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Le</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Le</surname>
              <given-names>T. M. C.</given-names>
            </name>
            <name>
              <surname>Selamat</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Nguyen</surname>
              <given-names>H. D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1080/24751839.2024.2367387</pub-id>
          <article-title>Powering AI-driven car damage identification based on VeHIDE dataset</article-title>
          <source>J. Inf. Telecommun.</source>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <volume>16</volume>
          <page-range>211</page-range>
          <issue>3</issue>
          <year>2025</year>
          <person-group person-group-type="author">
            <name>
              <surname>Ramazhan</surname>
              <given-names>Muhammad Remzy Syah</given-names>
            </name>
            <name>
              <surname>Bustamam</surname>
              <given-names>Alhadi</given-names>
            </name>
            <name>
              <surname>Buyung</surname>
              <given-names>Rinaldi Anwar</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.3390/info16030211</pub-id>
          <article-title>Smart car damage assessment using enhanced YOLO algorithm and image processing techniques</article-title>
          <source>Information</source>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <volume>36</volume>
          <page-range>015408</page-range>
          <issue>1</issue>
          <year>2024</year>
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Y. He Z. Zhang Y. Liu X. Li</surname>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <pub-id pub-id-type="doi">10.1088/1361-6501/ad80b5</pub-id>
          <article-title>Research on detection and classification of automotive body surface defects based on image processing and machine learning</article-title>
          <source>Meas. Sci. Technol.</source>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>