<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.3 20210610//EN" "JATS-journalpublishing1-3.dtd">
<article xml:lang="en" dtd-version="1.3" article-type="review-article" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML">
  <front>
    <journal-meta>
      <journal-id journal-id-type="publisher-id">ATAIML</journal-id>
      <journal-id journal-id-type="doi">10.56578</journal-id>
      <journal-title-group>
        <journal-title>Acadlore Transactions on AI and Machine Learning</journal-title>
        <abbrev-journal-title abbrev-type="issn">Acadlore Trans. Mach. Learn.</abbrev-journal-title>
        <abbrev-journal-title abbrev-type="publisher">ATAIML</abbrev-journal-title>
      </journal-title-group>
      <issn publication-format="electronic">2957-9570</issn>
      <issn publication-format="print">2957-9562</issn>
      <publisher>
        <publisher-name>Acadlore</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="publisher-id">AR-a_WVKLdlmSXXFciJScBuLddCfnMNc80e</article-id>
      <article-id pub-id-type="doi">10.56578/ataiml040304</article-id>
      <article-categories>
        <subj-group>
          <subject>Articles</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Artificial Intelligence in Electroencephalography: A Comprehensive Survey of Methods, Challenges, and Applications</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0009-0005-2671-8832</contrib-id>
          <name>
            <surname>Mutlu</surname>
            <given-names>Abdulvahap</given-names>
          </name>
          <email>241144107@firat.edu.tr</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0001-9677-5684</contrib-id>
          <name>
            <surname>DoğAn</surname>
            <given-names>şEngüL</given-names>
          </name>
          <email>sdogan@firat.edu.tr</email>
        </contrib>
        <contrib contrib-type="author" rid="aff_1">
          <contrib-id contrib-id-type="orcid" authenticated="true">https://orcid.org/0000-0002-5126-6445</contrib-id>
          <name>
            <surname>Tuncer</surname>
            <given-names>TüRker</given-names>
          </name>
          <email>turkertuncer@firat.edu.tr</email>
        </contrib>
        <aff id="aff_1">Department of Digital Forensics Engineering, Technology Faculty, Firat University, 23119 Elazig, Turkey</aff>
      </contrib-group>
      <pub-date publication-format="electronic" date-type="pub">
        <day>15</day>
        <month>09</month>
        <year>2025</year>
      </pub-date>
      <volume>4</volume>
      <issue>3</issue>
      <fpage>186</fpage>
      <lpage>218</lpage>
      <page-range>186-218</page-range>
      <history>
        <date date-type="received">
          <day>21</day>
          <month>07</month>
          <year>2025</year>
        </date>
        <date date-type="accepted">
          <day>11</day>
          <month>09</month>
          <year>2025</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>©2025 by the author(s)</copyright-statement>
        <copyright-year>2025</copyright-year>
        <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
          <license-p> Published by Acadlore Publishing Services Limited, Hong Kong. This article is available for free download and can be reused and cited, provided that the original published version is credited, under the CC BY 4.0 license.</license-p>
        </license>
      </permissions>
      <abstract><p>Electroencephalography (EEG) provides a non-invasive approach for capturing brain dynamics and has become a cornerstone in clinical diagnostics, cognitive neuroscience, and neuroengineering. The inherent complexity, low signal-to-noise ratio, and variability of EEG signals have historically posed substantial challenges for interpretation. In recent years, artificial intelligence (AI), encompassing both classical machine learning (ML) and advanced deep learning (DL) methodologies, has transformed EEG analysis by enabling automatic feature extraction, robust classification, regression-based state estimation, and synthetic data generation. This survey synthesizes developments up to 2025, structured along three dimensions. The first dimension is task category, e.g., classification, regression, generation and augmentation, clustering and anomaly detection. The second dimension is the methodological framework, e.g., shallow learners, Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Transformers, Graph Neural Networks (GNNs), and hybrid approaches. The third dimension is application domain, e.g., neurological disease diagnosis, brain-computer interfaces (BCIs), affective computing, cognitive workload monitoring, and specialized tasks such as sleep staging and artifact removal. Publicly available EEG datasets and benchmarking initiatives that have catalyzed progress were reviewed in this study. The strengths and limitations of current AI models were critically evaluated, including constraints related to data scarcity, inter-subject variability, noise sensitivity, limited interpretability, and challenges of real-world deployment. Future research directions were highlighted, including federated learning (FL) and privacy-preserving learning, self-supervised pretraining of Transformer-based architectures, explainable artificial intelligence (XAI) tailored to neurophysiological signals, multimodal fusion with complementary biosignals, and the integration of lightweight on-device AI for continuous monitoring. By bridging historical foundations with cutting-edge innovations, this survey aims to provide a comprehensive reference for advancing the development of accurate, robust, and transparent AI-driven EEG systems.</p></abstract>
      <kwd-group>
        <kwd>Electroencephalography</kwd>
        <kwd>Artificial intelligence</kwd>
        <kwd>Deep learning</kwd>
        <kwd>Machine learning</kwd>
        <kwd>Brain-computer interface</kwd>
        <kwd>Seizure detection</kwd>
        <kwd>Emotion recognition</kwd>
        <kwd>Explainable artificial intelligence</kwd>
      </kwd-group>
      <counts>
        <count count-type="contributors" count="3"/>
        <fig-count count="3"/>
        <table-count count="4"/>
        <ref-count count="224"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro">
      <title>1. Introduction</title>
      <p>EEG is a non-invasive technique that records the brain’s electrical activity via electrodes placed on the scalp [<xref ref-type="bibr" rid="ref_1">1</xref>], [<xref ref-type="bibr" rid="ref_2">2</xref>]. Due to its portability and relative simplicity, EEG has become central to both clinical neuroscience and BCI applications. However, raw EEG signals are complex and noisy, posing significant challenges for interpretation by human experts [<xref ref-type="bibr" rid="ref_3">3</xref>], [<xref ref-type="bibr" rid="ref_4">4</xref>]. In recent years, AI techniques—particularly ML and DL—have been increasingly employed to automatically analyze EEG data and uncover subtle patterns beyond the reach of traditional analyses [<xref ref-type="bibr" rid="ref_5">5</xref>], [<xref ref-type="bibr" rid="ref_6">6</xref>]. These AI-driven advances promise improved diagnostic tools and new interactive technologies that utilize brain signals [<xref ref-type="bibr" rid="ref_7">7</xref>]. The rapid development of AI for EEG analysis warrants a comprehensive survey to consolidate recent findings. Research in this area has grown explosively, especially in the last five years, fueled by larger EEG datasets and more powerful algorithms. A survey can illuminate the state of the art, from early ML approaches to modern deep neural networks, and highlight how these techniques are transforming EEG-based research and applications. This study aims to bridge historical context with cutting-edge developments, providing researchers a clear roadmap of how AI methods have evolved to meet EEG analysis challenges.</p><p>EEG is widely used to study brain function and to diagnose neurological conditions (e.g., epilepsy and dementia) because it directly reflects brain dynamics in real time [<xref ref-type="bibr" rid="ref_8">8</xref>]. However, the interpretation of EEG traditionally requires laborious manual inspection or hand-crafted feature extraction by experts. AI offers a powerful alternative: algorithms can learn complex EEG patterns and make accurate classifications or predictions, improving both speed and accuracy of the analysis [<xref ref-type="bibr" rid="ref_9">9</xref>]. For example, modern ML can sift through thousands of EEG recordings to detect signatures of Alzheimer’s disease (AD) that clinicians might miss [<xref ref-type="bibr" rid="ref_10">10</xref>], [<xref ref-type="bibr" rid="ref_11">11</xref>]. DL models, by automatically learning features, avoid the biases of manual feature selection and have the potential to make EEG a more robust tool for clinical diagnosis [<xref ref-type="bibr" rid="ref_12">12</xref>]. Beyond medicine, AI-powered EEG analysis is enabling new BCI systems where computers interpret users’ mental commands or emotional states for assistive technology and human-computer interaction [<xref ref-type="bibr" rid="ref_13">13</xref>].</p><p>This study provides a comprehensive survey of AI techniques applied to EEG signals. It covers both classical ML and contemporary DL methods, emphasizing developments through 2025. After introducing a background on EEG signals and AI methodologies, a multi-faceted categorization of AI techniques was proposed for EEG analysis: task category (e.g., classification vs. regression), methodological framework, e.g., Support Vector Machines (SVMs) vs. CNNs, and application domain (e.g., medical diagnosis, emotion recognition, seizure prediction, and cognitive workload assessment). For each category, representative methods were summarized, comparing performance on common datasets (with tables of benchmarks and metrics) and discussing strengths and weaknesses. Then the major public EEG datasets and benchmarks that have driven progress in the field were reviewed. Finally, current challenges (such as data quality, variability, and interpretability issues) were analyzed and promising future directions were outlined. In particular, emphasis was placed on Transformers for long-range temporal modeling, FL and privacy-preserving learning to enable collaborative yet secure EEG analysis across institutions, and XAI tailored to neurophysiological signals. That could shape the next generation of EEG-based technologies. The objective of this study is to provide an authoritative reference that both newcomers and experienced researchers can use to understand the landscape of AI in EEG analysis and to identify opportunities for future research in this interdisciplinary domain.</p>
    </sec>
    <sec sec-type="">
      <title>2. Background and preliminaries</title>
      <p>EEG measures voltage fluctuations generated by ionic currents in neurons, captured at the scalp. EEG signals are characterized by multiple frequency components corresponding to different brain rhythms. They are conventionally divided into frequency bands—delta (approximately 0.5–4 Hz), theta (approximately 4–8 Hz), alpha (approximately 8–12 Hz), beta (approximately 12–30 Hz), and gamma (approximately 30–50+ Hz)—each associated with different brain states [<xref ref-type="bibr" rid="ref_14">14</xref>]. These frequency bands are not only physiologically distinct but also serve as crucial biomarkers in AI-based EEG applications. For example, delta rhythms dominate in deep sleep and are leveraged in sleep stage classification models. Theta activity is linked with memory and drowsiness, making it useful in cognitive workload and driver-fatigue monitoring. Alpha rhythms reflect relaxation and attentional states, and asymmetry in alpha power is widely used in affective computing for emotion recognition. Beta activity is associated with motor control and concentration, forming the basis of motor imagery (MI) BCIs. Gamma oscillations, linked with higher-order cognition, are explored in studies of working memory and perceptual binding. EEG recordings typically have low amplitude (tens of microvolts) and can be recorded from dozens of electrodes placed according to the international 10–20 system across the scalp. During EEG acquisition, signals are often sampled at rates from 128 Hz up to 1000 Hz or more and require amplification and filtering. However, EEG is highly prone to noise and artifacts from muscle activity, eye blinks, electrical interference, etc. [<xref ref-type="bibr" rid="ref_15">15</xref>]. This inherent noisiness and non-stationarity make analysis difficult—preprocessing steps like filtering, artifact removal, e.g., via independent component analysis (ICA), and segmentation are usually applied to isolate meaningful neural signals [<xref ref-type="bibr" rid="ref_16">16</xref>]. Despite these challenges, EEG remains a key tool for its millisecond-level temporal resolution and its ability to reflect cognitive or clinical states in real time [<xref ref-type="bibr" rid="ref_17">17</xref>], [<xref ref-type="bibr" rid="ref_18">18</xref>]. <xref ref-type="fig" rid="fig_1">Figure 1</xref> illustrates an example of a raw EEG segment and its decomposition into these canonical sub-bands. Each band exhibits a characteristic oscillatory pattern that serves as a biomarker in AI-based EEG applications such as sleep staging, emotion recognition, and MI BCIs.</p>
      
        <fig id="fig_1">
          <label>Figure 1</label>
          <caption>
            <title>Examples of EEG waveforms in different frequency bands (delta, theta, alpha, beta, and gamma)</title>
          </caption>
          <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/8/img_7otYVivytV-VxFF6.png"/>
        </fig>
      
      <p>EEG is used in a wide range of applications in neuroscience and engineering. In clinical settings, EEG is indispensable for neurological diagnosis—for example, detecting epileptic seizures and abnormal brain waves, assessing depth of anesthesia, or aiding diagnosis of neurodegenerative diseases [<xref ref-type="bibr" rid="ref_19">19</xref>]. In cognitive and affective research, EEG allows monitoring of mental states such as workload, fatigue, or emotion [<xref ref-type="bibr" rid="ref_20">20</xref>], [<xref ref-type="bibr" rid="ref_21">21</xref>], [<xref ref-type="bibr" rid="ref_22">22</xref>]. In sleep medicine, EEG-based sleep staging classifies sleep phases such as Rapid Eye Movement (REM) and Non-Rapid Eye Movement (NREM) stages for disorders like insomnia or sleep apnea [<xref ref-type="bibr" rid="ref_23">23</xref>], [<xref ref-type="bibr" rid="ref_24">24</xref>]. EEG is also the backbone of BCIs, enabling direct communication or control of external devices via brain signals [<xref ref-type="bibr" rid="ref_25">25</xref>]. Classic BCI paradigms include MI (where users imagine movements to control prosthetics or cursors), P300 spellers using event-related potentials (ERPs) for communication, and neurofeedback systems [<xref ref-type="bibr" rid="ref_26">26</xref>], [<xref ref-type="bibr" rid="ref_27">27</xref>], [<xref ref-type="bibr" rid="ref_28">28</xref>]. These diverse applications each impose different requirements on EEG analysis methods\textemdash for instance, real-time responsiveness for BCIs, or high specificity for medical diagnosis—which in turn influence the choice of AI techniques.</p><p>The application of AI to EEG is not new—even early EEG studies employed pattern recognition algorithms [<xref ref-type="bibr" rid="ref_29">29</xref>]. Traditional ML approaches in EEG analysis often involved manual feature extraction followed by classifiers like Linear Discriminant Analysis (LDA) or SVM [<xref ref-type="bibr" rid="ref_30">30</xref>]. Handcrafted feature pipelines remain valuable, especially when data are limited [<xref ref-type="bibr" rid="ref_31">31</xref>], [<xref ref-type="bibr" rid="ref_32">32</xref>]. Common handcrafted EEG features include power spectral densities in specific bands, wavelet transform coefficients, common spatial patterns (CSP) for BCIs, and connectivity or coherence measures [<xref ref-type="bibr" rid="ref_33">33</xref>], [<xref ref-type="bibr" rid="ref_34">34</xref>]. These features, once extracted, are fed into ML classifiers (SVM, LDA, k-nearest neighbors, random forests, etc.) to perform tasks like detection of a condition or classification of mental states [<xref ref-type="bibr" rid="ref_29">29</xref>], [<xref ref-type="bibr" rid="ref_35">35</xref>]. Such pipelines have shown success in many studies; for example, LDA and SVM were long employed as reliable classifiers for EEG-based BCI systems. However, a limitation of this “feature engineering” approach is that it relies on human expertise to choose the right features and may miss complex patterns [<xref ref-type="bibr" rid="ref_6">6</xref>].</p><p>The rise of DL has shifted the paradigm towards end-to-end feature learning from raw data. In the last decade, as larger EEG datasets became available and computational power increased, researchers started training deep neural networks to automatically learn features directly from EEG signals. Notably, CNNs and RNNs have become prevalent [<xref ref-type="bibr" rid="ref_36">36</xref>], [<xref ref-type="bibr" rid="ref_37">37</xref>], [<xref ref-type="bibr" rid="ref_38">38</xref>]. CNNs are adept at learning spatial and temporal filters from multichannel EEG; they can be applied either on raw time-series or on time-frequency images of EEG (e.g., spectrograms) [<xref ref-type="bibr" rid="ref_39">39</xref>]. RNNs, including popular variants like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs), excel at modeling the temporal dynamics of EEG sequences [<xref ref-type="bibr" rid="ref_40">40</xref>], [<xref ref-type="bibr" rid="ref_41">41</xref>]. CNNs and RNNs can also be combined (e.g., a CNN for feature extraction per time window, feeding into an LSTM for sequence modeling) [<xref ref-type="bibr" rid="ref_42">42</xref>]. More recently, advanced architectures such as Transformers (which use self-attention mechanisms) have been introduced to EEG, showing promise in capturing long-range dependencies in the data [<xref ref-type="bibr" rid="ref_43">43</xref>], [<xref ref-type="bibr" rid="ref_44">44</xref>], [<xref ref-type="bibr" rid="ref_45">45</xref>]. In parallel, hybrid models have emerged—for instance, combining DL with traditional methods or combining multiple neural network types—to use complementary strengths [<xref ref-type="bibr" rid="ref_46">46</xref>]. Throughout this survey, various model types and their adaptations for EEG were encountered.</p><p>Since many AI-on-EEG advancements were driven by BCI research, this study briefly notes the basics of a BCI system [<xref ref-type="bibr" rid="ref_47">47</xref>]. A BCI acquires brain signals (often EEG due to its non-invasive nature), processes them to extract features, then classifies or translates them into commands that drive an external device or computer interface. For example, in an MI BCI, when a user imagines moving their left hand, the EEG rhythms (typically mu and beta rhythms over the motor cortex) desynchronize in a characteristic way; a trained AI model can recognize this pattern and cause a robotic arm to move left [<xref ref-type="bibr" rid="ref_48">48</xref>], [<xref ref-type="bibr" rid="ref_49">49</xref>]. Early BCI implementations used simple ML classifiers on a few handcrafted features (like band power changes), but modern BCIs increasingly rely on DL to improve accuracy and reduce the need for user-specific calibration [<xref ref-type="bibr" rid="ref_50">50</xref>], [<xref ref-type="bibr" rid="ref_51">51</xref>]. The requirement for BCIs to operate in real time with high reliability underscores the importance of robust and computationally efficient AI algorithms for EEG [<xref ref-type="bibr" rid="ref_52">52</xref>]. Additionally, because BCIs directly affect user experience or safety (in assistive devices, for instance), interpretability of the AI’s decisions and generalization across users are critical considerations—issues discussed in later sections of this study [<xref ref-type="bibr" rid="ref_53">53</xref>].</p><p>In summary, EEG signals present unique opportunities and challenges for AI. The rest of this study delves into how various AI techniques are categorized and applied to EEG, what benchmarks and datasets are advancing the field, and how researchers are tackling the inherent hurdles while pushing toward more powerful and generalizable EEG-based AI systems.</p>
    </sec>
    <sec sec-type="">
      <title>3. Categorization of ai techniques for eeg analysis</title>
      <p>AI techniques applied to EEG can be categorized from multiple perspectives. In this study, the methods were categorized into three complementary dimensions: (a) task category (what kind of problem is being solved with EEG data), (b) methodological framework (which type of AI/ML model is used), and (c) application domain (the functional area or purpose of the analysis). This multi-axis categorization helps clarify how certain algorithms align with specific tasks and applications [<xref ref-type="bibr" rid="ref_54">54</xref>]. For each category, typical methods were summarized, providing examples (including datasets and metrics where applicable) and discussing the strengths and weaknesses of the approaches. Comparative tables were included to highlight key methods and performance on benchmark datasets.</p>
      
        <sec>
          
            <title>3.1. Task category</title>
          
          <p>EEG analysis tasks can be broadly divided by the nature of the output that the AI model produces or the problem being addressed.</p>
          
            <sec>
              
                <title>3.1.1 Classification tasks</title>
              
              <p>The majority of EEG studies formulate a classification problem—the AI model assigns the EEG data to one of several discrete classes or categories. Examples include diagnosing a condition (seizure vs. non-seizure EEG, or patient vs. healthy control), recognizing a mental state (e.g., “imagining left hand movement” vs. “right hand movement” in MI BCI, or high vs. low cognitive workload), or detecting an event (like the presence of a P300 ERP vs. no event) [<xref ref-type="bibr" rid="ref_55">55</xref>], [<xref ref-type="bibr" rid="ref_56">56</xref>], [<xref ref-type="bibr" rid="ref_57">57</xref>], [<xref ref-type="bibr" rid="ref_58">58</xref>]. Robust automatic classification of EEG signals is crucial for practical applications [<xref ref-type="bibr" rid="ref_51">51</xref>]. Historically, classification tasks were tackled with traditional ML on extracted features, but recent DL models have reported improved accuracy [<xref ref-type="bibr" rid="ref_59">59</xref>]. For instance, CNNs or RNNs can classify emotional states from EEG with higher accuracy than earlier methods. Classification performance is typically measured by accuracy, precision/recall, or area under the curve (AUC) for each class, depending on the application. A strength of classification formulations is their simplicity and direct relevance to decision-making (e.g., a seizure happening or not). A weakness, however, is that complex brain states may not fall into clear-cut categories (e.g., gradations of emotion or cognitive load), and training classifiers requires sufficient labeled examples of each class [<xref ref-type="bibr" rid="ref_60">60</xref>].</p>
            </sec>
          
          
            <sec>
              
                <title>3.1.2 Regression and continuous estimation tasks</title>
              
              <p>In some cases, the goal is to predict a continuous value or time series from EEG such as estimating a person’s level of drowsiness or cognitive load on a continuous scale, tracking the depth of anesthesia, or reconstructing a continuous hand trajectory from EEG for a prosthetic device [<xref ref-type="bibr" rid="ref_61">61</xref>], [<xref ref-type="bibr" rid="ref_62">62</xref>]. Regression tasks output a numerical value (or a set of values). They are less common than classification in EEG research, but important for scenarios where brain states vary along a spectrum rather than in distinct categories. Common approaches include linear regression models on EEG features, as well as neural networks that output continuous values (e.g., using a linear output layer) [<xref ref-type="bibr" rid="ref_63">63</xref>]. Recurrent networks (LSTM/GRU) are particularly suited for continuous sequence prediction from EEG, as they can learn temporal mappings (e.g., predicting a future EEG trend or an external signal like respiratory rate from EEG). The performance of regression models is measured by metrics like mean squared error (MSE), correlation coefficient ($r$), or mean absolute error [<xref ref-type="bibr" rid="ref_62">62</xref>]. One challenge in regression with EEG is obtaining ground truth for continuous mental states, which often rely on subjective scales or indirect proxies.</p>
            </sec>
          
          
            <sec>
              
                <title>3.1.3 Generation and data augmentation tasks</title>
              
              <p>An emerging task category involves generating or synthesizing EEG signals using AI, typically with generative models. The motivation is often to augment limited EEG datasets or to simulate EEG for scenarios where data are hard to collect. Techniques like Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) have been applied to create realistic EEG-like signals or to enhance data variability [<xref ref-type="bibr" rid="ref_64">64</xref>], [<xref ref-type="bibr" rid="ref_65">65</xref>], [<xref ref-type="bibr" rid="ref_66">66</xref>], [<xref ref-type="bibr" rid="ref_67">67</xref>]. For example, GANs have been used to generate artificial EEG segments to balance class distributions (such as creating “fake” seizure examples to supplement real data in training) and have shown improvements in classifier performance [<xref ref-type="bibr" rid="ref_68">68</xref>], [<xref ref-type="bibr" rid="ref_69">69</xref>], [<xref ref-type="bibr" rid="ref_70">70</xref>]. Recently, diffusion models—which have revolutionized image generation—have been explored for EEG. One study has used a diffusion model to remove artifacts from EEG and improve cross-subject generalization [<xref ref-type="bibr" rid="ref_71">71</xref>], [<xref ref-type="bibr" rid="ref_72">72</xref>]. Another application of generation is style transfer or transformation of EEG (e.g., converting one subject’s EEG style to that of another to facilitate domain adaptation) [<xref ref-type="bibr" rid="ref_73">73</xref>]. While generative models can produce highly realistic EEG patterns, a challenge is ensuring physiological plausibility and that synthetic data cover meaningful variations [<xref ref-type="bibr" rid="ref_74">74</xref>]. Data augmentation through simpler means is also widely used. Sliding windows, signal segmentation and recombination, and noise injection are common strategies to enlarge training data [<xref ref-type="bibr" rid="ref_75">75</xref>]. These conventional augmentations are easier to implement but limited in diversity; by contrast, deep generative augmentations (GAN/VAE) can introduce novel and plausible variations.</p><p>The strength of generation tasks is in addressing data scarcity and improving model robustness; the weakness lies in the risk of generating artifacts or overfitting to synthetic patterns that don’t generalize to real data. Recent innovations have applied diffusion-based models to EEG tasks. For instance, a flexible Denoising Diffusion Probabilistic Model (DDPM) framework synthesized realistic EEG/electrocorticography (ECoG)/Local Field Potential (LFP) signals with preserved spectral and coupling characteristics [<xref ref-type="bibr" rid="ref_76">76</xref>]. EEGDfus, a conditional diffusion architecture combining CNN and Transformer branches, achieved state-of-the-art denoising (correlations up to 0.992) [<xref ref-type="bibr" rid="ref_77">77</xref>]. Transformer-based diffusion models utilizing generated vicinal data enhanced classification accuracy across diverse datasets by 2.5–14% [<xref ref-type="bibr" rid="ref_78">78</xref>]. Multimodal augmentation methods, e.g., EEG functional near-infrared spectroscopy (fNIRS) EFDA CDG, yield significant performance gains such as approximately 98% accuracy on motor tasks and drug-addiction discrimination [<xref ref-type="bibr" rid="ref_79">79</xref>]. ERP-focused conditional diffusion models enabled subject- and session-specific signal generation with domain-aware fidelity metrics [<xref ref-type="bibr" rid="ref_80">80</xref>]. Finally, EEGDM, a diffusion-pretrained representation learning model, achieved superior results on Temple University Hospital (TUH) EEG tasks with a lightweight architecture [<xref ref-type="bibr" rid="ref_81">81</xref>].</p><p>While generative models such as GANs, VAEs, and diffusion networks offer promising solutions for data scarcity and augmentation, they also introduce ethical and safety risks. Synthetic EEG signals may be difficult to distinguish from authentic recordings, raising concerns about data provenance and verification in shared repositories. In clinical contexts, reliance on artificially generated data without proper validation could increase the risk of misdiagnosis or inappropriate treatment recommendations, especially if models learn spurious features present only in synthetic data. Furthermore, adversarial misuse—such as intentionally generating deceptive EEG to manipulate diagnostic systems—is a potential threat. To mitigate these risks, researchers emphasize the need for clear labeling of synthetic vs. real data, rigorous physiological plausibility checks, and transparent reporting standards when using generative EEG in scientific or clinical applications. Ethical frameworks and auditing mechanisms should therefore complement technical advances in generative EEG research to ensure responsible deployment.</p>
            </sec>
          
          
            <sec>
              
                <title>3.1.4 Clustering and anomaly detection</title>
              
              <p>In unsupervised or semi-supervised settings, one may cluster EEG data into groups or detect anomalous EEG segments without explicit labels. For example, clustering could be used to find recurring EEG microstates or to group patients by EEG features [<xref ref-type="bibr" rid="ref_82">82</xref>]. Anomaly detection is relevant in monitoring scenarios (flagging an abnormal EEG pattern that might indicate an impending seizure or equipment malfunction) [<xref ref-type="bibr" rid="ref_83">83</xref>]. Techniques include clustering algorithms (k-means on EEG features, or dimensionality reduction + clustering) and autoencoder (AE)-based anomaly detectors that learn to reconstruct normal EEG and raise an alert when reconstruction error is high (indicating an out-of-distribution pattern). While not as prominent in literature as supervised tasks, these methods are valuable for exploratory analysis and situations with few labels. Their performance is harder to quantify, often requiring qualitative validation or use of surrogate labels.</p>
            </sec>
          
          
            <sec>
              
                <title>3.1.5 Summary of task categories</title>
              
              <p> <xref ref-type="table" rid="table_1">Table 1</xref> provides an overview of common EEG task formulations with examples. Classification is the most prevalent task in literature, but other task types address important needs like continuous monitoring and data scarcity.</p>
              
                <table-wrap id="table_1">
                  <label>Table 1</label>
                  <caption>
                    <title>Summary of EEG analysis tasks and examples</title>
                  </caption>
                  <table><tr><th >Task Category</th><th >Typical Objectives</th><th >Example Applications</th><th >Common Metrics</th></tr><tr><td >Classification</td><td >Assign a label to the EEG segment/trial</td><td >Seizure vs. non-seizure detection; mental state (e.g., emotion) classification; MI command recognition; sleep stage classification</td><td >Accuracy, F1-score, AUC, and kappa</td></tr><tr><td >Regression</td><td >Predict continuous value or time series</td><td >Workload level estimation, drowsiness level, and reconstructing movement kinematics from EEG</td><td >MSE, correlation ( $r$ ), and mean absolute error (MAE)</td></tr><tr><td >Generation (augmentation)</td><td >Produce synthetic EEG or transform EEG (often for data augmentation or denoising)</td><td >Creating additional training data for rare events (e.g., seizures); removing artifacts (e.g., via diffusion model); simulating EEG for BCI training</td><td >Quality judged by realism (visual inspection) or improvement in downstream task performance</td></tr><tr><td >Clustering / unsupervised</td><td >Discover structure without labels; anomaly detection</td><td >Grouping EEG features into clusters (e.g., EEG microstates); detecting abnormal EEG epochs in long recordings</td><td >Clustering validity indices; anomaly detection Receiver Operating Characteristic (ROC) (if ground truth anomalies are available)</td></tr></table>
                </table-wrap>
              
              <p>Each task type may demand different algorithmic solutions—for instance, classification often relies on discriminative models, while generation uses generative models. Next, this study examines AI techniques by methodological framework, noting how they align with these tasks.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.2. Methodological framework</title>
          
          <p>AI algorithms applied to EEG range from established shallow learners to cutting-edge deep architectures. This study categorizes them into subgroups and highlights key characteristics, strengths, and weaknesses of each.</p>
          
            <sec>
              
                <title>3.2.1 Traditional ml (shallow models)</title>
              
              <p>This group includes methods like LDA, SVM, naive Bayes, decision trees, random forests, k-Nearest Neighbors (k-NN), and ensemble methods [<xref ref-type="bibr" rid="ref_84">84</xref>], [<xref ref-type="bibr" rid="ref_85">85</xref>], [<xref ref-type="bibr" rid="ref_86">86</xref>]. These algorithms typically require a feature extraction step from EEG signals (e.g., mean band power, coherence between channels, and entropy measures). They are often fast to train and can work well with limited data, which was advantageous in early EEG studies where datasets were small. For example, SVM was popular in EEG classification and could achieve high accuracy in BCI tasks when combined with well-chosen features. A major benefit of shallow models is their interpretability (somewhat) and lower risk of overfitting on small data—e.g., decision tree ensembles can highlight which features are important, and linear models give weight coefficients. Shallow ensembles remain surprisingly competitive with deep networks on several EEG problems when paired with well-crafted features [<xref ref-type="bibr" rid="ref_87">87</xref>]. However, their capacity to model complex nonlinear patterns is limited compared to deep networks. They also rely heavily on the quality of input features; if the handcrafted features omit critical information, the model cannot recover it. In summary, traditional ML methods serve as a strong baseline and are still used in scenarios with very scarce data or when model interpretability is paramount. However, as data sizes and problem complexity grew, their performance plateaued in comparison to DL approaches.</p>
            </sec>
          
          
            <sec>
              
                <title>3.2.2 Artificial neural networks (anns) and dl</title>
              
              <p>This category encompasses multi-layer neural networks capable of automatic feature learning. The simplest form, Multilayer Perceptrons (MLPs), consisting of fully connected layers, were applied to EEG in the past but found limited use alone (they often underperformed feature-based methods on raw EEG). The revolution came with specialized architectures.</p><p>CNNs</p>
              <p>CNNs are now a dominant model for EEG signal classification. They apply learned convolution filters across the input, which in EEG can capture local temporal patterns or spatial combinations of channels. CNNs are adept at learning translation-invariant features and have been extensively used for tasks like seizure detection, where a CNN can learn waveform patterns of seizures, or for MI, where CNNs can learn spatial filters akin to CSP. Notably, a survey found that CNNs (often with various architectural tweaks) constituted over 90% of DL papers in EEG classification in recent years [<xref ref-type="bibr" rid="ref_72">72</xref>]. The strengths of CNNs are their ability to automatically extract relevant features (avoiding manual feature engineering) and their efficiency due to weight sharing (making them faster to train than fully connected networks with similar layers). Some CNN architectures for EEG incorporate domain knowledge—for instance, filters that first convolve across time and then across channels to separate temporal and spatial filtering [<xref ref-type="bibr" rid="ref_7">7</xref>]. <xref ref-type="fig" rid="fig_2">Figure 2</xref> shows the Deep ConvNet architecture proposed by Schirrmeister et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] for BCI, which applies an initial temporal convolution to capture frequency-specific patterns, followed by spatial filters across electrodes. Subsequent convolution-pooling blocks learn increasingly abstract representations of EEG, and a final dense layer performs classification. This design mimics classical preprocessing steps (band-pass filtering and spatial filtering) while enabling end-to-end feature learning.</p>
              
                <fig id="fig_2">
                  <label>Figure 2</label>
                  <caption>
                    <title>Deep ConvNet for BCI [<xref ref-type="bibr" rid="ref_7">7</xref>]</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/8/img_CC2tkHDKHCyqI5EZ.png"/>
                </fig>
              
              <p>CNNs have achieved state-of-the-art performance on many benchmarks, sometimes rivaling human expert accuracy in tasks like sleep stage scoring or detecting specific EEG patterns [<xref ref-type="bibr" rid="ref_88">88</xref>], [<xref ref-type="bibr" rid="ref_89">89</xref>]. However, CNNs require sufficient training data to learn effectively; with very small EEG datasets, they risk overfitting. They also act as “black boxes,” making it hard to interpret what features have been learned (though techniques exist to visualize CNN filters or use class activation maps). Additionally, many CNNs treat EEG as an image or grid of channels, which may not optimally account for the true 3D geometry of the head. Some newer models address this via Graph Convolution Neural Networks (GCNNs), as shown in <xref ref-type="fig" rid="fig_3">Figure 3</xref>. Overall, CNNs are powerful for spatial-temporal pattern recognition in EEG and remain an active area of model innovation (with variations like deep vs. shallow CNNs, residual connections, separable convolutions for efficiency, etc.).</p>
              
                <fig id="fig_3">
                  <label>Figure 3</label>
                  <caption>
                    <title>GCNs-Net for EEG MI [<xref ref-type="bibr" rid="ref_90">90</xref>]</title>
                  </caption>
                  <graphic xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://media.acadlore.com/assets/media/2025/8/img_WuK-ny4xXQBCXNfJ.png"/>
                </fig>
              
              <p>RNNs</p><p>RNNs, especially LSTM and GRU networks, have been widely used for sequential modeling of EEG data. EEG is inherently time-series, and RNNs can maintain an internal memory to accumulate information over time. For example, an LSTM can integrate evidence over several seconds of EEG to decide if a cognitive event has occurred, potentially improving detection of subtle events that unfold over time. LSTMs and GRUs address the vanishing gradient problem of basic RNNs, enabling learning of longer temporal dependencies. RNNs naturally handle variable-length sequences and can model temporal dynamics like oscillatory phase or ERP shapes. They have been used for tasks like predicting upcoming seizures (by analyzing temporal patterns preceding a seizure) or for continuous sleep stage tracking due to the rise of attention mechanisms and Transformers, which often outperform RNNs in capturing long-range dependency. However, RNNs can be slower to train (due to sequential processing) and may struggle with very long sequences unless truncated or hierarchical approaches are used. In practice, many EEG studies combine CNNs and RNNs (using CNNs to extract per-window features and then RNNs to capture sequences of those features). Pure RNN architectures have somewhat fallen out of favor for some EEG tasks.</p><p>Transformers and attention-based models</p><p>Transformers are a newer class of models that rely on self-attention mechanisms instead of recurrent connections to process sequential data [<xref ref-type="bibr" rid="ref_91">91</xref>]. In the last couple of years, Transformers have rapidly influenced EEG research. Their ability to attend to relationships between any two time points (or channels) in the sequence is advantageous for EEG, where important patterns (like spike-wave discharges in epilepsy or ERP components) may be dispersed in time. Transformers can also integrate information from multi-channel data effectively by treating the multi-channel EEG as a sequence of combined feature vectors. Studies have proposed adaptations like the time-series Transformer (treating EEG time points similarly to words in a sentence), the Vision Transformer applied to EEG time-frequency images, and even the Graph Transformer for EEG where nodes represent electrodes [<xref ref-type="bibr" rid="ref_92">92</xref>], [<xref ref-type="bibr" rid="ref_93">93</xref>], [<xref ref-type="bibr" rid="ref_94">94</xref>]. Transformers excel at capturing long-range dependencies and complex interactions in the data. They have achieved state-of-the-art performance in some application benchmarks, reportedly outperforming CNN/RNN in MI classification, emotion recognition, and seizure detection tasks. They are also highly parallelizable (allowing faster training on GPUs than RNNs which are sequential). However, a key drawback is the need for large training datasets—Transformers have many more parameters and can easily overfit to smaller EEG datasets. EEG datasets are often smaller than the big data regimes where Transformers shine (e.g., ImageNet or large text corpora). Therefore, without careful regularization or pretraining, Transformers might not realize their full potential on EEG. Another issue is interpretability. While the attention weights in Transformers can sometimes be visualized to infer which time periods or channels the model deemed important, interpreting them in a neurophysiological sense is non-trivial. Nonetheless, the emergence of Transformers in EEG analysis is a notable trend, and researchers are actively exploring optimized Transformer architectures for EEG (such as combining convolutional front-ends with Transformer back-ends or using Transformer encoders for spatial filtering).</p><p>Despite their strengths, Transformers are particularly sensitive to EEG artifacts and non-stationarity. Because attention mechanisms treat all time points as potential contributors to prediction, spurious events such as eye blinks, muscle contractions, or electrode drifts can disproportionately influence the learned attention weights. Unlike CNNs that can localize features or RNNs that smooth temporal context, Transformers may overemphasize transient noise if not explicitly constrained. Moreover, EEG’s non-stationary nature—variability across sessions, subjects, or recording conditions—can cause Transformers to overfit to session-specific noise patterns, reducing cross-subject generalization. Large model capacity also exacerbates this risk when training data are limited. Recent work attempts to address these issues by incorporating artifact-aware preprocessing, attention regularization, and hybrid CNN-Transformer designs that first extract robust local features before global modeling. Nevertheless, the challenge of ensuring Transformer robustness to real-world EEG noise remains an open problem and a key consideration for future research.</p><p>Other DL models</p><p>There are additional architectures worth mentioning. AEs have been used in unsupervised learning on EEG such as learning low-dimensional representations or denoising signals [<xref ref-type="bibr" rid="ref_95">95</xref>], [<xref ref-type="bibr" rid="ref_96">96</xref>]. Stacked AEs and deep belief networks (DBNs) were some of the earlier deep models applied to EEG, with moderate success in feature learning [<xref ref-type="bibr" rid="ref_97">97</xref>], [<xref ref-type="bibr" rid="ref_98">98</xref>]. GNNs, including Graph Convolutional Networks (GCN) and graph attention networks, treat EEG channels as nodes in a graph (edges might represent physical adjacency or functional connectivity) and perform learning on this graph structure [<xref ref-type="bibr" rid="ref_99">99</xref>], [<xref ref-type="bibr" rid="ref_100">100</xref>], [<xref ref-type="bibr" rid="ref_101">101</xref>], [<xref ref-type="bibr" rid="ref_102">102</xref>], [<xref ref-type="bibr" rid="ref_103">103</xref>]. GNNs can better exploit the non-grid topology of electrodes and have shown promise in emotion recognition and seizure detection by modeling relationships between electrode signals. Hybrid models combine modalities or algorithmic approaches. For instance, a model might use a CNN to extract features and then an SVM as the final classifier (a convolutional+deep hybrid), or combine EEG with other signals like fNIRS or electrooculography (EOG) in a multi-modal network. Another growing area is transfer learning models—due to the variability in EEG, researchers have explored pretraining a deep model on a large dataset and fine-tuning it on a smaller target dataset, or using domain adaptation networks to transfer knowledge across subjects or tasks. The hybrid and transfer approaches are attempts to boost performance when data are limited or when a single model type alone is insufficient.</p><p>Beyond supervised architectures such as CNNs, RNNs, and Transformers, a growing line of work leverages self-supervised learning (SSL) to exploit large amounts of unlabeled EEG data. SSL frameworks, often built upon deep encoders, pretrain feature representations through contrastive or predictive objectives before fine-tuning on downstream tasks. For example, contrastive SSL has improved emotion recognition under limited labeled data and enabled cross-subject transfer in MI classification [<xref ref-type="bibr" rid="ref_104">104</xref>]. Graph-based SSL methods (e.g., EEG-DisGCMAE) further adapt to varying electrode densities, facilitating knowledge transfer between high- and low-density EEG setups [<xref ref-type="bibr" rid="ref_105">105</xref>]. These approaches consistently demonstrate greater label efficiency, robustness to inter-subject variability, and enhanced generalization compared to purely supervised training. Integrating SSL into EEG analysis thus expands the applicability of DL to domains where data scarcity is a critical bottleneck, complementing the advances achieved by supervised and semi-supervised models.</p>
            </sec>
          
          
            <sec>
              
                <title>3.2.3 Strengths vs. weaknesses</title>
              
              <p>To summarize the algorithmic families, <xref ref-type="table" rid="table_2">Table 2</xref> provides a comparison, highlighting their typical pros and cons in the EEG context.</p>
              <p>While <xref ref-type="table" rid="table_2">Table 2</xref> summarizes general strengths and weaknesses, several direct comparisons on benchmark EEG datasets illustrate performance gaps more concretely. For example, on the CHB-MIT pediatric epilepsy dataset, classical ML pipelines—e.g., Least Squares Support Vector Machine (LS-SVM) with Radial Basis Function (RBF) kernels using handcrafted time-frequency features—achieve approximately 97–98% accuracy with sensitivity around 97% and specificity around 99% [<xref ref-type="bibr" rid="ref_106">106</xref>]. Meanwhile, modern DL models such as hybrid CNN-LSTM architectures reported accuracy near 94.8%, sensitivity around 90.2%, and specificity up to 99.5% [<xref ref-type="bibr" rid="ref_107">107</xref>]; other methods push above 97.5% accuracy with high sensitivity (approximately 98.9%) and low false positive rates (approximately 2%) [<xref ref-type="bibr" rid="ref_108">108</xref>]; and a recent 1D Residual Convolutional Neural Network (ResCNN) model achieves near-perfect performance (accuracy approximately 99.7% and sensitivity approximately 99.6%, specificity approximately 99.6%) alongside extremely low false-positive rates [<xref ref-type="bibr" rid="ref_109">109</xref>].</p>
              
                <table-wrap id="table_2">
                  <label>Table 2</label>
                  <caption>
                    <title>Comparison of algorithmic approaches for EEG analysis</title>
                  </caption>
                  <table><tbody><tr><th colspan="1" rowspan="1"><p>Algorithm Class</p></th><th colspan="1" rowspan="1"><p>Examples</p></th><th colspan="1" rowspan="1"><p>Strengths</p></th><th colspan="1" rowspan="1"><p>Weaknesses</p></th></tr><tr><td colspan="1" rowspan="1"><p>Traditional (shallow)</p></td><td colspan="1" rowspan="1"><p>LDA, SVM, k-NN, random forest, etc.</p></td><td colspan="1" rowspan="1"><p>Simple, fast training; interpretable (for some); works on small data; a well-established theory</p></td><td colspan="1" rowspan="1"><p>Requires manual feature extraction; limited modeling capacity for complex patterns; may not fully exploit raw data</p></td></tr><tr><td colspan="1" rowspan="1"><p>CNN-based networks</p></td><td colspan="1" rowspan="1"><p>Deep ConvNets, EEGNet, etc.</p></td><td colspan="1" rowspan="1"><p>Learns spatial-temporal features automatically; state-of-the-art accuracy in many tasks; efficient convolution operations</p></td><td colspan="1" rowspan="1"><p>Needs large data; can overfit if not regularized; acts as a black box (features not directly interpretable)</p></td></tr><tr><td colspan="1" rowspan="1"><p>RNN-based networks</p></td><td colspan="1" rowspan="1"><p>LSTM, GRU, etc.</p></td><td colspan="1" rowspan="1"><p>Captures temporal dependencies; suitable for sequence data; can model context over time (useful for ERP or trend analysis)</p></td><td colspan="1" rowspan="1"><p>Training can be slow; there is difficulty with very long sequences; older RNNs can suffer vanishing gradients; and they are less used now compared to attention models</p></td></tr><tr><td colspan="1" rowspan="1"><p>Transformer attention</p></td><td colspan="1" rowspan="1"><p>EEGformer, EEGViT [<xref ref-type="bibr" rid="ref_110">110</xref>], Vision Transformer, etc.</p></td><td colspan="1" rowspan="1"><p>Models long-range interactions effectively; often highest performance when data abundant; highly parallelizable; flexible input representations</p></td><td colspan="1" rowspan="1"><p>Very data-hungry; large model size; interpretability of attention weights isn't straightforward; computationally heavy if not optimized</p></td></tr><tr><td colspan="1" rowspan="1"><p>Hybrid / specialized models</p></td><td colspan="1" rowspan="1"><p>GNNs, hybrid CNNRNN, and ensemble methods combining deep and shallow</p></td><td colspan="1" rowspan="1"><p>Can incorporate domain knowledge (e.g., electrode layout in GNN), utilize strengths of multiple methods, and often improve generalization</p></td><td colspan="1" rowspan="1"><p>Added complexity in design; harder to analyze; may require tuning multiple components; not as widely adopted yet</p></td></tr></tbody></table>
                </table-wrap>
              
              <p>In MI BCIs (BCI Competition IV-2a), classical CSP-based pipelines typically yield four-class accuracy in the range of approximately 60–75%. For instance, baseline models often hover around 70% accuracy [<xref ref-type="bibr" rid="ref_111">111</xref>]. DL pipelines such as Filter Bank Convolutional Network (FBCNet)—a CNN variant—achieve around 76.2% accuracy; Transformer-infused architectures like EEG-TCNet reach approximately 77.35%; larger hybrid CNN-Transformer models such as CLTNet obtain around 83.0% accuracy [<xref ref-type="bibr" rid="ref_112">112</xref>]; and state-of-the-art multi-scale Transformer models, e.g., Multi-Scale Convolutional Transformer (MSCFormer), reach approximately 83–83.0% average accuracy, with some reports reaching up to approximately 88% in enhanced configurations [<xref ref-type="bibr" rid="ref_113">113</xref>].</p><p>Similarly, in emotion recognition using the DEAP benchmark, traditional subject-independent pipelines using handcrafted features (e.g., differential entropy) generally reach moderate accuracy levels—often around 70% or lower; although explicit SVM-based numbers weren’t located, these rates are in line with historical baselines. In contrast, deep architectures show striking improvements: CNN-LSTM networks with feature matrices achieve valence/arousal classification accuracy of 96.9% and 97.4%, respectively (presumably subject-dependent) [<xref ref-type="bibr" rid="ref_114">114</xref>], and Transformer-based systems in binary classification exceed 95% accuracy in subject-dependent settings.</p><p>These examples highlight that while traditional methods remain useful for smaller datasets or interpretable pipelines, DL architectures—especially with CNNs, hybrid models, and Transformers—generally dominate when sufficient training data and model capacity are available. In practice, the choice of algorithm depends on the task requirements and available data. For example, a hospital deploying an EEG-based diagnostic tool might favor an interpretable model or one tested for robustness, whereas a competition for BCI accuracy might push for complex deep ensembles to squeeze out extra percentage points of accuracy. The next section looks at EEG-focused AI methods by application domain, which often dictates which algorithms and task formulations are suitable.</p>
            </sec>
          
        </sec>
      
      
        <sec>
          
            <title>3.3. Application domain</title>
          
          <p>Another way to organize AI in EEG research is by application or analytical purpose. Different applications emphasize different aspects of EEG signals and pose distinct challenges. This study highlights several major application domains where AI has been applied to EEG, noting the typical approaches and performance in each.</p>
          
            <sec>
              
                <title>3.3.1 Medical diagnosis and neurological disorders</title>
              
              <p>Using AI to interpret clinical EEGs is a rapidly growing area. EEG is critical in diagnosing conditions like epilepsy, where neurologists examine EEG for epileptiform spikes or seizures, and it’s useful in assessing neurodegenerative diseases (dementia, Parkinson's, etc.), detecting encephalopathy, and more. AI methods are being developed to assist or automate these interpretations.</p><p>Epileptic seizure detection and prediction</p><p>This has been a flagship problem for EEG-based AI [<xref ref-type="bibr" rid="ref_115">115</xref>]. Many studies have segmented EEG into short windows and classified each as “seizure” or “non-seizure.” Classical approaches extract features such as band power, entropy or wavelet coefficients and feed them into SVMs or tree-based classifiers, yielding average accuracies around 75–80%. DL, particularly CNNs, has substantially outperformed these methods. A December 2024 meta-analysis reported pooled DL accuracy of <inline-formula>
  <mml:math id="m7f2jt8v46">
    <mml:mo>≈</mml:mo>
  </mml:math>
</inline-formula> 89% (sensitivity <inline-formula>
  <mml:math id="mcc2epji6z">
    <mml:mo>≈</mml:mo>
  </mml:math>
</inline-formula> 89% and specificity <inline-formula>
  <mml:math id="momqiizs9z">
    <mml:mo>≈</mml:mo>
  </mml:math>
</inline-formula> 91%) versus <inline-formula>
  <mml:math id="m02wmubniu">
    <mml:mo>≈</mml:mo>
  </mml:math>
</inline-formula> 78% accuracy for traditional ML on validation sets [<xref ref-type="bibr" rid="ref_116">116</xref>]. Moreover, state-of-the-art single-study CNNs on the CHB-MIT pediatric dataset routinely exceed 95%, with some fully convolutional networks achieving &gt;99% accuracy, &gt;99.6% specificity, and false-alarm rates below 0.5/h [<xref ref-type="bibr" rid="ref_117">117</xref>], [<xref ref-type="bibr" rid="ref_118">118</xref>]. Alongside deep models, hybrid feature-engineering pipelines have also achieved near-perfect detection in cross-validation. For example, the explainable FriendPat-centric XFE model attained 99.61% accuracy using 10-fold Cross-Validation (CV) (and 79.92% with leave-one-subject-out CV) on a 10,356-signal Turkish epilepsy EEG dataset [<xref ref-type="bibr" rid="ref_119">119</xref>]. Similarly, the neonatal-focused TATPat ensemble obtained 99.1% 10-fold-CV accuracy on the open Helsinki NICU corpus while retaining full interpretability [<xref ref-type="bibr" rid="ref_120">120</xref>], [<xref ref-type="bibr" rid="ref_121">121</xref>]. Beyond detection, seizure prediction—forecasting events minutes to hours in advance—remains more challenging but has also seen rapid progress. Early-warning CNNs evaluated on CHB-MIT reported AUCs of 98.8% with sensitivity &gt;93.5% and false-prediction rates as low as 0.074/h [<xref ref-type="bibr" rid="ref_122">122</xref>], [<xref ref-type="bibr" rid="ref_123">123</xref>]. More recent hybrid CNN-Transformer architectures push sensitivities to approximately 99.3% with &gt;95% specificity and average warning times exceeding one hour [<xref ref-type="bibr" rid="ref_124">124</xref>], [<xref ref-type="bibr" rid="ref_125">125</xref>].</p><p>While AI models for epilepsy detection often report high accuracy in controlled experimental settings, translation into real-world clinical workflows remains non-trivial. Regulatory approval processes, such as U.S. Food and Drug Administration (FDA) clearance, CE marking in Europe, and equivalent certifications in other regions, impose stringent requirements on safety, reliability, reproducibility, and interpretability. In particular, FDA’s Software as a Medical Device (SaMD) framework demands not only proof of accuracy but also robustness across diverse populations, transparent documentation of training datasets, and post-market performance monitoring. Furthermore, dataset limitations (e.g., imbalance between ictal vs. interictal events and scarcity of rare seizure types) and generalizability concerns (across different acquisition hardware, electrode setups, or patient demographics) hinder clinical reliability. Another bottleneck is the “black-box” nature of DL models, which makes regulatory reviewers cautious unless interpretability modules (e.g., saliency maps linked to electrode activity) are integrated.</p><p>Neurodegenerative and cognitive disorders</p><p>EEG changes have been observed in conditions like AD, Parkinson’s disease, mild cognitive impairment (MCI), and attention-deficit/hyperactivity disorder (ADHD). AI is used to detect these changes which might be hard to quantify by eye. For example, Alzheimer’s patients often show shifts in EEG power spectra (more delta/theta and less alpha) and altered connectivity [<xref ref-type="bibr" rid="ref_126">126</xref>], [<xref ref-type="bibr" rid="ref_127">127</xref>]. ML models trained on resting-state EEG have been developed to distinguish Alzheimer’s or MCI patients from healthy aging individuals. A recent study analyzed over 12,000 routine EEG recordings with ML and could identify distinct EEG patterns corresponding to stages of cognitive decline, successfully differentiating patients with Alzheimer’s and Lewy body dementia from normal controls [<xref ref-type="bibr" rid="ref_128">128</xref>]. Impressively, their AI method did this without requiring manual selection of channels or frequency bands, reducing human bias. The ability to learn directly from raw EEG is a strength of modern AI—it may surface novel biomarkers. AI-based diagnosis in this domain is still in exploratory phases but shows potential as a cheap, accessible screening tool (since EEG is widely available and non-invasive). Similarly, for ADHD or autism, EEG ML models have been tested to detect developmental differences (often using EEG during cognitive tasks) [<xref ref-type="bibr" rid="ref_129">129</xref>], [<xref ref-type="bibr" rid="ref_130">130</xref>]. One challenge in these applications is variability—diseases like AD have heterogeneous EEG presentations. Therefore, large training datasets are needed for robustness. The Mayo Clinic study above is a good example of utilizing a huge dataset to achieve generalizable results.</p><p>Other clinical uses</p><p>EEG is routinely used to monitor depth of anesthesia, detect cerebral ischemia, and diagnose sleep disorders, and AI has been applied successfully across all these domains [<xref ref-type="bibr" rid="ref_131">131</xref>], [<xref ref-type="bibr" rid="ref_132">132</xref>]. In automated sleep staging, end-to-end DL models now routinely match or exceed expert consensus: DetectsleepNet—a lightweight, interpretable CNN built on sleep-expert priors—achieved 80.9% accuracy on SHHS and 88.0% on Physio2018 without any hand-crafted preprocessing, while ZleepAnlystNet reported 87.0% overall accuracy and 82.1% macro-F1 on Sleep-EDF-13 [<xref ref-type="bibr" rid="ref_133">133</xref>], [<xref ref-type="bibr" rid="ref_134">134</xref>], [<xref ref-type="bibr" rid="ref_135">135</xref>], [<xref ref-type="bibr" rid="ref_136">136</xref>], [<xref ref-type="bibr" rid="ref_137">137</xref>], [<xref ref-type="bibr" rid="ref_138">138</xref>]. Although overall accuracy remains in the high 80s, Transformer-enhanced architectures and hybrid designs combining CNN with Bidirectional Long Short-Term Memory (BiLSTM) are beginning to push per-stage F1-scores higher and improve cross-cohort robustness [<xref ref-type="bibr" rid="ref_139">139</xref>], [<xref ref-type="bibr" rid="ref_140">140</xref>], [<xref ref-type="bibr" rid="ref_141">141</xref>], [<xref ref-type="bibr" rid="ref_142">142</xref>].</p><p>In the Intensive Care Unit (ICU), AI‐driven EEG alarms are streamlining clinicians’ workflows by delivering fatigue-free, preliminary analyses. Real-time burst-suppression segmentation algorithms—based on adaptive thresholding of local voltage variance—achieved agreement rates with expert readers (<inline-formula>
  <mml:math id="merf8yakh9">
    <mml:mi>κ</mml:mi>
    <mml:mo>≈</mml:mo>
  </mml:math>
</inline-formula> 0.90) [<xref ref-type="bibr" rid="ref_143">143</xref>]. DL models combining CNNs and RNNs for detecting levels of sedation in ICU patients yielded test AUCs of approximately 0.80 [<xref ref-type="bibr" rid="ref_144">144</xref>]. Patient-specific intracranial-EEG seizure detectors—using features such as the Hilbert-Huang transform with Bayesian network classifiers—achieved 96.6% sensitivity at just 0.21 false alarms per hour [<xref ref-type="bibr" rid="ref_145">145</xref>]. Together with emerging EEG-based delirium tracking systems, these tools automatically flag critical events—burst suppression, seizure onsets, or delirium—enabling faster, more reliable alerts [<xref ref-type="bibr" rid="ref_146">146</xref>], [<xref ref-type="bibr" rid="ref_147">147</xref>].</p><p>The primary strengths of AI in medical EEG are consistency (no fatigue or intra-reader bias) and the ability to uncover subtle multivariate patterns directly from raw signals. However, high-reliability clinical deployment demands rigorous validation, regulatory approval, and, critically, model explainability. Interpretable designs—such as the expert-inspired modules in DetectsleepNet that highlight salient channels and temporal features—are essential to build clinician trust. Overall, AI in medical EEG is poised to improve early detection and personalized monitoring. But these applications demand high reliability and interpretability. Black-box models are less acceptable in medicine, prompting research into XAI for EEG (as addressed in Sections 5 and 6).</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.2 Emotion recognition and affective computing</title>
              
              <p>Using EEG to infer human affective states has become a cornerstone of affective computing. Emotional arousal and valence manifest in EEG rhythms—for example, frontal alpha asymmetry reliably indexes positive versus negative affect [<xref ref-type="bibr" rid="ref_148">148</xref>]. Public benchmarks such as DEAP and SEED have catalyzed this field, with participants’ EEG recorded while watching music videos or film clips annotated along valence-arousal dimensions [<xref ref-type="bibr" rid="ref_149">149</xref>]. Early approaches extracted hand-crafted features (e.g., differential entropy in canonical bands) and fed them into SVMs or similar classifiers, yielding modest subject-dependent accuracies of approximately 60–70% on DEAP [<xref ref-type="bibr" rid="ref_150">150</xref>]. Deep CNNs operating on time-frequency or 3D temporal-spectral-spatial representations later pushed subject-dependent performance above 90%: the dual-scale EEG-Mixer attained &gt;95% on DEAP and approximately 93.7% on SEED, and the Attention-based Multiple Dimensions EEG Transformer (AMDET) model reached 97.48% (arousal) and 96.85% (valence) on DEAP plus 97.17% on SEED [<xref ref-type="bibr" rid="ref_151">151</xref>], [<xref ref-type="bibr" rid="ref_152">152</xref>].</p><p>However, real-world use demands subject-independent generalization. In leave-one-subject-out tests on DEAP, accuracies dropped to approximately 65.9% for valence and 69.5% for arousal and to approximately 76.7% for binary positive-negative classification [<xref ref-type="bibr" rid="ref_153">153</xref>]. This gap highlights the challenge of inter-subject variability. Emerging methods—including FL frameworks that achieved 90.7% on DEAP in a distributed setup and graph- or attention-augmented hybrids—are actively tackling cross-subject robustness [<xref ref-type="bibr" rid="ref_154">154</xref>], [<xref ref-type="bibr" rid="ref_155">155</xref>], [<xref ref-type="bibr" rid="ref_156">156</xref>], [<xref ref-type="bibr" rid="ref_157">157</xref>]. In summary, while modern CNN and Transformer models can exceed 95% accuracy in subject-dependent settings, subject-independent EEG emotion recognition typically remains in the 65–80% range, underscoring the need for larger, more diverse training cohorts and novel architectures to bridge this generalization gap.</p><p>Emotion EEG signals can be subtle and easily confounded by unrelated mental activity or noise. There is also considerable individual variability—people’s EEG responses to emotional stimuli differ. Therefore, models may overfit to specific participants. To address this, researchers have explored subject-independent models (trained on data from multiple participants and evaluated on unseen individuals) and transfer learning. Another challenge is the reliability of labels. Emotions are often self-reported, which can be subjective or inconsistent. Despite these issues, progress is steady. The strength of DL is in capturing complex, distributed patterns (perhaps a combination of frequency changes across multiple regions) that correlate with emotional states, something difficult to do with manual features alone. Emotion recognition from EEG has applications in adaptive user interfaces (e.g., games that respond to a player’s frustration level), mental health (monitoring mood), and marketing (measuring engagement). However, performance in real-life settings (outside lab stimuli) is still an open question—thus research often continues on improving the robustness of these models (through techniques like FL to gather diverse data without violating privacy).</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.3 Bcis and mi</title>
              
              <p>Clinical intervention (CI) applications form a substantial portion of EEG-AI research. In MI BCIs, users imagine limb movements, eliciting characteristic desynchronization in the <inline-formula>
  <mml:math id="mxz0ffja0c">
    <mml:mi>μ</mml:mi>
  </mml:math>
</inline-formula> (8–12 Hz) and <inline-formula>
  <mml:math id="mj3e77hh5z">
    <mml:mi>β</mml:mi>
  </mml:math>
</inline-formula> (13–30 Hz) bands over the sensorimotor cortex. Traditional pipelines combined CSP with LDA or SVM classifiers. DL quickly overtook these. The Deep ConvNet proposed by Schirrmeister et al. [<xref ref-type="bibr" rid="ref_7">7</xref>] reached approximately 75% accuracy on the four-class BCI Competition IV-2a task under a within-subject protocol, and the compact EEGNet model matched CSP+LDA baselines while remaining lightweight. Specialized architectures then set new state-of-the-art benchmarks: FBCNet achieved 76.20% four-class accuracy on IV-2a [<xref ref-type="bibr" rid="ref_158">158</xref>]. MBMANet further improved to 83.18% (<inline-formula>
  <mml:math id="mh34ccnswc">
    <mml:mi>κ</mml:mi>
  </mml:math>
</inline-formula> = 0.776) on the same dataset [<xref ref-type="bibr" rid="ref_159">159</xref>]. CTNet, a convolutional-Transformer hybrid, reached 78.66% [<xref ref-type="bibr" rid="ref_160">160</xref>]. CCLNet reported within-subject accuracies up to 95.87% on IV-2a [<xref ref-type="bibr" rid="ref_161">161</xref>]. Despite these gains, subject-independent MI still lags—cross-subject accuracies typically fall to 60–80%, highlighting inter-subject variability as a major hurdle.</p><p>BCI paradigms beyond MI include ERPs and steady-state visual evoked potentials (SSVEPs). For P300 spellers, CNN+LSTM ensembles achieved &gt;98.7% single-trial accuracy on BCI Competition II data, vastly outpacing manual averaging methods [<xref ref-type="bibr" rid="ref_162">162</xref>]. In SSVEP-based BCIs, an LSTM model decoding drone-control signals reached 96.8% accuracy, while compact CNNs decoding 12-class asynchronous SSVEP achieve approximately 80% cross-subject accuracy (chance = 8.3%), with subject-specific fine-tuning pushing some reports above 97% [<xref ref-type="bibr" rid="ref_163">163</xref>], [<xref ref-type="bibr" rid="ref_164">164</xref>]. These rapid advances—in specialized CNNs, Transformer hybrids, and end-to-end feature learning—are driving real-time, robust BCI closer to both clinical and assistive-technology deployment.</p><p>A major advantage of DL in BCI is reducing calibration time—some deep models can be pre-trained on large datasets and adapt to new users with minimal additional data (through transfer learning), addressing a long-standing BCI issue of lengthy per-user training. In addition, deep models can integrate multiple modalities of input (e.g., EEG + EOG) to simultaneously handle artifacts, as one hybrid approach did by feeding both EEG and an eye-blink channel into a network to make it robust to ocular artifacts. The weakness is that BCIs demand real-time performance; large models with high computation or latency might not be feasible in an online setting. Researchers have tackled this by model compression, using smaller networks (like SSVEP-Net and EEGNet) or optimizing code for deployment. Another challenge is subject variability—a model trained on one set of subjects may perform poorly on a new user due to differences in skull anatomy, electrode contact, etc. Domain adaptation techniques (calibrating the model with a few minutes of data from the new user, or using adversarial training to make the model invariant to subject-specific features) are being explored to mitigate this.</p><p>Nonetheless, DL has revitalized BCI research by substantially improving the accuracy and reliability of decoding intentions from EEG. This brings BCIs closer to practical use in assisting disabled users. For example, a recent study combined a Residual Network (ResNet) (for spatial feature extraction) with a GRU (for temporal smoothing) to let severely motor-impaired users control a robotic arm; they achieved reliable device control without the need for a handcrafted feature pipeline [<xref ref-type="bibr" rid="ref_165">165</xref>]. As BCI applications expand (from medical to gaming to virtual reality), the flexibility of AI models to adapt and learn complex mappings will be increasingly important.</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.4 Cognitive workload and attention monitoring</title>
              
              <p>EEG is often used to monitor cognitive states such as workload, fatigue, or attention in high-stakes settings (e.g., an operator in a control room or a drowsy driver). AI models have evolved from using simple <inline-formula>
  <mml:math id="mpvp4qstxt">
    <mml:mi>θ</mml:mi>
  </mml:math>
</inline-formula> / <inline-formula>
  <mml:math id="me4avbuezt">
    <mml:mi>β</mml:mi>
  </mml:math>
</inline-formula> or <inline-formula>
  <mml:math id="md3rjw6gmj">
    <mml:mi>α</mml:mi>
  </mml:math>
</inline-formula> / <inline-formula>
  <mml:math id="mwc10gr6pf">
    <mml:mi>θ</mml:mi>
  </mml:math>
</inline-formula> power‐ratio thresholds and SVMs to deep networks that capture subtler, multivariate patterns. CNNs, for example, can learn combined slow‐wave increases and blink-artifact signatures of fatigue, while RNNs track the temporal evolution of attention dips. Hossain et al. [<xref ref-type="bibr" rid="ref_166">166</xref>] specifically demonstrated deep‐learning models for driver-attention recognition using short (2–5 s) EEG windows. Time sensitivity is critical—models must flag cognitive decline with minimal delay—and researchers have mitigated inter-subject variability via transfer learning or by fusing EEG with modalities like eye-tracking or heart rate.</p><p>Traditional pipelines achieved roughly 70–85% accuracy in driver-fatigue and workload classification. Today’s state-of-the-art deep models routinely exceed 95% on benchmark datasets. TFAC-Net, a single-channel temporal-frequency attention CNN, delivered approximately 95.3% real-time fatigue detection without bulky setups [<xref ref-type="bibr" rid="ref_167">167</xref>]. A ConvNeXt adaptation on the STEW dataset attained 95.76% accuracy for binary workload and 95.11% for three-class load estimation [<xref ref-type="bibr" rid="ref_168">168</xref>]. EEG-CogNet, which combines multi-band features in a deep framework, reported 96.53% accuracy for mental workload, 98.40% for attention state, and 97.86% for fatigue detection [<xref ref-type="bibr" rid="ref_169">169</xref>]. Multimodal fusion, i.e., EEG + electrocardiography (ECG) + facial video, on the DROZY dataset achieves 98.41% accuracy (F1 98.38%) [<xref ref-type="bibr" rid="ref_170">170</xref>]. These models were used to process 1–5 s windows to minimize detection delay, making them viable for real-time alerts. Nevertheless, environmental confounders and subject variability still challenge subject-independent deployment, spurring ongoing work in transfer learning and robust multimodal integration. Real-world deployment remains nascent, but one can envision smart vehicles or workstations that adapt dynamically—e.g., prompting a break when driver fatigue is detected.</p><p>Beyond driving and operator vigilance, cognitive workload monitoring with EEG has begun to influence diverse domains such as aviation, education, and gaming. In aviation, EEG-driven workload indices are being integrated into pilot training simulators and cockpit monitoring systems, enabling adaptive autopilot engagement during periods of high workload or fatigue. In education, real-time EEG-based attention tracking has been used to personalize digital learning platforms, dynamically adjusting difficulty levels or presenting breaks when students show declining focus. Similarly, in immersive environments like virtual reality (VR) and augmented reality (AR), EEG-informed adaptive interfaces help prevent simulator sickness and optimize engagement by monitoring mental effort and attentional load. In professional e-sports and gaming, EEG is leveraged for both performance enhancement and neurofeedback, offering players feedback on focus maintenance and stress regulation. These cross-domain applications highlight how advances in lightweight headsets, mobile EEG, and multimodal AI pipelines are making cognitive monitoring more practical outside the laboratory. Despite promising results, challenges remain in ecological validity—classifiers trained on controlled datasets often degrade in noisy, real-world conditions—necessitating further progress in domain adaptation, XAI, and seamless integration with unobtrusive wearable sensors.</p>
            </sec>
          
          
            <sec>
              
                <title>3.3.5 Other notable applications</title>
              
              <p>There are many niche but important areas where EEG and AI intersect. A few examples are as follows:</p><p>•Sleep stage classification: As touched on earlier, this involves classifying EEG, often along with EOG and electromyography (EMG) channels, into sleep stages (wake, N1, N2, N3, and REM). It’s a critical task in sleep medicine and traditionally done manually by experts. Deep CNNs have been very successful in this aspect, achieving human-level accuracy on large sleep datasets by learning characteristic waveforms like spindles or K-complexes for stage N2, delta waves for N3, etc. [<xref ref-type="bibr" rid="ref_171">171</xref>]. More recently, hybrid CNN-Transformer models and Transformer-based models have emerged, and hybrid models use small convolutional front-ends to extract local spectral features before feeding 30 s epochs as “tokens” into self-attention layers that model long-range stage transitions in parallel [<xref ref-type="bibr" rid="ref_172">172</xref>], [<xref ref-type="bibr" rid="ref_173">173</xref>], [<xref ref-type="bibr" rid="ref_174">174</xref>]. These Transformer-based architectures not only match or exceed CNN+LSTM performance but also provide natural attention-based interpretability and greater flexibility when integrating multiple polysomnography (PSG) channels. Together, they represent the current state of the art in automatic sleep staging.</p><p>•Event detection in neuroscience experiments: Researchers have used AI to detect specific neural events in EEG, such as detecting error-related potentials (brain signals when a person makes a mistake), or markers of memory encoding. These help in brain-state-triggered experiments or neurofeedback. For instance, real-time closed-loop experiments might use a classifier to detect when a subject’s brain indicates detection of a rare stimulus (P300) and then respond accordingly. Such event-based decoding is being extended to experiments on decision-making, emotion recognition, and learning, where AI enables millisecond-level detection and precise intervention. This allows researchers to probe causal brain-behavior relationships that were previously inaccessible.</p><p>•Artifact recognition and removal: While not an “application” to an end-user, an important use of AI is to automatically identify artifacts in EEG (eye blinks and muscle noise) and remove or correct them [<xref ref-type="bibr" rid="ref_175">175</xref>]. Methods like AEs or GANs have been used to filter out artifacts without losing underlying brain signals. This automation reduces the need for manual preprocessing, accelerating research pipelines and improving the reliability of clinical monitoring. In mobile or wearable EEG setups\textemdash where noise is more prevalent\textemdash AI-based artifact correction is particularly valuable for enabling real-time applications such as BCI or fatigue detection.</p><p>•Neurofeedback and rehabilitation: EEG-driven AI can personalize neurofeedback (training individuals to modulate their own brain activity). For example, AI could adaptively determine which brain patterns to reinforce for a patient recovering from stroke, based on EEG, and adjust the feedback stimuli accordingly. Recent work has explored deep reinforcement learning to optimize training schedules and feedback modalities, making sessions more effective and patient-specific. Beyond stroke, neurofeedback has shown promise in managing ADHD, anxiety, and chronic pain, where AI-driven EEG adaptation improves both user engagement and therapeutic outcomes.</p><p>Each application domain tends to use a tailored set of techniques and has unique benchmarks. <xref ref-type="table" rid="table_3">Table 3</xref> summarizes a few key application areas, their common model types, and representative outcomes reported. Performance figures are approximate from recent literature and can vary with dataset and setup.</p>
              
                <table-wrap id="table_3">
                  <label>Table 3</label>
                  <caption>
                    <title>Representative applications of EEG AI with typical methods and performance</title>
                  </caption>
                  <table><tbody><tr><th colspan="1" rowspan="1"><p>Application</p></th><th colspan="1" rowspan="1"><p>Typical AI Methods</p></th><th colspan="1" rowspan="1"><p>Example Dataset / Benchmark</p></th><th colspan="1" rowspan="1"><p>Performance (Reported)</p></th></tr><tr><td colspan="1" rowspan="1"><p>Epileptic seizure detection</p></td><td colspan="1" rowspan="1"><p>CNN on raw or spectrogram; CNN-LSTM hybrids; ensemble of CNN+SVM; Transformer’s emerging</p></td><td colspan="1" rowspan="1"><p>TUH EEG Seizure Corpus; CHB-MIT; Bonn EEG</p></td><td colspan="1" rowspan="1"><p>90–99% accuracy in controlled teats (e.g., &gt;95% sensitivity on CHB-MIT); real-time systems &lt;1s delay with few false alarms</p></td></tr><tr><td colspan="1" rowspan="1"><p>Alzheimer’s / MCT diagnosis</p></td><td colspan="1" rowspan="1"><p>Feature-based ML (e.g, SVM on bandpower); 1D CNN on EEG; GCNN on coherence networks</p></td><td colspan="1" rowspan="1"><p>Local clinical EEG datasets (e.g., the Mayo Clinic dataset of 12k EEGs)</p></td><td colspan="1" rowspan="1"><p>Distinguigh ADMCI vs. healthy with approximately 80–90% accuracy in research settings (needs external validation for clinical use)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Emotion recognition</p></td><td colspan="1" rowspan="1"><p>CNN on topographic feature maps, RNN on time-series, GCNN, and Transformers (recent)</p></td><td colspan="1" rowspan="1"><p>DEAP (emotion EEG); SEED; DREAMER</p></td><td colspan="1" rowspan="1"><p>Approximately 95–100% accuracy for binary high/low arousal or valence; approximately 90–95% for three or more emotion classes (harder task)</p></td></tr><tr><td colspan="1" rowspan="1"><p>MI BCI</p></td><td colspan="1" rowspan="1"><p>Filter-bank CSP + SVM (traditional); Deep CorroNet EEGWet (CNN); CNN+LSTM; Transformerbased MI networks</p></td><td colspan="1" rowspan="1"><p>BCI Competition IV (2a, 2b); Phasionet EEG Motor Movement dataset</p></td><td colspan="1" rowspan="1"><p>Approximately 76–93% classification accuracy for four-class MI; near 95–100% accuracy for two-class tasks in some subjects; advanced deep models outperform traditional Filter Bank Common Spatial Pattern (FBCSP) + SVM pipelines by up to approximately 20 %</p></td></tr><tr><td colspan="1" rowspan="1"><p>Cognitive load / drowsiness</p></td><td colspan="1" rowspan="1"><p>Shallow ANN/SVM; random forest / XGBe95t ensembles; DBNs; 1D CNN and Temporal Convolutional Networks (TCNs); LSTM ( <mml:math id="mxv0h4et7l">
  <mml:mo>±</mml:mo>
</mml:math> attention); graph neural nets; self-supervised pretraining</p></td><td colspan="1" rowspan="1"><p>NASA-Mental Workload dataset; Driving fatigue EEG set; MAT and STEW cognitive taska</p></td><td colspan="1" rowspan="1"><p>Approximately 84–98.7% accuracy distinguishing low v3. high workload or alert vs. fatigued (e.g., 98.7% with random forest ensembles; 97.4% on MAT; 96.1% on STEW; DBNs <mml:math id="mmwhhin93n">
  <mml:mo>≈</mml:mo>
</mml:math> 92%); typical SVM/ANN/1D CNN/LSTM models (80–95%)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Sleep stage classification</p></td><td colspan="1" rowspan="1"><p>Deep CCNNs (SleepNet and U-Sleep); CNN + BiLSIM sequence models; Seasleephet and SleepTransformer fully convolutional U-Nets (UTime); TCNs; selfsupervised pretraining</p></td><td colspan="1" rowspan="1"><p>Sleep-EDF; MASS dataset; SHHS; CAP Sleep</p></td><td colspan="1" rowspan="1"><p>Approximately 82–88% overall accuracy for fivestage classification on SleepEDF / MASS; advanced models reach approximately 87–89% on Sleep-EDF and approximately 88–90% on SHHS with Cohen’s <mml:math id="ms52b57wir">
  <mml:mi>κ</mml:mi>
</mml:math> <mml:math id="m4ui8xsall">
  <mml:mo>≈</mml:mo>
</mml:math> 0.75–0.82; cascaded RNN pipelines report approximately 95% on sixclass CAP Sleep</p></td></tr></tbody></table>
                </table-wrap>
              
              <p>In summary, AI techniques have permeated virtually every application area of EEG. The success in each domain depends on the interplay between algorithm capabilities and the nature of the EEG signatures for that domain (e.g., distinct large transient events like seizures are easier to detect than diffuse mood states). Across domains, one trend is clear: DL now often provides state-of-the-art results given sufficient data, while integrating domain knowledge (through hybrid models or specialized architectures) and addressing generalization remain active research themes. The next section focuses on the data itself—the public datasets and benchmarks that have catalyzed progress in EEG AI.</p>
            </sec>
          
        </sec>
      
    </sec>
    <sec sec-type="">
      <title>4. Public datasets and benchmarks</title>
      <p>Availability of high-quality EEG datasets has been a crucial factor in advancing AI research on EEG. In the past, many studies used private or small datasets, limiting generalization. Over the last decade, numerous public EEG datasets have been released, covering a variety of tasks from BCI to clinical diagnostics, and several benchmark competitions have been organized. These shared resources enable reproducible research and head-to-head comparison of algorithms. The following is a summary of major EEG datasets and benchmarks, along with their characteristics:</p><p>•BCI competition datasets: A cornerstone of BCI research has been the series of international BCI competitions (I, II, III, IV) that released datasets for tasks like MI, P300 spellers, SSVEP, and more [<xref ref-type="bibr" rid="ref_176">176</xref>], [<xref ref-type="bibr" rid="ref_177">177</xref>], [<xref ref-type="bibr" rid="ref_178">178</xref>], [<xref ref-type="bibr" rid="ref_179">179</xref>], [<xref ref-type="bibr" rid="ref_180">180</xref>]. For example, BCI Competition IV-2a is a popular MI dataset (9 subjects, 22-channel EEG, and four classes of imagined movements) often used to benchmark classification algorithms.</p><p>These datasets typically come with standardized train/test splits, allowing fair comparisons. Researchers continue to use them to report performance of new algorithms, making them de facto benchmarks (e.g., DeepConvNet was evaluated on BCI competition IV-2a, etc.). Performance on these has steadily improved, demonstrating algorithmic progress. The BNCI Horizon 2020 initiative aggregated many BCI datasets in a unified repository, including competition data and other BCI experiment data (like error potentials, etc.) [<xref ref-type="bibr" rid="ref_181">181</xref>].</p><p>•Clinical EEG databases: One of the largest is the TUH EEG Corpus, which contains over 60,000 EEG recordings, with a subset labeled for seizures (TUH Seizure Corpus) [<xref ref-type="bibr" rid="ref_182">182</xref>], [<xref ref-type="bibr" rid="ref_183">183</xref>]. This is a game-changer for training deep models in a clinical context. TUH has been used in contests, e.g., the seizure detection challenge of the Institute of Electrical and Electronics Engineers (IEEE), and in studies to train robust seizure detectors. Another notable one is the CHB-MIT dataset (Children’s Hospital Boston), 23 pediatric patients (24 cases) with epilepsy, each with many hours of EEG and annotated seizures; it’s smaller (CHB-MIT: approximately 969 hours and 173 seizures) but widely used in proof-of-concept algorithm tests [<xref ref-type="bibr" rid="ref_117">117</xref>]. The Freiburg EEG dataset provides long-term EEG from epilepsy patients, including intracranial recordings (useful for seizure prediction research). There’s also the Bonn EEG dataset—small, with segments of EEG labeled normal vs. seizure—often used in early ML papers for quick testing, though it’s very limited in size and diversity [<xref ref-type="bibr" rid="ref_184">184</xref>]. For sleep, the Sleep-EDF and expanded sleep datasets (like MASS) provide many nights of PSG for training sleep stage classifiers [<xref ref-type="bibr" rid="ref_134">134</xref>].</p><p>•Affective and cognitive datasets: As mentioned, DEAP (32 subjects, 32-channel EEG + peripheral signals, and watching music videos) is a benchmark for emotion recognition. SEED (Chinese dataset: 15 subjects, 62-channel EEG, and film clips for positive/neutral/negative emotion) is another [<xref ref-type="bibr" rid="ref_185">185</xref>]. DREAMER is a smaller one with EEG and other signals for emotion [<xref ref-type="bibr" rid="ref_186">186</xref>]. The PhysioNet EEG Motor Movement/Imagery Dataset is essentially an MI dataset with 109 subjects (from the Wadsworth Center) performing various tasks—it’s a valuable resource for MI and has been used to pretrain networks that are later fine-tuned on smaller BCI data [<xref ref-type="bibr" rid="ref_187">187</xref>]. There are also datasets for specialized tasks like ERP CORE (a recent set of datasets for different evoked potential paradigms, useful for testing AI on ERPs) [<xref ref-type="bibr" rid="ref_188">188</xref>].</p><p>•Open data repositories: Platforms such as OpenNeuro and IEEE DataPort host EEG datasets contributed by labs worldwide, ranging from resting-state EEG recordings to task-based EEG [<xref ref-type="bibr" rid="ref_189">189</xref>]. The field has moved toward sharing data in standardized formats (e.g., EEG-BIDS format) to facilitate reuse [<xref ref-type="bibr" rid="ref_190">190</xref>]. The Neuroelectromagnetic Data Archive and Tools Resource (NEMAR), built on OpenNeuro, specifically supports EEG/MEG data sharing and analysis in the cloud [<xref ref-type="bibr" rid="ref_191">191</xref>].</p><p>In addition to the widely used datasets summarized above, several large-scale and multimodal resources have been released in recent years (2023–2025) that broaden the benchmark landscape:</p><p>•Expanded TUH sub-libraries: TUH continues to grow with modular releases (TUSZ v2.0.3 seizures; TUAB v3.0.1 abnormal; TUAR v3.0.1 artifacts; TUEP v2.0.1 epilepsy; TUEV v2.0.1 events; TUSL v2.0.1 slowing). These standardized subsets support task-specific benchmarking in clinical EEG.</p><p>•Population-scale EEG (HBN-EEG): The Healthy Brain Network has released more than 2,600 pediatric/young adult EEG sessions (BIDS, OpenNeuro/NEMAR), with deep phenotyping. A 2025 EEG Challenge built on these data promotes generalizable and fairness-aware benchmarks [<xref ref-type="bibr" rid="ref_192">192</xref>].</p><p>•Wearable epilepsy data (SeizeIT2, 2025): More than 11,000 hours of behind-the-ear EEG plus ECG/EMG/IMU, with 886 focal seizures, enabling real-world ambulatory seizure detection research [<xref ref-type="bibr" rid="ref_193">193</xref>].</p><p>•EEG-fNIRS multimodal sets: New open datasets include a Stroop task, a visual motivation dataset, and multi-joint MI, supporting hybrid BCI and fusion learning [<xref ref-type="bibr" rid="ref_194">194</xref>], [<xref ref-type="bibr" rid="ref_195">195</xref>].</p><p>•Naturalistic Multimodal (NOD, 2025): EEG/MEG/fMRI from the same participants viewing 57k natural images, shared in BIDS with OpenNeuro access, enabling cross-modal representation learning [<xref ref-type="bibr" rid="ref_196">196</xref>].</p><p>•Repository Growth: OpenNeuro now hosts approximately 1,400 public datasets (more than 62k participants), including new EEG/iEEG releases, while NEMAR provides browser-based access and compute tools.</p><p>These resources extend traditional benchmarks with greater scale, multimodality, and ecological validity and are expected to shape the next wave of EEG-AI research. To give a quick reference, <xref ref-type="table" rid="table_4">Table 4</xref> lists a selection of widely used EEG datasets and their key features.</p>
      
        <table-wrap id="table_4">
          <label>Table 4</label>
          <caption>
            <title>Selected public EEG datasets and corresponding characteristics</title>
          </caption>
          <table><tbody><tr><th colspan="1" rowspan="1"><p>Dataset (Year)</p></th><th colspan="1" rowspan="1"><p>Description</p></th><th colspan="1" rowspan="1"><p>Subjects</p></th><th colspan="1" rowspan="1"><p>Channels</p></th><th colspan="1" rowspan="1"><p>Use Case</p></th></tr><tr><td colspan="1" rowspan="1"><p>BCI Competition IV2a (2008)</p></td><td colspan="1" rowspan="1"><p>MI (L / R hand, L / R foot), 9 subjects, and 4 classes</p></td><td colspan="1" rowspan="1"><p>9</p></td><td colspan="1" rowspan="1"><p>22(EEG) + 3 EOG</p></td><td colspan="1" rowspan="1"><p>BCI (MI classification)</p></td></tr><tr><td colspan="1" rowspan="1"><p>PhysioNet EEG Motor Movement / Imagery (2009)</p></td><td colspan="1" rowspan="1"><p>Various MI tasks (open / close fist, etc.)</p></td><td colspan="1" rowspan="1"><p>109</p></td><td colspan="1" rowspan="1"><p>64</p></td><td colspan="1" rowspan="1"><p>BCI/MI, widely used as a baseline for transfer learning</p></td></tr><tr><td colspan="1" rowspan="1"><p>CHB-MIT Scalp EEG (PhysioNet, 2010)</p></td><td colspan="1" rowspan="1"><p>Pediatric epilepsy and continuous EEG with seizures</p></td><td colspan="1" rowspan="1"><p>22</p></td><td colspan="1" rowspan="1"><p>23</p></td><td colspan="1" rowspan="1"><p>Seizure detection (clinical)</p></td></tr><tr><td colspan="1" rowspan="1"><p>TUH EEG Corpus (TUEG: v2.0.1 (2002–2017))</p></td><td colspan="1" rowspan="1"><p>Large clinical EEG database and diverse pathologies</p></td><td colspan="1" rowspan="1"><p>26,846 from more than 10,000 unique patients</p></td><td colspan="1" rowspan="1"><p>24–36 EEG per session</p></td><td colspan="1" rowspan="1"><p>General EEG analysis, seizure subset widely used, and baseline for DL models on real-world clinical data</p></td></tr><tr><td colspan="1" rowspan="1"><p>DEAP (2012)</p></td><td colspan="1" rowspan="1"><p>EEG + peripheral signals during music videos (emotion labels)</p></td><td colspan="1" rowspan="1"><p>32</p></td><td colspan="1" rowspan="1"><p>32 EEG + 8 peripheral physiological channels</p></td><td colspan="1" rowspan="1"><p>Emotion classification (valence / arousal)</p></td></tr><tr><td colspan="1" rowspan="1"><p>SEED (2015)</p></td><td colspan="1" rowspan="1"><p>EEG during film clips (3 emotion classes)</p></td><td colspan="1" rowspan="1"><p>15</p></td><td colspan="1" rowspan="1"><p>62</p></td><td colspan="1" rowspan="1"><p>Emotion classification</p></td></tr><tr><td colspan="1" rowspan="1"><p>Sleep-EDF (Expanded, 2013)</p></td><td colspan="1" rowspan="1"><p>Whole-night sleep EEG with hypnograms (sleep stages)</p></td><td colspan="1" rowspan="1"><p>42</p></td><td colspan="1" rowspan="1"><p>2 EEG + other</p></td><td colspan="1" rowspan="1"><p>Sleep stage classification</p></td></tr><tr><td colspan="1" rowspan="1"><p>BNCI Horizon 2020 (Launched 2014)</p></td><td colspan="1" rowspan="1"><p>Repository of 28 open-access BCI datasets (MI, SSVEP, P300, etc.)</p></td><td colspan="1" rowspan="1"><p>8–109 (various)</p></td><td colspan="1" rowspan="1"><p>8–64 (various)</p></td><td colspan="1" rowspan="1"><p>BCI benchmark suite</p></td></tr><tr><td colspan="1" rowspan="1"><p>EEG Eye State (UCI, 2013)</p></td><td colspan="1" rowspan="1"><p>EEG during continuous recording with eye open/closed labels</p></td><td colspan="1" rowspan="1"><p>1</p></td><td colspan="1" rowspan="1"><p>14</p></td><td colspan="1" rowspan="1"><p>Simple binary classification (open vs. closed)</p></td></tr></tbody></table>
        </table-wrap>
      
      <p>Using these datasets, the community has established certain benchmarks—for example, classification accuracy on DEAP two-class emotion, or kappa score on Sleep-EDF for sleep staging, or AUC on TUH for seizure detection—that new AI models strive to improve. It’s important to note that raw performance numbers are not the only consideration; robustness across datasets is also valued. There is a push towards evaluating models on multiple datasets to ensure they generalize (e.g., testing a seizure detector trained on TUH on CHB-MIT data). This has revealed that some models that do well on a single dataset might not generalize without adaptation, highlighting the need for techniques like transfer learning. Benchmarks also include computational efficiency and latency, especially relevant for real-time BCI. For instance, an algorithm might be benchmarked by information transfer rate (bits/min) rather than just accuracy to account for how quickly and confidently it makes predictions. In research papers, however, accuracy and F1-score remain common comparative metrics.</p><p>In summary, the availability of rich EEG datasets and organized challenges has accelerated progress. As data resources continue to grow (with initiatives encouraging data sharing), it is expected that AI models will further improve by learning from diverse EEG data encompassing many subjects and conditions. With benchmarks in place, the field can track improvements in a concrete way. Next, this study discusses the persistent challenges that even the best current methods face and then explores future directions aimed at overcoming those challenges.</p><p>Despite their widespread use, many public EEG datasets exhibit demographic and geographic biases that constrain the generalizability of trained models. For example, the CHB-MIT Scalp EEG Database is one of the most cited resources for seizure detection, yet it is derived almost exclusively from pediatric patients at a single U.S. hospital, limiting its representativeness across adult populations or non-Western cohorts. Similarly, several emotion recognition and BCI datasets disproportionately sample Western, educated, industrialized, rich, and democratic (WEIRD) populations, overlooking cultural, ethnic, and neurophysiological diversity. Such biases risk producing models that perform well on benchmark datasets but fail to generalize in clinical or global applications. For instance, variations in electrode montages, genetic backgrounds, lifestyle factors, and clinical practices may all affect EEG signals, yet these factors are underrepresented in current datasets. To mitigate these issues, researchers are increasingly advocating for:</p><p>•Cross-cultural and multi-site data collection,</p><p>•Incorporation of age- and sex-balanced cohorts,</p><p>•Development of open multimodal datasets (e.g., EEG-fNIRS and EEG-MRI) that cover broader populations.</p><p>Recognizing and addressing this “Western population bias” is essential for building EEG-AI systems that are equitable, clinically reliable, and globally deployable.</p>
    </sec>
    <sec sec-type="">
      <title>5. Challenges and limitations</title>
      <p>Despite considerable advancements, applying AI to EEG data still faces a number of fundamental challenges and limitations. These issues stem both from the nature of EEG signals and from the characteristics of AI models. Recognizing these challenges is important for properly interpreting current results and for directing future research. Key challenges are as follows:</p><p>•Noise, artifacts, and data quality: EEG signals are notoriously noisy. They are susceptible to interference from muscle activity (EMG), eye blinks/movements (EOG), electrical noise from equipment, and environmental artifacts. Poor contact or impedance can introduce drift and noise. AI models trained on data with certain noise characteristics may not generalize if the noise changes (for instance, a model may misclassify if the test data have more muscle artifact than the training data). While preprocessing can attenuate artifacts, complete elimination is hard, and aggressive filtering can distort true brain signals. This makes building robust AI difficult. Signal processing methods like ICA or wavelet denoising are commonly used as a front-end to AI models to isolate artifacts [<xref ref-type="bibr" rid="ref_197">197</xref>]. Some DL approaches incorporate artifact removal within the model (e.g., an AE learning to separate brain vs. noise). Nevertheless, data quality remains a bottleneck. Ensuring high-quality EEG (through good hardware and acquisition protocols) and using data augmentation to teach models to handle noise are active areas of work.</p><p>•Inter-subject and inter-session variability: Perhaps the biggest challenge for generalized EEG AI is the variability across individuals and even across sessions for the same individual. Each person’s brain anatomy and functional patterns differ, meaning that the same mental state can manifest differently in EEG features across individuals. Moreover, electrode placement variations or slight impedance differences in each session can change the signal. This variability often causes a model trained on one group to perform poorly on another—the classic domain shift problem [<xref ref-type="bibr" rid="ref_198">198</xref>]. Designing subject-independent models is difficult. For example, a deep network might latch onto idiosyncratic features of the training subjects that don’t transfer (e.g., the sharp drop from approximately 93% (10-fold) to approximately 74% (LOSO) in QuadTPat stress detection and from 99.6% to 79.9% in FriendPat epilepsy highlights how subject-specific patterns still dominate handcrafted descriptors, underscoring the need for stronger domain adaptation) [<xref ref-type="bibr" rid="ref_199">199</xref>]. A universal classifier that works for all users without calibration remains unsolved in BCI and other areas. Techniques to address this include adaptive learning (fine-tuning the model on a new subject with a small amount of data), domain adaptation (aligning feature distributions between source and target via adversarial training or transfer components), and meta-learning (training models that can quickly adapt to new subjects) [<xref ref-type="bibr" rid="ref_200">200</xref>], [<xref ref-type="bibr" rid="ref_201">201</xref>]. Some non-deep approaches use normalization of EEG per subject (z-scoring features within each subject) to remove global differences [<xref ref-type="bibr" rid="ref_202">202</xref>]. While many studies have reported high accuracy, it’s often observed in within-subject cross-validation; the drop in cross-subject testing is significant. Overcoming this remains a major research focus because practical EEG applications need models that can be deployed to new users or patients reliably. Low cross-subject accuracy has been pointed out as a key limitation hindering real-life use of EEG DL.</p><p>•Data scarcity and class imbalance: High-quality labeled EEG data is expensive and time-consuming to collect. Many medical applications suffer from limited data—e.g., collecting thousands of EEGs for a rare disease is challenging, and labeling events like seizures requires expert clinicians. DL models thrive on big data and can otherwise overfit on small datasets. The community has tackled this through data augmentation (as discussed, using techniques like sliding windows and synthetic data generation) and by pooling data (creating consortia for multi-center EEG data). Still, some domains have inherent scarcity (e.g., EEG during a very specific cognitive task). Class imbalance is also common. For example, in seizure detection, most of the EEG is non-seizure, with only brief seizure segments. Models can get biased towards the majority class. Methods like oversampling, focal loss, Synthetic Minority Over-sampling Technique (SMOTE), or generating more minority samples with GANs have been used to mitigate this [<xref ref-type="bibr" rid="ref_203">203</xref>], [<xref ref-type="bibr" rid="ref_204">204</xref>], [<xref ref-type="bibr" rid="ref_205">205</xref>]. Transfer learning from larger unrelated EEG datasets is another approach (e.g., pretrained on a large EEG dataset with surrogate tasks, then fine-tuned on the small target dataset). While these strategies help, limited data remain a core limitation, especially compared to fields like image or text where enormous labeled datasets exist. In addition, regulatory and privacy issues can limit data sharing in clinical contexts, though FL aims to address that. As discussed in Section 6, emerging directions such as FL and self-supervised pretraining aim to mitigate data scarcity by enabling collaborative learning without centralizing data and by utilizing large unlabeled EEG corpora.</p><p>•Model interpretability: Most high-performing AI models for EEG (deep neural networks and Transformers) are complex black boxes. This raises concerns in scientific, clinical, and user-facing contexts. For instance, a neurologist might hesitate to trust AI's diagnosis suggestion if the model cannot explain which EEG features led to that conclusion. Lack of interpretability also hampers scientific insight—people might care why a certain brain-state classification is working (to discover neural correlates), not just that it works. Currently, understanding deep EEG models is difficult. There have been attempts such as visualizing CNN filters to see if they align with known EEG rhythms or using saliency maps to highlight parts of EEG that influenced a decision. Some models incorporate attention mechanisms which can be analyzed to infer the importance of time segments or channels. But these are indirect explanations. As it stands, the opaque nature of deep models is a limitation, and interpretability is “particularly crucial” for user trust and ethical use of EEG AI [<xref ref-type="bibr" rid="ref_206">206</xref>], [<xref ref-type="bibr" rid="ref_207">207</xref>]. The black-box issue is also closely linked to generalization: if it is unclear what the model has learned, it cannot be determined whether it is capturing biologically meaningful signals or merely exploiting trivial biases or noise. For example, a model might latch onto power line noise patterns that coincidentally correlate with a class in the training set—it would perform well on similar data but fail elsewhere. Without interpretability, such failure modes are hard to anticipate. This challenge is being increasingly recognized, and research on XAI specific to EEG is emerging (Section 6).</p><p>•Robustness and reliability: Beyond accuracy, robustness to various factors is a limitation. AI models can be sensitive to slight changes. For example, a classifier might perform poorly if electrodes are re-referenced differently or if there's a slight timing shift in signals. Adversarial robustness is also a consideration—studies have shown that adding small perturbations to EEG (within a physiological noise range) can fool a deep classifier, which is problematic for security/safety if someone could maliciously interfere with EEG-based systems [<xref ref-type="bibr" rid="ref_207">207-208</xref>]. Models are also often narrow in scope: a network trained to detect seizures might not know how to handle EEG with some other pathology (out-of-distribution data), leading to unpredictable outputs. For BCIs, reliability over time is key—changes in the user’s condition or headset position day to day can degrade performance (the “non-stationarity” problem). Continual learning or periodic recalibration might be needed, but many current models do not support efficient updates without retraining from scratch (if retrained, they might forget old data—the catastrophic forgetting issue). Thus, maintaining performance over time and across conditions is a practical limitation.</p><p>•Computational demands: Some advanced models (like Transformers or very deep CNNs) are computationally heavy, with millions of parameters. Training them can require GPUs and a lengthy time, and running them in real time on portable devices (e.g., a wearable EEG) may be infeasible without significant power. While this is a technical limitation that tends to diminish as hardware improves, it’s still relevant. Researchers are investigating model compression (pruning and quantization) and more efficient architectures (like TinyML approaches for EEG) to allow on-device processing, especially for BCIs or mobile health applications [<xref ref-type="bibr" rid="ref_209">209-210</xref>]. For example, using separable convolutions and smaller kernels (as in EEGNet) can drastically reduce parameter count while keeping good accuracy. Nonetheless, a complicated model might have latency that's too high for an interactive BCI (which often needs less than 100 ms from brain event to action). Therefore, there’s a trade-off between model complexity and deployment feasibility.</p><p>•Evaluation and reproducibility issues: Lastly, a meta-challenge is that comparing models across studies can be difficult due to differing evaluation protocols. Some works use within-subject cross-validation, others use leave-one-subject-out, and some use different preprocessing. If not standardized, claims of improvement might be hard to verify. There’s a push for open-source code and standardized benchmarks (Section 4) to ensure fair comparison [<xref ref-type="bibr" rid="ref_211">211</xref>], [<xref ref-type="bibr" rid="ref_212">212</xref>]. Reproducibility is improving thanks to shared datasets, but sometimes details like how EEG is filtered or how deep models are initialized can affect outcomes. This is a community challenge to address by adhering to rigorous validation (e.g., reporting results on held-out datasets, not just on the same data used for training/tuning).</p><p>In summary, the challenges outlined above are not isolated obstacles but active drivers of current research. Each limitation maps directly onto the future directions discussed in Section 6. For example, data scarcity motivates FL and SSL, inter-subject variability calls for domain adaptation and meta-learning, interpretability concerns drive XAI, and robustness issues are addressed through adversarial training, multimodal fusion, and continual learning. Establishing this logical loop strengthens the survey’s coherence by showing how today’s obstacles shape tomorrow’s innovations.</p>
    </sec>
    <sec sec-type="">
      <title>6. Future directions</title>
      <p>Looking ahead, the intersection of AI and EEG is poised to continue evolving rapidly. Researchers have been actively exploring new methodologies to tackle the challenges outlined and to open up novel applications. This study highlights several promising future directions and emerging trends that are likely to shape the field in the coming years as follows:</p><p>(a) Transformers and advanced deep architectures: Transformers have made a big splash in EEG analysis recently, but this is likely just the beginning. Future work will refine Transformer architectures to better suit EEG. This could involve hybrid models that combine CNNs (for local feature extraction) with Transformer encoders (for global context), or the development of lightweight Transformers that can handle smaller data regimes. There is also interest in pretraining Transformers on large unlabeled EEG datasets using SSL which is analogous to Bidirectional Encoder Representations from Transformers (BERT) in Natural Language Processing (NLP)—for example, forcing it to learn general EEG representations by masking parts of an EEG sequence and training the model to reconstruct them. Such pretraining could then be fine-tuned for specific tasks (seizure detection, BCI, etc.), potentially improving performance when labeled data is scarce. Given Transformers’ success in other domains, it is expected that they will become a staple for EEG tasks requiring long-range temporal modeling or integration of multimodal data. However, making them data-efficient and computationally feasible (perhaps through sparsity, factorized attention, or smaller patch-based models) will be key areas of innovation.</p><p>(b) FL and privacy-preserving learning: In clinical EEG especially, privacy is a major concern—patient data cannot always be centralized for training an AI model. FL offers a solution by allowing models to be trained collaboratively across multiple hospitals or devices, without sharing raw data. In FL, each center computes model updates on its local data and only those updates (not the EEG signals) are sent to a central server to be aggregated. This preserves privacy while tapping on data volume. Initial studies have shown that FL can be effective for EEG-based emotion recognition, such as training on data from multiple users or institutions to get a better general model. More applications of FL for building robust EEG models (e.g., a federated network for seizure prediction that learns from patients across different clinics) can be foreseen. There are challenges like handling non-independent and identically distributed (non-iid) data (EEG from different sites may have different characteristics) and communication overhead, but active research is making FL more efficient and robust [<xref ref-type="bibr" rid="ref_213">213</xref>]. In addition, techniques like differential privacy could be applied to ensure that models do not inadvertently leak personal information (like an outlier EEG pattern unique to one individual) [<xref ref-type="bibr" rid="ref_214">214</xref>]. The drive for privacy will also encourage on-device AI for EEG (like running a model directly on a wearable or mobile device), which ties into making models efficient.</p><p>(c) Interpretable and XAI for EEG: As highlighted, interpretability is crucial, and a surge in methods to make EEG AI more transparent is expected. This includes developing visualization tools and metrics specifically for EEG networks. For example, techniques to map a deep network’s features back to EEG time-frequency space could help reveal if the model is focusing on known physiological patterns (like alpha oscillations or sleep spindles). A recent direction is concept-based interpretability, where one defines human-understandable concepts (e.g., “delta wave activity” or “frontal asymmetry”) and tests how strongly those concepts influence the model’s decisions. Another is training inherently interpretable models: one idea is a network that outputs intermediate representations that correspond to features experts recognize (such as detecting spikes, then using those to classify epilepsy). Prototype learning is another promising approach—e.g., a network can be designed to learn prototypical EEG patterns for each class, and then classification is based on similarity to these prototypes (which can be visualized as representative signals) [<xref ref-type="bibr" rid="ref_215">215</xref>], [<xref ref-type="bibr" rid="ref_216">216</xref>]. Attention mechanisms also naturally lend some interpretability by indicating which parts of the signal are important. Zhou et al. [<xref ref-type="bibr" rid="ref_207">207</xref>] focused on interpretable and robust AI for EEG, indicating growing interest. It can be predicted that future models, especially for clinical use, will come with “explanation modules”—perhaps using methods like Local Interpretable Model-agnostic Explanations (LIME) or SHapley Additive exPlanations (SHAP) adapted to time-series—to provide reasons for their outputs. This not only builds trust but can potentially lead to scientific discovery (e.g., an AI model “discovering” an EEG biomarker that was not obvious to humans by consistently pointing to a particular pattern that indicates a disease).</p><p>A clarification is necessary between interpretability and explainability, which are often used interchangeably in AI literature but differ in scope—particularly for EEG-based applications. Interpretability refers to the degree to which the internal logic of a model is transparent and traceable, for example, whether a linear classifier's decision boundary or a CNN's learned filters can be directly examined. In contrast, explainability emphasizes whether the outputs of a (possibly complex) model can be made understandable to humans, often through post hoc methods such as saliency maps, feature attribution, or scalp-topography visualizations. In EEG-AI, this distinction is crucial: interpretability ensures that the computational process itself can be audited for validity (e.g., verifying whether spectral features used by the model correspond to physiologically plausible patterns), while explainability focuses on translating complex outputs into forms that clinicians, neuroscientists, or end-users can readily comprehend. Maintaining this distinction helps prevent conflating technical model transparency with user-facing understanding, thereby aligning methodological rigor with practical usability.</p><p>Beyond generic XAI methods such as saliency maps or SHAP values, several interpretation techniques have emerged that tap into EEG’s spatio-temporal and neurophysiological structure. For example, deep models employing prototype-based methods—like ProtoEEGNet—store representative EEG waveforms and perform classification by comparing new inputs to learned prototypes [<xref ref-type="bibr" rid="ref_217">217</xref>]. Similarly, adaptations of self-attention prototype methods (originally from ECG) applied to EEG for sleep-stage classification reveal prototypical components such as alpha spindles and slow waves, hinting at interpretable biomarkers [<xref ref-type="bibr" rid="ref_218">218</xref>]. Additionally, CNN-based feature visualization pipelines for Multivariate Pattern Analysis (MVPA) allow examination of discriminative activations on a trial-by-trial basis, revealing when and where certain spatial-temporal EEG patterns drive decisions [<xref ref-type="bibr" rid="ref_219">219</xref>]. A broader evaluation of XAI techniques—including saliency mapping, guided backpropagation, integrated gradients, Layer-wise Relevance Propagation (LRP), and more—has led to recommended best practices for presenting model interpretations in EEG-based BCI settings. Further, GNN architectures—which model electrode layouts as graphs—enhance neuroscientific interpretability by illuminating how different electrode regions contribute to decisions [<xref ref-type="bibr" rid="ref_220">220</xref>]. While these EEG-adapted interpretability strategies significantly enhance transparency and trust, several intuitive extensions remain underexplored. Explicit scalp-topography visualizations of model attention or weight maps, frequency-domain attribution (e.g., LRP identifying delta vs. alpha contributions), and ERP-component-based interpretability (e.g., highlighting reliance on P300 or N400 signals) have not yet been robustly demonstrated in the literature. Nonetheless, integrating domain knowledge into XAI remains a crucial frontier—vital for fostering clinically viable and scientifically insightful EEG-AI systems.</p><p>(d) Robustness and domain adaptation: Future research will continue tackling the generalization issue. Techniques from domain adaptation and transfer learning will be further refined. One likely direction is meta-learning (learning to learn) where a model is trained on a variety of tasks or subjects and learns a good initialization that can quickly adapt to new ones. This has shown promise in a few BCI studies. Another approach is synthetic-to-real adaptation: using generated EEG data to augment training and then adapting models to real data. People might see more use of adversarial training—not only to defend against malicious attacks but also as a way to make models robust to noise/artifact variations (train the model with adversarial examples of noise so it learns to be invariant). In addition, continuous learning algorithms could allow a deployed EEG model to keep improving as it sees more data from a user, without forgetting past knowledge. Research into architectures that support incremental updates will be valuable (for instance, dynamically expanding networks, or using Bayesian approaches to update weights with new data while preserving old performance).</p><p>(e) Multimodal and context-aware EEG analysis: The brain does not operate in isolation, and neither should models if additional data is available. Future AI for EEG may increasingly incorporate other data streams: combining EEG with fNIRS in hybrid BCIs, or with eye-tracking data, or physiological signals like ECG/GSR for emotion recognition [<xref ref-type="bibr" rid="ref_221">221</xref>]. Multimodal models can compensate for weaknesses of one modality (e.g., EEG might be noisy but another signal isn’t, or EEG gives high temporal resolution while fNIRS gives more spatial information). Recent advances have moved beyond simple feature concatenation. Attention-based fusion architectures now allow modality-specific encoders (e.g., CNNs for EEG and temporal convolutions for fNIRS) to interact through cross-attention layers, dynamically weighting each modality according to signal quality. Transformer-based fusion aligns temporal embeddings from EEG and EOG, enabling context-aware selection of relevant signals for tasks such as emotion recognition. Meanwhile, GNNs have been adapted for multimodal learning, where nodes represent EEG channels and fNIRS optodes, and edges encode inter-modality correlations—capturing richer spatial-temporal dependencies. These approaches demonstrate how architectural innovations can explicitly capture complementary information across modalities. DL frameworks are well-suited to learn from multimodal data, and research has shown improved results in emotion recognition when fusing EEG with peripheral signals. Contextual information is another modality of sorts—for instance, knowing the timing of external events (stimuli) can help in analyzing EEG. Future systems might incorporate context through techniques like encoding stimulus features and feeding them alongside EEG into a model (a rudimentary example: combining EEG with a driving simulator’s context to better predict if a brain pattern indicates hazard response or just random distraction). Essentially, moving beyond treating EEG in isolation could significantly enhance AI’s interpretative power.</p><p>(f) Emerging hardware and on-chip AI: On the hardware side, as dry EEG electrodes and wearable EEG devices become more common, there will be a drive to implement AI on hardware (e.g., chips in headbands). Neuromorphic computing and dedicated EEG processing chips could run simplified spiking neural networks or compressed models in real time with low power [<xref ref-type="bibr" rid="ref_222">222</xref>], [<xref ref-type="bibr" rid="ref_223">223</xref>]. This ties into edge computing for EEG—rather than sending data to the cloud for analysis (which can be slow and raises privacy concerns), the analysis might happen locally on the device. Collaborations between hardware designers and AI researchers might be seen to create models that are co-optimized with EEG sensors (for example, selecting channel subsets on the fly that maximize model confidence, reducing data dimensions). While somewhat outside the traditional scope of AI algorithms, this direction ensures that the sophisticated models being developed can actually be deployed in mobile neurotechnology.</p><p>(g) Domain-specific innovations: Various subfields will push their own specialized innovations. For instance, there is interest in adaptive BCIs that can not only decode but also actively query the user or adjust parameters based on confidence (an application of reinforcement learning or active learning). In clinical AI, integrating EEG analysis with electronic health records or imaging could provide more holistic diagnostic models (multimodal in a different sense: EEG + MRI, etc., where AI finds correlations). In addition, the concept of brain state decoding might shift from discrete classification to more fluid tracking, and AI might be used to drive neurofeedback (i.e., closed-loop systems where AI both reads and helps modulate brain activity in real time, perhaps using generative models to suggest stimuli that move the brain towards a desired state).</p><p>(h) Standardization and reproducibility efforts: A somewhat meta future direction is the continued development of standards (like EEG-BIDS for data and common frameworks for training/testing models). Tools and libraries (for example, MOABB—mother of all BCI benchmarks—a package that lets people evaluate multiple algorithms on many BCI datasets easily) will be refined, making it easier for researchers to test new models across a battery of datasets [<xref ref-type="bibr" rid="ref_224">224</xref>]. This will encourage the field to favor models that are not just tuned to one dataset but work broadly. It is expected that winning approaches will be those that generalize well, as evidenced by performance on community benchmark platforms (PapersWithCode trends already reflect this as people report state-of-the-art across benchmarks).</p><p>In summary, the future of AI in EEG analysis is very promising. More accurate, robust, and transparent models that can be deployed in real-world settings can be foreseen. The synergy between new algorithmic developments (like Transformers and FL) and the unique demands of EEG (nonstationarity, individual differences, and need for interpretability) will define the next wave of research. If successful, these future advances will not only solve technical challenges but also unlock new applications, ranging from brain-monitoring wearables for wellness to brain-controlled smart environments and AI-assisted neurological diagnosis that is faster and more personalized. The ultimate vision is a new generation of cognitive neurodynamic technologies—powered by AI—that seamlessly integrate with human brain function for health and enhancement, all while being reliable and understood by their users.</p>
    </sec>
    <sec sec-type="">
      <title>7. Conclusion</title>
      <p>AI has become an indispensable tool for EEG signal analysis, bringing significant improvements in automation and accuracy to a domain historically reliant on manual inspection and handcrafted methods. This survey traced the development of AI techniques for EEG, from early ML classifiers to state-of-the-art DL models. In addition, this study reviewed how these techniques are applied across various tasks (classification, regression, and generation) and algorithms (SVMs, neural networks, Transformers, etc.) and examined their use in key application areas, including medical diagnosis, emotion recognition, BCIs, and beyond. Recent years have seen DL approaches achieve remarkable success on benchmark datasets—for example, convolutional networks detecting seizures or classifying MI with high accuracy—demonstrating the potential for AI to decode the complex patterns in EEG that underpin cognitive and clinical phenomena.</p><p>However, this review also emphasized that challenges remain. EEG data pose unique difficulties such as noise, inter-subject variability, and limited availability of labeled data, which can constrain model performance and generalization. Furthermore, issues of interpretability and trust in AI decisions are especially pronounced in neurophysiological contexts. These challenges underscore that raw performance metrics, while important, are not the sole determinant of a method’s utility in practice. Robustness, transparency, and the ability to handle real-world variability are equally critical benchmarks that the next generation of EEG AI methods must meet. Encouragingly, the field is actively responding to these challenges. Emerging trends such as Transformer-based architectures are opening new frontiers in performance, FL is addressing data privacy and scarcity by enabling collaborative model training, and XAI techniques are beginning to peel back the curtain on black-box models to reveal the brain features driving their decisions. At the same time, the continued expansion of public EEG datasets and competitions is fostering a more rigorous and comparative research environment, accelerating progress. Future advances may well come from interdisciplinary collaboration—combining insights from neuroscience (e.g., knowledge of brain networks) with novel ML strategies (e.g., GNNs or SSL) to create models that are both powerful and physiologically interpretable.</p><p>In conclusion, the synergy of AI and EEG has greatly advanced the ability to interpret the electrical language of the brain. What began with modest ML experiments has evolved into sophisticated DL systems that can discern intricate neural patterns, often in real time. This progress holds immense promise: more effective clinical diagnostics for neurological conditions, more immersive and reliable brain-controlled devices, and deeper scientific understanding of brain function through data-driven discovery. Achieving these promises will require ongoing efforts to surmount current limitations and ensure that AI models are used judiciously and transparently in sensitive applications. The trajectory of recent research is highly encouraging—with each passing year, AI algorithms become more adept at handling EEG’s complexities, and EEG data science becomes more integrated with mainstream AI developments.</p><p>Looking ahead, new opportunities may arise beyond conventional clinical and cognitive domains. Extreme environments such as space missions, deep-sea exploration, or high-risk industrial operations demand continuous monitoring of human performance under stress, fatigue, or altered physiology. AI-powered EEG systems could provide real-time neurocognitive assessment for astronauts adapting to microgravity, divers facing high-pressure conditions, or workers in hazardous industries where safety depends on vigilance. Developing such systems will require robust models that can withstand noisy conditions, adapt across individuals, and operate on portable hardware. These cross-domain applications highlight the potential of EEG-AI not only as a clinical or research tool but also as a cornerstone of human-machine integration in the most challenging contexts. It is expected that the coming years will solidify the role of AI-powered EEG analysis as a cornerstone of cognitive neurodynamics research and its translational applications, ultimately enabling technologies that can beneficially interface human brains with the digital world in ways once confined to science fiction.</p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgments</title>
      
    </ack>
    <app-group>
      <app>
        <title>Appendix</title>
        <p style="text-align: center">Table A. Glossary of key interdisciplinary terms</p><table><tbody><tr><td colspan="1" rowspan="1"><p style="text-align: center">Term</p></td><td colspan="1" rowspan="1"><p style="text-align: center">EEG / Neuroscience Meaning</p></td><td colspan="1" rowspan="1"><p style="text-align: center">ML / AI Meaning</p></td><td colspan="1" rowspan="1"><p style="text-align: center">Clinical Relevance</p></td></tr><tr><td colspan="1" rowspan="1"><p>Artifact</p></td><td colspan="1" rowspan="1"><p>Non-neural contamination (eye blinks, EMG, and line noise)</p></td><td colspan="1" rowspan="1"><p>Input noise / perturbations that mislead models</p></td><td colspan="1" rowspan="1"><p>Risk of false alarms (e.g., mistaking a blink for an epileptic spike)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Nonstationarity</p></td><td colspan="1" rowspan="1"><p>EEG statistics change over time, sessions, or subjects</p></td><td colspan="1" rowspan="1"><p>Distribution drift / domain shift</p></td><td colspan="1" rowspan="1"><p>Reduces cross-subject generalization ; requires recalibration</p></td></tr><tr><td colspan="1" rowspan="1"><p>Generalization</p></td><td colspan="1" rowspan="1"><p>Crosssubject / session applicability of EEG models</p></td><td colspan="1" rowspan="1"><p>The model’s ability to perform on unseen data</p></td><td colspan="1" rowspan="1"><p>Ensures deployment reliability across diverse populations</p></td></tr><tr><td colspan="1" rowspan="1"><p>Sensitivity</p></td><td colspan="1" rowspan="1"><p>True positive rate (detecting real seizures)</p></td><td colspan="1" rowspan="1"><p>Equivalent to recall</p></td><td colspan="1" rowspan="1"><p>High sensitivity avoids missed diagnoses</p></td></tr><tr><td colspan="1" rowspan="1"><p>Specificity</p></td><td colspan="1" rowspan="1"><p>True negative rate (rejecting nonseizure activity)</p></td><td colspan="1" rowspan="1"><p>Related to true negative rate</p></td><td colspan="1" rowspan="1"><p>High specificity avoids false alarms</p></td></tr><tr><td colspan="1" rowspan="1"><p>Interpretability</p></td><td colspan="1" rowspan="1"><p>Physiological plausibility (are features brainrelevant?)</p></td><td colspan="1" rowspan="1"><p>Understanding model reasoning (saliency, SHAP, etc.)</p></td><td colspan="1" rowspan="1"><p>Improves trust from clinicians and neuroscientists</p></td></tr><tr><td colspan="1" rowspan="1"><p>XAI</p></td><td colspan="1" rowspan="1"><p>EEG-specific methods: electrode-localized maps and frequency-aware attribution</p></td><td colspan="1" rowspan="1"><p>General framework for making black-box models transparent</p></td><td colspan="1" rowspan="1"><p>Confirms reliance on valid biomarkers (e.g., P300 and alpha rhythms)</p></td></tr><tr><td colspan="1" rowspan="1"><p>Domain adaptation</p></td><td colspan="1" rowspan="1"><p>Adjusting for subject, session, or device variability</p></td><td colspan="1" rowspan="1"><p>Adapting models across datasets with shifted distributions</p></td><td colspan="1" rowspan="1"><p>Reduces costly perpatient calibration</p></td></tr><tr><td colspan="1" rowspan="1"><p>Transfer learning</p></td><td colspan="1" rowspan="1"><p>Using pretrained EEG models (e.g., TUH $\rightarrow$ CHBMIT fine-tuning)</p></td><td colspan="1" rowspan="1"><p>Reuse of knowledge from large source datasets to small target datasets</p></td><td colspan="1" rowspan="1"><p>Critical for rare-disease or lowsample EEG datasets</p></td></tr><tr><td colspan="1" rowspan="1"><p>Class imbalance</p></td><td colspan="1" rowspan="1"><p>Few seizure vs. many non-seizure samples</p></td><td colspan="1" rowspan="1"><p>Uneven distribution across categories</p></td><td colspan="1" rowspan="1"><p>Requires oversampling, GAN augmentation, or anomaly detection</p></td></tr><tr><td colspan="1" rowspan="1"><p>Robustness</p></td><td colspan="1" rowspan="1"><p>Stability under noise, electrode shifts, and artifacts</p></td><td colspan="1" rowspan="1"><p>Model performance under perturbations or adversarial noise</p></td><td colspan="1" rowspan="1"><p>Essential for ICU / BCI reliability</p></td></tr><tr><td colspan="1" rowspan="1"><p>Overfitting</p></td><td colspan="1" rowspan="1"><p>EEG models memorizing subject quirks rather than true markers</p></td><td colspan="1" rowspan="1"><p>Poor generalization due to memorizing training data</p></td><td colspan="1" rowspan="1"><p>Leads to failed clinical deployment</p></td></tr><tr><td colspan="1" rowspan="1"><p>Multimodal fusion</p></td><td colspan="1" rowspan="1"><p>EEG + fNIRS, ECG, and eyetracking integration</p></td><td colspan="1" rowspan="1"><p>Combining multiple data modalities</p></td><td colspan="1" rowspan="1"><p>Improves affective computing, workload monitoring, and diagnostics</p></td></tr><tr><td colspan="1" rowspan="1"><p>FL</p></td><td colspan="1" rowspan="1"><p>Hospitals train locally and share only model updates</p></td><td colspan="1" rowspan="1"><p>Distributed ML without raw data sharing</p></td><td colspan="1" rowspan="1"><p>Preserves privacy while enabling large-scale EEG model training</p></td></tr></tbody></table>
      </app>
    </app-group>
    <notes>
      <title>Author Contributions</title>
      
    </notes>
    <notes>
      <title>Funding</title>
      
    </notes>
    <notes>
      <title>Informed Consent Statement</title>
      
    </notes>
    <notes>
      <title>Data Availability Statement</title>
      <p>The data used to support the findings of this study are available from the corresponding author upon request.</p>
    </notes>
    <notes notes-type="COI-statement">
      <title>Conflicts of Interest</title>
      <p>The authors declare that they have no conflicts of interest.</p>
    </notes>
    <ref-list>
      <title>References</title>
      <ref id="ref_1">
        <label>1.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Teplan</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Fundamentals of EEG measurement</article-title>
          <source>Meas. Sci. Rev.</source>
          <year>2002</year>
          <volume>2</volume>
          <issue>2</issue>
          <page-range>1-11</page-range>
        </element-citation>
      </ref>
      <ref id="ref_2">
        <label>2.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Schomer</surname>
              <given-names>D. L.</given-names>
            </name>
            <name>
              <surname>Lopes da Silva</surname>
              <given-names>F. H.</given-names>
            </name>
          </person-group>
          <source>Niedermeyer's Electroencephalography: Basic Principles, Clinical Applications, and Related Fields</source>
          <publisher-name>Oxford University Press</publisher-name>
          <year>2017</year>
        </element-citation>
      </ref>
      <ref id="ref_3">
        <label>3.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mumtaz</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Rasheed</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Irfan</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Review of challenges associated with the EEG artifact removal methods</article-title>
          <source>Biomed. Signal Process. Control</source>
          <year>2021</year>
          <volume>68</volume>
          <page-range>102741</page-range>
          <pub-id pub-id-type="doi">10.1016/j.bspc.2021.102741</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_4">
        <label>4.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Al-Qazzaz</surname>
              <given-names>N. K.</given-names>
            </name>
            <name>
              <surname>Aldoori</surname>
              <given-names>A. A.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>S. H. B. M.</given-names>
            </name>
            <name>
              <surname>Ahmad</surname>
              <given-names>S. A.</given-names>
            </name>
            <name>
              <surname>Mohammed</surname>
              <given-names>A. K.</given-names>
            </name>
            <name>
              <surname>Mohyee</surname>
              <given-names>M. I.</given-names>
            </name>
          </person-group>
          <article-title>EEG signal complexity measurements to enhance BCI-based stroke patients' rehabilitation</article-title>
          <source>Sensors</source>
          <year>2023</year>
          <volume>23</volume>
          <issue>8</issue>
          <page-range>3889</page-range>
          <pub-id pub-id-type="doi">10.3390/s23083889</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_5">
        <label>5.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Craik</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Contreras-Vidal</surname>
              <given-names>J. L.</given-names>
            </name>
          </person-group>
          <article-title>Deep learning for electroencephalogram (EEG) classification tasks: A review</article-title>
          <source>J. Neural Eng.</source>
          <year>2019</year>
          <volume>16</volume>
          <issue>3</issue>
          <page-range>031001</page-range>
          <pub-id pub-id-type="doi">10.1088/1741-2552/ab0ab5</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_6">
        <label>6.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Roy</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Banville</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Albuquerque</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Gramfort</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Falk</surname>
              <given-names>T. H.</given-names>
            </name>
            <name>
              <surname>Faubert</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Deep learning-based electroencephalography analysis: A systematic review</article-title>
          <source>J. Neural Eng.</source>
          <year>2019</year>
          <volume>16</volume>
          <issue>5</issue>
          <page-range>051001</page-range>
          <pub-id pub-id-type="doi">10.1088/1741-2552/ab260c</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_7">
        <label>7.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schirrmeister</surname>
              <given-names>R. T.</given-names>
            </name>
            <name>
              <surname>Springenberg</surname>
              <given-names>J. T.</given-names>
            </name>
            <name>
              <surname>Fiederer</surname>
              <given-names>L. D. J.</given-names>
            </name>
            <name>
              <surname>Glasstetter</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Eggensperger</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Tangermann</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hutter</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Burgard</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Ball</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Deep learning with convolutional neural networks for EEG decoding and visualization</article-title>
          <source>Hum. Brain Mapp.</source>
          <year>2017</year>
          <volume>38</volume>
          <issue>11</issue>
          <page-range>5391-5420</page-range>
          <pub-id pub-id-type="doi">10.1002/hbm.23730</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_8">
        <label>8.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Q. Q.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>X. Q.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>W. G.</given-names>
            </name>
            <name>
              <surname>Bai</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Han</surname>
              <given-names>J. X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>Z. H.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Cong</surname>
              <given-names>F. Y.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>J. Q.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>X. L.</given-names>
            </name>
          </person-group>
          <article-title>The applied principles of EEG analysis methods in neuroscience and clinical neurology</article-title>
          <source>Mil. Med. Res.</source>
          <year>2023</year>
          <volume>10</volume>
          <issue>1</issue>
          <page-range>67</page-range>
          <pub-id pub-id-type="doi">10.1186/s40779-023-00502-7</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_9">
        <label>9.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Saeidi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Karwowski</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Farahani</surname>
              <given-names>F. V.</given-names>
            </name>
            <name>
              <surname>Fiok</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Taiar</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Hancock</surname>
              <given-names>P. A.</given-names>
            </name>
            <name>
              <surname>Al-Juaid</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Neural decoding of EEG signals with machine learning: A systematic review</article-title>
          <source>Brain Sci.</source>
          <year>2021</year>
          <volume>11</volume>
          <issue>11</issue>
          <page-range>1525</page-range>
          <pub-id pub-id-type="doi">10.3390/brainsci11111525</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_10">
        <label>10.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>AlSharabi</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Salamah</surname>
              <given-names>Y. Bin</given-names>
            </name>
            <name>
              <surname>Abdurraqeeb</surname>
              <given-names>A. M.</given-names>
            </name>
            <name>
              <surname>Aljalal</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Alturki</surname>
              <given-names>F. A.</given-names>
            </name>
          </person-group>
          <article-title>EEG Signal Processing for Alzheimer's Disorders Using Discrete Wavelet Transform and Machine Learning Approaches</article-title>
          <source>IEEE Access</source>
          <year>2022</year>
          <volume>10</volume>
          <page-range>89781-89797</page-range>
          <pub-id pub-id-type="doi">10.1109/ACCESS.2022.3198988</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_11">
        <label>11.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>AlSharabi</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Salamah</surname>
              <given-names>Y. B.</given-names>
            </name>
            <name>
              <surname>Aljalal</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Abdurraqeeb</surname>
              <given-names>A. M.</given-names>
            </name>
            <name>
              <surname>Alturki</surname>
              <given-names>F. A.</given-names>
            </name>
          </person-group>
          <article-title>EEG-based clinical decision support system for Alzheimer's disorders diagnosis using EMD and deep learning techniques</article-title>
          <source>Front. Hum. Neurosci.</source>
          <year>2023</year>
          <volume>17</volume>
          <page-range>1190203</page-range>
          <pub-id pub-id-type="doi">10.3389/fnhum.2023.1190203</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_12">
        <label>12.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Carvajal-Dossman</surname>
              <given-names>J. P.</given-names>
            </name>
            <name>
              <surname>Guio</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>García-Orjuela</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Guzmán-Porras</surname>
              <given-names>J. J.</given-names>
            </name>
            <name>
              <surname>Garces</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Naranjo</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Maradei-Anaya</surname>
              <given-names>S. J.</given-names>
            </name>
            <name>
              <surname>Duitama</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Retraining and evaluation of machine learning and deep learning models for seizure classification from EEG data</article-title>
          <source>Sci. Rep.</source>
          <year>2025</year>
          <volume>15</volume>
          <issue>1</issue>
          <page-range>15345</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-025-98389-y</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_13">
        <label>13.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>X. Y.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>Z. Y.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>H. J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>T. K.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>K. X.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>C. T.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>L. X.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>X. H.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>D. R.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>The combination of brain-computer interfaces and artificial intelligence: Applications and challenges</article-title>
          <source>Ann. Transl. Med.</source>
          <year>2020</year>
          <volume>8</volume>
          <issue>11</issue>
          <page-range>712</page-range>
          <pub-id pub-id-type="doi">10.21037/atm.2019.11.109</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_14">
        <label>14.</label>
        <element-citation publication-type="book">
          <person-group person-group-type="author">
            <name>
              <surname>Nayak</surname>
              <given-names>C. S.</given-names>
            </name>
            <name>
              <surname>Anilkumar</surname>
              <given-names>A. C.</given-names>
            </name>
          </person-group>
          <source>EEG Normal Waveforms</source>
          <publisher-name>StatPearls Publishing</publisher-name>
          <year>2025</year>
        </element-citation>
      </ref>
      <ref id="ref_15">
        <label>15.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jiang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Bian</surname>
              <given-names>G. B.</given-names>
            </name>
            <name>
              <surname>Tian</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <article-title>Removal of Artifacts from EEG Signals: A Review</article-title>
          <source>Sensors</source>
          <year>2019</year>
          <volume>19</volume>
          <issue>5</issue>
          <page-range>987</page-range>
          <pub-id pub-id-type="doi">10.3390/s19050987</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_16">
        <label>16.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ouyang</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Protocol for semi-automatic EEG preprocessing incorporating independent component analysis and principal component analysis</article-title>
          <source>STAR Protoc.</source>
          <year>2025</year>
          <volume>6</volume>
          <issue>1</issue>
          <page-range>103682</page-range>
          <pub-id pub-id-type="doi">10.1016/j.xpro.2025.103682</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_17">
        <label>17.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gevins</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>M. E.</given-names>
            </name>
            <name>
              <surname>McEvoy</surname>
              <given-names>L. K.</given-names>
            </name>
            <name>
              <surname>Leong</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Le</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Electroencephalographic imaging of higher brain function</article-title>
          <source>Phil. Trans. R. Soc. Lond. B</source>
          <year>1999</year>
          <volume>354</volume>
          <issue>1387</issue>
          <page-range>1125-1134</page-range>
          <pub-id pub-id-type="doi">10.1098/rstb.1999.0468</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_18">
        <label>18.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Light</surname>
              <given-names>G. A.</given-names>
            </name>
            <name>
              <surname>Williams</surname>
              <given-names>L. E.</given-names>
            </name>
            <name>
              <surname>Minow</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Sprock</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Rissling</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sharp</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Braff</surname>
              <given-names>D. L.</given-names>
            </name>
          </person-group>
          <article-title>Electroencephalography (EEG) and event-related potentials (ERPs) with human participants</article-title>
          <source>Curr. Protoc. Neurosci.</source>
          <year>2010</year>
          <volume>52</volume>
          <issue>1</issue>
          <page-range>6-25</page-range>
          <pub-id pub-id-type="doi">10.1002/0471142301.ns0625s52</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_19">
        <label>19.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jiao</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>R. H.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Qing</surname>
              <given-names>K. Q.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Pan</surname>
              <given-names>H. F.</given-names>
            </name>
            <name>
              <surname>Lei</surname>
              <given-names>Y. Q.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>W. J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X. A.</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>X. W.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>X. X.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Q. J.</given-names>
            </name>
            <name>
              <surname>Liao</surname>
              <given-names>X. X.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Y. F.</given-names>
            </name>
            <name>
              <surname>Fang</surname>
              <given-names>L. J.</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>Y. B.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Y. H.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>H. Y.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Shen</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Neural biomarker diagnosis and prediction to mild cognitive impairment and Alzheimer's disease using EEG technology</article-title>
          <source>Alz. Res. Therapy</source>
          <year>2023</year>
          <volume>15</volume>
          <page-range>32</page-range>
          <pub-id pub-id-type="doi">10.1186/s13195-023-01181-1</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_20">
        <label>20.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ndaro</surname>
              <given-names>N. Z.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>S. Y.</given-names>
            </name>
          </person-group>
          <article-title>Effects of Fatigue Based on Electroencephalography Signal during Laparoscopic Surgical Simulation</article-title>
          <source>Minim. Invasive Surg.</source>
          <year>2018</year>
          <volume>2018</volume>
          <issue>1</issue>
          <page-range>2389158</page-range>
          <pub-id pub-id-type="doi">10.1155/2018/2389158</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_21">
        <label>21.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gannouni</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Aledaily</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Belwafi</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Aboalsamh</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>Emotion detection using electroencephalography signals and a zero-time windowing-based epoch estimation and relevant electrode identification</article-title>
          <source>Sci. Rep.</source>
          <year>2021</year>
          <volume>11</volume>
          <issue>1</issue>
          <page-range>7071</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-021-86345-5</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_22">
        <label>22.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chikhi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Matton</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Blanchet</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>EEG power spectral measures of cognitive workload: A meta‐analysis</article-title>
          <source>Psychophysiology</source>
          <year>2022</year>
          <volume>59</volume>
          <issue>6</issue>
          <page-range>e14009</page-range>
          <pub-id pub-id-type="doi">10.1111/psyp.14009</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_23">
        <label>23.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lambert</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Peter-Derex</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Spotlight on sleep stage classification based on EEG</article-title>
          <source>Nat. Sci. Sleep</source>
          <year>2023</year>
          <volume>15</volume>
          <page-range>479-490</page-range>
          <pub-id pub-id-type="doi">10.2147/NSS.S401270</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_24">
        <label>24.</label>
        <element-citation publication-type="webpage">
          <article-title>Physiology, sleep stages</article-title>
          <source>, http://www.ncbi.nlm.nih.gov/books/NBK526132/</source>
          <year>2024</year>
        </element-citation>
      </ref>
      <ref id="ref_25">
        <label>25.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rashid</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Sulaiman</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Majeed</surname>
              <given-names>A. P. P. A.</given-names>
            </name>
            <name>
              <surname>Musa</surname>
              <given-names>R. M.</given-names>
            </name>
            <name>
              <surname>Nasir</surname>
              <given-names>A. F. A.</given-names>
            </name>
            <name>
              <surname>Bari</surname>
              <given-names>B. S.</given-names>
            </name>
            <name>
              <surname>Khatun</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Current status, challenges, and possible solutions of EEG-based brain-computer interface: A comprehensive review</article-title>
          <source>Front. Neurorob.</source>
          <year>2020</year>
          <volume>14</volume>
          <page-range>25</page-range>
          <pub-id pub-id-type="doi">10.3389/fnbot.2020.00025</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_26">
        <label>26.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Marzbani</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Marateb</surname>
              <given-names>H. R.</given-names>
            </name>
            <name>
              <surname>Mansourian</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Neurofeedback: A comprehensive review on system design, methodology and clinical applications</article-title>
          <source>Basic Clin. Neurosci.</source>
          <year>2016</year>
          <volume>7</volume>
          <issue>2</issue>
          <page-range>143-158</page-range>
          <pub-id pub-id-type="doi">10.15412/J.BCN.03070208</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_27">
        <label>27.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pan</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>X. N.</given-names>
            </name>
            <name>
              <surname>Ban</surname>
              <given-names>N. M.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>J. S.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>J. Y.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>H. Y.</given-names>
            </name>
          </person-group>
          <article-title>Advances in P300 brain–computer interface spellers: Toward paradigm design and performance evaluation</article-title>
          <source>Front. Hum. Neurosci.</source>
          <year>2022</year>
          <volume>16</volume>
          <page-range>1077717</page-range>
          <pub-id pub-id-type="doi">10.3389/fnhum.2022.1077717</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_28">
        <label>28.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Saibene</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Caglioni</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Corchs</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Gasparini</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>EEG-based BCIs on motor imagery paradigm using wearable technologies: A systematic review</article-title>
          <source>Sensors</source>
          <year>2023</year>
          <volume>23</volume>
          <issue>5</issue>
          <page-range>2798</page-range>
          <pub-id pub-id-type="doi">10.3390/s23052798</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_29">
        <label>29.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lotte</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Congedo</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Lécuyer</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Lamarche</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Arnaldi</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>A review of classification algorithms for EEG-based brain–computer interfaces</article-title>
          <source>J. Neural Eng.</source>
          <year>2007</year>
          <volume>4</volume>
          <issue>2</issue>
          <page-range>R1</page-range>
          <pub-id pub-id-type="doi">10.1088/1741-2560/4/2/R01</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_30">
        <label>30.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cortes</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Vapnik</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <article-title>Support-vector networks</article-title>
          <source>Mach. Learn.</source>
          <year>1995</year>
          <volume>20</volume>
          <issue>3</issue>
          <page-range>273-297</page-range>
          <pub-id pub-id-type="doi">10.1007/BF00994018</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_31">
        <label>31.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ramoser</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Muller-Gerking</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Pfurtscheller</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Optimal spatial filtering of single trial EEG during imagined hand movement</article-title>
          <source>IEEE Trans. Rehab. Eng.</source>
          <year>2000</year>
          <volume>8</volume>
          <issue>4</issue>
          <page-range>441-446</page-range>
          <pub-id pub-id-type="doi">10.1109/86.895946</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_32">
        <label>32.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tuncer</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Dogan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Subasi</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>LEDPatNet19: Automated emotion recognition model based on nonlinear LED pattern feature extraction function using EEG signals</article-title>
          <source>Cogn. Neurodyn.</source>
          <year>2022</year>
          <volume>16</volume>
          <issue>4</issue>
          <page-range>779-790</page-range>
          <pub-id pub-id-type="doi">10.1007/s11571-021-09748-0</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_33">
        <label>33.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Subasi</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>EEG signal classification using wavelet feature extraction and a mixture of expert model</article-title>
          <source>Expert Syst. Appl.</source>
          <year>2007</year>
          <volume>32</volume>
          <issue>4</issue>
          <page-range>1084-1093</page-range>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2006.02.005</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_34">
        <label>34.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bastos</surname>
              <given-names>A. M.</given-names>
            </name>
            <name>
              <surname>Schoffelen</surname>
              <given-names>J.-M.</given-names>
            </name>
          </person-group>
          <article-title>A tutorial review of functional connectivity analysis methods and their interpretational pitfalls</article-title>
          <source>Front. Syst. Neurosci.</source>
          <year>2016</year>
          <volume>9</volume>
          <page-range>175</page-range>
          <pub-id pub-id-type="doi">10.3389/fnsys.2015.00175</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_35">
        <label>35.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Breiman</surname>
              <given-names>L.</given-names>
            </name>
          </person-group>
          <article-title>Random forests</article-title>
          <source>Mach. Learn.</source>
          <year>2001</year>
          <volume>45</volume>
          <issue>1</issue>
          <page-range>5-32</page-range>
          <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_36">
        <label>36.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lecun</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Bottou</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Bengio</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Haffner</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Gradient-based learning applied to document recognition</article-title>
          <source>Proc. IEEE</source>
          <year>2002</year>
          <volume>86</volume>
          <issue>11</issue>
          <page-range>2278-2324</page-range>
          <pub-id pub-id-type="doi">10.1109/5.726791</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_37">
        <label>37.</label>
        <element-citation publication-type="conference-proceedings">
          <person-group person-group-type="author">
            <name>
              <surname>Roy</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Kiral-Kornek</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Harrer</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>ChronoNet: A deep recurrent neural network for abnormal EEG identification</article-title>
          <source>, https://doi.org/10.1007/978-3-030-21642-9_8</source>
          <year>2019</year>
          <volume>11526</volume>
          <page-range>47-56</page-range>
          <pub-id pub-id-type="doi">10.1007/978-3-030-21642-9_8</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_38">
        <label>38.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lawhern</surname>
              <given-names>V. J.</given-names>
            </name>
            <name>
              <surname>Solon</surname>
              <given-names>A. J.</given-names>
            </name>
            <name>
              <surname>Waytowich</surname>
              <given-names>N. R.</given-names>
            </name>
            <name>
              <surname>Gordon</surname>
              <given-names>S. M.</given-names>
            </name>
            <name>
              <surname>Hung</surname>
              <given-names>C. P.</given-names>
            </name>
            <name>
              <surname>Lance</surname>
              <given-names>B. J.</given-names>
            </name>
          </person-group>
          <article-title>EEGNet: A compact convolutional neural network for EEG-based brain–computer interfaces</article-title>
          <source>J. Neural Eng.</source>
          <year>2018</year>
          <volume>15</volume>
          <issue>5</issue>
          <page-range>056013</page-range>
          <pub-id pub-id-type="doi">10.1088/1741-2552/aace8c</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_39">
        <label>39.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bashivan</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Rish</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Yeasin</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Codella</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>Learning representations from EEG with deep recurrent-convolutional neural networks</article-title>
          <source>arXiv</source>
          <year>2015</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.1511.06448</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_40">
        <label>40.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hochreiter</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Schmidhuber</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Long short-term memory</article-title>
          <source>Neural Comput.</source>
          <year>1997</year>
          <volume>9</volume>
          <issue>8</issue>
          <page-range>1735-1780</page-range>
          <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_41">
        <label>41.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cho</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Merrienboer</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Bahdanau</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Bengio</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>On the properties of neural machine translation: Encoder-decoder approaches</article-title>
          <source>arXiv</source>
          <year>2014</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.1409.1259</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_42">
        <label>42.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Omar</surname>
              <given-names>S. M.</given-names>
            </name>
            <name>
              <surname>Kimwele</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Olowolayemo</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kaburu</surname>
              <given-names>D. M.</given-names>
            </name>
          </person-group>
          <article-title>Enhancing EEG signals classification using LSTM‐CNN architecture</article-title>
          <source>Eng. Rep.</source>
          <year>2024</year>
          <volume>6</volume>
          <issue>9</issue>
          <page-range>e12827</page-range>
          <pub-id pub-id-type="doi">10.1002/eng2.12827</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_43">
        <label>43.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wan</surname>
              <given-names>Z. J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>M. Y.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>S. C.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>J. J.</given-names>
            </name>
            <name>
              <surname>Tan</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Duan</surname>
              <given-names>W. F.</given-names>
            </name>
          </person-group>
          <article-title>EEGformer: A transformer–based brain activity classification method using EEG signal</article-title>
          <source>Front. Neurosci.</source>
          <year>2023</year>
          <volume>17</volume>
          <page-range>1148855</page-range>
          <pub-id pub-id-type="doi">10.3389/fnins.2023.1148855</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_44">
        <label>44.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Vaswani</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Shazeer</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Parmar</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Uszkoreit</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Jones</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Gomez</surname>
              <given-names>A. N.</given-names>
            </name>
            <name>
              <surname>Kaiser</surname>
              <given-names>Ł.</given-names>
            </name>
            <name>
              <surname>Polosukhin</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>Attention is all You need</article-title>
          <source>arXiv</source>
          <year>2023</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.1706.03762</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_45">
        <label>45.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Vafaei</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Hosseini</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Transformers in EEG Analysis: A review of architectures and applications in motor imagery, seizure, and emotion classification</article-title>
          <source>Sensors</source>
          <year>2025</year>
          <volume>25</volume>
          <issue>5</issue>
          <page-range>1293</page-range>
          <pub-id pub-id-type="doi">10.3390/s25051293</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_46">
        <label>46.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Albaqami</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Hassan</surname>
              <given-names>G. M.</given-names>
            </name>
            <name>
              <surname>Datta</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>MP-SeizNet: A multi-path CNN Bi-LSTM network for seizure-type classification using EEG</article-title>
          <source>Biomed. Signal Process. Control</source>
          <year>2023</year>
          <volume>84</volume>
          <page-range>104780</page-range>
          <pub-id pub-id-type="doi">10.1016/j.bspc.2023.104780</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_47">
        <label>47.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Machado</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Araújo</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Paes</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Velasques</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Cunha</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Budde</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Basile</surname>
              <given-names>L. F.</given-names>
            </name>
            <name>
              <surname>Anghinah</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Arias-Carrión</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Cagy</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Piedade</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Graaf</surname>
              <given-names>T. A.</given-names>
            </name>
            <name>
              <surname>Sack</surname>
              <given-names>A. T.</given-names>
            </name>
            <name>
              <surname>Ribeiro</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>EEG-based brain-computer interfaces: An overview of basic concepts and clinical applications in neurorehabilitation</article-title>
          <source>Rev. Neurosci.</source>
          <year>2010</year>
          <volume>21</volume>
          <issue>6</issue>
          <page-range>451-468</page-range>
          <pub-id pub-id-type="doi">10.1515/REVNEURO.2010.21.6.451</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_48">
        <label>48.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>A survey on robots controlled by motor imagery brain-computer interfaces</article-title>
          <source>Cogn. Robot.</source>
          <year>2021</year>
          <volume>1</volume>
          <page-range>12-24</page-range>
          <pub-id pub-id-type="doi">10.1016/j.cogr.2021.02.001</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_49">
        <label>49.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hayta</surname>
              <given-names>Ü.</given-names>
            </name>
            <name>
              <surname>Irimia</surname>
              <given-names>D. C.</given-names>
            </name>
            <name>
              <surname>Guger</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Erkutlu</surname>
              <given-names>Ĭ.</given-names>
            </name>
            <name>
              <surname>Güzelbey</surname>
              <given-names>Ĭ. H.</given-names>
            </name>
          </person-group>
          <article-title>Optimizing motor imagery parameters for robotic arm control by brain-computer interface</article-title>
          <source>Brain Sci.</source>
          <year>2022</year>
          <volume>12</volume>
          <issue>7</issue>
          <page-range>833</page-range>
          <pub-id pub-id-type="doi">10.3390/brainsci12070833</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_50">
        <label>50.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Singh</surname>
              <given-names>A. K.</given-names>
            </name>
            <name>
              <surname>Krishnan</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Trends in EEG signal feature extraction applications</article-title>
          <source>Front. Artif. Intell.</source>
          <year>2023</year>
          <volume>5</volume>
          <page-range>1072801</page-range>
          <pub-id pub-id-type="doi">10.3389/frai.2022.1072801</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_51">
        <label>51.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pawan</surname>
            </name>
            <name>
              <surname>Dhiman</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Machine learning techniques for electroencephalogram based brain-computer interface: A systematic literature review</article-title>
          <source>Meas. Sens.</source>
          <year>2023</year>
          <volume>28</volume>
          <page-range>100823</page-range>
          <pub-id pub-id-type="doi">10.1016/j.measen.2023.100823</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_52">
        <label>52.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>An</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wong</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Ling</surname>
              <given-names>S. H.</given-names>
            </name>
          </person-group>
          <article-title>Development of real-time brain-computer interface control system for robot</article-title>
          <source>Appl. Soft Comput.</source>
          <year>2024</year>
          <volume>159</volume>
          <page-range>111648</page-range>
          <pub-id pub-id-type="doi">10.1016/j.asoc.2024.111648</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_53">
        <label>53.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rajpura</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Cecotti</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Meena</surname>
              <given-names>Y. K.</given-names>
            </name>
          </person-group>
          <article-title>Explainable artificial intelligence approaches for brain-computer interfaces: A review and design space</article-title>
          <source>J. Neural Eng.</source>
          <year>2024</year>
          <volume>21</volume>
          <issue>4</issue>
          <page-range>0401003</page-range>
          <pub-id pub-id-type="doi">10.1088/1741-2552/ad6593</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_54">
        <label>54.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhou</surname>
              <given-names>X. L.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>C. Y.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>J. N.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z. R.</given-names>
            </name>
            <name>
              <surname>Zhai</surname>
              <given-names>L. M.</given-names>
            </name>
            <name>
              <surname>Jia</surname>
              <given-names>Z. Y.</given-names>
            </name>
            <name>
              <surname>Guan</surname>
              <given-names>C. T.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Interpretable and robust AI in EEG systems: A survey</article-title>
          <source>arXiv</source>
          <year>2023</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2304.10755</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_55">
        <label>55.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Siddiqui</surname>
              <given-names>M. K.</given-names>
            </name>
            <name>
              <surname>Morales-Menendez</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Hussain</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>A review of epileptic seizure detection using machine learning classifiers</article-title>
          <source>Brain Inf.</source>
          <year>2020</year>
          <volume>7</volume>
          <issue>1</issue>
          <page-range>5</page-range>
          <pub-id pub-id-type="doi">10.1186/s40708-020-00105-1</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_56">
        <label>56.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>M. Q.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z. L.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>J. L.</given-names>
            </name>
          </person-group>
          <article-title>A P300-detection method based on logistic regression and a convolutional neural network</article-title>
          <source>Front. Comput. Neurosci.</source>
          <year>2022</year>
          <volume>16</volume>
          <page-range>909553</page-range>
          <pub-id pub-id-type="doi">10.3389/fncom.2022.909553</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_57">
        <label>57.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ramírez-Arias</surname>
              <given-names>F. J.</given-names>
            </name>
            <name>
              <surname>García-Guerrero</surname>
              <given-names>E. E.</given-names>
            </name>
            <name>
              <surname>Tlelo-Cuautle</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Colores-Vargas</surname>
              <given-names>J. M.</given-names>
            </name>
            <name>
              <surname>García-Canseco</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>López-Bonilla</surname>
              <given-names>O. R.</given-names>
            </name>
            <name>
              <surname>Galindo-Aldana</surname>
              <given-names>G. M.</given-names>
            </name>
            <name>
              <surname>Inzunza-González</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Evaluation of machine learning algorithms for classification of EEG signals</article-title>
          <source>Technologies</source>
          <year>2022</year>
          <volume>10</volume>
          <issue>4</issue>
          <page-range>79</page-range>
          <pub-id pub-id-type="doi">10.3390/technologies10040079</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_58">
        <label>58.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wong</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Simmons</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Rivera‐Villicana</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Barnett</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Sivathamboo</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Perucca</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Ge</surname>
              <given-names>Z. Y.</given-names>
            </name>
            <name>
              <surname>Kwan</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Kuhlmann</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Vasa</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Mouzakis</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>O’Brien</surname>
              <given-names>T. J.</given-names>
            </name>
          </person-group>
          <article-title>EEG datasets for seizure detection and prediction—A review</article-title>
          <source>Epilepsia Open</source>
          <year>2023</year>
          <volume>8</volume>
          <issue>2</issue>
          <page-range>252-267</page-range>
          <pub-id pub-id-type="doi">10.1002/epi4.12704</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_59">
        <label>59.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ma</surname>
              <given-names>W. Z.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>Y. J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>T. H.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Z. P.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L. J.</given-names>
            </name>
          </person-group>
          <article-title>A comprehensive review of deep learning in EEG-based emotion recognition: Classifications, trends, and practical implications</article-title>
          <source>PeerJ Comput. Sci.</source>
          <year>2024</year>
          <volume>10</volume>
          <page-range>e2065</page-range>
          <pub-id pub-id-type="doi">10.7717/peerj-cs.2065</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_60">
        <label>60.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>J. J.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>C. H.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Y. H.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>T. G.</given-names>
            </name>
            <name>
              <surname>Jung</surname>
              <given-names>T. P.</given-names>
            </name>
            <name>
              <surname>Wan</surname>
              <given-names>X. L.</given-names>
            </name>
            <name>
              <surname>Duan</surname>
              <given-names>D. N.</given-names>
            </name>
            <name>
              <surname>Wen</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>A novel AI-driven EEG generalized classification model for cross-subject and cross-scene analysis</article-title>
          <source>Adv. Eng. Inform.</source>
          <year>2025</year>
          <volume>63</volume>
          <page-range>102971</page-range>
          <pub-id pub-id-type="doi">10.1016/j.aei.2024.102971</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_61">
        <label>61.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kumar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Anand</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>A depth of anaesthesia index from linear regression of EEG parameters</article-title>
          <source>J. Clin. Monit. Comput.</source>
          <year>2006</year>
          <volume>20</volume>
          <issue>2</issue>
          <page-range>67-73</page-range>
          <pub-id pub-id-type="doi">10.1007/s10877-005-9004-x</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_62">
        <label>62.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yuan</surname>
              <given-names>D. Y.</given-names>
            </name>
            <name>
              <surname>Yue</surname>
              <given-names>J. W.</given-names>
            </name>
            <name>
              <surname>Xiong</surname>
              <given-names>X. F.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>Y. B.</given-names>
            </name>
            <name>
              <surname>Zan</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>C. Y.</given-names>
            </name>
          </person-group>
          <article-title>A regression method for EEG-based cross-dataset fatigue detection</article-title>
          <source>Front. Physiol.</source>
          <year>2023</year>
          <volume>14</volume>
          <page-range>1196919</page-range>
          <pub-id pub-id-type="doi">10.3389/fphys.2023.1196919</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_63">
        <label>63.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Rivas</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Sierra-Garcia</surname>
              <given-names>J. E.</given-names>
            </name>
            <name>
              <surname>Camara</surname>
              <given-names>J. M.</given-names>
            </name>
          </person-group>
          <article-title>Comparison of LSTM- and GRU-type RNN networks for attention and meditation prediction on raw EEG data from low-cost Headsets</article-title>
          <source>Electronics</source>
          <year>2025</year>
          <volume>14</volume>
          <issue>4</issue>
          <page-range>707</page-range>
          <pub-id pub-id-type="doi">10.3390/electronics14040707</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_64">
        <label>64.</label>
        <element-citation publication-type="conference-proceedings">
          <person-group person-group-type="author">
            <name>
              <surname>Goodfellow</surname>
              <given-names>I. J.</given-names>
            </name>
            <name>
              <surname>Pouget-Abadie</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Mirza</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Warde-Farley</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Ozair</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Courville</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Bengio</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Generative adversarial networks</article-title>
          <source>, https://proceedings.neurips.cc/paper_files/paper/2014/file/f033ed80deb0234979a61f95710dbe25-Paper.pdf</source>
          <year>2014</year>
        </element-citation>
      </ref>
      <ref id="ref_65">
        <label>65.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kingma</surname>
              <given-names>D. P.</given-names>
            </name>
            <name>
              <surname>Welling</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Auto-Encoding Variational Bayes</article-title>
          <source>arXiv</source>
          <year>2022</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.1312.6114</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_66">
        <label>66.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Habashi</surname>
              <given-names>A. G.</given-names>
            </name>
            <name>
              <surname>Azab</surname>
              <given-names>A. M.</given-names>
            </name>
            <name>
              <surname>Eldawlatly</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Aly</surname>
              <given-names>G. M.</given-names>
            </name>
          </person-group>
          <article-title>Generative adversarial networks in EEG analysis: An overview</article-title>
          <source>J. Neuroeng. Rehabil.</source>
          <year>2023</year>
          <volume>20</volume>
          <issue>1</issue>
          <page-range>40</page-range>
          <pub-id pub-id-type="doi">10.1186/s12984-023-01169-w</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_67">
        <label>67.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>You</surname>
              <given-names>Z. S.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>Y. Z.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X. L.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Y. F.</given-names>
            </name>
          </person-group>
          <article-title>Virtual electroencephalogram acquisition: A review on eectroencephalogram generative methods</article-title>
          <source>Sensors</source>
          <year>2025</year>
          <volume>25</volume>
          <issue>10</issue>
          <page-range>3178</page-range>
          <pub-id pub-id-type="doi">10.3390/s25103178</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_68">
        <label>68.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Song</surname>
              <given-names>J. X.</given-names>
            </name>
            <name>
              <surname>Zhai</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>J. Z.</given-names>
            </name>
          </person-group>
          <article-title>EEGGAN-Net: Enhancing EEG signal classification through data augmentation</article-title>
          <source>Front. Hum. Neurosci.</source>
          <year>2024</year>
          <volume>18</volume>
          <page-range>1430086</page-range>
          <pub-id pub-id-type="doi">10.3389/fnhum.2024.1430086</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_69">
        <label>69.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Mao</surname>
              <given-names>X. Q.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>Y. D.</given-names>
            </name>
          </person-group>
          <article-title>ATGAN: Attention-based temporal GAN for EEG data augmentation in personal identification</article-title>
          <source>EURASIP J. Adv. Signal Process.</source>
          <year>2024</year>
          <volume>2024</volume>
          <issue>1</issue>
          <page-range>94</page-range>
          <pub-id pub-id-type="doi">10.1186/s13634-024-01188-2</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_70">
        <label>70.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mutlu</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Doğan</surname>
              <given-names>Ş.</given-names>
            </name>
            <name>
              <surname>Tuncer</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Synthetic ALS-EEG data augmentation for ALS diagnosis using conditional WGAN with weight clipping</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2506.16243</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_71">
        <label>71.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Duan</surname>
              <given-names>Y. Q.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>J. Z.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Chang</surname>
              <given-names>Y. C.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y. K.</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>C. T.</given-names>
            </name>
          </person-group>
          <article-title>Domain-Specific Denoising Diffusion Probabilistic Models for Brain Dynamics</article-title>
          <source>arXiv</source>
          <year>2023</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2305.04200</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_72">
        <label>72.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sun</surname>
              <given-names>C. Z.</given-names>
            </name>
            <name>
              <surname>Mou</surname>
              <given-names>C. Z.</given-names>
            </name>
          </person-group>
          <article-title>Survey on the research direction of EEG-based signal processing</article-title>
          <source>Front. Neurosci.</source>
          <year>2023</year>
          <volume>17</volume>
          <page-range>1203059</page-range>
          <pub-id pub-id-type="doi">10.3389/fnins.2023.1203059</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_73">
        <label>73.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>She</surname>
              <given-names>Q. S.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>X. S.</given-names>
            </name>
            <name>
              <surname>Fang</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>Y. L.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y. C.</given-names>
            </name>
          </person-group>
          <article-title>Cross-subject EEG emotion recognition using multi-source domain manifold feature selection</article-title>
          <source>Comput. Biol. Med.</source>
          <year>2023</year>
          <volume>159</volume>
          <page-range>106860</page-range>
          <pub-id pub-id-type="doi">10.1016/j.compbiomed.2023.106860</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_74">
        <label>74.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bao</surname>
              <given-names>G. C.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Tong</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Shu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L. Y.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zeng</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Data augmentation for EEG-based emotion recognition using generative adversarial networks</article-title>
          <source>Front. Comput. Neurosci.</source>
          <year>2021</year>
          <volume>15</volume>
          <page-range>723843</page-range>
          <pub-id pub-id-type="doi">10.3389/fncom.2021.723843</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_75">
        <label>75.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>George</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Smith</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Madiraju</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Yahyasoltani</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Ahamed</surname>
              <given-names>S. I.</given-names>
            </name>
          </person-group>
          <article-title>Data augmentation strategies for EEG-based motor imagery decoding</article-title>
          <source>Heliyon</source>
          <year>2022</year>
          <volume>8</volume>
          <issue>8</issue>
          <pub-id pub-id-type="doi">10.1016/j.heliyon.2022.e10240</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_76">
        <label>76.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Vetter</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Macke</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Generating realistic neurophysiological time series with denoising diffusion probabilistic models</article-title>
          <source>Patterns</source>
          <year>2024</year>
          <volume>5</volume>
          <issue>9</issue>
          <page-range>101047</page-range>
          <pub-id pub-id-type="doi">10.1016/j.patter.2024.101047</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_77">
        <label>77.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Huang</surname>
              <given-names>X. Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>A. P.</given-names>
            </name>
            <name>
              <surname>Qian</surname>
              <given-names>R. B.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>EEGDfus: A conditional diffusion model for fine-grained EEG denoising</article-title>
          <source>IEEE J. Biomed. Health Inform.</source>
          <year>2025</year>
          <volume>29</volume>
          <issue>4</issue>
          <page-range>2557-2569</page-range>
          <pub-id pub-id-type="doi">10.1109/JBHI.2024.3504717</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_78">
        <label>78.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>M. Z.</given-names>
            </name>
            <name>
              <surname>Gui</surname>
              <given-names>Y. Y.</given-names>
            </name>
            <name>
              <surname>Su</surname>
              <given-names>Y. Q.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Y. S.</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>G. B.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Y. C.</given-names>
            </name>
          </person-group>
          <article-title>Improving EEG classification through randomly reassembling original and generated data with transformer-based diffusion models</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2407.20253</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_79">
        <label>79.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Yin</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Gu</surname>
              <given-names>X. L.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>X. W.</given-names>
            </name>
            <name>
              <surname>Cao</surname>
              <given-names>X. S.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>C. J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>X. O.</given-names>
            </name>
          </person-group>
          <article-title>Neurophysiological data augmentation for EEG-fNIRS multimodal features based on a denoising diffusion probabilistic model</article-title>
          <source>Comput. Methods Programs Biomed.</source>
          <year>2025</year>
          <volume>261</volume>
          <page-range>108594</page-range>
          <pub-id pub-id-type="doi">10.1016/j.cmpb.2025.108594</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_80">
        <label>80.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Klein</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Guetschel</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Silvestri</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Tangermann</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Synthesizing EEG signals from event-related potential paradigms with conditional diffusion models</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.3217/978-3-99161-014-4-077</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_81">
        <label>81.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Puah</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Goh</surname>
              <given-names>S. K.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Ye</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Chan</surname>
              <given-names>C. K.</given-names>
            </name>
            <name>
              <surname>Lim</surname>
              <given-names>K. S.</given-names>
            </name>
            <name>
              <surname>Fong</surname>
              <given-names>S. L.</given-names>
            </name>
            <name>
              <surname>Woon</surname>
              <given-names>K. S.</given-names>
            </name>
            <name>
              <surname>Guan</surname>
              <given-names>C. T.</given-names>
            </name>
          </person-group>
          <article-title>EEGDM: EEG Representation Learning via Generative Diffusion Model</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2508.14086</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_82">
        <label>82.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wegner</surname>
              <given-names>F. Von</given-names>
            </name>
            <name>
              <surname>Knaut</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Laufs</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>EEG microstate sequences from different clustering algorithms are information-theoretically invariant</article-title>
          <source>Front. Comput. Neurosci.</source>
          <year>2018</year>
          <volume>12</volume>
          <page-range>70</page-range>
          <pub-id pub-id-type="doi">10.3389/fncom.2018.00070</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_83">
        <label>83.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Neloy</surname>
              <given-names>A. A.</given-names>
            </name>
            <name>
              <surname>Turgeon</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>A comprehensive study of auto-encoders for anomaly detection: Efficiency and trade-offs</article-title>
          <source>Mach. Learn. Appl.</source>
          <year>2024</year>
          <volume>17</volume>
          <page-range>100572</page-range>
          <pub-id pub-id-type="doi">10.1016/j.mlwa.2024.100572</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_84">
        <label>84.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Subasi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Gursoy</surname>
              <given-names>M. Ismail</given-names>
            </name>
          </person-group>
          <article-title>EEG signal classification using PCA, ICA, LDA and support vector machines</article-title>
          <source>Expert Syst. Appl.</source>
          <year>2010</year>
          <volume>37</volume>
          <issue>12</issue>
          <page-range>8659-8666</page-range>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2010.06.065</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_85">
        <label>85.</label>
        <element-citation publication-type="conference-proceedings">
          <person-group person-group-type="author">
            <name>
              <surname>Sha'abani</surname>
              <given-names>M. N. A. H.</given-names>
            </name>
            <name>
              <surname>Fuad</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Jamal</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Ismail</surname>
              <given-names>M. F.</given-names>
            </name>
          </person-group>
          <article-title>kNN and SVM classification for EEG: A review</article-title>
          <source>, https://doi.org/10.1007/978-981-15-2317-5_47</source>
          <year>2020</year>
          <volume>632</volume>
          <page-range>555-565</page-range>
          <pub-id pub-id-type="doi">10.1007/978-981-15-2317-5_47</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_86">
        <label>86.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bilucaglia</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Duma</surname>
              <given-names>G. M.</given-names>
            </name>
            <name>
              <surname>Mento</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Semenzato</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Tressoldi</surname>
              <given-names>P. E.</given-names>
            </name>
          </person-group>
          <article-title>Applying machine learning EEG signal classification to emotion‑related brain anticipatory activity</article-title>
          <source>F1000Res.</source>
          <year>2021</year>
          <volume>9</volume>
          <page-range>173</page-range>
          <pub-id pub-id-type="doi">10.12688/f1000research.22202.3</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_87">
        <label>87.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Subasi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Tuncer</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Dogan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Tanko</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Sakoglu</surname>
              <given-names>U.</given-names>
            </name>
          </person-group>
          <article-title>EEG-based emotion recognition using tunable Q wavelet transform and rotation forest ensemble classifier</article-title>
          <source>Biomed. Signal Process. Control</source>
          <year>2021</year>
          <volume>68</volume>
          <page-range>102648</page-range>
          <pub-id pub-id-type="doi">10.1016/j.bspc.2021.102648</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_88">
        <label>88.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Riyad</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Khalil</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Adib</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>MI-EEGNET: A novel convolutional neural network for motor imagery classification</article-title>
          <source>J. Neurosci. Methods</source>
          <year>2021</year>
          <volume>353</volume>
          <page-range>109037</page-range>
          <pub-id pub-id-type="doi">10.1016/j.jneumeth.2020.109037</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_89">
        <label>89.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Berrich</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Guennoun</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <article-title>EEG-based epilepsy detection using CNN-SVM and DNN-SVM with feature dimensionality reduction by PCA</article-title>
          <source>Sci. Rep.</source>
          <year>2025</year>
          <volume>15</volume>
          <issue>1</issue>
          <page-range>14313</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-025-95831-z</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_90">
        <label>90.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hou</surname>
              <given-names>Y. M.</given-names>
            </name>
            <name>
              <surname>Jia</surname>
              <given-names>S. Y.</given-names>
            </name>
            <name>
              <surname>Lun</surname>
              <given-names>X. M.</given-names>
            </name>
            <name>
              <surname>Hao</surname>
              <given-names>Z. Q.</given-names>
            </name>
            <name>
              <surname>Shi</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zeng</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Lv</surname>
              <given-names>J. L.</given-names>
            </name>
          </person-group>
          <article-title>GCNs-Net: A graph convolutional neural network approach for decoding time-resolved EEG motor imagery signals</article-title>
          <source>IEEE Trans. Neural Netw. Learn. Syst.</source>
          <year>2022</year>
          <volume>35</volume>
          <issue>6</issue>
          <page-range>7312-7323</page-range>
          <pub-id pub-id-type="doi">10.1109/TNNLS.2022.3202569</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_91">
        <label>91.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Du</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>EEG temporal–spatial transformer for person identification</article-title>
          <source>Sci. Rep.</source>
          <year>2022</year>
          <volume>12</volume>
          <issue>1</issue>
          <page-range>14378</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-022-18502-3</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_92">
        <label>92.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Dosovitskiy</surname>
              <given-names>Alexey</given-names>
            </name>
            <name>
              <surname>Beyer</surname>
              <given-names>Lucas</given-names>
            </name>
            <name>
              <surname>Kolesnikov</surname>
              <given-names>Alexander</given-names>
            </name>
            <name>
              <surname>Weissenborn</surname>
              <given-names>Dirk</given-names>
            </name>
            <name>
              <surname>Zhai</surname>
              <given-names>X. H.</given-names>
            </name>
            <name>
              <surname>Unterthiner</surname>
              <given-names>Thomas</given-names>
            </name>
            <name>
              <surname>Dehghani</surname>
              <given-names>Mostafa</given-names>
            </name>
            <name>
              <surname>Minderer</surname>
              <given-names>Matthias</given-names>
            </name>
            <name>
              <surname>Heigold</surname>
              <given-names>Georg</given-names>
            </name>
            <name>
              <surname>Gelly</surname>
              <given-names>Sylvain</given-names>
            </name>
            <name>
              <surname>Uszkoreit</surname>
              <given-names>Jakob</given-names>
            </name>
            <name>
              <surname>Houlsby</surname>
              <given-names>Neil</given-names>
            </name>
          </person-group>
          <article-title>An image is worth 16x16 words: Transformers for image recognition at scale</article-title>
          <source>International Conference on Learning Representations (ICLR)</source>
          <year>2021</year>
        </element-citation>
      </ref>
      <ref id="ref_93">
        <label>93.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Lee</surname>
              <given-names>Y. E.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>S. H.</given-names>
            </name>
          </person-group>
          <article-title>EEG-Transformer: Self-attention from transformer architecture for decoding EEG of imagined speech</article-title>
          <source>2022 10th International Winter Conference on Brain-Computer Interface (BCI), Gangwon-do, South Korea</source>
          <year>2022</year>
          <page-range>1-4</page-range>
          <pub-id pub-id-type="doi">10.1109/BCI53720.2022.9735124</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_94">
        <label>94.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cui</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>MVGT: A multi-view graph transformer based on spatial relations for EEG emotion recognition</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2407.03131</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_95">
        <label>95.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Mahaseni</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Khan</surname>
              <given-names>N. M.</given-names>
            </name>
          </person-group>
          <article-title>EEG signal denoising using beta-variational autoencoder</article-title>
          <source>2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source>
          <year>2024</year>
          <page-range>1-4</page-range>
          <pub-id pub-id-type="doi">10.1109/EMBC53108.2024.10782962</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_96">
        <label>96.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Cui</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Ji</surname>
              <given-names>T. Y.</given-names>
            </name>
            <name>
              <surname>Luo</surname>
              <given-names>J. J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>W. L.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>Z. F.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>W. G.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>Y. X.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>Y. W.</given-names>
            </name>
            <name>
              <surname>Hong</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>VAEEG: Variational auto-encoder for extracting EEG representation</article-title>
          <source>NeuroImage</source>
          <year>2024</year>
          <volume>304</volume>
          <page-range>120946</page-range>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2024.120946</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_97">
        <label>97.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hinton</surname>
              <given-names>G. E.</given-names>
            </name>
            <name>
              <surname>Osindero</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Teh</surname>
              <given-names>Y. W.</given-names>
            </name>
          </person-group>
          <article-title>A fast learning algorithm for deep belief nets</article-title>
          <source>Neural Comput.</source>
          <year>2006</year>
          <volume>18</volume>
          <issue>7</issue>
          <page-range>1527-1554</page-range>
          <pub-id pub-id-type="doi">10.1162/neco.2006.18.7.1527</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_98">
        <label>98.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Movahedi</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Coyle</surname>
              <given-names>J. L.</given-names>
            </name>
            <name>
              <surname>Sejdic</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>Deep belief networks for electroencephalography: A review of recent contributions and future outlooks</article-title>
          <source>IEEE J. Biomed. Health Inform.</source>
          <year>2017</year>
          <volume>22</volume>
          <issue>3</issue>
          <page-range>642-652</page-range>
          <pub-id pub-id-type="doi">10.1109/JBHI.2017.2727218</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_99">
        <label>99.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Scarselli</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Gori</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Tsoi</surname>
              <given-names>A. C.</given-names>
            </name>
            <name>
              <surname>Hagenbuchner</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Monfardini</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>The Graph Neural Network Model</article-title>
          <source>IEEE Trans. Neural Netw.</source>
          <year>2008</year>
          <volume>20</volume>
          <issue>1</issue>
          <page-range>61-80</page-range>
          <pub-id pub-id-type="doi">10.1109/TNN.2008.2005605</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_100">
        <label>100.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kipf</surname>
              <given-names>T. N.</given-names>
            </name>
            <name>
              <surname>Welling</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Semi-supervised classification with graph convolutional networks</article-title>
          <source>arXiv</source>
          <year>2016</year>
          <pub-id pub-id-type="doi">10.48550/ARXIV.1609.02907</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_101">
        <label>101.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Veličković</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Cucurull</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Casanova</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Romero</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Liò</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Bengio</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Graph attention networks</article-title>
          <source>arXiv</source>
          <year>2017</year>
          <pub-id pub-id-type="doi">10.48550/ARXIV.1710.10903</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_102">
        <label>102.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Z. D.</given-names>
            </name>
            <name>
              <surname>Hwang</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>K. Q.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Ji</surname>
              <given-names>T. K.</given-names>
            </name>
          </person-group>
          <article-title>Graph-generative neural network for EEG-based epileptic seizure detection via discovery of dynamic brain functional connectivity</article-title>
          <source>Sci. Rep.</source>
          <year>2022</year>
          <volume>12</volume>
          <issue>1</issue>
          <page-range>18998</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-022-23656-1</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_103">
        <label>103.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Klepl</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Graph neural network-based EEG classification: A Survey</article-title>
          <source>IEEE Trans. Neural Syst. Rehabil. Eng.</source>
          <year>2024</year>
          <volume>32</volume>
          <page-range>493-503</page-range>
          <pub-id pub-id-type="doi">10.1109/TNSRE.2024.3355750</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_104">
        <label>104.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Hu</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Dai</surname>
              <given-names>R. J.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>W. T.</given-names>
            </name>
            <name>
              <surname>Yin</surname>
              <given-names>H. L.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>B. L.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>W. L.</given-names>
            </name>
          </person-group>
          <article-title>Contrastive self-supervised EEG representation learning for emotion classification</article-title>
          <source>2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</source>
          <year>2024</year>
          <page-range>1-4</page-range>
          <pub-id pub-id-type="doi">10.1109/EMBC53108.2024.10781579</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_105">
        <label>105.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wei</surname>
              <given-names>X. X.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>K. H.</given-names>
            </name>
            <name>
              <surname>Jiao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Xie</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>L. F.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Pre-training graph contrastive masked autoencoders are strong distillers for EEG</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2411.19230</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_106">
        <label>106.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Thangarajoo</surname>
              <given-names>R. G.</given-names>
            </name>
            <name>
              <surname>Reaz</surname>
              <given-names>M. B. I.</given-names>
            </name>
            <name>
              <surname>Srivastava</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Haque</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>S. H. M.</given-names>
            </name>
            <name>
              <surname>Bakar</surname>
              <given-names>A. A. A.</given-names>
            </name>
            <name>
              <surname>Bhuiyan</surname>
              <given-names>M. A. S.</given-names>
            </name>
          </person-group>
          <article-title>Machine learning-based epileptic seizure detection methods using wavelet and EMD-based decomposition techniques: A review</article-title>
          <source>Sensors</source>
          <year>2021</year>
          <volume>21</volume>
          <issue>24</issue>
          <pub-id pub-id-type="doi">10.3390/s21248485</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_107">
        <label>107.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Atlam</surname>
              <given-names>H. F.</given-names>
            </name>
            <name>
              <surname>Aderibigbe</surname>
              <given-names>G. E.</given-names>
            </name>
            <name>
              <surname>Nadeem</surname>
              <given-names>M. S.</given-names>
            </name>
          </person-group>
          <article-title>Effective epileptic seizure detection with hybrid feature selection and SMOTE-based data balancing using SVM classifier</article-title>
          <year>2025</year>
          <volume>15</volume>
          <issue>9</issue>
          <page-range>4690</page-range>
          <pub-id pub-id-type="doi">10.3390/app15094690</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_108">
        <label>108.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Shen</surname>
              <given-names>M. K.</given-names>
            </name>
            <name>
              <surname>Wen</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Song</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Real-time epilepsy seizure detection based on EEG using tunable-Q wavelet transform and convolutional neural network</article-title>
          <year>2023</year>
          <volume>82</volume>
          <page-range>104566</page-range>
          <pub-id pub-id-type="doi">10.1016/j.bspc.2022.104566</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_109">
        <label>109.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kumar</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Upadhyay</surname>
              <given-names>P. K.</given-names>
            </name>
          </person-group>
          <article-title>A hybrid optimization-enhanced 1D-ResCNN framework for epileptic spike detection in scalp EEG signals</article-title>
          <year>2025</year>
          <volume>15</volume>
          <issue>1</issue>
          <page-range>5707</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-025-90164-3</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_110">
        <label>110.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>R. Q.</given-names>
            </name>
            <name>
              <surname>Modesitt</surname>
              <given-names>E.</given-names>
            </name>
          </person-group>
          <article-title>ViT2EEG: Leveraging hybrid pretrained vision transformers for EEG data</article-title>
          <source>arXiv</source>
          <year>2023</year>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2308.00454</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_111">
        <label>111.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lionakis</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Karampidis</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Papadourakis</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Current trends, challenges, and future research directions of hybrid and deep learning techniques for motor imagery brain-computer interface</article-title>
          <source>Multimodal Technol. Interact.</source>
          <year>2023</year>
          <volume>7</volume>
          <issue>10</issue>
          <page-range>95</page-range>
          <pub-id pub-id-type="doi">10.3390/mti7100095</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_112">
        <label>112.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>T. W.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>M. Y.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>CLTNet: A hybrid deep learning model for motor imagery classification</article-title>
          <source>Brain Sci.</source>
          <year>2025</year>
          <volume>15</volume>
          <issue>2</issue>
          <page-range>124</page-range>
          <pub-id pub-id-type="doi">10.3390/brainsci15020124</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_113">
        <label>113.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>B. C.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>H. F.</given-names>
            </name>
            <name>
              <surname>Wei</surname>
              <given-names>D. Z.</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>C. X.</given-names>
            </name>
            <name>
              <surname>Lan</surname>
              <given-names>Q.</given-names>
            </name>
          </person-group>
          <article-title>Multi-scale convolutional transformer network for motor imagery brain-computer interface</article-title>
          <source>Sci. Rep.</source>
          <year>2025</year>
          <volume>15</volume>
          <issue>1</issue>
          <page-range>12935</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-025-96611-5</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_114">
        <label>114.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yao</surname>
              <given-names>X. Z.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>T. W.</given-names>
            </name>
            <name>
              <surname>Ding</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Gong</surname>
              <given-names>A. M.</given-names>
            </name>
            <name>
              <surname>Nan</surname>
              <given-names>W. Y.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>Y. F.</given-names>
            </name>
          </person-group>
          <article-title>Emotion classification based on transformer and CNN for EEG spatial-temporal feature learning</article-title>
          <source>Brain Sci.</source>
          <year>2024</year>
          <volume>14</volume>
          <issue>3</issue>
          <page-range>268</page-range>
          <pub-id pub-id-type="doi">10.3390/brainsci14030268</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_115">
        <label>115.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Shoeibi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Khodatars</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Ghassemi</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Jafari</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Moridian</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Alizadehsani</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Panahiazar</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Khozeimeh</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Zare</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Hosseini-Nejad</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Khosravi</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Atiya</surname>
              <given-names>A. F.</given-names>
            </name>
            <name>
              <surname>Aminshahidi</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Hussain</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Rouhani</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Nahavandi</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Acharya</surname>
              <given-names>U. R.</given-names>
            </name>
          </person-group>
          <article-title>Epileptic seizures detection using deep learning techniques: A review</article-title>
          <source>Int. J. Environ. Res. Public Health</source>
          <year>2021</year>
          <volume>18</volume>
          <issue>11</issue>
          <pub-id pub-id-type="doi">10.3390/ijerph18115780</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_116">
        <label>116.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zou</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>D. Q.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>F. J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>X. H.</given-names>
            </name>
          </person-group>
          <article-title>Accuracy of machine learning in detecting pediatric epileptic seizures: Systematic review and meta-analysis</article-title>
          <source>J. Med. Internet Res.</source>
          <year>2024</year>
          <volume>26</volume>
          <page-range>e55986</page-range>
          <pub-id pub-id-type="doi">10.2196/55986</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_117">
        <label>117.</label>
        <element-citation publication-type="webpage">
          <article-title>CHB-MIT scalp EEG database</article-title>
          <source>, https://doi.org/10.13026/C2K01R</source>
          <year>2010</year>
        </element-citation>
      </ref>
      <ref id="ref_118">
        <label>118.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ibrahim</surname>
              <given-names>F. E.</given-names>
            </name>
            <name>
              <surname>Emara</surname>
              <given-names>H. M.</given-names>
            </name>
            <name>
              <surname>El‐Shafai</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Elwekeil</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Rihan</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Eldokany</surname>
              <given-names>I. M.</given-names>
            </name>
            <name>
              <surname>Taha</surname>
              <given-names>E. T.</given-names>
            </name>
            <name>
              <surname>El-Fishawy</surname>
              <given-names>A. S.</given-names>
            </name>
            <name>
              <surname>El-Rabaie</surname>
              <given-names>E. S. M.</given-names>
            </name>
            <name>
              <surname>Abdellatef</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>El‐Samie</surname>
              <given-names>F. E. Abd</given-names>
            </name>
          </person-group>
          <article-title>Deep‐learning‐based seizure detection and prediction from electroencephalography signals</article-title>
          <source>Int. J. Numer. Methods Biomed. Eng.</source>
          <year>2022</year>
          <volume>38</volume>
          <issue>6</issue>
          <page-range>e3573</page-range>
          <pub-id pub-id-type="doi">10.1002/cnm.3573</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_119">
        <label>119.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tuncer</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Dogan</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>An explainable EEG epilepsy detection model using friend pattern</article-title>
          <source>Sci. Rep.</source>
          <year>2025</year>
          <volume>15</volume>
          <issue>1</issue>
          <page-range>16951</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-025-01747-z</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_120">
        <label>120.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Stevenson</surname>
              <given-names>N. J.</given-names>
            </name>
            <name>
              <surname>Tapani</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Lauronen</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Vanhatalo</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>A dataset of neonatal EEG recordings with seizure annotations</article-title>
          <source>Sci. Data</source>
          <year>2019</year>
          <volume>6</volume>
          <issue>1</issue>
          <page-range>190039</page-range>
          <pub-id pub-id-type="doi">10.1038/sdata.2019.39</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_121">
        <label>121.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tuncer</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Dogan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Tasci</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Tasci</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Hajiyeva</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>TATPat based explainable EEG model for neonatal seizure detection</article-title>
          <source>Sci. Rep.</source>
          <year>2024</year>
          <volume>14</volume>
          <issue>1</issue>
          <page-range>26688</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-024-77609-x</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_122">
        <label>122.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Xu</surname>
              <given-names>Y. K.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>S. Q.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>H. M.</given-names>
            </name>
            <name>
              <surname>Sawan</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>An end-to-end deep learning approach for epileptic seizure prediction</article-title>
          <source>arXiv</source>
          <year>2021</year>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2108.07453</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_123">
        <label>123.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Godoy</surname>
              <given-names>R. V.</given-names>
            </name>
            <name>
              <surname>Reis</surname>
              <given-names>T. J.</given-names>
            </name>
            <name>
              <surname>Polegato</surname>
              <given-names>P. H.</given-names>
            </name>
            <name>
              <surname>Lahr</surname>
              <given-names>G. J.</given-names>
            </name>
            <name>
              <surname>Saute</surname>
              <given-names>R. L.</given-names>
            </name>
            <name>
              <surname>Nakano</surname>
              <given-names>F. N.</given-names>
            </name>
            <name>
              <surname>Machado</surname>
              <given-names>H. R.</given-names>
            </name>
            <name>
              <surname>Sakamoto</surname>
              <given-names>A. C.</given-names>
            </name>
            <name>
              <surname>Becker</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Caurin</surname>
              <given-names>G. A.</given-names>
            </name>
          </person-group>
          <article-title>EEG-based epileptic seizure prediction using temporal multi-channel transformers</article-title>
          <source>arXiv</source>
          <year>2022</year>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2209.11172</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_124">
        <label>124.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hussein</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Ward</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Multi-channel vision transformer for epileptic seizure prediction</article-title>
          <source>Biomedicines</source>
          <year>2022</year>
          <volume>10</volume>
          <issue>7</issue>
          <page-range>1551</page-range>
          <pub-id pub-id-type="doi">10.3390/biomedicines10071551</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_125">
        <label>125.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Koutsouvelis</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Chybowski</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Gonzalez-Sulser</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Abdullateef</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Escudero</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>Preictal period optimization for deep learning-based epileptic seizure prediction</article-title>
          <source>J. Neural Eng.</source>
          <year>2024</year>
          <volume>21</volume>
          <issue>6</issue>
          <page-range>066040</page-range>
          <pub-id pub-id-type="doi">10.1088/1741-2552/ad9ad0</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_126">
        <label>126.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ehteshamzad</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Assessing the potential of EEG in early detection of Alzheimer’s disease: A systematic comprehensive review (2000–2023)</article-title>
          <source>J. Alzheimer's Dis. Rep.</source>
          <year>2024</year>
          <volume>8</volume>
          <issue>1</issue>
          <page-range>1153-1169</page-range>
          <pub-id pub-id-type="doi">10.3233/ADR-230159</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_127">
        <label>127.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Akbar</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Taj</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Usman</surname>
              <given-names>S. M.</given-names>
            </name>
            <name>
              <surname>Imran</surname>
              <given-names>A. S.</given-names>
            </name>
            <name>
              <surname>Khalid</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Ihsan</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Yasin</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Unlocking the potential of EEG in Alzheimer’s disease research: Current status and pathways to precision detection</article-title>
          <source>Brain Res. Bull.</source>
          <year>2025</year>
          <volume>223</volume>
          <page-range>111281</page-range>
          <pub-id pub-id-type="doi">10.1016/j.brainresbull.2025.111281</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_128">
        <label>128.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Varatharajah</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Dicks</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Barnard</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Brinkmann</surname>
              <given-names>B. H.</given-names>
            </name>
            <name>
              <surname>Crepeau</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Worrel</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Fan</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Kremers</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Boeve</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Botha</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Gogineni</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Jones</surname>
              <given-names>D. T.</given-names>
            </name>
          </person-group>
          <article-title>Data-driven retrieval of population-level EEG features and their role in neurodegenerative diseases</article-title>
          <source>Brain Commun.</source>
          <year>2024</year>
          <volume>6</volume>
          <issue>4</issue>
          <page-range>fcae227</page-range>
          <pub-id pub-id-type="doi">10.1093/braincomms/fcae227</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_129">
        <label>129.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Lenartowicz</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Loo</surname>
              <given-names>S. K.</given-names>
            </name>
          </person-group>
          <article-title>Use of EEG to diagnose ADHD</article-title>
          <source>Curr. Psychiatry Rep.</source>
          <year>2014</year>
          <volume>16</volume>
          <issue>11</issue>
          <page-range>498</page-range>
          <pub-id pub-id-type="doi">10.1007/s11920-014-0498-0</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_130">
        <label>130.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cao</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Martin</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>X.</given-names>
            </name>
          </person-group>
          <article-title>Machine learning in attention-deficit/hyperactivity disorder: New approaches toward understanding the neural mechanisms</article-title>
          <source>Transl. Psychiatry</source>
          <year>2023</year>
          <volume>13</volume>
          <issue>1</issue>
          <page-range>236</page-range>
          <pub-id pub-id-type="doi">10.1038/s41398-023-02536-w</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_131">
        <label>131.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Fürbass</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Herta</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Koren</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Westover</surname>
              <given-names>M. B.</given-names>
            </name>
            <name>
              <surname>Hartmann</surname>
              <given-names>M. M.</given-names>
            </name>
            <name>
              <surname>Gruber</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Baumgartner</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Kluge</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>Monitoring burst suppression in critically ill patients: Multi-centric evaluation of a novel method</article-title>
          <source>Clin. Neurophysiol.</source>
          <year>2016</year>
          <volume>127</volume>
          <issue>4</issue>
          <page-range>2038-2046</page-range>
          <pub-id pub-id-type="doi">10.1016/j.clinph.2016.02.001</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_132">
        <label>132.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yue</surname>
              <given-names>H. J.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Z. Q.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>W. B.</given-names>
            </name>
            <name>
              <surname>Sun</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Dai</surname>
              <given-names>Y. D.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y. M.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>W. J.</given-names>
            </name>
            <name>
              <surname>Fan</surname>
              <given-names>X. M.</given-names>
            </name>
            <name>
              <surname>Wen</surname>
              <given-names>W. P.</given-names>
            </name>
            <name>
              <surname>Lei</surname>
              <given-names>W. B.</given-names>
            </name>
          </person-group>
          <article-title>Research and application of deep learning-based sleep staging: Data, modeling, validation, and clinical practice</article-title>
          <source>Sleep Med. Rev.</source>
          <year>2024</year>
          <volume>74</volume>
          <page-range>101897</page-range>
          <pub-id pub-id-type="doi">10.1016/j.smrv.2024.101897</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_133">
        <label>133.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Quan</surname>
              <given-names>S. F.</given-names>
            </name>
            <name>
              <surname>Howard</surname>
              <given-names>B. V.</given-names>
            </name>
            <name>
              <surname>Iber</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Kiley</surname>
              <given-names>J. P.</given-names>
            </name>
            <name>
              <surname>Nieto</surname>
              <given-names>F. J.</given-names>
            </name>
            <name>
              <surname>O’Connor</surname>
              <given-names>G. T.</given-names>
            </name>
            <name>
              <surname>Rapoport</surname>
              <given-names>D. M.</given-names>
            </name>
            <name>
              <surname>Redline</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Robbins</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Samet</surname>
              <given-names>J. M.</given-names>
            </name>
            <name>
              <surname>Wahl</surname>
              <given-names>P. W.</given-names>
            </name>
          </person-group>
          <article-title>The sleep heart health study: Design, rationale, and methods</article-title>
          <source>Sleep</source>
          <year>1997</year>
          <volume>20</volume>
          <issue>12</issue>
          <page-range>1077-1085</page-range>
          <pub-id pub-id-type="doi">10.1093/sleep/20.12.1077</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_134">
        <label>134.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kemp</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Zwinderman</surname>
              <given-names>A. H.</given-names>
            </name>
            <name>
              <surname>Tuk</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Kamphuisen</surname>
              <given-names>H. A. C.</given-names>
            </name>
            <name>
              <surname>Oberye</surname>
              <given-names>J. J. L.</given-names>
            </name>
          </person-group>
          <article-title>Analysis of a sleep-dependent neuronal feedback loop: The slow-wave microcontinuity of the EEG</article-title>
          <source>IEEE Trans. Biomed. Eng.</source>
          <year>2000</year>
          <volume>47</volume>
          <issue>9</issue>
          <page-range>1185-1194</page-range>
          <pub-id pub-id-type="doi">10.1109/10.867928</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_135">
        <label>135.</label>
        <element-citation publication-type="webpage">
          <article-title>The Sleep-EDF Database [Expanded]</article-title>
          <source>, https://physionet.org/content/sleep-edf/1.0.0/</source>
          <year>2013</year>
        </element-citation>
      </ref>
      <ref id="ref_136">
        <label>136.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>G. Q.</given-names>
            </name>
            <name>
              <surname>Cui</surname>
              <given-names>L. C.</given-names>
            </name>
            <name>
              <surname>Mueller</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Tao</surname>
              <given-names>S. Q.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Rueschman</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Mariani</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Mobley</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Redline</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>The national sleep research resource: Towards a sleep data commons</article-title>
          <source>J. Am. Med. Inform. Assoc.</source>
          <year>2018</year>
          <volume>25</volume>
          <issue>10</issue>
          <page-range>1351-1358</page-range>
          <pub-id pub-id-type="doi">10.1093/jamia/ocy064</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_137">
        <label>137.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jirakittayakorn</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Wongsawat</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Mitrirattanakul</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>ZleepAnlystNet: A novel deep learning model for automatic sleep stage scoring based on single-channel raw EEG data using separating training</article-title>
          <source>Sci. Rep.</source>
          <year>2024</year>
          <volume>14</volume>
          <issue>1</issue>
          <page-range>9859</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-024-60796-y</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_138">
        <label>138.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Guo</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>An interpretable and efficient sleep staging algorithm: DetectsleepNet</article-title>
          <source>arXiv</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2406.19246</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_139">
        <label>139.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pradeepkumar</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Anandakumar</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kugathasan</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Suntharalingham</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Kappel</surname>
              <given-names>S. L.</given-names>
            </name>
            <name>
              <surname>De Silva</surname>
              <given-names>A. C.</given-names>
            </name>
            <name>
              <surname>Edussooriya</surname>
              <given-names>C. U.</given-names>
            </name>
          </person-group>
          <article-title>Towards interpretable sleep stage classification using cross-modal transformers</article-title>
          <source>IEEE Trans. Neural Syst. Rehabil. Eng.</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.1109/TNSRE.2024.3438610</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_140">
        <label>140.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sadik</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Raihan</surname>
              <given-names>M. T.</given-names>
            </name>
            <name>
              <surname>Rashid</surname>
              <given-names>R. B.</given-names>
            </name>
            <name>
              <surname>Rahman</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Abdal</surname>
              <given-names>S. M.</given-names>
            </name>
            <name>
              <surname>Ahmed</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Mahmud</surname>
              <given-names>T. I.</given-names>
            </name>
          </person-group>
          <article-title>A multi constrained transformer-BiLSTM guided network for automated sleep stage classification from single-channel EEG</article-title>
          <source>arXiv</source>
          <year>2023</year>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2309.10542</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_141">
        <label>141.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Kwak</surname>
              <given-names>H. G.</given-names>
            </name>
            <name>
              <surname>Kweon</surname>
              <given-names>Y. S.</given-names>
            </name>
            <name>
              <surname>Shin</surname>
              <given-names>G. H.</given-names>
            </name>
          </person-group>
          <article-title>Siamese sleep transformer for robust sleep stage scoring with self-knowledge distillation and selective batch sampling</article-title>
          <source>2023 11th International Winter Conference on Brain-Computer Interface (BCI)</source>
          <year>2023</year>
          <page-range>1-5</page-range>
          <pub-id pub-id-type="doi">10.1109/BCI57258.2023.10078532</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_142">
        <label>142.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Al-akkam</surname>
              <given-names>O. A. A. S. M.</given-names>
            </name>
          </person-group>
          <article-title>Hybrid deep learning model based on transformer encoder for sleep stages classification</article-title>
          <source>Bilad Alrafidain J. Eng. Sci. Technol.</source>
          <year>2025</year>
          <volume>4</volume>
          <issue>1</issue>
          <page-range>113-126</page-range>
          <pub-id pub-id-type="doi">10.56990/bajest/2025.040110</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_143">
        <label>143.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Westover</surname>
              <given-names>M. B.</given-names>
            </name>
            <name>
              <surname>Shafi</surname>
              <given-names>M. M.</given-names>
            </name>
            <name>
              <surname>Ching</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Chemali</surname>
              <given-names>J. J.</given-names>
            </name>
            <name>
              <surname>Purdon</surname>
              <given-names>P. L.</given-names>
            </name>
            <name>
              <surname>Cash</surname>
              <given-names>S. S.</given-names>
            </name>
            <name>
              <surname>Brown</surname>
              <given-names>E. N.</given-names>
            </name>
          </person-group>
          <article-title>Real-time segmentation of burst suppression patterns in critical care EEG monitoring</article-title>
          <source>J. Neurosci. Methods</source>
          <year>2013</year>
          <volume>219</volume>
          <issue>1</issue>
          <page-range>131-141</page-range>
          <pub-id pub-id-type="doi">10.1016/j.jneumeth.2013.07.003</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_144">
        <label>144.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Nagaraj</surname>
              <given-names>S. B.</given-names>
            </name>
            <name>
              <surname>McClain</surname>
              <given-names>L. M.</given-names>
            </name>
            <name>
              <surname>Boyle</surname>
              <given-names>E. J.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>D. W.</given-names>
            </name>
            <name>
              <surname>Ramaswamy</surname>
              <given-names>S. M.</given-names>
            </name>
            <name>
              <surname>Biswal</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Akeju</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Purdon</surname>
              <given-names>P. L.</given-names>
            </name>
            <name>
              <surname>Westover</surname>
              <given-names>M. B.</given-names>
            </name>
          </person-group>
          <article-title>Electroencephalogram based detection of deep sedation in ICU patients using atomic decomposition</article-title>
          <source>IEEE Trans. Biomed. Eng.</source>
          <year>2018</year>
          <volume>65</volume>
          <issue>12</issue>
          <page-range>2684-2691</page-range>
          <pub-id pub-id-type="doi">10.1109/TBME.2018.2813265</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_145">
        <label>145.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>X. S.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>G. H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>Z. H.</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>F. Y.</given-names>
            </name>
            <name>
              <surname>Cong</surname>
              <given-names>F. Y.</given-names>
            </name>
          </person-group>
          <article-title>One-dimensional convolutional neural networks combined with channel selection strategy for seizure prediction using long-term intracranial EEG</article-title>
          <source>Int. J. Neur. Syst.</source>
          <year>2022</year>
          <volume>32</volume>
          <issue>02</issue>
          <page-range>2150048</page-range>
          <pub-id pub-id-type="doi">10.1142/S0129065721500489</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_146">
        <label>146.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sun</surname>
              <given-names>H. Q.</given-names>
            </name>
            <name>
              <surname>Kimchi</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Akeju</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Nagaraj</surname>
              <given-names>S. B.</given-names>
            </name>
            <name>
              <surname>McClain</surname>
              <given-names>L. M.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>D. W.</given-names>
            </name>
            <name>
              <surname>Boyle</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>W. L.</given-names>
            </name>
            <name>
              <surname>Ge</surname>
              <given-names>W. D.</given-names>
            </name>
            <name>
              <surname>Westover</surname>
              <given-names>M. B.</given-names>
            </name>
          </person-group>
          <article-title>Automated tracking of level of consciousness and delirium in critical illness using deep learning</article-title>
          <source>NPJ Digit. Med.</source>
          <year>2019</year>
          <volume>2</volume>
          <issue>1</issue>
          <page-range>89</page-range>
          <pub-id pub-id-type="doi">10.1038/s41746-019-0167-0</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_147">
        <label>147.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Shanker</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Abel</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Schamberg</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Brown</surname>
              <given-names>E. N.</given-names>
            </name>
          </person-group>
          <article-title>Etiology of burst suppression EEG patterns</article-title>
          <source>Front. Psychol.</source>
          <year>2021</year>
          <volume>12</volume>
          <page-range>673529</page-range>
          <pub-id pub-id-type="doi">10.3389/fpsyg.2021.673529</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_148">
        <label>148.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Koelstra</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Muhl</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Soleymani</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>J. S.</given-names>
            </name>
            <name>
              <surname>Yazdani</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ebrahimi</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Pun</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Nijholt</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Patras</surname>
              <given-names>I.</given-names>
            </name>
          </person-group>
          <article-title>DEAP: A database for emotion analysis; Using physiological signals</article-title>
          <source>IEEE Trans. Affective Comput.</source>
          <year>2012</year>
          <volume>3</volume>
          <issue>1</issue>
          <page-range>18-31</page-range>
          <pub-id pub-id-type="doi">10.1109/T-AFFC.2011.15</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_149">
        <label>149.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zheng</surname>
              <given-names>W. L.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>B. L.</given-names>
            </name>
          </person-group>
          <article-title>Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks</article-title>
          <source>IEEE Trans. Auton. Mental Dev.</source>
          <year>2015</year>
          <volume>7</volume>
          <issue>3</issue>
          <page-range>162-175</page-range>
          <pub-id pub-id-type="doi">10.1109/TAMD.2015.2431497</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_150">
        <label>150.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Tong</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Zeng</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Exploration of effective electroencephalography features for the recognition of different valence emotions</article-title>
          <source>Front. Neurosci.</source>
          <year>2022</year>
          <volume>16</volume>
          <page-range>1010951</page-range>
          <pub-id pub-id-type="doi">10.3389/fnins.2022.1010951</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_151">
        <label>151.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Xu</surname>
              <given-names>Y. L.</given-names>
            </name>
            <name>
              <surname>Du</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Lai</surname>
              <given-names>H. H.</given-names>
            </name>
            <name>
              <surname>Zou</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>T. Y.</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>L. S.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>P. C.</given-names>
            </name>
          </person-group>
          <article-title>AMDET: Attention based multiple dimensions EEG transformer for emotion recognition</article-title>
          <source>IEEE Trans. Affect. Comput.</source>
          <year>2023</year>
          <volume>15</volume>
          <issue>3</issue>
          <page-range>1067-1077</page-range>
          <pub-id pub-id-type="doi">10.1109/TAFFC.2023.3318321</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_152">
        <label>152.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>X. Y.</given-names>
            </name>
            <name>
              <surname>Cheng</surname>
              <given-names>X. K.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>TPRO-NET: An EEG-based emotion recognition method reflecting subtle changes in emotion</article-title>
          <source>Sci. Rep.</source>
          <year>2024</year>
          <volume>14</volume>
          <issue>1</issue>
          <page-range>13491</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-024-62990-4</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_153">
        <label>153.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>J. C.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>S. Q.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Pan</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>F.</given-names>
            </name>
          </person-group>
          <article-title>Cross-subject EEG emotion recognition with self-organized graph neural network</article-title>
          <source>Front. Neurosci.</source>
          <year>2021</year>
          <volume>15</volume>
          <page-range>611653</page-range>
          <pub-id pub-id-type="doi">10.3389/fnins.2021.611653</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_154">
        <label>154.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Xu</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Dang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J. B.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>DAGAM: A domain adversarial graph attention model for subject-independent EEG-based emotion recognition</article-title>
          <source>J. Neural Eng.</source>
          <year>2023</year>
          <volume>20</volume>
          <issue>1</issue>
          <page-range>016022</page-range>
          <pub-id pub-id-type="doi">10.1088/1741-2552/acae06</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_155">
        <label>155.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Xu</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Qi</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>EEG emotion recognition based on federated learning framework</article-title>
          <source>Electronics</source>
          <year>2022</year>
          <volume>11</volume>
          <issue>20</issue>
          <page-range>3316</page-range>
          <pub-id pub-id-type="doi">10.3390/electronics11203316</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_156">
        <label>156.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Ye</surname>
              <given-names>W. S.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Z. G.</given-names>
            </name>
            <name>
              <surname>Teng</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Ni</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>F. L.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>Z.</given-names>
            </name>
          </person-group>
          <article-title>Semi-supervised dual-stream self-attentive adversarial graph contrastive learning for cross-subject EEG-based emotion recognition</article-title>
          <source>IEEE Trans. Affect. Comput.</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.1109/TAFFC.2024.3433470</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_157">
        <label>157.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Chan</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Zheng</surname>
              <given-names>Q. Q.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>C. J.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Q.</given-names>
            </name>
            <name>
              <surname>Heng</surname>
              <given-names>P. A.</given-names>
            </name>
          </person-group>
          <article-title>Adaptive federated learning for EEG emotion recognition</article-title>
          <source>2024 International Joint Conference on Neural Networks (IJCNN)</source>
          <year>2024</year>
          <page-range>1-8</page-range>
          <pub-id pub-id-type="doi">10.1109/IJCNN60899.2024.10650004</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_158">
        <label>158.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Mane</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Chew</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Chua</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Ang</surname>
              <given-names>K. K.</given-names>
            </name>
            <name>
              <surname>Robinson</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Vinod</surname>
              <given-names>A. P.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>S. W.</given-names>
            </name>
            <name>
              <surname>Guan</surname>
              <given-names>C. T.</given-names>
            </name>
          </person-group>
          <article-title>FBCNet: A multi-view convolutional neural network for brain-computer interface</article-title>
          <source>arXiv</source>
          <year>2021</year>
          <pub-id pub-id-type="doi">10.48550/ARXIV.2104.01233</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_159">
        <label>159.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Deng</surname>
              <given-names>H. D.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>M. F.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J. D.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>M. M.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>M. M.</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>G. Z.</given-names>
            </name>
          </person-group>
          <article-title>A robust multi-branch multi-attention-mechanism EEGNet for motor imagery BCI decoding</article-title>
          <source>J. Neurosci. Meth.</source>
          <year>2024</year>
          <volume>405</volume>
          <page-range>110108</page-range>
          <pub-id pub-id-type="doi">10.1016/j.jneumeth.2024.110108</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_160">
        <label>160.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhao</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>X. L.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>B. C.</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>S. X.</given-names>
            </name>
            <name>
              <surname>Weng</surname>
              <given-names>S. J.</given-names>
            </name>
          </person-group>
          <article-title>CTNet: A convolutional transformer network for EEG-based motor imagery classification</article-title>
          <source>Sci. Rep.</source>
          <year>2024</year>
          <volume>14</volume>
          <issue>1</issue>
          <page-range>20237</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-024-71118-7</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_161">
        <label>161.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Singh</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Singha</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Bhalaik</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>CCLNet: Multiclass motor imagery EEG decoding through extended common spatial patterns and CNN-LSTM hybrid network</article-title>
          <source>J. Supercomput.</source>
          <year>2025</year>
          <volume>81</volume>
          <issue>7</issue>
          <page-range>805</page-range>
          <pub-id pub-id-type="doi">10.1007/s11227-025-07319-2</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_162">
        <label>162.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Havaei</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Zekri</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Mahmoudzadeh</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Rabbani</surname>
              <given-names>H.</given-names>
            </name>
          </person-group>
          <article-title>An efficient deep learning framework for P300 evoked related potential detection in EEG signal</article-title>
          <source>Comput. Methods Programs Biomed.</source>
          <year>2023</year>
          <volume>229</volume>
          <page-range>107324</page-range>
          <pub-id pub-id-type="doi">10.1016/j.cmpb.2022.107324</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_163">
        <label>163.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Kobayashi</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Ishizuka</surname>
              <given-names>K.</given-names>
            </name>
          </person-group>
          <article-title>LSTM-based classification of multiflicker-SSVEP in single channel dry-EEG for low-power/high-accuracy quadcopter-BMI system</article-title>
          <source>2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)</source>
          <year>2019</year>
          <page-range>2160-2165</page-range>
          <pub-id pub-id-type="doi">10.1109/SMC.2019.8914015</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_164">
        <label>164.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bassi</surname>
              <given-names>P. R. A. S.</given-names>
            </name>
            <name>
              <surname>Rampazzo</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Attux</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>Transfer learning and SpecAugment applied to SSVEP based BCI classification</article-title>
          <source>Biomed. Signal Process. Control</source>
          <year>2021</year>
          <volume>67</volume>
          <page-range>102542</page-range>
          <pub-id pub-id-type="doi">10.1016/j.bspc.2021.102542</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_165">
        <label>165.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Vorontsova</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Menshikov</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Zubov</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Orlov</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Rikunov</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Zvereva</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Flitman</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Lanikin</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sokolova</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Markov</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Bernadotte</surname>
              <given-names>A.</given-names>
            </name>
          </person-group>
          <article-title>Silent EEG-speech recognition using convolutional and recurrent neural network with 85% accuracy of 9 words classification</article-title>
          <source>Sensors</source>
          <year>2021</year>
          <volume>21</volume>
          <issue>20</issue>
          <page-range>6744</page-range>
          <pub-id pub-id-type="doi">10.3390/s21206744</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_166">
        <label>166.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hossain</surname>
              <given-names>K. M.</given-names>
            </name>
            <name>
              <surname>Islam</surname>
              <given-names>Md. A.</given-names>
            </name>
            <name>
              <surname>Hossain</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Nijholt</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Ahad</surname>
              <given-names>M. A. R.</given-names>
            </name>
          </person-group>
          <article-title>Status of deep learning for EEG-based brain–computer interface applications</article-title>
          <source>Front. Comput. Neurosci.</source>
          <year>2023</year>
          <volume>16</volume>
          <page-range>1006763</page-range>
          <pub-id pub-id-type="doi">10.3389/fncom.2022.1006763</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_167">
        <label>167.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gong</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Wen</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>TFAC-Net: A temporal-frequential attentional convolutional network for driver drowsiness recognition with single-channel EEG</article-title>
          <source>IEEE Trans. Intell. Transport. Syst.</source>
          <year>2024</year>
          <volume>25</volume>
          <issue>7</issue>
          <page-range>7004-7016</page-range>
          <pub-id pub-id-type="doi">10.1109/TITS.2023.3347075</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_168">
        <label>168.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Siddhad</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Roy</surname>
              <given-names>P. P.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>B. G.</given-names>
            </name>
          </person-group>
          <article-title>Neural networks meet neural activity: Utilizing EEG for mental workload estimation</article-title>
          <source>International Conference on Pattern Recognition, Cham, Germany</source>
          <year>2025</year>
          <page-range>325-339</page-range>
          <pub-id pub-id-type="doi">10.1007/978-3-031-78195-7_22</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_169">
        <label>169.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Panwar</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Pandey</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Roy</surname>
              <given-names>P. P.</given-names>
            </name>
          </person-group>
          <article-title>EEG-CogNet: A deep learning framework for cognitive state assessment using EEG brain connectivity</article-title>
          <source>Biomed. Signal Process. Control</source>
          <year>2024</year>
          <volume>98</volume>
          <page-range>106770</page-range>
          <pub-id pub-id-type="doi">10.1016/j.bspc.2024.106770</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_170">
        <label>170.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cao</surname>
              <given-names>S. L.</given-names>
            </name>
            <name>
              <surname>Feng</surname>
              <given-names>P. H.</given-names>
            </name>
            <name>
              <surname>Kang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Z. Y.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>Optimized driver fatigue detection method using multimodal neural networks</article-title>
          <source>Sci. Rep.</source>
          <year>2025</year>
          <volume>15</volume>
          <issue>1</issue>
          <page-range>12240</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-025-86709-1</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_171">
        <label>171.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Supratak</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Dong</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Guo</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>DeepSleepNet: A model for automatic sleep stage scoring based on raw single-channel EEG</article-title>
          <source>IEEE Trans. Neural Syst. Rehabil. Eng.</source>
          <year>2017</year>
          <volume>25</volume>
          <issue>11</issue>
          <page-range>1998-2008</page-range>
          <pub-id pub-id-type="doi">10.1109/TNSRE.2017.2721116</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_172">
        <label>172.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Phan</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Mikkelsen</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>O. Y.</given-names>
            </name>
            <name>
              <surname>Koch</surname>
              <given-names>A. M.</given-names>
            </name>
            <name>
              <surname>Mertins</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>De Vos</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>SleepTransformer: Automatic sleep staging with interpretability and uncertainty quantification</article-title>
          <source>IEEE Trans. Biomed. Eng.</source>
          <year>2022</year>
          <volume>69</volume>
          <issue>8</issue>
          <page-range>2456-2467</page-range>
          <pub-id pub-id-type="doi">10.1109/TBME.2022.3147187</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_173">
        <label>173.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Guo</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Nowakowski</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Dai</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>FlexSleepTransformer: A transformer-based sleep staging model with flexible input channel configurations</article-title>
          <source>Sci. Rep.</source>
          <year>2024</year>
          <volume>14</volume>
          <issue>1</issue>
          <page-range>26312</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-024-76197-0</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_174">
        <label>174.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>J. Q.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>H. T.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Y. X.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>Z. H.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>S. J.</given-names>
            </name>
            <name>
              <surname>Pan</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>CareSleepNet: A hybrid deep learning network for automatic sleep staging</article-title>
          <source>IEEE J. Biomed. Health Inform.</source>
          <year>2024</year>
          <volume>28</volume>
          <issue>12</issue>
          <page-range>7392-7405</page-range>
          <pub-id pub-id-type="doi">10.1109/JBHI.2024.3426939</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_175">
        <label>175.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Geng</surname>
              <given-names>X. Z.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>D. Z.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>H. L.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Yan</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Yue</surname>
              <given-names>M. Z.</given-names>
            </name>
          </person-group>
          <article-title>An improved feature extraction algorithms of EEG signals based on motor imagery brain-computer interface</article-title>
          <source>Alexandria Eng. J.</source>
          <year>2022</year>
          <volume>61</volume>
          <issue>6</issue>
          <page-range>4807-4820</page-range>
          <pub-id pub-id-type="doi">10.1016/j.aej.2021.10.034</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_176">
        <label>176.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Blankertz</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Muller</surname>
              <given-names>K. R.</given-names>
            </name>
            <name>
              <surname>Krusienski</surname>
              <given-names>D. J.</given-names>
            </name>
            <name>
              <surname>Schalk</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Wolpaw</surname>
              <given-names>J. R.</given-names>
            </name>
            <name>
              <surname>Schlogl</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Pfurtscheller</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Millan</surname>
              <given-names>Jd.R.</given-names>
            </name>
            <name>
              <surname>Schroder</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Birbaumer</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>The BCI competition III: Validating alternative approaches to actual BCI problems</article-title>
          <source>IEEE Trans. Neural Syst. Rehabil. Eng.</source>
          <year>2006</year>
          <volume>14</volume>
          <issue>2</issue>
          <page-range>153-159</page-range>
          <pub-id pub-id-type="doi">10.1109/TNSRE.2006.875642</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_177">
        <label>177.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Blankertz</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Dornhege</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Krauledat</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Muller</surname>
              <given-names>K.-R.</given-names>
            </name>
            <name>
              <surname>Curio</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>The non-invasive Berlin Brain–Computer Interface: Fast acquisition of effective performance in untrained subjects</article-title>
          <source>NeuroImage</source>
          <year>2007</year>
          <volume>37</volume>
          <issue>2</issue>
          <page-range>539-550</page-range>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.01.051</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_178">
        <label>178.</label>
        <element-citation publication-type="conference-proceedings">
          <person-group person-group-type="author">
            <name>
              <surname>Brunner</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Leeb</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Muller-Putz</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Schlögl</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Pfurtscheller</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>BCI Competition 2008–Graz data set A</article-title>
          <source>, </source>
          <year>2008</year>
          <volume>16</volume>
          <issue>34</issue>
          <page-range>1-6</page-range>
        </element-citation>
      </ref>
      <ref id="ref_179">
        <label>179.</label>
        <element-citation publication-type="conference-proceedings">
          <person-group person-group-type="author">
            <name>
              <surname>Leeb</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Brunner</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Muller-Putz</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Schlögl</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Pfurtscheller</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>BCI Competition 2008–Graz data set B</article-title>
          <source>, </source>
          <year>2008</year>
          <volume>16</volume>
          <page-range>1-6</page-range>
        </element-citation>
      </ref>
      <ref id="ref_180">
        <label>180.</label>
        <element-citation publication-type="conference-proceedings">
          <person-group person-group-type="author">
            <name>
              <surname>Miller</surname>
              <given-names>K. J.</given-names>
            </name>
            <name>
              <surname>Schalk</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Prediction of finger flexion: 4th brain-computer interface data competition</article-title>
          <source>, </source>
          <year>2008</year>
          <volume>1</volume>
          <page-range>1-2</page-range>
        </element-citation>
      </ref>
      <ref id="ref_181">
        <label>181.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Brunner</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Birbaumer</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Blankertz</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Guger</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Kübler</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Mattia</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Millán</surname>
              <given-names>Jd. R.</given-names>
            </name>
            <name>
              <surname>Miralles</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Nijholt</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Opisso</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Ramsey</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Salomon</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Müller-Putz</surname>
              <given-names>G. R.</given-names>
            </name>
          </person-group>
          <article-title>BNCI Horizon 2020: Towards a roadmap for the BCI community</article-title>
          <source>Brain-Comput. Interfaces</source>
          <year>2015</year>
          <volume>2</volume>
          <issue>1</issue>
          <page-range>1-10</page-range>
          <pub-id pub-id-type="doi">10.1080/2326263X.2015.1008956</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_182">
        <label>182.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Obeid</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Picone</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>The Temple University Hospital EEG data corpus</article-title>
          <source>Front. Neurosci.</source>
          <year>2016</year>
          <volume>10</volume>
          <page-range>196</page-range>
          <pub-id pub-id-type="doi">10.3389/fnins.2016.00196</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_183">
        <label>183.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Shah</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Weltin</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Lopez</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>McHugh</surname>
              <given-names>J. R.</given-names>
            </name>
            <name>
              <surname>Veloso</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Golmohammadi</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Obeid</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Picone</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>The Temple University Hospital seizure detection corpus</article-title>
          <source>Front. Neuroinform.</source>
          <year>2018</year>
          <volume>12</volume>
          <page-range>83</page-range>
          <pub-id pub-id-type="doi">10.3389/fninf.2018.00083</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_184">
        <label>184.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Andrzejak</surname>
              <given-names>R. G.</given-names>
            </name>
            <name>
              <surname>Lehnertz</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Mormann</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Rieke</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>David</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Elger</surname>
              <given-names>C. E.</given-names>
            </name>
          </person-group>
          <article-title>Indications of nonlinear deterministic and finite-dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state</article-title>
          <source>Phys. Rev. E</source>
          <year>2001</year>
          <volume>64</volume>
          <issue>6</issue>
          <page-range>061907</page-range>
          <pub-id pub-id-type="doi">10.1103/PhysRevE.64.061907</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_185">
        <label>185.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Duan</surname>
              <given-names>R. N.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>J. Y.</given-names>
            </name>
            <name>
              <surname>Lu</surname>
              <given-names>B. L.</given-names>
            </name>
          </person-group>
          <article-title>Differential entropy feature for EEG-based emotion classification</article-title>
          <source>2013 6th International IEEE/EMBS Conference on Neural Engineering (NER), San Diego, USA</source>
          <year>2013</year>
          <page-range>81-84</page-range>
          <pub-id pub-id-type="doi">10.1109/NER.2013.6695876</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_186">
        <label>186.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Katsigiannis</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Ramzan</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>DREAMER: A database for emotion recognition through EEG and ECG signals from wireless low-cost off-the-shelf devices</article-title>
          <source>IEEE J. Biomed. Health Inform.</source>
          <year>2018</year>
          <volume>22</volume>
          <issue>1</issue>
          <page-range>98-107</page-range>
          <pub-id pub-id-type="doi">10.1109/JBHI.2017.2688239</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_187">
        <label>187.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Schalk</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>McFarland</surname>
              <given-names>D. J.</given-names>
            </name>
            <name>
              <surname>Hinterberger</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Birbaumer</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Wolpaw</surname>
              <given-names>J. R.</given-names>
            </name>
          </person-group>
          <article-title>BCI2000: A general-purpose brain-computer interface (BCI) system</article-title>
          <source>IEEE Trans. Biomed. Eng.</source>
          <year>2004</year>
          <volume>51</volume>
          <issue>6</issue>
          <page-range>1034-1043</page-range>
          <pub-id pub-id-type="doi">10.1109/TBME.2004.827072</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_188">
        <label>188.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kappenman</surname>
              <given-names>E. S.</given-names>
            </name>
            <name>
              <surname>Farrens</surname>
              <given-names>J. L.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Stewart</surname>
              <given-names>A. X.</given-names>
            </name>
            <name>
              <surname>Luck</surname>
              <given-names>S. J.</given-names>
            </name>
          </person-group>
          <article-title>ERP CORE: An open resource for human event-related potential research</article-title>
          <source>NeuroImage</source>
          <year>2021</year>
          <volume>225</volume>
          <page-range>117465</page-range>
          <pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117465</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_189">
        <label>189.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Markiewicz</surname>
              <given-names>C. J.</given-names>
            </name>
            <name>
              <surname>Gorgolewski</surname>
              <given-names>K. J.</given-names>
            </name>
            <name>
              <surname>Feingold</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Blair</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Halchenko</surname>
              <given-names>Y. O.</given-names>
            </name>
            <name>
              <surname>Miller</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Hardcastle</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Wexler</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Esteban</surname>
              <given-names>O.</given-names>
            </name>
            <name>
              <surname>Goncavles</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Jwa</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Poldrack</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>The OpenNeuro resource for sharing of neuroscience data</article-title>
          <source>eLife</source>
          <year>2021</year>
          <volume>10</volume>
          <page-range>e71774</page-range>
          <pub-id pub-id-type="doi">10.7554/eLife.71774</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_190">
        <label>190.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Pernet</surname>
              <given-names>C. R.</given-names>
            </name>
            <name>
              <surname>Appelhoff</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Gorgolewski</surname>
              <given-names>K. J.</given-names>
            </name>
            <name>
              <surname>Flandin</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Phillips</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Delorme</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Oostenveld</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>EEG-BIDS, an extension to the brain imaging data structure for electroencephalography</article-title>
          <source>Sci. Data</source>
          <year>2019</year>
          <volume>6</volume>
          <issue>1</issue>
          <page-range>103</page-range>
          <pub-id pub-id-type="doi">10.1038/s41597-019-0104-8</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_191">
        <label>191.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Delorme</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Truong</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Youn</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Sivagnanam</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Stirm</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Yoshimoto</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Poldrack</surname>
              <given-names>R. A.</given-names>
            </name>
            <name>
              <surname>Majumdar</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Makeig</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>NEMAR: An open access data, tools, and compute resource operating on NeuroElectroMagnetic data</article-title>
          <source>Database</source>
          <year>2022</year>
          <volume>2022</volume>
          <page-range>baac096</page-range>
          <pub-id pub-id-type="doi">10.1093/database/baac096</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_192">
        <label>192.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Shirazi</surname>
              <given-names>S. Y.</given-names>
            </name>
            <name>
              <surname>Franco</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Scopel Hoffmann</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Esper</surname>
              <given-names>N. B.</given-names>
            </name>
            <name>
              <surname>Truong</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Delorme</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Milham</surname>
              <given-names>M. P.</given-names>
            </name>
            <name>
              <surname>Makeig</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>HBN-EEG: The FAIR implementation of the Healthy Brain Network (HBN) electroencephalography dataset</article-title>
          <source>bioRxiv</source>
          <year>2024</year>
          <pub-id pub-id-type="doi">10.1101/2024.10.03.615261</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_193">
        <label>193.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bhagubai</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Chatzichristos</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Swinnen</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Macea</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Lagae</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Jansen</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Schulze-Bonhage</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Sales</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Mahler</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Weber</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Paesschen</surname>
              <given-names>W. V.</given-names>
            </name>
            <name>
              <surname>De Vos</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>SeizeIT2: Wearable dataset of patients with focal epilepsy</article-title>
          <source>Sci. Data</source>
          <year>2025</year>
          <volume>12</volume>
          <issue>1</issue>
          <page-range>1228</page-range>
          <pub-id pub-id-type="doi">10.1038/s41597-025-05580-x</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_194">
        <label>194.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>Z. M.</given-names>
            </name>
            <name>
              <surname>Gao</surname>
              <given-names>C. Y..</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Ji</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>S. Y.</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>Open access dataset integrating EEG and fNIRS during Stroop tasks</article-title>
          <source>Sci. Data</source>
          <year>2023</year>
          <volume>10</volume>
          <issue>1</issue>
          <page-range>618</page-range>
          <pub-id pub-id-type="doi">10.1038/s41597-023-02524-1</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_195">
        <label>195.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Phukhachee</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Angsuwatanakul</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Iramina</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Kaewkamnerdpong</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>A simultaneous EEG-fNIRS dataset of the visual cognitive motivation study in healthy adults</article-title>
          <source>Data Brief</source>
          <year>2024</year>
          <volume>53</volume>
          <page-range>110260</page-range>
          <pub-id pub-id-type="doi">10.1016/j.dib.2024.110260</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_196">
        <label>196.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhang</surname>
              <given-names>G. H.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Zhen</surname>
              <given-names>S. Y.</given-names>
            </name>
            <name>
              <surname>Tang</surname>
              <given-names>S. H.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>Z.</given-names>
            </name>
            <name>
              <surname>Zhen</surname>
              <given-names>Z. L.</given-names>
            </name>
          </person-group>
          <article-title>A large-scale MEG and EEG dataset for object recognition in naturalistic scenes</article-title>
          <source>Sci. Data</source>
          <year>2025</year>
          <volume>12</volume>
          <issue>1</issue>
          <page-range>857</page-range>
          <pub-id pub-id-type="doi">10.1038/s41597-025-05174-7</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_197">
        <label>197.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jung</surname>
              <given-names>T. P.</given-names>
            </name>
            <name>
              <surname>Makeig</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Humphries</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>T. W.</given-names>
            </name>
            <name>
              <surname>McKeown</surname>
              <given-names>M. J.</given-names>
            </name>
            <name>
              <surname>Iragui</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Sejnowski</surname>
              <given-names>T. J.</given-names>
            </name>
          </person-group>
          <article-title>Removing electroencephalographic artifacts by blind source separation</article-title>
          <source>Psychophysiology</source>
          <year>2000</year>
          <volume>37</volume>
          <issue>2</issue>
          <page-range>163-178</page-range>
          <pub-id pub-id-type="doi">10.1111/1469-8986.3720163</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_198">
        <label>198.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jiménez-Guarneros</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Fuentes-Pineda</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Learning a robust unified domain adaptation framework for cross-subject EEG-based emotion recognition</article-title>
          <source>Biomed. Signal Process. Control</source>
          <year>2023</year>
          <volume>86</volume>
          <page-range>105138</page-range>
          <pub-id pub-id-type="doi">10.1016/j.bspc.2023.105138</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_199">
        <label>199.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cambay</surname>
              <given-names>V. Y.</given-names>
            </name>
            <name>
              <surname>Tasci</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Tasci</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Hajiyeva</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Dogan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Tuncer</surname>
              <given-names>T.</given-names>
            </name>
          </person-group>
          <article-title>QuadTPat: Quadruple transition pattern-based explainable feature engineering model for stress detection using EEG signals</article-title>
          <source>Sci. Rep.</source>
          <year>2024</year>
          <volume>14</volume>
          <issue>1</issue>
          <page-range>27320</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-024-78222-8</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_200">
        <label>200.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Huang</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>Z. H.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>S. R.</given-names>
            </name>
            <name>
              <surname>Hu</surname>
              <given-names>Z. X.</given-names>
            </name>
            <name>
              <surname>Fan</surname>
              <given-names>J. M.</given-names>
            </name>
            <name>
              <surname>Fu</surname>
              <given-names>M. S.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>J. L.</given-names>
            </name>
            <name>
              <surname>Xiao</surname>
              <given-names>Y. Q.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Dan</surname>
              <given-names>G.</given-names>
            </name>
          </person-group>
          <article-title>Discrepancy between inter- and intra-subject variability in EEG-based motor imagery brain-computer interface: Evidence from multiple perspectives</article-title>
          <source>Front. Neurosci.</source>
          <year>2023</year>
          <volume>17</volume>
          <page-range>1122661</page-range>
          <pub-id pub-id-type="doi">10.3389/fnins.2023.1122661</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_201">
        <label>201.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Han</surname>
              <given-names>J. W.</given-names>
            </name>
            <name>
              <surname>Bak</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>J. M.</given-names>
            </name>
            <name>
              <surname>Choi</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Shin</surname>
              <given-names>D. H.</given-names>
            </name>
            <name>
              <surname>Son</surname>
              <given-names>Y. H.</given-names>
            </name>
            <name>
              <surname>Kam</surname>
              <given-names>T. E.</given-names>
            </name>
          </person-group>
          <article-title>META-EEG: Meta-learning-based class-relevant EEG representation learning for zero-calibration brain–computer interfaces</article-title>
          <source>Expert Syst. Appl.</source>
          <year>2024</year>
          <volume>238</volume>
          <page-range>121986</page-range>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2023.121986</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_202">
        <label>202.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Apicella</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Isgrò</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Pollastro</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Prevete</surname>
              <given-names>R.</given-names>
            </name>
          </person-group>
          <article-title>On the effects of data normalization for domain adaptation on EEG data</article-title>
          <source>Eng. Appl. Artif. Intell.</source>
          <year>2023</year>
          <volume>123</volume>
          <page-range>106205</page-range>
          <pub-id pub-id-type="doi">10.1016/j.engappai.2023.106205</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_203">
        <label>203.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chawla</surname>
              <given-names>N. V.</given-names>
            </name>
            <name>
              <surname>Bowyer</surname>
              <given-names>K. W.</given-names>
            </name>
            <name>
              <surname>Hall</surname>
              <given-names>L. O.</given-names>
            </name>
            <name>
              <surname>Kegelmeyer</surname>
              <given-names>W. P.</given-names>
            </name>
          </person-group>
          <article-title>SMOTE: Synthetic minority over-sampling technique</article-title>
          <source>J. Artif. Intell. Res.</source>
          <year>2002</year>
          <volume>16</volume>
          <page-range>321-357</page-range>
          <pub-id pub-id-type="doi">10.1613/jair.953</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_204">
        <label>204.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Yang</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>IDA-GAN: A novel imbalanced data augmentation GAN</article-title>
          <source>2020 25th International Conference on Pattern Recognition (ICPR), Milan, Italy</source>
          <year>2021</year>
          <page-range>8299-8305</page-range>
          <pub-id pub-id-type="doi">10.1109/ICPR48806.2021.9411996</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_205">
        <label>205.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Abou-Abbas</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Henni</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Jemal</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Mezghani</surname>
              <given-names>N.</given-names>
            </name>
          </person-group>
          <article-title>Generative AI with WGAN-GP for boosting seizure detection accuracy</article-title>
          <source>Front. Artif. Intell.</source>
          <year>2024</year>
          <volume>7</volume>
          <page-range>1437315</page-range>
          <pub-id pub-id-type="doi">10.3389/frai.2024.1437315</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_206">
        <label>206.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Farahani</surname>
              <given-names>F. V.</given-names>
            </name>
            <name>
              <surname>Fiok</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Lahijanian</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Karwowski</surname>
              <given-names>W.</given-names>
            </name>
            <name>
              <surname>Douglas</surname>
              <given-names>P. K.</given-names>
            </name>
          </person-group>
          <article-title>Explainable AI: A review of applications to neuroimaging data</article-title>
          <source>Front. Neurosci.</source>
          <year>2022</year>
          <volume>16</volume>
          <page-range>906290</page-range>
          <pub-id pub-id-type="doi">10.3389/fnins.2022.906290</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_207">
        <label>207.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhou</surname>
              <given-names>X. L.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>C. Y.</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>J. N.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z. R.</given-names>
            </name>
            <name>
              <surname>Zhai</surname>
              <given-names>L. M.</given-names>
            </name>
            <name>
              <surname>Jia</surname>
              <given-names>Z. Y.</given-names>
            </name>
            <name>
              <surname>Guan</surname>
              <given-names>C. T.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Interpretable and robust AI in EEG systems: A survey</article-title>
          <source>arXiv</source>
          <year>2025</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2304.10755</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_208">
        <label>208.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Yu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Qiu</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Su</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Fan</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Cao</surname>
              <given-names>Y.</given-names>
            </name>
          </person-group>
          <article-title>Perturbing BEAMs: EEG adversarial attack to deep learning models for epilepsy diagnosing</article-title>
          <source>BMC Med. Inform. Decis. Mak.</source>
          <year>2023</year>
          <volume>23</volume>
          <issue>1</issue>
          <page-range>115</page-range>
          <pub-id pub-id-type="doi">10.1186/s12911-023-02212-5</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_209">
        <label>209.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Feng</surname>
              <given-names>L. C.</given-names>
            </name>
            <name>
              <surname>Shan</surname>
              <given-names>H. W.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Y. Q.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Z. M.</given-names>
            </name>
          </person-group>
          <article-title>An efficient model-compressed EEGNet accelerator for generalized brain-computer interfaces with near sensor intelligence</article-title>
          <source>IEEE Trans. Biomed. Circuits Syst.</source>
          <year>2022</year>
          <volume>16</volume>
          <issue>6</issue>
          <page-range>1239-1249</page-range>
          <pub-id pub-id-type="doi">10.1109/TBCAS.2022.3215962</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_210">
        <label>210.</label>
        <element-citation publication-type="conference-proceedings">
          <person-group person-group-type="author">
            <name>
              <surname>Bian</surname>
              <given-names>S. Z.</given-names>
            </name>
            <name>
              <surname>Kang</surname>
              <given-names>P. X.</given-names>
            </name>
            <name>
              <surname>Moosmann</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>M. X.</given-names>
            </name>
            <name>
              <surname>Bonazzi</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Rosipal</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Magno</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>On-device learning of EEGNet-based network for wearable motor imagery brain-computer interface</article-title>
          <source>, https://doi.org/10.1145/3675095.3676607</source>
          <year>2024</year>
          <page-range>9-16</page-range>
          <pub-id pub-id-type="doi">10.1145/3675095.3676607</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_211">
        <label>211.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Demirezen</surname>
              <given-names>G.</given-names>
            </name>
            <name>
              <surname>Taşkaya Temizel</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Brouwer</surname>
              <given-names>A. M.</given-names>
            </name>
          </person-group>
          <article-title>Reproducible machine learning research in mental workload classification using EEG</article-title>
          <source>Front. Neuroerg.</source>
          <year>2024</year>
          <volume>5</volume>
          <page-range>1346794</page-range>
          <pub-id pub-id-type="doi">10.3389/fnrgo.2024.1346794</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_212">
        <label>212.</label>
        <element-citation publication-type="conference-proceedings">
          <person-group person-group-type="author">
            <name>
              <surname>Kinahan</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Saidi</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Daliri</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Liss</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Berisha</surname>
              <given-names>V.</given-names>
            </name>
          </person-group>
          <article-title>Achieving Reproducibility in EEG-Based Machine Learning</article-title>
          <source>, https://doi.org/10.1145/3630106.3658983</source>
          <year>2024</year>
          <page-range>1464-1474</page-range>
          <pub-id pub-id-type="doi">10.1145/3630106.3658983</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_213">
        <label>213.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Grataloup</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Kurpicz-Briki</surname>
              <given-names>M.</given-names>
            </name>
          </person-group>
          <article-title>A systematic survey on the application of federated learning in mental state detection and human activity recognition</article-title>
          <source>Front. Digit. Health</source>
          <year>2024</year>
          <volume>6</volume>
          <page-range>1495999</page-range>
          <pub-id pub-id-type="doi">10.3389/fdgth.2024.1495999</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_214">
        <label>214.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Gahlan</surname>
              <given-names>N.</given-names>
            </name>
            <name>
              <surname>Sethia</surname>
              <given-names>D.</given-names>
            </name>
          </person-group>
          <article-title>AFLEMP: Attention-based federated learning for emotion recognition using multi-modal physiological data</article-title>
          <source>Biomed. Signal Process. Control</source>
          <year>2024</year>
          <volume>94</volume>
          <page-range>106353</page-range>
          <pub-id pub-id-type="doi">10.1016/j.bspc.2024.106353</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_215">
        <label>215.</label>
        <element-citation publication-type="conf-paper">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Y. X.</given-names>
            </name>
            <name>
              <surname>Qiu</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Zhao</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Yang</surname>
              <given-names>W. J.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>J. P.</given-names>
            </name>
            <name>
              <surname>Ma</surname>
              <given-names>X. L.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>H. G.</given-names>
            </name>
          </person-group>
          <article-title>EEG-based emotion recognition with prototype-based data representation</article-title>
          <source>2019 41st annual international conference of the IEEE engineering in medicine and biology society (EMBC), Berlin, Germany</source>
          <year>2019</year>
          <page-range>684-689</page-range>
          <pub-id pub-id-type="doi">10.1109/EMBC.2019.8857340</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_216">
        <label>216.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Han</surname>
              <given-names>D.-K.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>S.-W.</given-names>
            </name>
          </person-group>
          <article-title>Multi-layer prototype learning with Dirichlet mixup for open-set EEG recognition</article-title>
          <source>Expert Syst. Appl.</source>
          <year>2025</year>
          <volume>266</volume>
          <page-range>126047</page-range>
          <pub-id pub-id-type="doi">10.1016/j.eswa.2024.126047</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_217">
        <label>217.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Tang</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Willard</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Tegerdine</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Triplett</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Donnelly</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Moffett</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Semenova</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Barnett</surname>
              <given-names>A. J.</given-names>
            </name>
            <name>
              <surname>Jing</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Rudin</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Westover</surname>
              <given-names>B.</given-names>
            </name>
          </person-group>
          <article-title>ProtoEEGNet: An interpretable approach for detecting interictal epileptiform discharges</article-title>
          <source>arXiv</source>
          <year>2023</year>
          <pub-id pub-id-type="doi">10.48550/arXiv.2312.10056</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_218">
        <label>218.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Adey</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Habib</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Karmakar</surname>
              <given-names>C.</given-names>
            </name>
          </person-group>
          <article-title>Exploration of an intrinsically explainable self-attention based model for prototype generation on single-channel EEG sleep stage classification</article-title>
          <source>Sci. Rep.</source>
          <year>2024</year>
          <volume>14</volume>
          <issue>1</issue>
          <page-range>27612</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-024-79139-y</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_219">
        <label>219.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sujatha Ravindran</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Contreras-Vidal</surname>
              <given-names>J.</given-names>
            </name>
          </person-group>
          <article-title>An empirical comparison of deep learning explainability approaches for EEG using simulated ground truth</article-title>
          <source>Sci. Rep.</source>
          <year>2023</year>
          <volume>13</volume>
          <issue>1</issue>
          <page-range>17709</page-range>
          <pub-id pub-id-type="doi">10.1038/s41598-023-43871-8</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_220">
        <label>220.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Cui</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Yuan</surname>
              <given-names>L. Q.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Z. X.</given-names>
            </name>
            <name>
              <surname>Li</surname>
              <given-names>R. L.</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>T. Z.</given-names>
            </name>
          </person-group>
          <article-title>Towards best practice of interpreting deep learning models for EEG-based brain computer interfaces</article-title>
          <source>Front. Comput. Neurosci.</source>
          <year>2023</year>
          <volume>17</volume>
          <page-range>1232925</page-range>
          <pub-id pub-id-type="doi">10.3389/fncom.2023.1232925</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_221">
        <label>221.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>J. F.</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>K. W.</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>Y. F.</given-names>
            </name>
            <name>
              <surname>Bi</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Ji</surname>
              <given-names>X.</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>D. W.</given-names>
            </name>
          </person-group>
          <article-title>Strategic integration: A cross-disciplinary review of the fNIRS-EEG dual-modality imaging system for delivering multimodal neuroimaging to applications</article-title>
          <source>Brain Sci.</source>
          <year>2024</year>
          <volume>14</volume>
          <issue>10</issue>
          <page-range>1022</page-range>
          <pub-id pub-id-type="doi">10.3390/brainsci14101022</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_222">
        <label>222.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Covi</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Donati</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>X. P.</given-names>
            </name>
            <name>
              <surname>Kappel</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Heidari</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Payvand</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>W.</given-names>
            </name>
          </person-group>
          <article-title>Adaptive extreme edge computing for wearable devices</article-title>
          <source>Front. Neurosci.</source>
          <year>2021</year>
          <volume>15</volume>
          <page-range>611300</page-range>
          <pub-id pub-id-type="doi">10.3389/fnins.2021.611300</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_223">
        <label>223.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Shin</surname>
              <given-names>J. H.</given-names>
            </name>
            <name>
              <surname>Kwon</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>J. U.</given-names>
            </name>
            <name>
              <surname>Ryu</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Ok</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Kwon</surname>
              <given-names>S. J.</given-names>
            </name>
            <name>
              <surname>Park</surname>
              <given-names>H.</given-names>
            </name>
            <name>
              <surname>Kim</surname>
              <given-names>T. I.</given-names>
            </name>
          </person-group>
          <article-title>Wearable EEG electronics for a brain–AI closed-loop system to enhance autonomous machine decision-making</article-title>
          <source>npj Flex. Electron.</source>
          <year>2022</year>
          <volume>6</volume>
          <issue>1</issue>
          <page-range>32</page-range>
          <pub-id pub-id-type="doi">10.1038/s41528-022-00164-w</pub-id>
        </element-citation>
      </ref>
      <ref id="ref_224">
        <label>224.</label>
        <element-citation publication-type="journal">
          <person-group person-group-type="author">
            <name>
              <surname>Aristimunha</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Carrara</surname>
              <given-names>I.</given-names>
            </name>
            <name>
              <surname>Guetschel</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Sedlar</surname>
              <given-names>S.</given-names>
            </name>
            <name>
              <surname>Rodrigues</surname>
              <given-names>P.</given-names>
            </name>
            <name>
              <surname>Sosulski</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Narayanan</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Bjareholt</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Quentin</surname>
              <given-names>B.</given-names>
            </name>
            <name>
              <surname>Schirrmeister</surname>
              <given-names>R. T.</given-names>
            </name>
            <name>
              <surname>Kalunga</surname>
              <given-names>E.</given-names>
            </name>
            <name>
              <surname>Darmet</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Gregoire</surname>
              <given-names>C.</given-names>
            </name>
            <name>
              <surname>Hussain</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Gatti</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Goncharenko</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Thielen</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Moreau</surname>
              <given-names>T.</given-names>
            </name>
            <name>
              <surname>Roy</surname>
              <given-names>Y.</given-names>
            </name>
            <name>
              <surname>Jayaram</surname>
              <given-names>V.</given-names>
            </name>
            <name>
              <surname>Barachant</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Chevallier</surname>
              <given-names>S.</given-names>
            </name>
          </person-group>
          <article-title>Mother of all BCI benchmarks</article-title>
          <source>Zenodo</source>
          <year>2023</year>
          <pub-id pub-id-type="doi">10.5281/ZENODO.10034224</pub-id>
        </element-citation>
      </ref>
    </ref-list>
  </back>
</article>